[
    {
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_created_time":1585108798000,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":146,
        "Platform":"Github",
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_link_count":1.0,
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3421052632,
        "Challenge_watch_issue_ratio":0.1315789474
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [x] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [x] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nGetting this error, when invoking a MME on sagemaker setup using `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04` container image.\r\n\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=14448): Max retries exceeded with url: \/v1\/models\/d2295a7526f9df36354b8a2c4adc4f63 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70966dba50>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nTraceback (most recent call last):\r\n  File \"\/sagemaker\/python_service.py\", line 157, in _handle_load_model_post\r\n    self._wait_for_model(model_name)\r\n  File \"\/sagemaker\/python_service.py\", line 247, in _wait_for_model\r\n    response = session.get(url)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 546, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\r\n    raise ConnectionError(e, request=request)\r\n\r\n*DLC image\/dockerfile:*\r\n763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04\r\n*Current behavior:*\r\n\r\n*Expected behavior:*\r\nModel should load up and return prediction\r\n*Additional context:*\r\nI have setup a MME using the above mentioned container and invoking the endpoint using a lambda. The model files are in placed in S3 and are in the correct directory structure with a version number. ",
        "Challenge_closed_time":null,
        "Challenge_created_time":1602135852000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/170",
        "Challenge_link_count":1,
        "Challenge_readability":14.1,
        "Challenge_reading_time":30.6,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] : Model not loading while using existing container image to setup MME on sagemaker",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":238,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1048034934,
        "Challenge_watch_issue_ratio":0.1615720524
    },
    {
        "Challenge_adjusted_solved_time":1329.0686111111,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957138000,
        "Challenge_created_time":1567172491000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1329.0686111111,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":283,
        "Platform":"Github",
        "Solution_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Solution_link_count":1.0,
        "Solution_readability":11.3,
        "Solution_reading_time":8.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1048034934,
        "Challenge_watch_issue_ratio":0.1615720524
    },
    {
        "Challenge_adjusted_solved_time":5.8786111111,
        "Challenge_answer_count":1,
        "Challenge_body":"",
        "Challenge_closed_time":1615314058000,
        "Challenge_created_time":1615292895000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/175",
        "Challenge_link_count":0,
        "Challenge_readability":10.4,
        "Challenge_reading_time":1.84,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.8786111111,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":12,
        "Platform":"Github",
        "Solution_body":"Closing in favour of #174 ",
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":3,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_created_time":1615292808000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4770.0555555556,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":23,
        "Platform":"Github",
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":407.5572222222,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nSageMaker Operator Types, when included as part of KubeBuilder V2 custom CRD definition fail due to validation errors of unescaped regex patterns. \r\n\r\n```\r\n\/go\/bin\/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\".\/...\" output:crd:artifacts:config=config\/crd\/bases\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n```\r\n\r\n**What you expected to happen**:\r\nKubeBuilder should generate CRD specification which includes AWS SageMaker Operator Types\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n```\r\nimport (\r\n\tcommonv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\"\r\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\r\n)\r\n\r\n\/\/ GuestbookSpec defines the desired state of Guestbook\r\ntype GuestbookSpec struct {\r\n\t\/\/ INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\r\n\t\/\/ Important: Run \"make\" to regenerate code after modifying this file\r\n\r\n\tAlgorithmSpecification *commonv1.AlgorithmSpecification `json:\"algorithmSpecification\"`\r\n\r\n\tEnableInterContainerTrafficEncryption *bool `json:\"enableInterContainerTrafficEncryption,omitempty\"`\r\n\r\n\tEnableNetworkIsolation *bool `json:\"enableNetworkIsolation,omitempty\"`\r\n...\r\n\/\/Run make install with above  types in custom operator\r\nmake install \r\n```\r\n\r\n**Anything else we need to know?**:\r\nTried copying the above types and escaped the regex pattern with quotes (``\/\/ +kubebuilder:validation:Pattern='^(https|s3):\/\/([^\/]+)\/?(.*)$'``) and everything worked\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):Version: version.Version{KubeBuilderVersion:\"2.3.1\", KubernetesVendor:\"1.16.4\", GitCommit:\"8b53abeb4280186e494b726edf8f54ca7aa64a49\", BuildDate:\"2020-03-26T16:42:00Z\", GoOs:\"unknown\", GoArch:\"unknown\"}\r\n- Operator version (controller image tag):\tgithub.com\/aws\/amazon-sagemaker-operator-for-k8s v1.0.1-0.20200410212604-780c48ecb21a\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1596827786000,
        "Challenge_created_time":1595360580000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_readability":23.6,
        "Challenge_reading_time":75.11,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":407.5572222222,
        "Challenge_title":"SageMaker Operator Types fails KubeBuilder Pattern validation check",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":316,
        "Platform":"Github",
        "Solution_body":"Hi Nagaraj, I'll contact you directly to discuss this. It appears as though you are attempting to build your project with a newer version of `controller-gen` than we have supported in our CRDs. We are using an older version [`v0.2.0-beta.2`](https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/blob\/283886dd7c66adfd8c491bf452796fea698ceab8\/Makefile#L98) for our builds. We will need to update our CRDs (and maybe some controller logic) and our build scripts to support the newest version ([`v0.3.0`](https:\/\/github.com\/kubernetes-sigs\/controller-tools\/releases\/tag\/v0.3.0)). ",
        "Solution_link_count":2.0,
        "Solution_readability":10.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":2039.8183333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678360000,
        "Challenge_created_time":1592335014000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2039.8183333333,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":180,
        "Platform":"Github",
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_link_count":0.0,
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":15,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_created_time":1583773133000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":193,
        "Platform":"Github",
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_link_count":7.0,
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":227.0,
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### What did you do?\r\n\r\n<!--\r\n-->pip install stepfunctions fails in SageMaker Studio Notebook\r\n\r\nNotebook is using the Python3 (Data Science) kernel.\r\n\r\n\r\n\r\n\r\n### Reproduction Steps\r\n\r\n<!--\r\n--> pip install stepfunctions\r\n\r\n### What did you expect to happen?\r\n\r\n<!--\r\n-->I expected to be able to install AWS stepfunctions.\r\n\r\n### What actually happened?\r\n\r\n<!--\r\n-->\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\nCollecting stepfunctions\r\n  Using cached stepfunctions-2.3.0.tar.gz (67 kB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py egg_info did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [22 lines of output]\r\n      \/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py:760: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\r\n        % (opt, underscore_opt)\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 36, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"\/tmp\/pip-install-a9sl8pu9\/stepfunctions_fec8ededb6d5452993a38c0c5620f20d\/setup.py\", line 70, in <module>\r\n          \"IPython\",\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup\r\n          return distutils.core.setup(**attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup\r\n          _setup_distribution = dist = klass(attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__\r\n          for k, v in attrs.items()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__\r\n          self.finalize_options()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options\r\n          for ep in sorted(loaded, key=by_order):\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in <lambda>\r\n          loaded = map(lambda e: e.load(), filtered)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load\r\n          return functools.reduce(getattr, attrs, module)\r\n      AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\n\r\n### Environment\r\n\r\n  - **AWS Step Functions Data Science Python SDK version  : 2.3.0\r\n  - **Python Version:** <!-- Version of Python (run the command `python3 --version`) --> 3.7\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stack-traces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, slack, etc -->\r\n\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":null,
        "Challenge_created_time":1651588352000,
        "Challenge_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/188",
        "Challenge_link_count":0,
        "Challenge_readability":12.3,
        "Challenge_reading_time":42.53,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":78.0,
        "Challenge_repo_issue_count":190.0,
        "Challenge_repo_star_count":244.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"pip install stepfunctions fails in SageMaker Studio Notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":320,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1105263158,
        "Challenge_watch_issue_ratio":0.0947368421
    },
    {
        "Challenge_adjusted_solved_time":241.2261111111,
        "Challenge_answer_count":1,
        "Challenge_body":"TrainingPipeline needs to be updated to accommodate the `sagemaker.tensorflow.serving.Model` from Tensorflow package.\r\n\r\nRelated Thread: https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1201",
        "Challenge_closed_time":1579559963000,
        "Challenge_created_time":1578691549000,
        "Challenge_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/17",
        "Challenge_link_count":1,
        "Challenge_readability":20.6,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":78.0,
        "Challenge_repo_issue_count":190.0,
        "Challenge_repo_star_count":244.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":241.2261111111,
        "Challenge_title":"Support Tensorflow with the new sagemaker.tensorflow.serving.Model",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":20,
        "Platform":"Github",
        "Solution_body":"This PR https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/1252 fixes the issue.",
        "Solution_link_count":1.0,
        "Solution_readability":18.6,
        "Solution_reading_time":1.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":6.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1105263158,
        "Challenge_watch_issue_ratio":0.0947368421
    },
    {
        "Challenge_adjusted_solved_time":0.1422222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Challenge_closed_time":1668738306000,
        "Challenge_created_time":1668737794000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Challenge_link_count":3,
        "Challenge_readability":10.1,
        "Challenge_reading_time":15.87,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1422222222,
        "Challenge_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":160,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":22.5291666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThe banner message is shown on the top page of Studio Lab.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Studio Lab Top Page\r\n2. The message is shown.\r\n\r\n**Expected behavior**\r\nWe can use the Studio Lab as usual.\r\n\r\nI confirmed the following error.\r\n\r\n* We can start runtime but when clicking \"Open Project\", `ERR_EMPTY_RESPONSE` occurs in the browser.\r\n* When we click the start runtime, \"There was a problem when loading your project. This should be resolved shortly. Please try again later.\" occurred.\r\n\r\n**Screenshots**\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/202335625-de4d1505-97a7-4748-93d5-f0d6b0f5c597.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser Chrome\r\n\r\n**Additional context**\r\n\r\nAs the message suggests, we are working to restore the service. We apologize for any inconvenience.\r\nI'll announce after the service is back. \r\n",
        "Challenge_closed_time":1668731619000,
        "Challenge_created_time":1668650514000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/166",
        "Challenge_link_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":13.81,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":22.5291666667,
        "Challenge_title":"Starting 16th Nov 2022 04:00 PM PST, we are experiencing elevated error starting runtimes. The SageMaker Studio Lab team is working to restore the service. We apologize for any inconvenience.",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":153,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":52.6111111111,
        "Challenge_answer_count":4,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627477000,
        "Challenge_created_time":1667438077000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":52.6111111111,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":51,
        "Platform":"Github",
        "Solution_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":5.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":320.9530555556,
        "Challenge_answer_count":5,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933766000,
        "Challenge_created_time":1654778335000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":320.9530555556,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Solution_link_count":4.0,
        "Solution_readability":11.2,
        "Solution_reading_time":134.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":129.0,
        "Solution_word_count":1049.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":9,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_created_time":1647978773000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2124.5019444444,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_link_count":5.0,
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":349.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":5,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_created_time":1645584371000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6365.2066666667,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":91,
        "Platform":"Github",
        "Solution_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":5.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":3.7888888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nCloning a single notebook using the \"Open In in Sagemaker Studio Lab\" fails.  Cloning the whole repo works.  \r\n\r\nUsing sagemaker's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, I get this error:\r\n```\r\nUnable to copy notebook to project.\r\nThe link to this notebook is broken or blocked. If this is a private GitHub notebook, sign in to GitHub before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb\r\n```\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create MD cell with `[![Open in SageMaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb)`  and run it\r\n2. Click on the button that appears once you run the cell.  Will open new tab in browser\r\n3. In the new pop up tab, click \"Copy to Project\".  Will open new tab in browser\r\n4. In the new pop up tab's modal, select \"Copy Notebook Only\"\r\n5. Error will now appear\r\n\r\n**Expected behavior**\r\nMy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [Windows 11]\r\n - Browser [Chrome]\r\n - Version [97.0.4692.71]\r\n\r\n",
        "Challenge_closed_time":1643319424000,
        "Challenge_created_time":1643305784000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/54",
        "Challenge_link_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":19.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3.7888888889,
        "Challenge_title":"[BUG] \"Open In in Sagemaker Studio Lab\" button process fails when attempting to \"Copy Notebooks Only\"",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":177,
        "Platform":"Github",
        "Solution_body":"Interesting - this works fine on my end, but I'm using a Mac. I'll create a ticket for you. ",
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":139.3538888889,
        "Challenge_answer_count":5,
        "Challenge_body":"**Describe the bug**\r\nAmazon Sagemaker studio lab is not opening jupyter notebook. It is loading indefinitely at Preparing project run time after that i am getting `There was a problem when starting the project runtime. This should be resolved shortly.` Please try again later. It's been almost a week and it still hasn't been resolved. Even though I tried shifting the runtime from CPU to GPU but issue still persists. Any help would be appreciated.\r\n\r\n**The error i am getting is**\r\n![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1643050102000,
        "Challenge_created_time":1642548428000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/52",
        "Challenge_link_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":139.3538888889,
        "Challenge_title":"couldn't open Project on amazon sagemaker studio lab",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":"Hi - sorry you're running into this issue, we're taking a look.  I have also encountered this problem, which still exists at present.\r\n![image](https:\/\/user-images.githubusercontent.com\/37031974\/150719178-b4b46fa6-d912-44e3-a412-f605435d664c.png)\r\n I deleted my account and re-created it, which took around 5 minutes for them to accept my request for the second time. If need instance then this is the only approach I could find. I had not received any updates from this issue after a week, so I recreated my instance.\r\n I see - really sorry you're running into that. I've created a ticket with the team, but I'll close this issue for now since there is a known workaround.  Hello, I can't open project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? Thanks!",
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":12.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":1.1852777778,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Challenge_closed_time":1639666969000,
        "Challenge_created_time":1639662702000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1852777778,
        "Challenge_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":72,
        "Platform":"Github",
        "Solution_body":"Hi! The use case you are describing is exactly why we have this example notebook:\r\n- https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb \r\n\r\nYour net net is:\r\n1\/ Ensure you have proper training and authorization to use your AWS access and secret keys. If you are an AWS account admin, you can find this in your IAM console. If you are not, work with your admin to determine if this pattern is appropriate for you. \r\n\r\n2\/ If you are qualified to manage your AWS keys, create a new file called `~\/.aws\/credentials`. There, copy and paste in your access and secret keys.\r\n\r\n3\/ Remove any cells you ran in a notebook to create or verify those files.\r\n\r\n4\/ Create a SageMaker execution role. The easiest way to do this is via the console - you can create a new SageMaker execution role when create a notebook instance or a Studio user profile. Once this is done, paste in the arn (Amazon Resource Name), for this execution role.\r\n\r\n5\/ Go forth and scale up on SageMaker! After that,  once you are using the SageMaker Python SDK, you should be able to use all code-based features within SageMaker, such as training, hosting, tuning, autopilot, pipelines, etc.   Hi @EmilyWebber. Might you also have an example that doesn't require AWS account information yet avoids \"ValueError: Must setup local AWS configuration with a region supported by SageMaker\" in the code below for \"role\"?\r\n\r\n```\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.6.1',\r\n\tpytorch_version='1.7.1',\r\n\tpy_version='py36',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\nCode source: https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english, where Deploy is Amazon SageMaker rather than Accelerated Inference \u2014 the latter [currently returns an error](https:\/\/discuss.huggingface.co\/t\/distilbert-accelerated-inference-error\/15027) with or without SageMaker\r\n\r\nSince the SageMaker Studio Lab website emphasizes no costs and no need to sign up for an AWS account, a team and I are exploring whether to have students try. Thank you in advance. Hi @derekschan - thanks for the comment. This is actually expected behavior right now - Studio Lab defaults to not having access to any AWS API's unless explicitly granted. To date, that is solved only by the pattern mentioned above in this issue, explicitly installing AWS key permissions to utilize them. \r\n\r\nShould that ever change in the future we will be sure to let you know! I'll mark your comment as a feature enhancement.  Thank you, @EmilyWebber.",
        "Solution_link_count":3.0,
        "Solution_readability":9.3,
        "Solution_reading_time":31.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":372.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"In Pytorch images all the prints in stderr are not catched and are ignored:\r\n\r\n\r\n### Describe the problem\r\n\r\n### Minimal repro \/ logs\r\nEntrypoint.py:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    import sys\r\n    sys.stderr.write('Coucou stderr')\r\n    sys.stdout.write('Coucou stdout')\r\n```\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nestimator = PyTorch(entry_point='entrypoint.py',\r\n                    role=role,\r\n                    framework_version='1.1.0',\r\n                    train_instance_count=1,\r\n                    train_instance_type='local',\r\n                )\r\nestimator.fit({'config': 's3:\/\/sagemaker-eu-*************\/config\/test_sagemaker_1.json'})\r\n```\r\n\r\n<details><summary>LOGS<\/summary>\r\n<p>\r\n\r\nCreating tmpqp7i_4w3_algo-1-8gd7b_1 ... \r\nAttaching to tmpqp7i_4w3_algo-1-8gd7b_12mdone\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,345 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,349 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,363 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,365 sagemaker_pytorch_container.training INFO     Invoking user training script.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Module entrypoint does not provide a setup.py. \r\nalgo-1-8gd7b_1  | Generating setup.py\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating setup.cfg\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating MANIFEST.in\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,490 sagemaker-containers INFO     Installing module with the following command:\r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m pip install . \r\nalgo-1-8gd7b_1  | Processing \/opt\/ml\/code\r\nalgo-1-8gd7b_1  | Building wheels for collected packages: entrypoint\r\nalgo-1-8gd7b_1  |   Running setup.py bdist_wheel for entrypoint ... done\r\nalgo-1-8gd7b_1  |   Stored in directory: \/tmp\/pip-ephem-wheel-cache-44kbrxy0\/wheels\/35\/24\/16\/37574d11bf9bde50616c******356bc7164af8ca3\r\nalgo-1-8gd7b_1  | Successfully built entrypoint\r\nalgo-1-8gd7b_1  | Installing collected packages: entrypoint\r\nalgo-1-8gd7b_1  | Successfully installed entrypoint-1.0.0\r\nalgo-1-8gd7b_1  | You are using pip version 18.1, however version 19.3.1 is available.\r\nalgo-1-8gd7b_1  | You should consider upgrading via the 'pip install --upgrade pip' command.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,054 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,069 sagemaker-containers INFO     Invoking user script\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Training Env:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | {\r\nalgo-1-8gd7b_1  |     \"additional_framework_parameters\": {},\r\nalgo-1-8gd7b_1  |     \"channel_input_dirs\": {\r\nalgo-1-8gd7b_1  |         \"config\": \"\/opt\/ml\/input\/data\/config\"\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\r\nalgo-1-8gd7b_1  |     \"hosts\": [\r\nalgo-1-8gd7b_1  |         \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |     ],\r\nalgo-1-8gd7b_1  |     \"hyperparameters\": {},\r\nalgo-1-8gd7b_1  |     \"input_config_dir\": \"\/opt\/ml\/input\/config\",\r\nalgo-1-8gd7b_1  |     \"input_data_config\": {\r\nalgo-1-8gd7b_1  |         \"config\": {\r\nalgo-1-8gd7b_1  |             \"TrainingInputMode\": \"File\"\r\nalgo-1-8gd7b_1  |         }\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"input_dir\": \"\/opt\/ml\/input\",\r\nalgo-1-8gd7b_1  |     \"is_master\": true,\r\nalgo-1-8gd7b_1  |     \"job_name\": \"sagemaker-pytorch-2019-10-22-09-06-18-353\",\r\nalgo-1-8gd7b_1  |     \"log_level\": 20,\r\nalgo-1-8gd7b_1  |     \"master_hostname\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"model_dir\": \"\/opt\/ml\/model\",\r\nalgo-1-8gd7b_1  |     \"module_dir\": \"s3:\/\/sagemaker-eu-west-1-*********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\r\nalgo-1-8gd7b_1  |     \"module_name\": \"entrypoint\",\r\nalgo-1-8gd7b_1  |     \"network_interface_name\": \"eth0\",\r\nalgo-1-8gd7b_1  |     \"num_cpus\": 2,\r\nalgo-1-8gd7b_1  |     \"num_gpus\": 0,\r\nalgo-1-8gd7b_1  |     \"output_data_dir\": \"\/opt\/ml\/output\/data\",\r\nalgo-1-8gd7b_1  |     \"output_dir\": \"\/opt\/ml\/output\",\r\nalgo-1-8gd7b_1  |     \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\r\nalgo-1-8gd7b_1  |     \"resource_config\": {\r\nalgo-1-8gd7b_1  |         \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |         \"hosts\": [\r\nalgo-1-8gd7b_1  |             \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |         ]\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"user_entry_point\": \"entrypoint.py\"\r\nalgo-1-8gd7b_1  | }\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Environment variables:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | SM_HOSTS=[\"algo-1-8gd7b\"]\r\nalgo-1-8gd7b_1  | SM_NETWORK_INTERFACE_NAME=eth0\r\nalgo-1-8gd7b_1  | SM_HPS={}\r\nalgo-1-8gd7b_1  | SM_USER_ENTRY_POINT=entrypoint.py\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_PARAMS={}\r\nalgo-1-8gd7b_1  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]}\r\nalgo-1-8gd7b_1  | SM_INPUT_DATA_CONFIG={\"config\":{\"TrainingInputMode\":\"File\"}}\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\r\nalgo-1-8gd7b_1  | SM_CHANNELS=[\"config\"]\r\nalgo-1-8gd7b_1  | SM_CURRENT_HOST=algo-1-8gd7b\r\nalgo-1-8gd7b_1  | SM_MODULE_NAME=entrypoint\r\nalgo-1-8gd7b_1  | SM_LOG_LEVEL=20\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\r\nalgo-1-8gd7b_1  | SM_INPUT_DIR=\/opt\/ml\/input\r\nalgo-1-8gd7b_1  | SM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DIR=\/opt\/ml\/output\r\nalgo-1-8gd7b_1  | SM_NUM_CPUS=2\r\nalgo-1-8gd7b_1  | SM_NUM_GPUS=0\r\nalgo-1-8gd7b_1  | SM_MODEL_DIR=\/opt\/ml\/model\r\nalgo-1-8gd7b_1  | SM_MODULE_DIR=s3:\/\/sagemaker-eu-west-1-***********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\r\nalgo-1-8gd7b_1  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"\/opt\/ml\/input\/data\/config\"},\"current_host\":\"algo-1-8gd7b\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-8gd7b\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"config\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-10-22-09-06-18-353\",\"log_level\":20,\"master_hostname\":\"algo-1-8gd7b\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/sagemaker-eu-west-1-**********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]},\"user_entry_point\":\"entrypoint.py\"}\r\nalgo-1-8gd7b_1  | SM_USER_ARGS=[]\r\nalgo-1-8gd7b_1  | SM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\r\nalgo-1-8gd7b_1  | SM_CHANNEL_CONFIG=\/opt\/ml\/input\/data\/config\r\nalgo-1-8gd7b_1  | PYTHONPATH=\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Invoking script with the following command:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m entrypoint\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Coucou stdout2019-10-22 09:06:23,102 sagemaker-containers INFO     Reporting training SUCCESS\r\ntmpqp7i_4w3_algo-1-8gd7b_1 exited with code 0\r\nAborting on container exit...\r\n===== Job Complete =====\r\n<\/p>\r\n<\/details>\r\n\r\nAs you see the coucou stdout has been printed, stderr has been ignored. In distant mode same result.\r\n\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1571735353000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/issues\/37",
        "Challenge_link_count":0,
        "Challenge_readability":25.5,
        "Challenge_reading_time":98.29,
        "Challenge_repo_contributor_count":39.0,
        "Challenge_repo_fork_count":85.0,
        "Challenge_repo_issue_count":167.0,
        "Challenge_repo_star_count":340.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":null,
        "Challenge_title":"Pytorch Sagemaker Container STDERR output",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":426,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.2335329341,
        "Challenge_watch_issue_ratio":0.2275449102
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n`sagemaker-inference` recently (10\/15) released v1.5.3, which included [this commit](https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/commit\/8efb1672798d747cd623e5dd2eb7919af87a1b80) updating the name of the model server artifact and command from `mxnet-model-server` to `multi-model-server`.\r\n\r\nall containers defined in this repository install `sagemaker-inference` as a dependency of this repo itself, on lines\r\n\r\n```dockerfile\r\nRUN pip install --no-cache-dir \"sagemaker-pytorch-inference<2\"\r\n```\r\n\r\nand this repo's `setup.py` has an `install_requires` which includes `sagemaker-inference>=1.3.1`. as a result, `sagemaker-inference=1.5.3` installed.\r\n\r\nso while the `Dockerfile`'s `CMD` value (which calls `mxnet-model-server` directly) will succeed, attempts to use the `ENTRYPOINT` with `serve` as a build arg will fail with message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 22, in <module>\r\n    serving.main()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 39, in main\r\n    _start_model_server()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 49, in wrapped_f\r\n    return Retrying(*dargs, **dkw).call(f, *args, **kw)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 206, in call\r\n    return attempt.get(self._wrap_exception)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 247, in get\r\n    six.reraise(self.value[0], self.value[1], self.value[2])\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 200, in call\r\n    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 35, in _start_model_server\r\n    model_server.start_model_server(handler_service=HANDLER_SERVICE)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/model_server.py\", line 94, in start_model_server\r\n    subprocess.Popen(multi_model_server_cmd)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 709, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 1344, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'multi-model-server': 'multi-model-server'\r\n\r\n```\r\n\r\n**To reproduce**\r\n1. build any container\r\n1. mount a model and `inference.py` (e.g. `half_plus_three`) into `\/opt\/ml\/model`\r\n1. `docker run [tag name] serve`\r\n\r\n**Expected behavior**\r\ntensorflow serving serves the mounted model \/ `inference.py`\r\n\r\n**System information**\r\nA description of your system. Please provide:\r\n- **Toolkit version**: 2.0.5, but should apply to all versions\r\n- **Framework version**: 1.4, but should apply to all versions\r\n- **Python version**: 3.7\r\n- **CPU or GPU**: cpu, but should apply to both\r\n- **Custom Docker image (Y\/N)**: N",
        "Challenge_closed_time":null,
        "Challenge_created_time":1607044885000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/88",
        "Challenge_link_count":1,
        "Challenge_readability":13.6,
        "Challenge_reading_time":40.59,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":134.0,
        "Challenge_repo_star_count":88.0,
        "Challenge_repo_watch_count":25.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"renaming of mxnet-model-server in sagemaker-inference package 1.5.3 causing entrypoint with command `serve` to fail",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":287,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1492537313,
        "Challenge_watch_issue_ratio":0.1865671642
    },
    {
        "Challenge_adjusted_solved_time":790.3911111111,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1662449543000,
        "Challenge_created_time":1659604135000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.42,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":790.3911111111,
        "Challenge_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":98,
        "Platform":"Github",
        "Solution_body":"Already fixed",
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":653.7947222222,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Deploy SWB in hongkong reigon\r\n2. Create a Sagemaker workspace\r\n3. Click \"Stop\" button.\r\n5. workspace status show \"UNKNOWN\"\r\n",
        "Challenge_closed_time":1659958133000,
        "Challenge_created_time":1657604472000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/45",
        "Challenge_link_count":0,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":653.7947222222,
        "Challenge_title":"[Bug] In HongKong region, After user stop sagemaker workspace manually, web console show \"UNKNOWN\" status",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":54,
        "Platform":"Github",
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/cd99bd2a4f39291e3149b0abbc78ab5b3d650454)",
        "Solution_link_count":1.0,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":69.9002777778,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1657702977000,
        "Challenge_created_time":1657451336000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Challenge_link_count":0,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":69.9002777778,
        "Challenge_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":116,
        "Platform":"Github",
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/2339efe78d0f4705ef0cd6d2b1c5f06a810e6730) ",
        "Solution_link_count":1.0,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":95.0875,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\r\n\r\nGood day.\r\n\r\nCould you add to the Sagemaker section?:\r\n```\r\npip install urllib3==1.24.3\r\npip install PyYAML==3.13\r\npip install ipython\r\n```\r\nFrom https:\/\/medium.com\/@jonathantse\/train-deepracer-model-locally-with-gpu-support-29cce0bdb0f9. \r\n\r\nKeep up the good work!",
        "Challenge_closed_time":1564216116000,
        "Challenge_created_time":1563873801000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/36",
        "Challenge_link_count":1,
        "Challenge_readability":10.9,
        "Challenge_reading_time":3.69,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":95.0875,
        "Challenge_title":"Sagemaker dependencies",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":27,
        "Platform":"Github",
        "Solution_body":"I know ipython used to be used because of a couple lines in the python launch file, however it's been removed. Are you able to launch it without ipython?\n\nI'll double check the other two as they should come in through other dependencies. They used to be there but threw errors because of other dependencies requiring different versions What I did was go through your steps line by line on a fresh Ubuntu 18 install. And try and run sagemaker. Might be environment specific to Ubuntu? I\u2019m not too clued up on python and pip. But for sure I had to install those to get it to run.\n\n\n________________________________\nFrom: Chris Rhodes <notifications@github.com>\nSent: Wednesday, July 24, 2019 3:42 AM\nTo: crr0004\/deepracer\nCc: jarrett jordaan; Author\nSubject: Re: [crr0004\/deepracer] Sagemaker dependencies (#36)\n\n\nI know ipython used to be used because of a couple lines in the python launch file, however it's been removed. Are you able to launch it without ipython?\n\nI'll double check the other two as they should come in through other dependencies. They used to be there but threw errors because of other dependencies requiring different versions\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https:\/\/github.com\/crr0004\/deepracer\/issues\/36?email_source=notifications&email_token=AAF52S6N33S3XYV4OLVLAR3QA6XRLA5CNFSM4IGB77KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2U47FQ#issuecomment-514445206>, or mute the thread<https:\/\/github.com\/notifications\/unsubscribe-auth\/AAF52S3V6KVWPFHAOM5ODA3QA6XRLANCNFSM4IGB77KA>.\n I have updated sagemaker python sdk setup.py file for the dependencies. The errors won't actually do any harm. Just some conflicting version constraints.",
        "Solution_link_count":2.0,
        "Solution_readability":9.3,
        "Solution_reading_time":22.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":278.3016666667,
        "Challenge_answer_count":4,
        "Challenge_body":"I mistakenly put my model in pretrained folder but outside the model subfolder. In such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nWhile I did not fix the issue, I added logging to make it verbose. I will try to upload a patch.",
        "Challenge_closed_time":1562359564000,
        "Challenge_created_time":1561357678000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":278.3016666667,
        "Challenge_title":"When pretrained model is not found, sagemaker falls into an infinite silent loop",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":66,
        "Platform":"Github",
        "Solution_body":"```\r\nIndex: rl_coach\/src\/markov\/s3_client.py\r\nIDEA additional info:\r\nSubsystem: com.intellij.openapi.diff.impl.patch.CharsetEP\r\n<+>UTF-8\r\n===================================================================\r\n--- rl_coach\/src\/markov\/s3_client.py\t(revision 789d7553bdfda74d10dcfbcc0c0286fdb0b5b57f)\r\n+++ rl_coach\/src\/markov\/s3_client.py\t(date 1561357411000)\r\n@@ -60,10 +60,12 @@\r\n \r\n     def download_model(self, checkpoint_dir):\r\n         s3_client = self.get_client()\r\n+        logger.info(\"Downloading pretrained model from %s\/%s %s\" % (self.bucket, self.model_checkpoints_prefix, checkpoint_dir))\r\n         filename = \"None\"\r\n         try:\r\n             filename = os.path.abspath(os.path.join(checkpoint_dir, \"checkpoint\"))\r\n             if not os.path.exists(checkpoint_dir):\r\n+                logger.info(\"Model folder %s does not exist, creating\" % filename)\r\n                 os.makedirs(checkpoint_dir)\r\n \r\n             while True:\r\n@@ -73,13 +75,17 @@\r\n                 if \"Contents\" not in response:\r\n                     # If no lock is found, try getting the checkpoint\r\n                     try:\r\n+                        key = self._get_s3_key(\"checkpoint\")\r\n+                        logger.info(\"Downloading %s\" % key)\r\n                         s3_client.download_file(Bucket=self.bucket,\r\n-                                                Key=self._get_s3_key(\"checkpoint\"),\r\n+                                                Key=key,\r\n                                                 Filename=filename)\r\n                     except Exception as e:\r\n+                        logger.info(\"Something went wrong, will retry in 2 seconds %s\" % e)\r\n                         time.sleep(2)\r\n                         continue\r\n                 else:\r\n+                    logger.info(\"Found a lock file, waiting\")\r\n                     time.sleep(2)\r\n                     continue\r\n \r\n@@ -98,6 +104,8 @@\r\n                             filename = os.path.abspath(os.path.join(checkpoint_dir,\r\n                                                                     obj[\"Key\"].replace(self.model_checkpoints_prefix,\r\n                                                                                        \"\")))\r\n+\r\n+                            logger.info(\"Downloading model file %s\" % filename)\r\n                             s3_client.download_file(Bucket=self.bucket,\r\n                                                     Key=obj[\"Key\"],\r\n                                                     Filename=filename)\r\n\r\n``` Sadly, no file upload is available in issues You can open a pull request if you like, do you need help in doing so? It's definitely an issue and I've encountered it myself. It also happens when the checkpoint file references files that don't exist. Thank you @bhannebipro , I lost track :)",
        "Solution_link_count":0.0,
        "Solution_readability":12.4,
        "Solution_reading_time":24.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":164.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":945.8408333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Steps to reproduce:\r\nI followed instructions in the readme, but instead of `docker pull nabcrr\/sagemaker-rl-tensorflow:console` I did `docker pull nabcrr\/sagemaker-rl-tensorflow:nvidia` and then tagged it as instructed. Before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` I went to that file and commented out the line that Lonon mentioned in #17 \r\n\r\nExpected result:\r\nWhen running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nActual result:\r\n```\r\nalgo-1-vrm2i_1  | ERROR: ld.so: object '\/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | Reporting training FAILURE\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | Traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | ModuleNotFoundError: No module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | No module named 'tensorflow'\r\n```\r\n\r\nSystem info:\r\nUbuntu 18.04.2 LTS\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nMon Jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660M    Off  | 00000000:01:00.0 N\/A |                  N\/A |\r\n| N\/A   46C    P8    N\/A \/  N\/A |    266MiB \/  1999MiB |     N\/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Challenge_closed_time":1564215404000,
        "Challenge_created_time":1560810377000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_readability":11.7,
        "Challenge_reading_time":41.03,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":945.8408333333,
        "Challenge_title":"No tensorflow reported when trying to run nvidia image for sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":266,
        "Platform":"Github",
        "Solution_body":"Note: adding tensorflow-gpu==1.11.0 and rebuilding the image solves the issue Image has been updated for this",
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":6,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_created_time":1571075742000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":19,
        "Platform":"Github",
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_link_count":6.0,
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0769230769,
        "Challenge_watch_issue_ratio":0.1098901099
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/blob\/master\/hpo_pytorch_mnist\/hpo_pytorch_mnist.ipynb\r\n\r\nNeed to add \r\n```\r\n!pip install ipywidgets\r\n!jupyter nbextension enable --py widgetsnbextension\r\n```\r\n\r\nWith the command below, SageMaker local mode still show some errors: \r\n```\r\n!pip install docker-compose\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1590122594000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/issues\/32",
        "Challenge_link_count":1,
        "Challenge_readability":15.4,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":48.0,
        "Challenge_repo_issue_count":74.0,
        "Challenge_repo_star_count":103.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Error in PyTorch MNIST Notebook with SageMaker Studio ",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":34,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.2027027027,
        "Challenge_watch_issue_ratio":0.2567567568
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb\r\n\r\nResolve by adding `!apt-get install unzip -y`\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1590122435000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/issues\/31",
        "Challenge_link_count":1,
        "Challenge_readability":22.9,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":48.0,
        "Challenge_repo_issue_count":74.0,
        "Challenge_repo_star_count":103.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Error in Autopilot Notebook with SageMaker Studio ",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":15,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.2027027027,
        "Challenge_watch_issue_ratio":0.2567567568
    },
    {
        "Challenge_adjusted_solved_time":122.7663888889,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nWhen making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createModel\" when I run the pipeline from scratch. The error is:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png)\r\n\r\n**To Reproduce**\r\nThe only steps I took was to unfold it as it naturally comes. This bug prevented me from creating a sagemaker model for both the batch and realtime pipelines.\r\n\r\n**Expected behavior**\r\nThe ideal and expected behavior is that this error does not occur and you can create the model.\r\n\r\n**Solution to that moment**\r\nTry to fix the numpy versions issue by re-creating the `sagemaker_layer` layer, via pip installation of the libraries. However, there were conflicts with other modified libraries at the time of `pip install numpy`. For this reason, I had to choose to use the default AWS library that comes with numpy \"AWSLambda-Python38-SciPy1x-v29\". For this, I had to modify the code as follows:\r\n\r\nin deploy_actions.py \/ create_sagemaker_model - I add the layer:\r\n\"arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python38-SciPy1x:29\u201d\r\n\r\nWith this, I stop throwing that error at me. I think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. Please check if it still exists in the new versions.\r\n\r\n**Please complete the following information about the solution:**\r\n- [ ] Version: [e.g. v1.1.0]\r\n\r\n\r\nTo get the version of the solution, you can look at the description of the created CloudFormation stack. For example, \"(SO0136) - AWS MLOps Framework. Version v1.1.0\".\r\n\r\n- [ ] Region: [e.g. us-east-1]\r\n- [ ] Was the solution modified from the version published on this repository? No\r\n- [ ] If the answer to the previous question was yes, are the changes available on GitHub? -\r\n- [ ] Have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses?\r\n- [ ] Were there any errors in the CloudWatch Logs? Yes\r\n\r\n**Additional context**\r\nI am a Solution Architect of an advanced AWS partner company, and we are running a proof of concept with a real client.",
        "Challenge_closed_time":1620839615000,
        "Challenge_created_time":1620397656000,
        "Challenge_link":"https:\/\/github.com\/aws-solutions\/mlops-workload-orchestrator\/issues\/6",
        "Challenge_link_count":2,
        "Challenge_readability":10.2,
        "Challenge_reading_time":28.62,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":45.0,
        "Challenge_repo_issue_count":23.0,
        "Challenge_repo_star_count":115.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":122.7663888889,
        "Challenge_title":"Error with sagemaker_layer in lambda \"create_sagemakermodel\"",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":321,
        "Platform":"Github",
        "Solution_body":"Please mention if this continues to support you in correcting this error with a pull request. Also to know if it is something from my environment or if it is really a bug in the layer. Greetings to all! Hi @CthompsonCL , thank you for raising this issue. In the newly released version (v1.2.0), the createmodel lambda does not exist anymore. The model is created within a CloudFromation template. So, this issue is resolved in the new release. We will investigate\/fix the build of the sagemaker layer in the previous release.     @CthompsonCL, one more note. if you build the solution locally (i.e, custom build), the sagemaker layer must be built using an Amazon Linux image. Otherwise, you will have the reported error. Perfect! thanks for all. ",
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":9.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":125.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.347826087,
        "Challenge_watch_issue_ratio":0.6956521739
    },
    {
        "Challenge_adjusted_solved_time":1017.9547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Challenge_closed_time":1623053107000,
        "Challenge_created_time":1619388470000,
        "Challenge_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Challenge_link_count":1,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":428.0,
        "Challenge_repo_issue_count":16.0,
        "Challenge_repo_star_count":350.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1017.9547222222,
        "Challenge_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":136,
        "Platform":"Github",
        "Solution_body":"Resolved by fixing Sagemaker's version to 1.72.0.",
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":0.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.5,
        "Challenge_watch_issue_ratio":1.125
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"### What steps did you take\r\n\r\nIf node scales\/up down, the sagemaker component tries to create the same job which fails. Since sagemaker does not let create the same name job. Component controller should be able to detect this and resume the job from existing state. \r\n\r\n### What happened:\r\nthe job hangs\/fail \r\n\r\n### What did you expect to happen:\r\nI expect the job to resume from previous state. \r\n\r\n### Environment:\r\nkfp-1.6\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1639174458000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7040",
        "Challenge_link_count":0,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.62,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] Idempotency in kubeflow pipeline sagemaker component. ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"### What steps did you take\r\n\r\nCode gets stuck in infinite loop is SageMaker training job gets stopped (unhandled use case)\r\n\r\n### What happened:\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/sagemaker_training_component.py#L57-L66\r\n\r\nAbove code only caters for training job status `Completed` or `Failed`, so if the training job status is marked as `Stopped`, it causes an infinite loop in below code\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/sagemaker\/common\/sagemaker_component.py#L197-L201\r\n\r\n### What did you expect to happen:\r\n\r\nTraining job status `stopped` to be catered for\r\n\r\n### Environment:\r\n\r\n### Anything else you would like to add:\r\n\r\n\r\n### Labels\r\n<!-- Please include labels below by uncommenting them to help us better triage issues -->\r\n\r\n<!-- \/area frontend -->\r\n<!-- \/area backend -->\r\n<!-- \/area sdk -->\r\n<!-- \/area testing -->\r\n<!-- \/area samples -->\r\n<!-- \/area components -->\r\n\r\n\r\n---\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1630204381000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6465",
        "Challenge_link_count":2,
        "Challenge_readability":11.5,
        "Challenge_reading_time":15.25,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] Unhandled SageMaker training job status 'stopped' causing infinite loop",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":136,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":947.3408333333,
        "Challenge_answer_count":7,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093472000,
        "Challenge_created_time":1607683045000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":947.3408333333,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":304,
        "Platform":"Github",
        "Solution_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_link_count":4.0,
        "Solution_readability":9.7,
        "Solution_reading_time":16.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":9822.3969444444,
        "Challenge_answer_count":8,
        "Challenge_body":"### What steps did you take: Removed the HPO and Training Jobs only creating the model and batch transforming in SageMaker \r\n[Automatically taking the HPO and Training on SageMaker facing some issue while kfp compile]\r\n\r\n### What happened: Getting output properly in Kubefow. But I want to to see custom  model (ANY) output without HPO and Model training in Sagemaker\r\n\r\n### What did you expect to happen: Without HPO and batch job in SageMaker\r\n\r\n\r\n\r\n\r\n### Anything else you would like to add:\r\nAny open source loan data model using KF would be appriciated \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\/\/compile(kfp.compile ) \r\n\/\r\n",
        "Challenge_closed_time":1632453568000,
        "Challenge_created_time":1597092939000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352",
        "Challenge_link_count":0,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9822.3969444444,
        "Challenge_title":"Want to create ANY model and do batch transform in without Training in Amazon SageMaker ",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"\/assign @Jeffwan @PatrickXYS \r\nlooks like aws specific SageMaker KFP should works fine as regular KFP installed on EKS.\r\n\r\n@RedbackThomson @akartsky Any idea you can share here?  Hi @swarnaditya \r\nThank you for using SageMaker Components for Kubeflow Pipelines. Could you provide more details about the issue:\r\n- are you bringing your own model? Is it uploaded to S3?\r\n- are you bringing your own container or using one of the [pre-built SageMaker algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html)?\r\n- sample reproducible code\r\n\r\nHere is a sample pipeline with only model and batch transform job - https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/tests\/integration_tests\/resources\/definition\/transform_job_pipeline.py\r\n\r\nand here are the sample inputs for this pipeline - https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/tests\/integration_tests\/resources\/config\/kmeans-mnist-batch-transform\/config.yaml#L6-L23. Note: the variables enclosed in `(( ))` are replaced at runtime by our integration tests so you will not be able to run with these inputs directly but we can help you with your use-case\r\n\r\nIn case this helps, here is a customer blog who created a pipeline with model+Hosting - https:\/\/aws.amazon.com\/blogs\/machine-learning\/cisco-uses-amazon-sagemaker-and-kubeflow-to-create-a-hybrid-machine-learning-workflow\/ This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \/assign @surajkota  @RedbackThomson  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \/close\r\n\r\nsince there is no updates on the issue @surajkota: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352#issuecomment-926312361):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_link_count":7.0,
        "Solution_readability":14.4,
        "Solution_reading_time":30.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":255.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":147.3688888889,
        "Challenge_answer_count":4,
        "Challenge_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Challenge_closed_time":1589522860000,
        "Challenge_created_time":1588992332000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Challenge_link_count":0,
        "Challenge_readability":12.5,
        "Challenge_reading_time":10.43,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":147.3688888889,
        "Challenge_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":141,
        "Platform":"Github",
        "Solution_body":"Thank you @marwan116  for trying out the operator. Currently SageMaker has support for images hosted in ECR only. \r\nSageMaker has support for various frameworks like TensorFlow, XGBoost, PyTorch etc as well as some [in-built algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html).\r\n\r\nIf you have custom image, [here](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/docker-push-ecr-image.html) is the instruction to put them into ECR.\r\n  https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670 @marwan116 looks like question answered, I'm going to close this issue.\r\nBut feel free to reopen with `\/reopen` comment.\r\n\r\n\/close @Bobgy: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728#issuecomment-629047584):\n\n>@marwan116 looks like question answered, I'm going to close this issue.\r\n>But feel free to reopen with `\/reopen` comment.\r\n>\r\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_link_count":6.0,
        "Solution_readability":13.1,
        "Solution_reading_time":16.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":129.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":280.5022222222,
        "Challenge_answer_count":6,
        "Challenge_body":"### What steps did you take:\r\nI run a custom image using the sagemaker training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/sagemaker\/train\/component.yaml) and it ran fine. I am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nThe problem arises however if inside the custom script I use boto3 to manually download an object from s3 - then I get an error: Unable to locate credentials ...  \r\n\r\n### What happened:\r\nBelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\nINFO:root:Submitting Training Job to SageMaker...\r\nINFO:root:Created Training Job with name: TrainingJob-20200430232331-LPHY\r\nINFO:root:Training job in SageMaker: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/sagemaker\/home?region=us-west-2#\/jobs\/TrainingJob-20200430232331-LPHY\r\nINFO:root:CloudWatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=TrainingJob-20200430232331-LPHY;streamFilter=typeLogStreamPrefix\r\nINFO:root:Job request submitted. Waiting for completion...\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training failed with the following error: AlgorithmError: Exception during training: Unable to locate credentials\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  File \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packag\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  File \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise Exception('Training job failed')\r\nException: Training job failed\r\n```\r\n\r\n### What did you expect to happen:\r\nI would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### Environment:\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nI deployed kubeflow pipelines as part of my kubeflow deployment on AWS EKS:\r\n\r\nKFP version: \r\nBuild commit: 743746b\r\n\r\nKFP SDK version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1589303399000,
        "Challenge_created_time":1588293591000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670",
        "Challenge_link_count":3,
        "Challenge_readability":12.2,
        "Challenge_reading_time":47.45,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":280.5022222222,
        "Challenge_title":"Sagemaker Custom Training Job Error: Unable to locate botocore.credentials",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":390,
        "Platform":"Github",
        "Solution_body":"Thanks Marwan for trying out the component. \r\nI believe your script was buried inside your custom image, if so that custom image runs inside sagemaker which does not inherit the `use_aws_secret` values. So either you need to add permission to the role https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L10 or read it from AWS secret manager. \r\n\r\nWould you mind sharing your script or minimal reproducible code ?  @gautamkmr  - thank you for taking the time to respond to this issue\r\n\r\nPlease find below a very simplified version of the script I'd like to run but hopefully should be good enough to show where the issue is - please note the comments in the script\r\n```\r\nimport pathlib\r\nimport os\r\nimport boto3\r\nimport sys\r\n\r\n\r\ndef main():\r\n    try:\r\n        # Reading data that sagemaker has copied from s3\r\n        # works fine\r\n        prefix = pathlib.Path('\/opt\/ml\/')\r\n        input_path = prefix \/ 'input\/data\/train\/'\r\n\r\n        with open(input_path \/ 'test.txt', 'r') as f:\r\n            content = f.read()\r\n\r\n        assert 'hello world' in content\r\n\r\n        # the below portion is trying to read data from s3\r\n        # using boto3 but it fails\r\n        bucket_name = os.environ['AWS_BUCKET']\r\n        object_name = 'dummy_input\/test.txt'\r\n        file_name = 'test.txt'\r\n\r\n        s3 = boto3.client('s3')\r\n\r\n        # specifically the below line fails:\r\n        # botocore.exceptions.NoCredentialsError: Unable to locate credentials\r\n        s3.download_file(bucket_name, object_name, file_name)\r\n\r\n    except Exception:\r\n        sys.exit(255)\r\n```\r\n\r\nHere is the script for compiling the pipeline just in case you need it\r\n```\r\nimport kfp.compiler as compiler\r\nimport json\r\nimport os\r\nfrom kfp import components, dsl\r\nfrom kfp.aws import use_aws_secret\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"sm_kfp_example\",\r\n    description=\"sample sagemaker training job\"\r\n)\r\ndef sm_kfp_example():\r\n    bucket = os.environ.get('AWS_BUCKET')\r\n    role = os.environ.get('SAGEMAKER_ROLE_ARN')\r\n\r\n    train_channels = json.dumps([{\r\n        'ChannelName': 'train',\r\n        'DataSource': {\r\n            'S3DataSource': {\r\n                'S3Uri': f's3:\/\/{bucket}\/dummy_input\/',\r\n                'S3DataType': 'S3Prefix',\r\n                'S3DataDistributionType': 'FullyReplicated'\r\n            }\r\n        },\r\n        'ContentType': '',\r\n        'CompressionType': 'None',\r\n        'RecordWrapperType': 'None',\r\n        'InputMode': 'File'\r\n    }])\r\n\r\n    # use the sagemaker training operator defined by aws\r\n    # [a wrapper around Sagemaker CreateTrainingJob]\r\n    repo_path = 'https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines'\r\n    # commit hash of current version of kfp that we are using\r\n    commit = 'master'\r\n    suffix = 'components\/aws\/sagemaker\/train\/component.yaml'\r\n    path = f'{repo_path}\/{commit}\/{suffix}'\r\n\r\n    sagemaker_train_op = components.load_component_from_url(path)\r\n    output_path = f's3:\/\/{bucket}\/output'\r\n    account_id = os.environ.get('AWS_ACCOUNT_ID')\r\n    region = os.environ.get('AWS_REGION')\r\n    image = f'{account_id}.dkr.ecr.{region}.amazonaws.com\/sm_kfp_example:latest'\r\n\r\n    _ = sagemaker_train_op(\r\n        region=region,\r\n        endpoint_url='',\r\n        image=image,\r\n        training_input_mode='File',\r\n        hyperparameters='{}',\r\n        channels=train_channels,\r\n        instance_type='ml.m5.xlarge',\r\n        instance_count='1',\r\n        volume_size='20',\r\n        max_run_time='3600',\r\n        model_artifact_path=output_path,\r\n        output_encryption_key='',\r\n        network_isolation='True',\r\n        traffic_encryption='False',\r\n        spot_instance='False',\r\n        max_wait_time='3600',\r\n        checkpoint_config='{}',\r\n        role=role,\r\n    ).apply(\r\n        use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')\r\n    )\r\n\r\n\r\ndef compile(pipeline_func):\r\n    pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\r\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # compile the pipeline\r\n    compile(sm_kfp_example)\r\n```\r\n\r\nThe very strange thing is if I try to create the training job using the sagemaker python sdk (i.e. not using sagemaker's k8s training operator) - the script runs fine - i.e. the credentials are passed down to the container - below is the script in case you need it\r\n\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.estimator import Estimator\r\nimport os\r\n\r\naws_region = os.environ['AWS_REGION']\r\nalgorithm_name = \"sm_kfp_example\"\r\ns3_bucket = os.environ['AWS_BUCKET']\r\n\r\n# use the security token service to verify the account identity\r\nclient = boto3.client('sts')\r\naccount = client.get_caller_identity()['Account']\r\n\r\n# set the sagemaker role\r\nrole = os.environ['SAGEMAKER_ROLE_ARN']\r\n\r\n# create a boto_session\r\nboto_session = boto3.session.Session(\r\n    region_name=aws_region,\r\n)\r\n\r\n# get full training image url\r\ntraining_image = f\"{account}.dkr.ecr.{aws_region}.amazonaws.com\/{algorithm_name}:latest\"\r\n\r\n\r\n# specify location on s3_bucket to output the results\r\ns3_output_location = f's3:\/\/{s3_bucket}\/output'\r\n\r\n# create a sagemaker_session\r\nsagemaker_session = sagemaker.session.Session(boto_session=boto_session)\r\n\r\n# create an estimator\r\nestimator = Estimator(\r\n    training_image,\r\n    role,\r\n    train_instance_count=1,\r\n    train_instance_type='ml.m5.xlarge',\r\n    train_volume_size=10,  # 10 GB\r\n    train_max_run=600,  # 10 minutes = 600seconds\r\n    input_mode='File',\r\n    output_path=s3_output_location,\r\n    sagemaker_session=sagemaker_session,\r\n    hyperparameters={},\r\n    base_job_name=\"sagemaker-sample\",\r\n)\r\n\r\nestimator.fit(\r\n    inputs={\r\n        'train': f's3:\/\/{s3_bucket}\/dummy_input\/'\r\n    },\r\n    logs=True\r\n)\r\n```\r\n Note, to avoid opening other Sagemaker issues, I will list out some of the pain points I have faced trying to integrate sagemaker with Kubeflow here and let me know if there are any solutions to these - excuse me for not following protocol - but if these are deemed as valid issues -I would be glad to open up the relevant issues:\r\n\r\n- I can't seem to pass an image to sagemaker_training_op that is not hosted on ECR and if the image is hosted on ECR - it has to be in the same region as that specified for sagemaker_training_op ... \r\n\r\n- Currently, the sagemaker logs are being output to cloudwatch not to kubeflow (would be much easier if they can be forwarded to kubeflow)\r\n  There has been some undocumented change to the `load_component_*` functions. It used to return a `ContainerOp`, now it returns a `TaskSpec` instead.\r\n\r\nCurrently there are 2 possible workaround:\r\n\r\n1.  use the private func `_create_container_op_from_component_and_arguments` to generate ur containerop from taskspec\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\n\r\ncomponent_op = components.load_component_from_url(...)\r\ntaskspec = component_op(...)\r\ncontainerop = _create_container_op_from_component_and_arguments(\r\n  taskspec.component_ref.spec, \r\n  taskspec.arguments, \r\n  taskspec.component_ref\r\n)\r\n```\r\n\r\n2. overwrite the default `_default_container_task_constructor`\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\nimport kfp.components._components as _components\r\n\r\n_components._default_container_task_constructor = _create_container_op_from_component_and_arguments\r\n\r\n# now load_component will return a containerop\r\ncontainerop = components.load_component_from_url(...)\r\n```\r\n\r\nPS: @Ark-kun this is not the first time I seen this issue\/qns - what do u think? Hi @marwan116, I was able to reproduce the failure and the root cause is that the default value for `network_isolation` parameter is set to [False in python sdk](https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/bf48fb1219bd8ea22e78a913bfa091e544c57cc3\/src\/sagemaker\/estimator.py#L1112)  whereas in the pipeline definition you provided it is set to True, which is also the [default value](https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L73-L75) in training component\r\n\r\nCan you try set it to False and let us know if your issue has been resolved ?\r\n\r\n\r\n----\r\n\r\nHere are some clarifications based on posts on this thread: \r\n\r\n- The logs you posted in the issue initially [under whats happened section](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670#issue-610481941) (except the exception) is from the component pod and NOT from the training job itself. As you have already observed, for the training job logs you need to go to cloudwatch.\r\n  - the first log line which you see `INFO:botocore.credentials:Found credentials in environment variables.` is from boto session which is created by the component backend to call create_training_job API. It uses the credentials are from `aws-secret` that you would have created. These credentials are only used to invoke the job and are not passed to the instance in SageMaker\r\n\r\n- The SageMaker instance which runs in AWS assumes the credentials from the role ARN you provide in`SAGEMAKER_ROLE_ARN` not not from the secret\r\n\r\nLet us know if you have more questions.\r\n @surajkota  - thank you so much for taking the time to reproduce this - yes you are right it is because I had `network_isolation` set to `True` - (sorry I should have taken the time to understand what `network_isolation` does)\r\n\r\nAlso thank you for the clarifications!\r\n\r\nI saw @gautamkmr graciously took the time to open an issue concerning the logs - thank you @gautamkmr !\r\n\r\nI am closing this now as this particular issue is now resolved",
        "Solution_link_count":5.0,
        "Solution_readability":13.4,
        "Solution_reading_time":112.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":69.0,
        "Solution_word_count":954.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":1317.2311111111,
        "Challenge_answer_count":13,
        "Challenge_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Challenge_closed_time":1563269566000,
        "Challenge_created_time":1558527534000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Challenge_link_count":1,
        "Challenge_readability":21.6,
        "Challenge_reading_time":18.74,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1317.2311111111,
        "Challenge_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":116,
        "Platform":"Github",
        "Solution_body":"Hi @Jeffwan ,\r\n\r\nneed your support on this.\r\n\r\nI am using training image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" and it is throwing an error for mising values for parameters K and feature_dim. Although we are not using these parameters anywhere in pipeline.\r\n\r\nCan you please provide the solution ?\r\n\r\nRegards,\r\nVarun em. I may delete the configuration fields in clean up. Let me double check and come back to you @vackysh  I can reproduce this issue. \r\n\r\n`HyperParameters` was removed by me in this commit\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/commit\/26f2719c28a731d8925ae2ce96252be1df2562aa\r\n\r\nAdd it back will solve this problem Image has been rebuilt and it should be good now.  Hi @Jeffwan ,\r\n\r\nThanks for your response.\r\n\r\nI again executed the pipeline using image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" , but getting the same issue\r\n\r\n\"Training failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\"\r\n\r\nThe pipeline parameters are:\r\n\r\n@dsl.pipeline(\r\nname='MNIST Classification pipeline',\r\ndescription='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\nimage='382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1',\r\ndataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\ninstance_type='ml.c4.8xlarge',\r\ninstance_count='2',\r\nvolume_size='50',\r\nmodel_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\nbatch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\nbatch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\nrole_arn=''\r\n):\r\n\r\nPlease suggest how to get through it if issue has already fixed at your end.\r\n @vackysh I think the problem is your machine already has this image. could you go to the machine and do a force pull? \r\n```\r\nseedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05\r\n``` HI @Jeffwan ,\r\n\r\nI refreshed the image It is now working fine.\r\nThank you so much.\r\n\r\nRegards,\r\nVarun Hi @Jeffwan,\r\n\r\nWhere can i get the actual source code (ML code ) reading from the image seedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05 (not a docker file, but actual logic for train, predction) ?\r\n\r\nI actually working on similar automation and want to analyse the source code.\r\n\r\nRegards,\r\nVarun @vackysh This is a component example for training. \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/train.py\r\n\r\nNot sure if your work is internal or public. It would be perfect if you can make some contribution! Feel free to ping me on Slack or shoot me an email. Hi @Jeffwan  ,\r\n\r\nThanks for information. But i am looking for the main Kmean algorithms code that is used for training the model. I couldn't find that anywhere on path.\r\n\r\nI have a requirement where Scikit SVM model to get deploy on kubeflow pipeline using aws sagemaker services and S3. So i want to have a look on source ML code that has been passed through image as an input to pipeline.\r\n\r\nRegards,\r\nVarun\r\n\r\n @vackysh Now I get your point, in the example, I am using the container images from SageMaker. I think KMEANS one is first-party models and you might don't have access to it. What I suggest you to do is bring your own training image if you have customization request. This issue is resolved. Now closing > This issue is resolved. Now closing\r\n\r\nHave you figured out a way to make it? Did you try bring your own container? ",
        "Solution_link_count":2.0,
        "Solution_readability":9.1,
        "Solution_reading_time":43.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":7267.8997222222,
        "Challenge_answer_count":3,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Challenge_closed_time":1626207887000,
        "Challenge_created_time":1600043448000,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Challenge_link_count":4,
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.38,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7267.8997222222,
        "Challenge_title":"[bug] Sagemaker Remote Test reporting issues",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":244,
        "Platform":"Github",
        "Solution_body":"@saimidu mentioned that codebuild runs have a timeout of 90min. However, \r\n- codebuild should have shown status as timed out instead of Failed\r\n- PR commit status should have been failed instead of pending.\r\nSo that's still an open issue. Depends on #444 It appears this issue has been resolved by the PR mentioned above. Closing this ticket out.",
        "Solution_link_count":0.0,
        "Solution_readability":5.4,
        "Solution_reading_time":4.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.039824771,
        "Challenge_watch_issue_ratio":0.015133413
    },
    {
        "Challenge_adjusted_solved_time":184.4022222222,
        "Challenge_answer_count":0,
        "Challenge_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1598551848000,
        "Challenge_created_time":1597888000000,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Challenge_link_count":0,
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":184.4022222222,
        "Challenge_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":55,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.039824771,
        "Challenge_watch_issue_ratio":0.015133413
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"- [ X] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\nAutogluon 0.4.0 TextPredictor training on p3.8xl 4-GPU instance in Sagemaker Notebook terminal, with `env.num_gpus: 4` setting.  I get an error in spawning multiprocessing.  When I train with everything the same, but only on a single GPU within the same instance and setup, it trains without a problem.\r\n\r\n**Expected behavior**\r\nTrain across all 4 GPUs in the p3.8xl instance with no errors.\r\n\r\n**To Reproduce**\r\n* SageMaker Notebook p3.8xl instance\r\n* python 3.7.12.  \r\n* pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0\r\n* python train.py\r\n\r\nCode:\r\nin `train.py` file\r\n```from argparse import Namespace\r\n\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import TextPredictor\r\n\r\nargs = Namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\npredictor = TextPredictor(\r\n    label='label',\r\n    eval_metric=model_config['eval_metric']\r\n)\r\n\r\npredictor.fit(\r\n    train_data=train_df[model_config['features']],\r\n    hyperparameters=hyperparameters,\r\n    time_limit=model_config['time_limit']\r\n)\r\n\r\n```\r\n\r\n**Screenshots**\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/ec2-user\/SageMaker\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in <module>\r\n    time_limit=model_config['time_limit']\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit\r\n    seed=seed,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit\r\n    enable_progress_bar=self._enable_progress_bar,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit\r\n    ckpt_path=self._ckpt_path,  # this is to resume training that was broken accidentally\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\nIf you are using 0.4.0 and newer, please run the following code snippet:\r\n<details>\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-06\r\ntime                 : 15:22:16.975165\r\npython               : 3.7.12.final.0\r\nOS                   : Linux\r\nOS-release           : 4.14.252-131.483.amzn1.x86_64\r\nVersion              : #1 SMP Mon Nov 1 20:48:11 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 32\r\ncpu_ram_mb           : 245845\r\ncuda version         : 11.450.142.00\r\nnum_gpus             : 4\r\ngpu_ram_mb           : [8404, 8476, 16160, 16160]\r\navail_disk_size_mb   : 11391\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.34\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nmatplotlib           : 3.5.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.21.5\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : None\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorchmetrics         : 0.7.3\r\ntqdm                 : 4.64.0\r\ntransformers         : 4.16.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1649258575000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1650",
        "Challenge_link_count":0,
        "Challenge_readability":15.2,
        "Challenge_reading_time":104.31,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Unable to train on multiple GPUs in Sagemaker Notebook Terminal",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":636,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"- [x] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\n```python\r\n...\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\nIt appears that the SageMaker endpoint isn't able to find \/ open the model file. I was able to use the example code in the tutorial [Deploying AutoGluon Models with AWS SageMaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html) and managed to deploy an endpoint to SageMaker. But I get this error when I go to make predictions with test data. I wonder if this might be related to transition from MXNet to Pytorch and how their artifacts are typically stored? I'm using `v0.4.0` but the predictor object is `sagemaker.mxnet.model.MXNetPredictor`. This discrepancy in framework seems supported be a related error that I found in a GitHub issue [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1238).\r\n\r\nNote also that I am attempting to adapt the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset), because I'm ultimately try to deploy a multi-modal model and figure out how to pass image_paths to the SageMaker endpoint. \r\n\r\nHere's the full traceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nModelError                                Traceback (most recent call last)\r\n<ipython-input-51-abf97eb84e0a> in <module>\r\n----> 1 predictions = predictor.predict(test_data[:1].values)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\n    159             data, initial_args, target_model, target_variant, inference_id\r\n    160         )\r\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\r\n    162         return self._handle_response(response)\r\n    163 \r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\r\n    389                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    390             # The \"self\" in this scope is referring to the BaseClient.\r\n--> 391             return self._make_api_call(operation_name, kwargs)\r\n    392 \r\n    393         _api_call.__name__ = str(py_operation_name)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\r\n    717             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\r\n    718             error_class = self.exceptions.from_code(error_code)\r\n--> 719             raise error_class(parsed_response, operation_name)\r\n    720         else:\r\n    721             return parsed_response\r\n\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 110, in transform\r\n    self.validate_and_initialize(model_dir=model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 158, in validate_and_initialize\r\n    self._model = self._model_fn(model_dir)\r\n  File \"\/opt\/ml\/model\/code\/tabular_serve.py\", line 11, in model_fn\r\n    model = TabularPredictor.load(model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2816, in load\r\n    predictor = cls._load(path=path)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2772, in _load\r\n    predictor: TabularPredictor = load_pkl.load(path=path + cls.predictor_file_name)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/common\/loaders\/load_pkl.py\", line 37, in load\r\n    with compression_fn_map[compression_fn]['open'](validated_path, 'rb', **compression_fn_kwargs) as fin:\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**To Reproduce**\r\n\r\n1. Train a multi modal model, using code adapted from [Multimodal Data Tables: Tabular, Text, and Image Tutorial](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html): [petfinder_train.py](https:\/\/gist.github.com\/ijmiller2\/f5837977077674fe741fee031d2bad2a)\r\n2. Deploy the pet finder model: [petfinder_deploy.py](https:\/\/gist.github.com\/ijmiller2\/f6b21c2b0b40211161d1fb0252542189)\r\n\r\n**Screenshots**\r\nNA\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\n`0.4.0`\r\n<details>\r\n\r\n```python\r\n# Replace this code with the output of the following:\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-02\r\ntime                 : 19:45:52.692253\r\npython               : 3.9.7.final.0\r\nOS                   : Linux\r\nOS-release           : 5.4.0-66-generic\r\nVersion              : #74~18.04.2-Ubuntu SMP Fri Feb 5 11:17:31 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 12\r\ncpu_ram_mb           : 64324\r\ncuda version         : None\r\nnum_gpus             : 0\r\ngpu_ram_mb           : []\r\navail_disk_size_mb   : 289302\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.tabular    : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon.vision     : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.21\r\ncatboost             : 1.0.4\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nfastai               : 2.5.3\r\ngluoncv              : 0.11.0\r\nlightgbm             : 3.3.2\r\nmatplotlib           : 3.5.1\r\nnetworkx             : 2.7.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.22.3\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : 1.8.0\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorch                : 1.10.1+cpu\r\ntorchmetrics         : 0.7.2\r\ntqdm                 : 4.63.0\r\ntransformers         : 4.16.2\r\nxgboost              : 1.4.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\n\r\nI am attempting to follow the [tutorial to deploy a model via Sagemaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html), however, adapting to use the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset).\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1648949150000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1644",
        "Challenge_link_count":8,
        "Challenge_readability":14.6,
        "Challenge_reading_time":85.63,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":80,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] SageMaker endpoint appears unable to load model file \/ use image paths as features",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":618,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Has anyone figured out an easy way to make plot_ensemble_model() work in jupyter based environments? I'm having a lot of difficulty installing pygraphviz (think it might be related to pygraphviz not able to see where graphviz is being installed? but not sure)\r\n\r\nI've tried the following code without success: \r\n%pip install python3-dev\r\n%pip install graphviz\r\n%pip install libgraphviz-dev\r\n%pip install pkg-config\r\n\r\n%pip install pygraphviz\r\n\r\n\r\nThanks for the help!",
        "Challenge_closed_time":null,
        "Challenge_created_time":1645398079000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1551",
        "Challenge_link_count":0,
        "Challenge_readability":9.8,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make plot_ensemble_model() work in sagemaker (or any jupyter based env)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":79,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":8976.4125,
        "Challenge_answer_count":2,
        "Challenge_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1613263024000,
        "Challenge_created_time":1580947939000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Challenge_link_count":1,
        "Challenge_readability":14.4,
        "Challenge_reading_time":192.92,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":230,
        "Challenge_solved_time":8976.4125,
        "Challenge_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":999,
        "Platform":"Github",
        "Solution_body":"Thanks for submitting this issue!\r\n\r\nWe haven't looked closely at which boto versions are functional with AutoGluon, but I would suspect using the newer version wouldn't cause issues.\r\n\r\nWe will take a look.\r\n @zhuwenzhen In the latest mainline of AutoGluon, the boto version limitation has been removed. This may resolve your issue if you install AutoGluon from source, or wait for AutoGluon 0.1 to be released.",
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":10.1325,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Challenge_closed_time":1600271697000,
        "Challenge_created_time":1600235220000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Challenge_link_count":5,
        "Challenge_readability":13.9,
        "Challenge_reading_time":38.77,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":651.0,
        "Challenge_repo_issue_count":2147.0,
        "Challenge_repo_star_count":3215.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":10.1325,
        "Challenge_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":254,
        "Platform":"Github",
        "Solution_body":"According to [this](https:\/\/github.com\/iterative\/dvc\/issues\/1995), it could be that `enum34` is installed.\r\n\r\nCan you check whether this is also the case here? Thanks @jaheba ! That was the issue and by running `!pip uninstall -y enum34` it is resolved.",
        "Solution_link_count":1.0,
        "Solution_readability":4.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0423847229,
        "Challenge_watch_issue_ratio":0.032603633
    },
    {
        "Challenge_adjusted_solved_time":11.7994444444,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nUsing Amazon SageMaker on an AWS GPU-instance \"ml.p2.xlarge\", I was not able to run the example `benchmark_m4.py` script (copy\/pasted in SageMaker) on GPU. \r\n\r\n## To Reproduce\r\nAfter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nNext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        DeepAREstimator,\r\n        trainer=Trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on CPU)\r\n\r\n## Error Message\r\n\r\n```\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_quarterly.\r\nINFO:root:Start model training\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_yearly.\r\nINFO:root:Start model training\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[24000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"3M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=8, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='3M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='24000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=8>, train=<gluonts.dataset.common.FileDataset object at 0x7f9377c9e748>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x7f937812ce48>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"MASE\",\r\n     89             \"sMAPE\",\r\n---> 90             \"MSIS\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # When setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": True if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise KeyError(\r\n   1176                     \"None of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nKeyError: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n## Other\r\nIn addition, before installing gluonts (from https:\/\/beta.mxnet.io\/guide\/crash-course\/6-use_gpus.html): \r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n[[1. 1. 1. 1.]\r\n [1. 1. 1. 1.]\r\n [1. 1. 1. 1.]]\r\n<NDArray 3x4 @gpu(0)>\r\n```\r\n\r\nAfter installing gluonts: \r\n\r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nMXNetError                                Traceback (most recent call last)\r\n<ipython-input-16-749bd657d613> in <module>()\r\n      5 \r\n      6 \r\n----> 7 x = nd.ones((3,4), ctx=gpu())\r\n      8 x\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/ndarray.py in ones(shape, ctx, dtype, **kwargs)\r\n   2419     dtype = mx_real_t if dtype is None else dtype\r\n   2420     # pylint: disable= no-member, protected-access\r\n-> 2421     return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)\r\n   2422     # pylint: enable= no-member, protected-access\r\n   2423 \r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/register.py in _ones(shape, ctx, dtype, out, name, **kwargs)\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/_ctypes\/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/base.py in check_call(ret)\r\n    250     \"\"\"\r\n    251     if ret != 0:\r\n--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    253 \r\n    254 \r\n\r\nMXNetError: [22:29:51] src\/imperative\/imperative.cc:79: Operator _ones is not implemented for GPU.\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x9fb) [0x7f9397bb2abb]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f93d5b04e2e]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x12865) [0x7f93d5b05865]\r\n```\r\n\r\n## Environment\r\n\r\n- Amazon SageMaker, running on AWS instance \"ml.p2.xlarge\". \r\n- GluonTS version: 0.3.3 installed using pip.\r\n- Kernel: conda_mxnet_p36 \r\n\r\n",
        "Challenge_closed_time":1573208758000,
        "Challenge_created_time":1573166280000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/426",
        "Challenge_link_count":1,
        "Challenge_readability":24.8,
        "Challenge_reading_time":190.21,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":651.0,
        "Challenge_repo_issue_count":2147.0,
        "Challenge_repo_star_count":3215.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":11.7994444444,
        "Challenge_title":"Problems using GPU with Amazon SageMaker on AWS-instance \"ml.p2.xlarge\".",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":785,
        "Platform":"Github",
        "Solution_body":"After installing GluonTS, could you try running in a cell \r\n\r\n```\r\n!pip show mxnet\r\n```\r\n\r\nand report the result? `!pip show mxnet` results in:\r\n\r\n```python\r\nName: mxnet\r\nVersion: 1.4.1\r\nSummary: MXNet is an ultra-scalable deep learning framework. This version uses openblas.\r\nHome-page: https:\/\/github.com\/apache\/incubator-mxnet\r\nAuthor: UNKNOWN\r\nAuthor-email: UNKNOWN\r\nLicense: Apache 2.0\r\nLocation: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\r\nRequires: numpy, graphviz, requests\r\nRequired-by: gluonts\r\nYou are using pip version 10.0.1, however version 19.3.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```\r\n\r\n`!pip install mxnet --upgrade` led to conflicts with gluonts and numpy and the same error message with `GPU is not enabled`. \r\n @tm1611 hopefully this is solved with the upcoming `0.4` release (to be relased very soon), since #428 was merged. I think the problem is that with `gluonts<0.4` the vanilla mxnet package gets installed and replaces the one with built-in cuda for GPU processing. @tm1611 can you check if the problem is gone now when you install GluonTS from scratch on your instance? Yes, the problem with mxnet and dependencies was removed. Thanks a lot for the quick fix. \r\n\r\nUnrelated to this issue, but related to `m4_benchmark.py` and the sake of completeness: Using gluonts-version 0.4., one has to change `num_eval_samples` to `num_sampels` (see #421 ). ",
        "Solution_link_count":1.0,
        "Solution_readability":5.7,
        "Solution_reading_time":17.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":202.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0423847229,
        "Challenge_watch_issue_ratio":0.032603633
    },
    {
        "Challenge_adjusted_solved_time":21.5513888889,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791347000,
        "Challenge_created_time":1585713762000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":164.0,
        "Challenge_repo_fork_count":740.0,
        "Challenge_repo_issue_count":1942.0,
        "Challenge_repo_star_count":7884.0,
        "Challenge_repo_watch_count":102.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":21.5513888889,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":1973,
        "Platform":"Github",
        "Solution_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Solution_link_count":2.0,
        "Solution_readability":11.1,
        "Solution_reading_time":32.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":397.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0844490216,
        "Challenge_watch_issue_ratio":0.052523172
    },
    {
        "Challenge_adjusted_solved_time":992.0369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1623682749000,
        "Challenge_created_time":1620111416000,
        "Challenge_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Challenge_link_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":49.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":1491.0,
        "Challenge_repo_star_count":115.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":992.0369444444,
        "Challenge_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0328638498,
        "Challenge_watch_issue_ratio":0.0087189805
    },
    {
        "Challenge_adjusted_solved_time":15.6105555556,
        "Challenge_answer_count":2,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782683000,
        "Challenge_created_time":1657726485000,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2570.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":15.6105555556,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":829,
        "Platform":"Github",
        "Solution_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":5.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0482758621,
        "Challenge_watch_issue_ratio":0.0318965517
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nSWB 5.2.6 version - SageMaker jupyter notebook workspace can not mount a study.  see error in \/var\/log\/message\r\n\/usr\/local\/bin\/goofys[7248]: main.FATAL Mounting file system: Mount: mount: running fusermount: exec: \"fusermount\": executable file not found in $PATH#012#012stderr:\r\n\r\nLooks like the FUSE package failed to install during on-start.  if run \"sudo yum install fuse\" then you can run \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study. \r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n - Is the deployment from a forked version of the repository?\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1671836610000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1089",
        "Challenge_link_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":13.85,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] study fail to mount in SWB 5.2.6 SageMaker Jupyter Notebook",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":146,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":168.8386111111,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671209314000,
        "Challenge_created_time":1670601495000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Challenge_link_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.6,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":168.8386111111,
        "Challenge_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":"Hi @simranmakwana, thank you for creating this issue. There are currently no plans to enrich the error messages in the UI; the recommendation is for you to customize the error messages within your installation of SWB as you see fit. Please reply back if there are any concerns with this approach. Thank you! ",
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":3.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":53.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":191.5183333333,
        "Challenge_answer_count":5,
        "Challenge_body":"**Describe the bug**\r\n\r\nIdle Sagemaker Notebook instances do not stop after specified time.\r\n\r\nSWB runs autostop.py script to automatically stop Sagemaker Notebook instance. The script is used by `on-start` lifecycle rule of the instance CFN template. According to LifecycleConfigOnStart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Make sure AutoStopIdleTimeInMinutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. Create a new workspace with Sagemaker notebook instance\r\n3. Leave the instance idle for the time specified (AutoStopIdleTimeInMinutes )\r\n4. After the specified time see that the instance is not stopped\r\n\r\n**Expected behavior**\r\nIdle Sagemaker Notebook instance automatically stops after specified time.\r\n\r\n**Screenshots**\r\n<img width=\"1308\" alt=\"Screen Shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed v5.0.0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1671059684000,
        "Challenge_created_time":1670370218000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076",
        "Challenge_link_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":15.67,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":191.5183333333,
        "Challenge_title":"[Bug] Sagemaker instance does not stop automatically",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":"Thank you. We are aware of this issue and have a backlog item to resolve this! See https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065 for more information. Hi, please check the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5) for the fix to this issue.  Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! Hi Marianna,\nThank you. I am going to migrate to v5.2.5 tomorrow. If everything goes\nwell, I'll close the ticket soon.\n\n\nOn Tue, Dec 13, 2022 at 7:55 AM Marianna Ghirardelli <\n***@***.***> wrote:\n\n> Hi! I also want to note that you may need to stop and start any affected\n> instances after upgrade and deploying SWB v5.2.5.\n>\n> If this fixes your issue, please go ahead and close this issue. I am going\n> to mark as closing-soon-if-no-response so we will close in about 7 days if\n> we do not hear that this did not resolve the issue.\n>\n> Thank you for the report!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076#issuecomment-1347314897>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/ALKETLSEQCFIMAF5HX4CAT3WM6GN3ANCNFSM6AAAAAASWEAJP4>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n Closing this issue since the fix was merged, please feel free to reopen the issue if it persists on your end.",
        "Solution_link_count":4.0,
        "Solution_readability":6.9,
        "Solution_reading_time":21.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":244.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":386.0352777778,
        "Challenge_answer_count":11,
        "Challenge_body":"When a Sagemaker notebook is launched with an associated study, the study folders are not mounted. Digging into this, I see that in the `~\/.aws` folder there is no credentials file, which is generated by the `mount_s3.sh` script.\r\n\r\n**To Reproduce**\r\n- Create a new data source (or use an existing one).\r\n- Select it and create a new Sagemaker instance using those studies. (Ensure that the folder has files so you can see them if they mount.)\r\n- After the instance launches, connect to it and see if the study folders are connected. If not, open a terminal and run `ls ~\/.aws` to see if the credential file is there.\r\n\r\n**Expected behavior**\r\nThe study folders are mounted using the assumed roles in the AWS credentials file, generated by `mount_s3.sh` script.\r\n\r\n**Screenshots**\r\n<img width=\"702\" alt=\"Screen Shot 2022-11-30 at 11 27 45 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853707-789607d5-6fca-4a77-911d-50a5c36a549b.png\">\r\n<img width=\"526\" alt=\"Screen Shot 2022-11-30 at 11 27 36 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853710-824ddc98-dfb5-4c8b-abbe-f19fa2453640.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - Versions 5.0.0 & 4.3.1\r\n\r\n**Additional context**\r\nThis may or may not be associated with the other bug I noted with mounting s3 studies folders, [[Bug] SWB Sagemaker Study permission denied](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067). It seems both of these issues are new, within the last couple weeks. And the environment has had no new deployments or changes within that time. Previous to these last couple weeks we had no issues with Sagemaker and study folders mounting.\r\n",
        "Challenge_closed_time":1671215597000,
        "Challenge_created_time":1669825870000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1073",
        "Challenge_link_count":3,
        "Challenge_readability":8.3,
        "Challenge_reading_time":21.62,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":386.0352777778,
        "Challenge_title":"[Bug] Sagemaker AWS Credentials not Populating",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":232,
        "Platform":"Github",
        "Solution_body":"Thanks for the bug report! We are aware of the issue mounting studies to Sagemaker instances and are actively trying to resolve it. We will let you know if we need any more info to help in our debugging. Although I am able to successfully mount studies to RStudio instances, I have noticed that it is inconsistent. \r\nWhen an RStudio instance is first launched, the user must open the terminal. This triggers some code to run automatically which then mounts the studies, making them visible in the file viewer. However, we have noticed that sometimes the studies are never mounted and the sub-folders are not visible to the user. This appears to happen randomly. When we stop, restart, and click into the terminal tab, the issue is resolved. @srpiatt please check the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5) for the fix to this issue.  Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! I pulled the changes committed for v5.2.5 into our forked repository, which is locked at 5.0.0 version. Credentials file is created and the studies folder is able to mount. Thank you! :D Same happen to me in version 5.2.6, terminal trick didn't resolve the issue  This worked for a while, but I just tried launching another instance today to test if we're still encountering a different permissions issue, and encountered this bug again. We have the changes from the v5.2.5 SWB version pulled, as I noted before. I verified that we haven't regressed and that the changes are present in this instance. This is what we're seeing in this new instance, as of today.\r\n\r\n<img width=\"586\" alt=\"Screen Shot 2023-01-09 at 4 29 30 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/211414692-dd7f4d08-3564-43e3-93b8-839c46f33646.png\">\r\n\r\nCan anyone confirm that they're also seeing this bug again? A new issue popped up that cause this issue to appear again. Please try updating to version 5.2.7. Thanks! I'll give it a try. Is the change related to this commit, [https:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/bdf978b1c909c37207902f1b29b0fd05b5b24208](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/bdf978b1c909c37207902f1b29b0fd05b5b24208)? Until we update our fork to the latest version, we have to cherry pick these sorts of fixes. If yes, I can test that out today and close the ticket again if all good. :) yep, that's it! The changes worked for me. Thanks again, Tim! :D",
        "Solution_link_count":3.0,
        "Solution_readability":6.8,
        "Solution_reading_time":33.96,
        "Solution_score_count":3.0,
        "Solution_sentence_count":35.0,
        "Solution_word_count":421.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":24,
        "Challenge_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids, but the existing studies don't typically change.\r\n\r\nWhat could cause the fs role number to change for a study? What else could cause this permissions denied error? \r\n\r\nThis is a pretty big problem for us, as we have had people actively using SWB and all their work is gone on Sagemaker stop, because the folder they saved to isn't syncing.\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1668634688000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067",
        "Challenge_link_count":0,
        "Challenge_readability":15.2,
        "Challenge_reading_time":29.34,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] SWB Sagemaker Study permission denied",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":276,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":0.4308333333,
        "Challenge_answer_count":1,
        "Challenge_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids.\r\n\r\nMy question is: What could cause the fs role number to change for a study?\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1668634670000,
        "Challenge_created_time":1668633119000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1066",
        "Challenge_link_count":0,
        "Challenge_readability":16.9,
        "Challenge_reading_time":26.21,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.4308333333,
        "Challenge_title":"[Bug] SWB Sagemaker Study permission denied",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":231,
        "Platform":"Github",
        "Solution_body":"I'm elevating this to a bug.",
        "Solution_link_count":0.0,
        "Solution_readability":4.5,
        "Solution_reading_time":0.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":720.8744444444,
        "Challenge_answer_count":16,
        "Challenge_body":"**Describe the bug**\r\nWe encountered an interesting issue regarding the auto stop script. We had no code changes, but suddenly, Sagemaker instances started hanging around for days, with no use. Looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. When I look at the script, it has this line `print(f'Notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. However, the file on the repo, as well as the s3 bucket, does not contain this line. So, after some digging, I found that this line was introduced here, in this commit [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). What I don't understand is how it got into the Sagemaker notebook, and why it's not being overridden by the custom config start we have here [sagemaker-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/sagemaker-notebook-instance.cfn.yml#L264-L272) This script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**To Reproduce**\r\nLaunch a Sagemaker instance. You can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nThe AWS version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L96-L100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n```\r\nAnd on the `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L96-L101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n    print('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**Expected behavior**\r\nThe autostop script in the s3 bucket should be the one used for SWB Sagemaker instances.\r\n\r\n**Screenshots**\r\n<img width=\"1510\" alt=\"Screen Shot 2022-11-16 at 12 14 21 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1671215663000,
        "Challenge_created_time":1668620515000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065",
        "Challenge_link_count":5,
        "Challenge_readability":14.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":720.8744444444,
        "Challenge_title":"[Bug] Sagemaker autostop script not pulling from s3 bucket",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":279,
        "Platform":"Github",
        "Solution_body":"Just want to verify that the autostop script in your bucket had not been updated at some point in the past unexpectedly. Is that correct? Yes-- none of the files on the s3 bucket were changed in several months. I also downloaded the autostop script from the bucket to verify manually that it matches the SWB repo version. I was not able to replicate this in v5.2.2:\r\n<img width=\"937\" alt=\"Screen Shot 2022-11-30 at 3 34 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903064-013c1899-2763-4c88-8cce-7b39128b0240.png\">\r\n\r\nIf you look at the CloudWatch log group \/aws\/sagemaker\/NotebookInstances\/BasicNotebookInstance-<id>\/LifecycleConfigOnStart do you see the following output (would be towards the end):\r\n<img width=\"960\" alt=\"Screen Shot 2022-11-30 at 3 36 08 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903303-d180144c-62d9-4775-a233-83687012715b.png\">\r\nThis is triggered on start of the instance. The screenshot you posted was from your instance, correct? Do you see the lines, \r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n``` \r\nthis line comes from this repo [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L100-L101). It should be only the one line, like below\r\n```\r\nidle = False\r\n``` \r\nwhich comes from SWB here [awslabs\/service-workbench-on-aws](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L100)\r\n\r\nThe s3 file says it's downloaded, but for whatever reason it's not using that downloaded file, and instead using the file on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples. This introduces SWB to vulnerabilities when this code is changes and a bug is introduced in that repo. SWB should instead use the autostop script that it has saved in s3, because that is locked and changes that aren't intended wouldn't be introduced. The screenshot is from my instance, yes. SWB repo contains the lines:\r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n```\r\n[here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/61200d06d1a607b9c0a209240813b261ade2c5e9\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L105). It is the lines:\r\n```\r\nidle = False\r\nprint('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\nthat I thought you said were presenting the problem (that are in the samples repo but not SWB). Is that correct?\r\n\r\nHowever, I see that my instance is not stopping even though the autostop script is the same as the SWB repo.\r\n\r\nWhere did you see the error on the cron job for the autostop? Also, to clarify, you see that the Cloudwatch logs copy from the s3 bucket to local and that the s3 bucket file is the correct file? Yet, you see the wrong file when you less the file on the instance? Oh, you're right I was looking at the wrong line. My apologies! \r\n\r\nYes- for us it's successfully copying the correct file from s3 (which I downloaded to verify), but the autostop script (`\/usr\/local\/bin\/autostop.py`) is the wrong copy. Got it. And where do you see the cron job failing? Because it was not overwriting the autostop script with the one from our s3 bucket, and instead defaulting to the script from `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples`,  when the repo owners introduced this commit: https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e, the cron job failed on lines like `print(f'Notebook idle state set as {idle} since kernel connections are ignored.')`, stating that the `print(f` part was invalid syntax.\r\n\r\nI guess the key issue is really why it didn't overwrite this default script with the s3 one, considering it successfully downloaded the one from s3. Secondly, it seems odd that this repo is somehow the default autostop script that the Sagemaker system uses. This introduces bugs if there's a failure in that script or a malicious commit. Yes, I see the problem in the other repo's commit. I am still trying to debug how the script is on your Sagemaker instance.\r\n\r\nGot ~two~ three more questions:\r\n1. Where _in you account_ did you see that error message from the cron job? CloudWatch logs? Sagemaker? etc.\r\n2. Are you working with AppStream-enabled SWB? Does Sagemaker have to go through AppStream to connect?\r\n3. What is the output from running this command in a terminal on the sagemaker instance: `\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 300 --ignore-connections`? Sure. :D Yeah I don't know how the script was there automatically. I didn't see any code in our repo that would cause it to pull from there.\r\n\r\nWe do not have appstream enabled, but we do send traffic through a proxy lambda as well as a firewall instance. However, all the environment files that get downloaded for the bootstrap process were successful, so I don't think it was a network issue.\r\n\r\nThe reason I checked the autostop script was because I saw no note in the `\/var\/log\/autostop.log` file that the script is sent to via cron job. So I ran the script by hand.\r\n\r\nFor instance, right now autostop is not working. There's no messaging that tells you it's not working. When you look at the cron logs it shows this, with no errors. The `\/var\/log\/autostop.log` script doesn't show any messages or errors.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# grep autostop \/var\/log\/cron | tail -n 1\r\nDec  1 18:14:01 ip-10-10-57-235 CROND[9860]: (root) CMD (\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600 --ignore-connections >> \/var\/log\/autostop.log)\r\n```\r\n\r\nBut if you run the autostop script exactly as the cron job has it, like below, you get an error \/right now\/ related to a boto3 import issue.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# \/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/autostop.py\", line 18, in <module>\r\n    import boto3\r\nImportError: No module named boto3\r\n```\r\n\r\nBoto3 changes were introduced in a commit here, [in the on-start script on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/13b4023c9dca45fea58b2129fe5848619284653a#diff-54051e148aa00ee3fa158cc346d6c243418d14718a6760171ef562887977748f) Yup, so I also get that problem when I try to invoke the autostop script (and my autostop script matches the SWB one). I think that is the root cause of this problem. I will add a backlog item to figure out why boto3 is not being imported correctly so that sagemaker notebooks can use them for autostop. \r\n\r\nIt still does not explain why you got the amazon-sagemaker-notebook-instance-lifecycle-config-samples in the instance. Was that only present in one instance or all instances? Is it possible someone manually changed the files when trying to debug the autostop not working?\r\n\r\nThanks so much for working through this with me! Yeah, no problem-- thanks for your patience and attention! :D\r\n\r\nI don't believe that the files have been altered. I am currently the only person on my team actively responsible for doing admin\/infrastructure activities for these systems. I've tried this on new instances to rule out individual Sagemaker systems changes by users. We have two different version of SWB deployed-- maybe there's a difference between versions.  @srpiatt please upgrade your SWB installation to the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5). Sagemaker made a change that caused all new instances to be spun up with the AL2 operating system. New Sagemaker instances will no longer be able to mount studies or autostop without the fix in v5.2.5 Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! I pulled the changes committed for v5.2.5 into our forked repository, which is locked at 5.0.0 version. Auto stop works. Still not sure how the file got replaced with the one in that repo, but it's a non-issue at the moment. Thank you! :D",
        "Solution_link_count":8.0,
        "Solution_readability":8.9,
        "Solution_reading_time":108.09,
        "Solution_score_count":5.0,
        "Solution_sentence_count":81.0,
        "Solution_word_count":1171.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":219.2780555556,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1659686697000,
        "Challenge_created_time":1658897296000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Challenge_link_count":1,
        "Challenge_readability":19.7,
        "Challenge_reading_time":33.11,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":219.2780555556,
        "Challenge_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":223,
        "Platform":"Github",
        "Solution_body":"Further context - this only started happening approx. 32 hours ago. If I had to guess... maybe it should have never worked and something just happened to get 'fixed' in the IAM API yesterday? \ud83d\ude01  Hi @tdmalone is this still an issue? Hi @kcadette, it is, yes:\r\n\r\n```\r\nUser: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:xxxxxxxxxxxx:notebook-instance\/basicnotebookinstance-lqerepcrnmaw because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: 4f72193d-aa17-41b9-8ed0-ee381686cb5b; Proxy: null)}\r\n```\r\n\r\nIt should be a one-line fix, so I've submitted a PR: https:\/\/github.com\/awslabs\/service-workbench-on-aws\/pull\/1021",
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":11.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":529.4461111111,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460282000,
        "Challenge_created_time":1631554276000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":529.4461111111,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":4421.2883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923114000,
        "Challenge_created_time":1628006476000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":4421.2883333333,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":164,
        "Platform":"Github",
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_link_count":2.0,
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":431.3538888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen connecting to Sagemaker workspaces, there is an intermittent issue where a blank browser launches instead of Sagemaker.  The issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nSTEP 1: Login as an Admin \r\nSTEP 2: Create a workspace (SageMaker)\r\nSTEP 3: Start the Workspace \r\nSTEP 5: Click \"Connect\"\r\nSTEP 6: A new blank web browser tab opens \r\nSTEP 7: Click \"Connect\" again, another blank web browser tab opens\r\n\r\nUser receives a \"Something Went Wrong\" general error in SWB at Step 6\r\n\r\nIn the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"Error from cloudfront\"\r\n   \r\n\r\n**Expected behavior**\r\nSagemaker workspace launch in browser\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed 3.0.0\r\n\r\n**Additional context**\r\nUser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nThe issue is experienced approximately once a week. Sometimes clearing cache solves the issue. Other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Challenge_closed_time":1624287519000,
        "Challenge_created_time":1622734645000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/509",
        "Challenge_link_count":0,
        "Challenge_readability":11.2,
        "Challenge_reading_time":16.66,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":431.3538888889,
        "Challenge_title":"[Bug] Blank Page on Sagemaker Workspace Connect",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":210,
        "Platform":"Github",
        "Solution_body":"The relevant code snippet for this issue is [here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/7988c933d5f4d6c9878249a7521d64d891707824\/addons\/addon-base-raas-ui\/packages\/base-raas-ui\/src\/parts\/environments-sc\/parts\/ScEnvironmentHttpConnections.js#L60).\r\n\r\nI was unable to reproduce this issue (I'm using Chrome 90.0.4430.212) , but let's try to track this issue down.\r\n\r\nAre we unable to get the Sagemaker URL or is the browser failing to refer the user to the Sagemaker url?\r\n\r\nTo check if the browser is getting the Sagemaker URL correctly:\r\n1. Go to the Sagemaker workspace and click the connection button to open the \"HTTP Connections\" card \r\n2. Open the network tab in Chrome Inspect tool\r\n3. Clear all of the network activity\r\n4. Click on the `url` row on the sidebar\r\n5. In the new tab that opens up, select the sub tab for `response`.\r\n6. Check if a `url` is provided and try navigating to that `url` in the browser.\r\n\r\nIf the URL is valid and you can can navigate to the Sagemaker instance that way, then the issue is with the browser failing to redirect the user.\r\n\r\nhttps:\/\/user-images.githubusercontent.com\/3661906\/120824482-84a88d80-c526-11eb-883b-13d9b4cfa7f4.mov\r\nHere's a video of the process.\r\n\r\nPlease follow the instructions above to help us debug what is the cause of the issue. ",
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":16.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":183.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Calling `stop_training_job` from the SageMaker client against an existing by not \"InProgress\" job, causes the client to hang. This only seems to happen within the sm-executor though. \r\n\r\nHere's the output calling the method from the python interpreter within the pod:\r\n```\r\n# .\/python\r\nPython 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import boto3\r\n>>> client = boto3.client(\"sagemaker\")\r\n>>> client.stop_training_job(TrainingJobName=\"98cb7232-02b1-4a1b-a59e-55a8eca9e048\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the StopTrainingJob operation: The request was rejected because the training job is in status Stopped.\r\n>>>\r\n```\r\n\r\nHere is the DEBUG output from the sm-executor\r\n```\r\n2019-10-09 06:39:38,252 INFO: Attempting to stop training job 98cb7232-02b1-4a1b-a59e-55a8eca9e048\r\n2019-10-09 06:39:38,252 DEBUG: Event before-parameter-build.sagemaker.StopTrainingJob: calling handler <function generate_idempotent_uuid at 0x7f54d82671e0>\r\n2019-10-09 06:39:38,253 DEBUG: Event before-call.sagemaker.StopTrainingJob: calling handler <function inject_api_version_header_if_needed at 0x7f54d8268b70>\r\n2019-10-09 06:39:38,253 DEBUG: Making request for OperationModel(name=StopTrainingJob) with params: {'url_path': '\/', 'query_string': '', 'method': 'POST', 'headers': {'X-Amz-Target': 'SageMaker.StopTrainingJob', 'Content-Type': 'application\/x-amz-json-1.1', 'User-Agent': 'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221'}, 'body': b'{\"TrainingJobName\": \"98cb7232-02b1-4a1b-a59e-55a8eca9e048\"}', 'url': 'https:\/\/api.sagemaker.us-east-1.amazonaws.com\/', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x7f54bacde518>, 'has_streaming_input': False, 'auth_type': None}}\r\n2019-10-09 06:39:38,253 DEBUG: Event request-created.sagemaker.StopTrainingJob: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x7f54bacde4e0>>\r\n2019-10-09 06:39:38,254 DEBUG: Event choose-signer.sagemaker.StopTrainingJob: calling handler <function set_operation_specific_signer at 0x7f54d82670d0>\r\n2019-10-09 06:39:38,254 DEBUG: Calculating signature using v4 auth.\r\n2019-10-09 06:39:38,254 DEBUG: CanonicalRequest:\r\nPOST\r\n\/\r\n\r\ncontent-type:application\/x-amz-json-1.1\r\nhost:api.sagemaker.us-east-1.amazonaws.com\r\nx-amz-date:20191009T063938Z\r\nx-amz-security-token:FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF\r\nx-amz-target:SageMaker.StopTrainingJob\r\n\r\ncontent-type;host;x-amz-date;x-amz-security-token;x-amz-target\r\n84e242897f2f826cc224094427e7ba8bc4c2f559097741460b59e162e8114c40\r\n2019-10-09 06:39:38,254 DEBUG: StringToSign:\r\nAWS4-HMAC-SHA256\r\n20191009T063938Z\r\n20191009\/us-east-1\/sagemaker\/aws4_request\r\n4320231908e4cd91204a6044a6201b1a74c0a63a2f708f9c4c27df2d6a6344db\r\n2019-10-09 06:39:38,255 DEBUG: Signature:\r\nc074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae\r\n2019-10-09 06:39:38,255 DEBUG: Sending http request: <AWSPreparedRequest stream_output=False, method=POST, url=https:\/\/api.sagemaker.us-east-1.amazonaws.com\/, headers={'X-Amz-Target': b'SageMaker.StopTrainingJob', 'Content-Type': b'application\/x-amz-json-1.1', 'User-Agent': b'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221', 'X-Amz-Date': b'20191009T063938Z', 'X-Amz-Security-Token': b'FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF', 'Authorization': b'AWS4-HMAC-SHA256 Credential=ASIAYNI7SS57NFEDAZHQ\/20191009\/us-east-1\/sagemaker\/aws4_request, SignedHeaders=content-type;host;x-amz-date;x-amz-security-token;x-amz-target, Signature=c074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae', 'Content-Length': '59'}>\r\n2019-10-09 06:39:38,320 DEBUG: Response headers: {'x-amzn-RequestId': '03dd9de3-3672-4f4b-b575-a06d29e15e6b', 'Content-Type': 'application\/x-amz-json-1.1', 'Content-Length': '116', 'Date': 'Wed, 09 Oct 2019 06:39:37 GMT', 'Connection': 'close'}\r\n2019-10-09 06:39:38,320 DEBUG: Response body:\r\nb'{\"__type\":\"ValidationException\",\"message\":\"The request was rejected because the training job is in status Stopped.\"}'\r\n2019-10-09 06:39:38,320 DEBUG: Event needs-retry.sagemaker.StopTrainingJob: calling handler <botocore.retryhandler.RetryHandler object at 0x7f54bacde898>\r\n2019-10-09 06:39:38,321 DEBUG: No retry needed.\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1570606746000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/928",
        "Challenge_link_count":2,
        "Challenge_readability":21.2,
        "Challenge_reading_time":75.68,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":1054.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"[SM-Executor] SageMaker.stop_training_job hangs",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":346,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0132827324,
        "Challenge_watch_issue_ratio":0.0075901328
    },
    {
        "Challenge_adjusted_solved_time":326.7363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Challenge_closed_time":1571326528000,
        "Challenge_created_time":1570150277000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Challenge_link_count":3,
        "Challenge_readability":12.2,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":1054.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":326.7363888889,
        "Challenge_title":"Cannot run benchmark for sagemaker",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"3 tactics were used to address this issue (by @perdasilva): \r\nStop gap, watcher, error reporting in the status.\r\n@haohanchen-yagao - Please confirm and the close this issue.",
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0132827324,
        "Challenge_watch_issue_ratio":0.0075901328
    },
    {
        "Challenge_adjusted_solved_time":457.1169444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1621931178000,
        "Challenge_created_time":1620285557000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Challenge_link_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":25.0,
        "Challenge_repo_issue_count":1008.0,
        "Challenge_repo_star_count":130.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.1169444444,
        "Challenge_title":"The data path inside sagemaker notebook does not work",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":85,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0099206349,
        "Challenge_watch_issue_ratio":0.0178571429
    },
    {
        "Challenge_adjusted_solved_time":18.7375,
        "Challenge_answer_count":0,
        "Challenge_body":"Invoke Endpoint response time out. \r\n\r\n### Reproduction Steps\r\n\r\n{\r\n  \"trainingJob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instanceType\": \"ml.c5.9xlarge\",\r\n    \"timeoutInSeconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### Error Log\r\nIn Inference Lambda CloudWatch:\r\n\r\nTask timed out after 120.10 seconds\r\n\r\n\r\nIn Sagemaker Training CloudWatch:\r\n\r\n2021-04-09   04:53:46,902 [INFO ] main org.pytorch.serve.ModelServer - Loading initial   models: model.mar\r\n--\r\n2021-04-09 04:53:49,837 [INFO ] main   org.pytorch.serve.archive.ModelArchive - eTag   8ff2b3de4bed4fb1bc7fe969652117ff\r\n2021-04-09 04:53:49,847 [INFO ] main   org.pytorch.serve.wlm.ModelManager - Model model loaded.\r\n2021-04-09 04:53:49,865 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Inference server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Metrics server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,931 [INFO ] main   org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\r\nModel server started.\r\n2021-04-09 04:53:49,957 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on   port: \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker   started.\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime:   3.6.13\r\n2021-04-09 04:53:49,963 [INFO ]   W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to:   \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,972 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection   accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   CPUUtilization.Percent:33.3\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskAvailable.Gigabytes:19.622234344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUsage.Gigabytes:4.731609344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUtilization.Percent:19.4\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryAvailable.Megabytes:30089.12109375\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUsed.Megabytes:902.6953125\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUtilization.Percent:4.1\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Setting the   default backend to \"pytorch\". You can change it in the   ~\/.dgl\/config.json file or export the DGLBACKEND environment variable.\u00a0 Valid options are: pytorch, mxnet,   tensorflow (all lowercase)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   ------------------ Loading model -------------------\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker   process died.\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most   recent call last):\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 176, in <module>\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 worker.run_server()\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 148, in run_server\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self.handle_connection(cl_socket)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 112, in handle_connection\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service, result, code =   self.load_model(msg)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 85, in load_model\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service = model_loader.load(model_name,   model_dir, handler, gpu, batch_size)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py\", line   117, in load\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   model_service.initialize(service.context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/home\/model-server\/tmp\/models\/8ff2b3de4bed4fb1bc7fe969652117ff\/handler_service.py\",   line 51, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 super().initialize(context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py\",   line 66, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self._service.validate_and_initialize(model_dir=model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\",   line 158, in validate_and_initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self._model = self._model_fn(model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/ml\/model\/code\/fd_sl_deployment_entry_point.py\", line 149, in   model_fn\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 rgcn_model.load_state_dict(stat_dict)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\",   line 1045, in load_state_dict\r\n2021-04-09   04:53:51,251 [INFO ] W-9000-model_1-stdout   org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self.__class__.__name__, \"     \\t\".join(error_msgs)))\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError:   Error(s) in loading state_dict for HeteroRGCN:\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.weight: copying a   param with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.bias: copying a   param with shape torch.Size([2]) from checkpoint, the shape in current model   is torch.Size([16]).\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** <!-- Output of `cdk version` -->\r\n  - **Framework Version:**\r\n  - **Node.js Version:** <!-- Version of Node.js (run the command `node -v`) -->\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\nCause of this bug:\r\n\r\nBackend worker process died.\r\nSagemaker Endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1618279737000,
        "Challenge_created_time":1618212282000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/57",
        "Challenge_link_count":2,
        "Challenge_readability":17.5,
        "Challenge_reading_time":124.73,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":25.0,
        "Challenge_repo_issue_count":1008.0,
        "Challenge_repo_star_count":130.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":107,
        "Challenge_solved_time":18.7375,
        "Challenge_title":"sagemaker endpoint fail to deploy or time out server error(0) bug",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":650,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0099206349,
        "Challenge_watch_issue_ratio":0.0178571429
    },
    {
        "Challenge_adjusted_solved_time":142.7236111111,
        "Challenge_answer_count":7,
        "Challenge_body":"### System Info\n\n```Shell\npytorch: 1.10.2\r\npython:3.8\n```\n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] One of the scripts in the examples\/ folder of Accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSagemaker Multi-GPU distributed data training, while \"model.generate\" it always returns empty tensors.\n\n### Expected behavior\n\n```Shell\nI'm trying to run a distributed training in a Sagemaker training job, the inference is not working properly, I found it as a future work on huggingface documentation so I'm wondering If that's why it's not working yet on sagemaker Multi-GPU.\r\n\r\nThanks\n```\n",
        "Challenge_closed_time":1664255368000,
        "Challenge_created_time":1663741563000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/accelerate\/issues\/706",
        "Challenge_link_count":0,
        "Challenge_readability":13.6,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":76.0,
        "Challenge_repo_fork_count":280.0,
        "Challenge_repo_issue_count":921.0,
        "Challenge_repo_star_count":3349.0,
        "Challenge_repo_watch_count":64.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":142.7236111111,
        "Challenge_title":"Have accelerate for  Distributed Training: Data Parallelism feature working on AWS Sagemaker yet?",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":"Hello @HebaGamalElDin, please provide minimal reproducible example for us to deep dive and help you.  Hello @pacman100, I'm fine tuning a transformer model from the hub of huggingface.. below is the training function that utilizes the accelerator on sagemaker training jobs.\r\n\r\n```\r\ndef train(context: Context, num_epochs):\r\n    model = context.model\r\n    model = accelerator.prepare(model)\r\n    optimizer = AdamW(model.parameters(), lr=1e-3)\r\n    \r\n    num_training_steps = num_epochs * len(context.train_dataloader)\r\n    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\r\n    optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(optimizer, context.train_dataloader, context.val_dataloader, lr_scheduler)\r\n    \r\n    \r\n    losses = []\r\n    min_cer = 1.0\r\n    min_train_loss = 1.0\r\n    for epoch in range(num_epochs):\r\n        model.train()\r\n        for j, batch in enumerate(train_dataloader):\r\n            inputs: torch.Tensor = batch[\"input\"]#.to(accelerator.device)\r\n            labels: torch.Tensor = batch[\"label_tensor\"]#.to(accelerator.device)\r\n\r\n            outputs = model(pixel_values=inputs, labels=labels)\r\n            #print(outputs)\r\n            loss = outputs.loss\r\n            accelerator.backward(loss)\r\n            #loss.backward()\r\n\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n            losses.append(loss)\r\n            accelerator.print(f\"Epoch {epoch}-------Batch---{j}-----Loss---{loss}\")\r\n            \r\n        model.eval()\r\n        for i, batch in enumerate(eval_dataloader):\r\n            inputs: torch.Tensor = batch[\"input\"]#.to(accelerator.device)\r\n            with torch.no_grad():\r\n                predictions = accelerator.unwrap_model(model).generate(inputs)\r\n\r\n                generated_ids = accelerator.gather(predictions).cpu().numpy()\r\n                print(f\"Generated IDs: {generated_ids}\")\r\n                labels = accelerator.gather(batch[\"label_tensor\"]).cpu().numpy()\r\n                \r\n                generated_text = context.processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n                labels_text = context.processor.batch_decode(labels, skip_special_tokens=True)\r\n                \r\n                predictions, labels = postprocess_text(generated_text, labels_text)\r\n                \r\n                cer_metric.add_batch(predictions=predictions, references=labels)\r\n                wer_metric.add_batch(predictions=predictions, references=labels)\r\n                print(f\"Predictions: {predictions}-----------Labels: {labels}\")\r\n        cer = cer_metric.compute()\r\n        wer = wer_metric.compute()\r\n\r\n        accelerator.print(f\"Average CER: {cer}------ Average WER: {wer}\")\r\n```\r\n\r\nthe python estimator is as follows:\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nimport sagemaker\r\nrole = sagemaker.get_execution_role()\r\npt_estimator = PyTorch(\r\n    base_job_name=\"transformer-ocr-training\",\r\n    source_dir=\"source\",\r\n    entry_point=\"Train.py\",\r\n    role=role,    \r\n    py_version=\"py38\",\r\n    \r\n    image_uri =\"763104351884.dkr.ecr.us-east-1.amazonaws.com\/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\r\n\r\n    #framework_version=\"1.12.0\",\r\n\r\n    instance_count=1,\r\n    instance_type=\"ml.p3.16xlarge\"\r\n    #distribution={'smdistributed':{'dataparallel':{ 'enabled': True }}}\r\n)\r\n\r\npt_estimator.fit(\"s3:\/\/handwritten-ocr-training\")\r\n```\r\n\r\nExactly when I'm generate in for the evaluation set it always retrieves empty tensors. What am I missing here? However the number of processes is 8 GPUs so the accelerate has access to all of them however it's not generating in the validation all decoded strings are empty, appreciate your help! Hello @HebaGamalElDin, you are not using the \ud83e\udd17 Accelerate integration of AWS SageMaker correctly. To help you and others going forwards, I have spent time creating this repo https:\/\/github.com\/pacman100\/accelerate-aws-sagemaker which details on how to correctly use AWS SageMaker with \ud83e\udd17 Accelerate. it works correctly with generation `model.generate`. Please go through the README and files in the above repo and let us know if you still have issues.  Hello @pacman100 .. Thank you for the warm help.\r\nI have one question please, what I didn't get is how to configure accelerate inside the training job?\r\nmeaning where to run the command `accelerate config --config_file accelerate_config.yaml`? Have the [accelerate_config.yaml](https:\/\/github.com\/pacman100\/accelerate-aws-sagemaker\/blob\/af5caadcea0fa8186c11a784b6f86591c8fa5b3f\/src\/seq2seq\/accelerate_config.yaml) file should been replaced the python SDK estimator *PyTorch* in my case? Hello, you don't have to use any SageMaker estimator (PyTorch estimator in your case) as Accelerate internally uses Hugging Face SageMaker Estimator https:\/\/github.com\/huggingface\/accelerate\/blob\/main\/src\/accelerate\/commands\/launch.py#L776 along with all the necessary env variables to handle SageMaker DDP.\r\n\r\nJust create the accelerate config with command `accelerate config` on any virtual machine\/local machine\/sagemaker notebooks on which you have aws cli installed with aws credentials setup. After that when you run `accelerate launch` it will internally use HF estimator to create the training job on AWS SageMaker. I am running `accelerate config` and `accelerate launch` on a local machine with aws credentials setup.\r\n\r\n\r\n\r\n @pacman100 Okay I got that thank you.\r\nOne more question please, I'm encountering an issue when I'm testing, most of validation batches entirely are empty while some others are okay, this problem doesn't happen while training is on 1 GPU, What could be the problem here please?!\r\n**HINT: I'm logging the length of the text predictions coming by model.generate() for each batch, the majority is zero as shown in the below screenshot.**\r\n![image](https:\/\/user-images.githubusercontent.com\/36745656\/192077881-b5a598d0-1762-4335-9f2b-f07daa627318.png)\r\n",
        "Solution_link_count":4.0,
        "Solution_readability":16.5,
        "Solution_reading_time":70.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":54.0,
        "Solution_word_count":517.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0825190011,
        "Challenge_watch_issue_ratio":0.0694896851
    },
    {
        "Challenge_adjusted_solved_time":2.8202777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1666960895000,
        "Challenge_created_time":1666950742000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Challenge_link_count":0,
        "Challenge_readability":6.6,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.8202777778,
        "Challenge_title":"Sagemaker example DAG to use aws session token",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":120,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":18.1363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nXCom return value of `SageMakerTransformOperatorAsync`  and `SageMakerTrainingOperatorAsync` does not produce the expected output.\r\n\r\nIt seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a dag with traditional operators\r\n2. Run same dag with Async operators\r\n3. Compare outputs\r\n\r\n**Expected behavior**\r\nThe Xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n",
        "Challenge_closed_time":1666957435000,
        "Challenge_created_time":1666892144000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/736",
        "Challenge_link_count":0,
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.76,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.1363888889,
        "Challenge_title":"XCom Output of Sagemaker Async Operators",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":84,
        "Platform":"Github",
        "Solution_body":"@bharanidharan14  I tested locally with the branch for the fix and seems to be fixed with your patch. Thank you!",
        "Solution_link_count":0.0,
        "Solution_readability":7.6,
        "Solution_reading_time":1.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":40.4833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Challenge_closed_time":1666859588000,
        "Challenge_created_time":1666713848000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Challenge_link_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":40.4833333333,
        "Challenge_title":"Token error with Sagemaker Async Operators",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":127,
        "Platform":"Github",
        "Solution_body":"The issue is with the session token is not considered while the secrete and access key is given in the connection proper field, not in the extra config",
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Can you please confirm if Sagemaker Debugger works with HPO. I get errors when the code that works perfectly fine with SM script mode fails when extended to HPO.\r\n\r\n` FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/config\/debughookconfig.json'`",
        "Challenge_closed_time":null,
        "Challenge_created_time":1597167105000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/sagemaker-debugger\/issues\/325",
        "Challenge_link_count":0,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.69,
        "Challenge_repo_contributor_count":36.0,
        "Challenge_repo_fork_count":80.0,
        "Challenge_repo_issue_count":628.0,
        "Challenge_repo_star_count":139.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Debugger with HPO",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":41,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0573248408,
        "Challenge_watch_issue_ratio":0.0414012739
    },
    {
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_created_time":1609843354000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10230.7861111111,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":67,
        "Platform":"Github",
        "Solution_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Solution_link_count":0.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":933.8588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nHello,\r\n\r\nThis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nWe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nIt helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nUnexpectedStatusException: Error for Training job huggingface-pytorch-training-2022-06-29-04-04-58-606: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\r\nExitCode 1\r\nErrorMessage \":RuntimeError: Tensors must have same number of dimensions: got 4 and 3\r\n :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[41154,1],0]   Exit code:    1\"\r\nCommand \"mpirun --host algo-1:8 \r\n\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.\n\n### Who can help?\n\n[SageMakerAprilTraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/SageMakerAprilTraining.zip)\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning this attached file with the training python file\n\n### Expected behavior\n\nI have shared the notebook and the error raised in it for clarification",
        "Challenge_closed_time":1660575729000,
        "Challenge_created_time":1657213837000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18060",
        "Challenge_link_count":2,
        "Challenge_readability":14.3,
        "Challenge_reading_time":36.15,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":933.8588888889,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training #16890",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":355,
        "Platform":"Github",
        "Solution_body":"@omid0001 @kanwari3, \r\n\r\nWould it be possible for you to reproduce this issue (`not enough values to unpack`) without using SageMaker, i.e. just with a Python script?\r\n```bash\r\n[1,0]: bsz, seq_len = input_ids_shape[:2]\r\n[1,0]:ValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\nIt would be a good idea to verify what data is received by the model first. Usually the batches in data (`input_ids`) should be already of the format `(batch_size, sequence_length)`, and if you see the above error, it is likely the data or its processing pipeline has some issues. Using `torch.unsqueeze` is not really a good idea, as it implies you have only `batch_size` being 1.\r\n\r\nMy suggestion:\r\n- Try to run your training without SageMaker (and without the using the fix `torch.unsqueeze`)\r\n- Check what is received by the model, and check in the data pipeline if it prepares the correct input format\r\n  - If you still get the issue and can't figure it out:\r\n    - I could try to help if you could provide the training script + data processing script + a tiny portion of your data    \r\n  - If the issue only occurs when you wrap the training in SageMaker, I don't have the competence to help in this case, sorry. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_link_count":1.0,
        "Solution_readability":10.7,
        "Solution_reading_time":18.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":243.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":117.2208333333,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\ntransformer: 4.17.0\r\ntorch: 1.10.2\r\n\r\nPlatform: Sagemaker Deep Learning Container\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@NielsRogge\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nThe error only comes when training on Sagemaker using Huggingface.\r\n\r\nScripts to start training on Sagemaker:\r\n\r\nFolder organization:\r\n```\r\n.\/\r\n----sg_training.py\r\n----scripts\r\n-------requirements.txt\r\n-------train.py \r\n```\r\n\r\nsg_training.py:\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.huggingface import HuggingFace\r\n\r\nif __name__ == \"__main__\":\r\n    iam_client = boto3.client(...)\r\n\r\n    role = iam_client.get_role(...)['Role']['Arn']\r\n    sess = sagemaker.Session()\r\n\r\n    sagemaker_session_bucket = 's3-sagemaker-session'\r\n\r\n    hyperparameters = {'epochs': 20,\r\n                       'train_batch_size': 1,\r\n                       'model_name': \"microsoft\/layoutxlm-base\",\r\n                       'output_dir': '\/opt\/ml\/model\/',\r\n                       'checkpoints': '\/opt\/ml\/checkpoints\/',\r\n                       'combine_train_val': True,\r\n                       'exp_tracker': \"all\",\r\n                       'exp_name': 'Sagemaker Training'\r\n                       }\r\n\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location')\r\n\r\n    huggingface_estimator.fit()\r\n```\r\n\r\nEntrypoint scripts folder:\r\n\r\n\r\nrequirements.txt:\r\n```\r\ngit+https:\/\/github.com\/facebookresearch\/detectron2.git\r\n```\r\n\r\ntrain.py:\r\n```\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n\r\n\r\ndef run():\r\n    model = LayoutLMv2ForSequenceClassification.from_pretrained('microsoft\/layoutxlm-base',\r\n                                                                num_labels=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--epochs\", type=int, default=3)\r\n    parser.add_argument(\"--exp_name\", type=str, default=\"Sagemaker Training\")\r\n    parser.add_argument(\"--train-batch-size\", type=int, default=2)\r\n    parser.add_argument(\"--eval-batch-size\", type=int, default=1)\r\n    parser.add_argument(\"--warmup_steps\", type=int, default=500)\r\n    parser.add_argument(\"--model_name\", type=str)\r\n    parser.add_argument(\"--learning_rate\", type=str, default=1e-5)\r\n    parser.add_argument(\"--combine_train_val\", type=bool, default=False)\r\n    # Data, model, and output directories\r\n    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\r\n    parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\")\r\n    parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model')\r\n    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\r\n    args, _ = parser.parse_known_args()\r\n\r\n    logger = logging.getLogger(__name__)\r\n    logging.basicConfig(\r\n        level=logging.getLevelName(\"INFO\"),\r\n        handlers=[logging.StreamHandler(sys.stdout)],\r\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    )\r\n\r\n    run()\r\n\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n```shell\r\nHere the log on the error from AWS Cloud Watch:\r\n\r\nInvoking script with the following command:\r\n\/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val True --epochs 20 --exp_name Sagemaker_Training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in <module>\r\nfrom detectron2.modeling import META_ARCH_REGISTRY\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in <module>\r\nfrom detectron2.layers import ShapeSpec\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in <module>\r\nfrom .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm, CycleBatchNormList\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in <module>\r\n    from fvcore.nn.distributed import differentiable_all_reduce\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in <module>\r\n    from .focal_loss import (\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in <module>\r\n    sigmoid_focal_loss_jit: \"torch.jit.ScriptModule\" = torch.jit.script(sigmoid_focal_loss)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn\r\nreturn torch.jit.script(fn, _rcb=rcb)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\nRuntimeError: \r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 6, in <module>\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__\r\nvalue = getattr(module, name)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__\r\nmodule = self._get_module(self._class_to_module[name])\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module\r\nraise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):\r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\n\r\n```\r\n```\r\n",
        "Challenge_closed_time":1656429784000,
        "Challenge_created_time":1656007789000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17855",
        "Challenge_link_count":1,
        "Challenge_readability":16.7,
        "Challenge_reading_time":107.62,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":117.2208333333,
        "Challenge_title":"LayoutLMv2 training on sagemaker error: undefined value has_torch_function_variadic",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":580,
        "Platform":"Github",
        "Solution_body":"cc @philschmid (hope I am tagging correctly) @Natlem could you try adding `debugger_hook_config=False` to the `HuggingFace` estimator? \r\n\r\n```python\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location',\r\n                                        debugger_hook_config=False,\r\n)\r\n``` Hi @philschmid ,\r\n\r\nAdded the `debugger_hook_config=False`, the error is gone now. Thanks ! Awesome, closing the issue.  Feel free to reopen if you have more issues. @Natlem i forwarded the error to the AWS team to be able use the debugger soon.  @philschmid  Thanks ! @philschmid do you have any idea why this solves the problem? Is it documented by AWS anywhere?\r\n\r\nSagemaker Debugger has cost me multiple days of time in the mysterious problems it produces. Far more than anything else on Sagemaker. I posted an issue on awslabs about this awhile back and never got a reply. I would really like to know what is going on here\r\n\r\n**For anyone encountering this while using a HyperparameterTuner**\r\nPassing `debugger_hook_config=False` in the `Estimator` will not the solve the problem. Further, passing `environment={'USE_SMDEBUG':0}` also will not solve the problem. Somehow these settings never make it to a tuner's constituent training jobs.\r\n\r\nThe only way to solve it is to set `ENV USE_SMDEBUG=\"0\"` in the docker container that will be running the constituent training jobs. > Somehow these settings never make it to a tuner's constituent training jobs.\r\n\r\nAre you using the `HuggingFace` estimator or the `HyperparameterTuner`",
        "Solution_link_count":0.0,
        "Solution_readability":10.2,
        "Solution_reading_time":22.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":223.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":763.1580555556,
        "Challenge_answer_count":3,
        "Challenge_body":"### System Info\n\n```shell\nThis was verified today on a fresh SageMaker Studio instance running in us-west-2.\r\n\r\nIt's not a Transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on SageMaker Studio at some point.\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1) Open an SM Studio notebook\r\n\r\n2) Run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nThe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### Expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Challenge_closed_time":1654502136000,
        "Challenge_created_time":1651754767000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17096",
        "Challenge_link_count":0,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":763.1580555556,
        "Challenge_title":"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"Thanks for the issue @juliensimon, this should be fixed by https:\/\/github.com\/huggingface\/transformers\/pull\/17049. It will be in the next release which should drop early next week. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. Should be fixed now!",
        "Solution_link_count":2.0,
        "Solution_readability":8.5,
        "Solution_reading_time":6.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":913.5883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"### System Info\n\n```shell\nusing sagemaker \r\nmpi_options = {\r\n    \"enabled\" : True,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":True,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": True,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':HHousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = HuggingFace(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### Who can help?\n\n@ydshieh @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Create huggingface estimator\r\n2.     training_args = Seq2SeqTrainingArguments(\r\n        predict_with_generate=True,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=True,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nError I get:\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:ValueError: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[41156,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### Expected behavior\n\n```shell\nTraining on a sagemaker notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\nI can successfully train the LED model with my training data. Trying to get it to work with Huggingface estimator and sagemaker SMP I would assume the same outcome.\n```\n",
        "Challenge_closed_time":1653922922000,
        "Challenge_created_time":1650634004000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/16890",
        "Challenge_link_count":0,
        "Challenge_readability":18.2,
        "Challenge_reading_time":50.05,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":913.5883333333,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":296,
        "Platform":"Github",
        "Solution_body":"cc @philschmid  I would also suggest @kanwari3 to\r\n- try to use the same Python\/PyTorch\/transformers versions (and other libraries) on SageMaker that work locally (if possible)\r\n- if the above doesn't work, try to use on local machine the same versions as those used on SageMaker, and see if you still get errors\r\n\r\nSo we have a better idea about if this is indeed a SageMaker issue or libraries issue This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. cc @philschmid  , cc @ydshieh ,  cc @sgugger \r\nHi, \r\n\r\nThis is a follow up on this post with the same title. We are trying to fix the issue and are still getting the same error after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. We tried unsqueezing the input_ids and attention_masks but it didn\u2019t fix the error.\r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\n    return {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.",
        "Solution_link_count":1.0,
        "Solution_readability":10.8,
        "Solution_reading_time":20.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":1,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_created_time":1645497684000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":650.0,
        "Challenge_repo_issue_count":1974.0,
        "Challenge_repo_star_count":882.0,
        "Challenge_repo_watch_count":2756.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Solution_link_count":1.0,
        "Solution_readability":13.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0683890578,
        "Challenge_watch_issue_ratio":1.3961499493
    },
    {
        "Challenge_adjusted_solved_time":4.3669444444,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nSteps:\r\n1. Create a new AzureML workspace.\r\n    - Name: `azureml-test-workspace`\r\n    - Resource group: `recommenders_project_resources`\r\n    - Location: *Make sure you have enough quota in the location you choose*\r\n2. Create two new clusters: `cpu-cluster` and `gpu-cluster`. Go to compute, then compute cluster, then new.\r\n    - Select the CPU VM base. Anything above 32GB of RAM, and 8 cores should be fine.\r\n    - Select the GPU VM base. Anything above 56GB of RAM, and 6 cores, and an NVIDIA K80 should be fine.\r\n3. Add the subscription ID to GitHub action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). Create a new repository secret called `AZUREML_TEST_SUBID` and add the subscription ID as the value.\r\n4. Make sure you have installed [Azure CLI](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`.\r\n5. Select your subscription: `az account set -s $AZURE_SUBSCRIPTION_ID`.\r\n5. Create a Service Principal: `az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID --sdk-auth`.\r\n6. Add the output from the Service Principal (should be a JSON blob) as an action secret `AZUREML_TEST_CREDENTIALS`.\r\n\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1669646142000,
        "Challenge_created_time":1669630421000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1862",
        "Challenge_link_count":2,
        "Challenge_readability":8.1,
        "Challenge_reading_time":25.09,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":4.3669444444,
        "Challenge_title":"[BUG] Update test documentation to connect AzureML with GitHub actions",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":254,
        "Platform":"Github",
        "Solution_body":"Error:\r\n```\r\n$ az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID. --sdk-auth\r\nThis command or command group has been migrated to Microsoft Graph API. Please carefully review all breaking changes introduced during this migration: https:\/\/docs.microsoft.com\/cli\/azure\/microsoft-graph-migration\r\nOption '--sdk-auth' has been deprecated and will be removed in a future release.\r\nAADSTS530003: Your device is required to be managed to access this resource.\r\nTrace ID: XXXXXXXXXXXXXXXXXXXXXXX\r\nCorrelation ID: XXXXXXXXXXXXXXXXXXXXXXX\r\nTimestamp: 2022-11-28 10:02:57Z\r\nTo re-authenticate, please run:\r\naz login --scope https:\/\/graph.microsoft.com\/\/.default\r\n```\r\n",
        "Solution_link_count":2.0,
        "Solution_readability":11.8,
        "Solution_reading_time":9.09,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":77.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nAzureML tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3485981939\/jobs\/5832009213\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\nWe want to send back a signal to GitHub so if the tests fail, the badge is red and we are notified\r\n\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1668674781000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1852",
        "Challenge_link_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":13.88,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] AzureML test process is not failing if there is an error in the tests",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":2.4247222222,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n                EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n                BATCH_SIZE_SYNTHETIC=1024,\r\n                BATCH_SIZE_CRITEO=1024,\r\n                RANDOM_SEED=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\nE           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\nE             comparison failed\r\nE             Obtained: 0.5131\r\nE             Expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668600473000,
        "Challenge_created_time":1668591744000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1848",
        "Challenge_link_count":1,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.18,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.4247222222,
        "Challenge_title":"[BUG] xdeepfm error in AzureML test",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":162,
        "Platform":"Github",
        "Solution_body":"```\r\n pytest tests\/integration\/examples\/test_notebooks_gpu.py::test_xdeepfm_integration --disable-warnings --durations 0\r\n```\r\nwith \r\n```\r\n@pytest.mark.gpu\r\n@pytest.mark.notebooks\r\n@pytest.mark.integration\r\n@pytest.mark.parametrize(\r\n    \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n    [\r\n        (\r\n            15,\r\n            10,\r\n            {\r\n                \"res_syn\": {\"auc\": 0.9716, \"logloss\": 0.699},\r\n                \"res_real\": {\"auc\": 0.749, \"logloss\": 0.4926},\r\n            },\r\n            42,\r\n        )\r\n    ],\r\n)\r\ndef test_xdeepfm_integration(\r\n    notebooks,\r\n    output_notebook,\r\n    kernel_name,\r\n    syn_epochs,\r\n    criteo_epochs,\r\n    expected_values,\r\n    seed,\r\n):\r\n    notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n    pm.execute_notebook(\r\n        notebook_path,\r\n        output_notebook,\r\n        kernel_name=kernel_name,\r\n        parameters=dict(\r\n            EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n            EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n            BATCH_SIZE_SYNTHETIC=1024,\r\n            BATCH_SIZE_CRITEO=1024,\r\n            RANDOM_SEED=seed,\r\n        ),\r\n    )\r\n    results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n        \"data\"\r\n    ]\r\n\r\n    for key, value in expected_values.items():\r\n        assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\n        assert results[key][\"logloss\"] == pytest.approx(\r\n            value[\"logloss\"], rel=TOL, abs=ABS_TOL\r\n        )\r\n```",
        "Solution_link_count":0.0,
        "Solution_readability":19.8,
        "Solution_reading_time":15.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":139.4886111111,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668591607000,
        "Challenge_created_time":1668089448000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Challenge_link_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":139.4886111111,
        "Challenge_title":"[BUG] Error in some of the AzureML tests",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"@pradnyeshjoshi any thoughts for this error?",
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nRuntime of [tests\/integration\/examples\/test_notebooks_gpu.py::test_sasrec_quickstart_integration](https:\/\/github.com\/microsoft\/recommenders\/blob\/6987858116d21699f6d92661f03c1529383c7d88\/tests\/integration\/examples\/test_notebooks_gpu.py#L679) varies a lot on the following platforms:\r\n- As part of ADO pipeline, it takes ~562 sec to complete.\r\n- When run as an experiment on AzureML compute cluster triggered using a [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml), it takes ~7080 sec.\r\n\r\nWe need to investigate why this happens.\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nBoth the machines are of same type (NC6s_V2), and use the same CUDA and CuDNN versions:\r\n`cudatoolkit=11.2`\r\n`cudnn=8.1`\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nTrigger the [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml) manually and take a look at pytest logs in the dashboard to see the execution times.\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1652368299000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1716",
        "Challenge_link_count":3,
        "Challenge_readability":10.5,
        "Challenge_reading_time":22.27,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] SASRec integration test unusually long time on AzureML compute cluster",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":179,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_created_time":1596320174000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2127.9833333333,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":8.9,
        "Solution_reading_time":1.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":431.4666666667,
        "Challenge_answer_count":12,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Challenge_closed_time":1555413510000,
        "Challenge_created_time":1553860230000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Challenge_link_count":0,
        "Challenge_readability":9.7,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":431.4666666667,
        "Challenge_title":"[BUG] Remove contrib from azureml",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":106,
        "Platform":"Github",
        "Solution_body":"azureml_hyperdrive_wide_and_deep notebook does**n't** use any azureml contrib modules. papermill PR in progress is using azureml.contrib.notebook. If it's not desired in the yaml file, I can install this package only inside the notebook. Will this work? mmm, yeah that would be a workaround.  Not in the Hyperdrive notebooks. since @jingyanwangms is the only one using it, can you take care of this issue in your PR? Do we want to require people to install things at the beginning of notebooks, though?  azureml.contrib.notebook is required for submitting notebook through aml. But if we don't want azureml.contrib to install as default in base yaml files, @heatherbshapiro what would you recommend doing here? @miguelgfierro Sure. I can do it in my PR. Hey guys\r\nI am getting an error while trying to run this line \"from azureml.contrib.notebook import NotebookRunConfig, AzureMLNotebookHandler\".\r\n**The error is ModuleNotFoundError: No module named 'azureml.contrib'** although I have installed azureml-contrib-notebook from pip. What should I do?\r\n HI @Raman1121 , which file in the code is this line in? I am trying to experiment with jupyter notebook on Azure through API calls by following the code snippet given here - \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-notebook\/azureml.contrib.notebook.azuremlnotebookhandler?view=azure-ml-py Well, that code is not related to the Recommenders GitHub repo. Most likely you have not configured your conda or python settings appropriately.",
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":18.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":21.0,
        "Solution_word_count":210.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":142.3802777778,
        "Challenge_answer_count":3,
        "Challenge_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Challenge_closed_time":1548948415000,
        "Challenge_created_time":1548435846000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Challenge_link_count":2,
        "Challenge_readability":18.3,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":142.3802777778,
        "Challenge_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"hey @rastala thanks for the pointer, we are working on updating that notebook to a newer version of databricks and spark. @jreynolds01 is looking at this based on this issue https:\/\/github.com\/Microsoft\/Recommenders\/issues\/427 yes, this should be fixed with my PR. fixed with #438 ",
        "Solution_link_count":1.0,
        "Solution_readability":6.2,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I have trained a model locally using the R package locfit. I am now trying to run this in Azure Machine Learning.\r\n\r\nMost guides\/previous questions appear to be in relation to Azure Machine Learning (classic). Although I believe the process outlined in similar posts will be similar (e.g. here, here, I am still unable to get it to work.\r\n\r\nI have outlined the steps I have followed below:\r\n\r\nDownload locfit R package for windows Zip file from here\r\n\r\nPut this downloaded Zip file into a new Zip file entitled \"locfit_package\"\r\n\r\nI upload this \"locfit_package\" zip folder to AML as a dataset (Create Dataset > From Local Files > name: locfit_package dataset type: file > Upload the zip (\"locfit_package\") > Confirm upload is correct\r\n\r\nIn the R terminal I then execute the following code:\r\n\r\n```\r\ninstall.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, verbose = TRUE)\r\n\r\nlibrary(locfit_package, lib.loc=\".\", verbose=TRUE)\r\n\r\nlibrary(locfit)\r\n\r\n```\r\nThe following error message is then returned:\r\n\r\n```\r\nsystem (cmd0): \/usr\/lib\/R\/bin\/R CMD INSTALL\r\n\r\nWarning: invalid package \u2018src\/locfit_package.zip\u2019 Error: ERROR: no packages specified Warning message:\r\n\r\nIn install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, : installation of package \u2018src\/locfit_package.zip\u2019 had non-zero exit status Error in library(locfit_package, lib.loc = \".\", verbose = TRUE) : there is no package called \u2018locfit_package\u2019 Execution halted\r\n\r\n\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1631291354000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1590",
        "Challenge_link_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":18.3,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to open R locfit package in Azure Machine Learning",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":198,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello, \r\n\r\nWe are trying to mount an Azure Storage account in Azure ML. This works perfectly fine, until we start a child run. In the logs of the child run, we can see the following:\r\nSet Dataset input__c79bd306's target path to \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4\r\n\r\nBut when we try to access the mount, we get the following error: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4': No such file or directory\r\n\r\nThe code to start the child run can be found below.\r\nThank you for your help.\r\n\r\n`child_config = ScriptRunConfig(source_directory='.',\r\n                                       script='src\/main_child.py',\r\n                                       arguments=arguments,\r\n                                       environment=environment,\r\n                                       docker_runtime_config=DockerConfiguration(use_docker=True),\r\n                                       compute_target=compute_target)\r\nrun.submit_child(child_config)`\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1631278836000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_readability":17.3,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML mounting Storage Account",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Pandas dataframes with arrays as column values seem to be incorrectly persisted. An example:\r\n\r\n```python\r\ntest_df = pd.DataFrame({'x': [np.random.rand(1000) for _ in range(1000)]})\r\nds = Datastore.get_default(ws)\r\nDataset.Tabular.register_pandas_dataframe(test_df, ds, 'test_dataset')\r\n\r\ntest_df.head()\r\n###\r\n\tx\r\n0\t[0.5044850335733219, 0.6054305053424696, 0.669...\r\n1\t[0.41759815476145723, 0.266477750018155, 0.511...\r\n2\t[0.6777708610872593, 0.16925324567267985, 0.16...\r\n3\t[0.4268294269387616, 0.6540643485117185, 0.033...\r\n4\t[0.6560106490417036, 0.5804652379458484, 0.582...\r\n\r\nDataset.get_by_name(ws, 'test_dataset').to_pandas_dataframe().head()\r\n###\r\nx\r\n0\tERROR\r\n1\tERROR\r\n2\tERROR\r\n3\tERROR\r\n4\tERROR\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1630657501000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1587",
        "Challenge_link_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":10.17,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Pandas dataframes with array column values are not correctly persisted as AzureML datasets",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":622.6719444444,
        "Challenge_answer_count":6,
        "Challenge_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Challenge_closed_time":1632248052000,
        "Challenge_created_time":1630006433000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Challenge_link_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":622.6719444444,
        "Challenge_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":204,
        "Platform":"Github",
        "Solution_body":"[70_driver_log.txt](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/7062339\/70_driver_log.txt)\r\n\r\nError generated in new compute that uses the V1.33 SDK Any updates to this? I had created another AML Workspace and issue disappeared but for some other subscriptions it still doesnt work and errors with the same thing as in the logs. Everything works perfectly fine in V1.32 of the SDK so not sure if new update changed some sort of Identity SDK used in Azure? The driver log had error message \"Compute has no identity provisioned.\" Try updating the compute to enable managed identity, and grant managed identity access to the data storage. Ah ok I was under the impression only the compute clusters had MI and not the compute instance. I'll take a look at the docs and will also re-configure the datastore which might be issue. @rudizhou428 we had some new feature for Compute Instance, which can use your identity in the CI, but, you need to re-create the CI as it won't automatically update the existing one. @chunyli0328 Ah ok cool, I ended up creating a new Azure ML Workspace and moved all my files over and since you need to recreate the CI and clusters, I'm guessing thats why it started to work again. Closing this issue, thanks for the help everyone!",
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":15.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":384.0669444444,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Challenge_closed_time":1629244160000,
        "Challenge_created_time":1627861519000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Challenge_link_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":12.03,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":384.0669444444,
        "Challenge_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":119,
        "Platform":"Github",
        "Solution_body":"0.17.5 introduced a breaking change hence there is an upperbound Thank you @vizhur! @areed1192, I'm closing this issue. Please reopen if you still have questions.",
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi!\r\n\r\nWhen trying to download a registered model from the AMLS workspace, I'm getting the following traceback. The file shows up in the `target_dir` (and ADLS path) however the size is 0 bytes, so it is making the file, however no data is being transferred into it.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    432         try:\r\n--> 433             return exec_func()\r\n    434         except exceptions as request_exception:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in exec_func()\r\n    212                                                           max_connections=max_concurrency,\r\n--> 213                                                           validate_content=_validate_check_sum)\r\n    214             file_size = os.stat(path).st_size\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/baseblobservice.py in get_blob_to_path(self, container_name, blob_name, file_path, open_mode, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout)\r\n   1855 \r\n-> 1856         with open(file_path, open_mode) as stream:\r\n   1857             blob = self.get_blob_to_stream(\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAzureMLException                          Traceback (most recent call last)\r\n<command-3894832347418984> in <module>\r\n----> 1 existing_model.download(target_dir=\"\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\")\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in download(self, target_dir, exist_ok, exists_ok)\r\n    999 \r\n   1000         # download files using sas\r\n-> 1001         file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\r\n   1002         if len(file_paths) == 0:\r\n   1003             raise WebserviceException(\"Illegal state. Unpack={}, Paths in target_dir is \"\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in _download_model_files(self, sas_to_relative_download_path, target_dir, exist_ok)\r\n    940                                           \"{}\".format(target_path), logger=module_logger)\r\n    941             sas_to_relative_download_path[sas] = target_path\r\n--> 942             download_file(sas, target_path, stream=True)\r\n    943 \r\n    944         if self.unpack:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in download_file(source_uri, path, max_retries, stream, protocol, session, _validate_check_sum, max_concurrency)\r\n    219                                        'present in blob.'.format(file_size, content_length))\r\n    220 \r\n--> 221         return _retry(exec_func, max_retries=max_retries)\r\n    222 \r\n    223     # download using requests.Session\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    443             else:\r\n    444                 module_logger.error('Failed to download file with error: {}'.format(request_exception))\r\n--> 445                 raise AzureMLException('Download of file failed with error: {}'.format(request_exception))\r\n    446         finally:\r\n    447             clean_up_func()\r\n\r\nAzureMLException: AzureMLException:\r\n\tMessage: Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\"\r\n    }\r\n}\r\n\r\n```\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/7530947\/125011096-a8c32700-e01c-11eb-83b4-4305be4095df.png)\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1625795312000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1545",
        "Challenge_link_count":1,
        "Challenge_readability":20.3,
        "Challenge_reading_time":51.46,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureMLException with model.download",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":281,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":1430.2352777778,
        "Challenge_answer_count":1,
        "Challenge_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":1630367426000,
        "Challenge_created_time":1625218579000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Challenge_link_count":2,
        "Challenge_readability":13.3,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1430.2352777778,
        "Challenge_title":"Bug: Failure while loading azureml_run_type_providers",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":129,
        "Platform":"Github",
        "Solution_body":"Thanks for the report! azureml-sdk==1.13.0 does specify docker<5.0.0, while mlflow==1.18.0 requires 5.0.0. \r\n\r\nI'm going to close this issue as there is no action for azureml-sdk.",
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":386.3980555556,
        "Challenge_answer_count":2,
        "Challenge_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1626388206000,
        "Challenge_created_time":1624997173000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Challenge_link_count":3,
        "Challenge_readability":25.6,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":386.3980555556,
        "Challenge_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"Thanks for submitting the issue. I am fixing this broken link now.  The links should be fixed on next SDK release on Aug 3rd",
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":519.1605555556,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\n\r\nI have installed the azure ml using below environment yml, installation happened without any issues but when I import the azureml.core I am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**Exception**\r\nimport azureml.core\r\n`Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).`\r\n\r\nAzure ML SDK Version:  1.31.0\r\n\r\n\r\nPlease help.\r\nThanks",
        "Challenge_closed_time":1626388114000,
        "Challenge_created_time":1624519136000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1523",
        "Challenge_link_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":519.1605555556,
        "Challenge_title":"Warning while loading the azureml.core",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":101,
        "Platform":"Github",
        "Solution_body":"Can you try installing [azureml-core](https:\/\/pypi.org\/project\/azureml-core\/) instead? This is unfortunately a package conflict issue. I was able to create the conda environment by removing all the version pinning on `matplotlib`, `numpy`, ... all the way to `pytorch`, and changed `azureml-sdk` to `azureml-core`.  \r\n\r\n```yml\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib\r\n- numpy\r\n- seaborn\r\n- urllib3\r\n- scipy\r\n- scikit-learn\r\n- pandas\r\n- py-xgboost\r\n- jupyterlab\r\n- ipykernel\r\n- pytorch\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-core\r\n\r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]                               \r\n```\r\n\r\n```bash\r\n(base) \u279c  jiazho_playground \u2717 conda activate ati_reranking_automl_py36\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground git:(split-merge) \u2717 python\r\nPython 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)\r\n[GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import azureml.core\r\n>>>\r\n\r\n\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground \u2717 pip freeze | grep azureml.core\r\nazureml-core==1.32.0\r\n```\r\n",
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":15.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":120.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":674.2494444444,
        "Challenge_answer_count":7,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719342000,
        "Challenge_created_time":1624292044000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":674.2494444444,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":249,
        "Platform":"Github",
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":257.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":890.5602777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Challenge_closed_time":1626798489000,
        "Challenge_created_time":1623592472000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Challenge_link_count":0,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":890.5602777778,
        "Challenge_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":130,
        "Platform":"Github",
        "Solution_body":"Extra docker steps is no longer supported. Please create an environment instead where you can inject files as you wish and use that environment for deployment. Here is a sample.\r\n\r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\r\n",
        "Solution_link_count":1.0,
        "Solution_readability":16.7,
        "Solution_reading_time":4.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":2342.6461111111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Challenge_closed_time":1631108652000,
        "Challenge_created_time":1622675126000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Challenge_link_count":2,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.59,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":2342.6461111111,
        "Challenge_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":292,
        "Platform":"Github",
        "Solution_body":"After a long while I've tried the exact same script in the same compute instance again. I don't experience the connection error anymore so I will close this ticket. I don't know what changed.",
        "Solution_link_count":0.0,
        "Solution_readability":5.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":524.3688888889,
        "Challenge_answer_count":10,
        "Challenge_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1622654301000,
        "Challenge_created_time":1620766573000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Challenge_link_count":12,
        "Challenge_readability":29.1,
        "Challenge_reading_time":35.07,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":524.3688888889,
        "Challenge_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":112,
        "Platform":"Github",
        "Solution_body":"Hi,\r\n\r\nI have some updates:\r\n\r\n- I put the code below to set the environment.\r\n\r\nfrom azureml.core.environment import Environment\r\n\r\nenv = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\nmyenv = env.clone(\"automl_env\")\r\n\r\ntrain_steps = AutoMLPipelineBuilder.get_many_models_train_steps(experiment=experiment,\r\n                                                                automl_settings=automl_settings,\r\n                                                                train_data=dataset_input,\r\n                                                                compute_target=compute_target,\r\n                                                                partition_column_names=partition_column_names,\r\n                                                                node_count=1,\r\n                                                                process_count_per_node=2,\r\n                                                                run_invocation_timeout=3700,\r\n                                                                train_env=myenv)\r\n\r\n- The environment problem has been resolved, but now the process displays the message **ValueError: None is not in list**. I don't know what this means.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/118014360-82894f80-b329-11eb-8e6a-558d6606d7b1.png)\r\n\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 3.0.0 (\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 212, in <module>\r\n    logs = run(data_file_path, args, automl_settings, current_step_run)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 100, in run\r\n    data = pd.read_csv(file_path, parse_dates=[timestamp_column])\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 676, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 448, in _read\r\n    parser = TextFileReader(fp_or_buf, **kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 880, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1114, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1949, in __init__\r\n    self._set_noconvert_columns()\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2015, in _set_noconvert_columns\r\n    _set(val)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2005, in _set\r\n    x = names.index(x)\r\nValueError: None is not in list\r\n\r\nBest regards,\r\nCristina @crisansou is there any error surfaced in 70_driver_log? \r\nHi @shbijlan ,\r\n\r\nI deleted the workspace. I tried to reproduce the steps again but I couldn't even create the experiment, below is the error message. Can you tell which is the recommended version to use this solution?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119876912-c752e000-befe-11eb-9a18-c8f03afae48c.png)\r\n\r\nI don't know if it's related, but I realized that now compute instance is using version 1.29.\r\n\r\n!pip install --upgrade azureml-sdk[automl]\r\n!pip install azureml-contrib-automl-pipeline-steps\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119877097-03864080-beff-11eb-8134-07d22a243284.png)\r\n\r\n\r\n\r\n\r\n> @crisansou is there any error surfaced in 70_driver_log?\r\n\r\n from azureml.core. import Environment\r\ntrain_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n\r\nCan you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n\r\nAlso this solution only supports 'forecasting' and needs a time_column_name passed in automl settings Hi @deeptim123 ,\r\n\r\nThanks for the instructions! After including the environment I was able to run the cell, but the pipeline ended with an error because I am using a regression model.\r\n\r\nIn the next release, in addition to fixing the bug, will it be possible to use regression?\r\n\r\n> from azureml.core. import Environment\r\n> train_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n> \r\n> Can you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n> \r\n> Also this solution only supports 'forecasting' and needs a time_column_name passed in automl settings\r\n\r\n There are currently no plans to support regression. @cartacioS  for visibility of this ask @deeptim123 ,\r\n\r\nThanks for the info. I think it's important to add this functionality for regression and classification as well.\r\n\r\n> There are currently no plans to support regression. @cartacioS for visibility of this ask\r\n\r\n @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at sabina.cartacio@microsoft.com and we can further discuss.\r\n\r\nThanks! Hi @cartacioS ,\r\n\r\nGot it, thanks for the info! The project is starting now, but if it is really necessary to use the multiple models solution for regression I'll send you an email.\r\n\r\n> @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at [sabina.cartacio@microsoft.com](mailto:sabina.cartacio@microsoft.com) and we can further discuss.\r\n> \r\n> Thanks!\r\n\r\n Closing as this is being tracked offline as a feature request for MANY MODELS, by Sabina.",
        "Solution_link_count":3.0,
        "Solution_readability":11.4,
        "Solution_reading_time":82.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":57.0,
        "Solution_word_count":664.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":24.7769444444,
        "Challenge_answer_count":2,
        "Challenge_body":"In Azure Machine Learning Service, when we deploy a Model as an AKS Webservice Endpoint, how can we raise exceptions to let the end-user get proper feedback if their API call is unsuccessful? Azure mentions using `azureml.exceptions.WebserviceException` in their documentation. However, how do we use this class to raise exceptions in case the API call cannot be processed properly and the end-user is responsible for it?",
        "Challenge_closed_time":1617344140000,
        "Challenge_created_time":1617254943000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1413",
        "Challenge_link_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":6.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":24.7769444444,
        "Challenge_title":"How to effectively use azureml.exceptions.WebserviceException for efficient REST API Error Management?",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":76,
        "Platform":"Github",
        "Solution_body":"@anirbansaha96 can you link to the doc you mention? The only doc I'm aware of about authoring errors is here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#binary-data\r\n \r\nWhich mentions using AMLResponse in the scoring file.\r\n\r\nAMLResponse will allow the writer of the scoring file to set a custom error message and api response code. Yes `return AMLResponse(\"Message\", status-code)` is what I was looking for. The documentation I was referring to: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.webserviceexception?view=azure-ml-py",
        "Solution_link_count":2.0,
        "Solution_readability":13.4,
        "Solution_reading_time":7.89,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":65.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":2331.6441666667,
        "Challenge_answer_count":6,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/74793968\/110592377-36daee00-81a0-11eb-8fb0-de7e2ba93af1.png)\r\n\r\nThis issue wasn't present until a few days ago. Issue shows up when we submit an experiment to azure ml workspace in the image build logs. We are using mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 base image",
        "Challenge_closed_time":1623755236000,
        "Challenge_created_time":1615361317000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1387",
        "Challenge_link_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2331.6441666667,
        "Challenge_title":"azure ml Python SDK 1.24.0 image build fails with the error failed to get layer was working fine before",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":51,
        "Platform":"Github",
        "Solution_body":"I would think that is some transient issue. \r\nSide note, mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 was deprecated in 2019, please use \r\nmcr.microsoft.com\/azureml\/intelmpi2018.3-ubuntu16.04 or better default cpu image from your client version that is pinned to a versioned tag Still facing the same issue. Kind of blocked is their some way i could fix this. Updated the image thanks for that is it local build? if yes, try to remove the image Nope it is remote compute build. It is in AML Compute cluster The issue got resolved we were earlier creating an image and storing it in Azure Container Registry. Now we don't pass it to RunConfiguration() object. We create it directly in the AML Build process and that has fixed the issue though now the image is not cached anymore so that is problematic. @MAQ-Ravijit-Ramana it would be great to get some details of your scenario, like the script you running",
        "Solution_link_count":0.0,
        "Solution_readability":7.4,
        "Solution_reading_time":11.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"\r\nAs, title says: there are no instructions on where to submit bugs\/glitches related to the Python azureml-sdk. \r\nIs it through github somewhere? Is it through an Azure support ticket? \r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":null,
        "Challenge_created_time":1615198296000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1378",
        "Challenge_link_count":2,
        "Challenge_readability":12.5,
        "Challenge_reading_time":12.4,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"No instructions on where to submit bugs\/glitches related to the Python azureml-sdk",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":86,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":91.9480555556,
        "Challenge_answer_count":2,
        "Challenge_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Challenge_closed_time":1614604742000,
        "Challenge_created_time":1614273729000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Challenge_link_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":91.9480555556,
        "Challenge_title":"Azure ML Run docker command to pull public image failed ",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":179,
        "Platform":"Github",
        "Solution_body":"please try this \r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1247#issuecomment-738887772 Seems like it solved the issue. Thanks!",
        "Solution_link_count":1.0,
        "Solution_readability":13.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":82.9133333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Upload data to a datastore**\r\n![AzureUpload](https:\/\/user-images.githubusercontent.com\/947785\/108407641-3e325b80-71e1-11eb-85df-58479ed8db52.png)\r\n\r\nNow that you have determined the available datastores, you can upload files from your local file system to a datastore so that it will be accessible to experiments running in the workspace, regardless of where the experiment script is actually being run.\r\n\r\n_default_ds.upload_files(files=['.\/data\/diabetes.csv', '.\/data\/diabetes2.csv'], # Upload the diabetes csv files in \/data\r\n                       target_path='diabetes-data\/', # Put it in a folder path in the datastore\r\n                       overwrite=True, # Replace existing files of the same name\r\n                       show_progress=True)_\r\n\r\nUploading an estimated of 2 files\r\nUploading .\/data\/diabetes.csv\r\nUploading .\/data\/diabetes2.csv\r\nUploaded 0 files\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 332, in handler\r\n    result = future.result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in <lambda>\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 463, in create_blob_from_path\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 582, in create_blob_from_stream\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 971, in _put_blob\r\n    return self._perform_request(request, _parse_base_properties)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 381, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 306, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 292, in _perform_request\r\n    HTTPError(response.status, response.message, response.headers, response.body))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\r\n    raise ex\r\nazure.common.AzureHttpError: This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\r\nRequestId:a9ffd72c-c01e-00d9-5220-064b2e000000\r\nTime:2021-02-18T18:02:54.3372191Z<\/Message><\/Error>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 994, in emit\r\n    msg = self.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 840, in format\r\n    return fmt.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 577, in format\r\n    record.message = record.getMessage()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/traitlets\/config\/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelapp.py\", line 612, in start\r\n    self.io_loop.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/platform\/asyncio.py\", line 199, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 814, in inner\r\n    self.ctx_run(self.run)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 775, in run\r\n    yielded = self.gen.send(value)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 362, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 265, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2867, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2895, in _run_cell\r\n    return runner(coro)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3072, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3263, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3343, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-0f28dc9194af>\", line 4, in <module>\r\n    show_progress=True)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in upload_files\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 321, in _start_upload_task\r\n    tq.add_task(async_task)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 55, in __exit__\r\n    self.flush(self.identity)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in flush\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in <genexpr>\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/async_task.py\", line 58, in wait\r\n    res = self._handler(self._future, self._logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 340, in handler\r\n    exception_handler(e, logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 304, in exception_handler\r\n    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\r\nMessage: 'Upload failed, please make sure target_path does not start with invalid characters.'\r\nArguments: (AzureHttpError('This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\\nRequestId:a9ffd72c-c01e-00d9-5220-064b2e000000\\nTime:2021-02-18T18:02:54.3372191Z<\/Message><\/Error>',),)\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 332, in handler\r\n    result = future.result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in <lambda>\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 463, in create_blob_from_path\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 582, in create_blob_from_stream\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 971, in _put_blob\r\n    return self._perform_request(request, _parse_base_properties)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 381, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 306, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 292, in _perform_request\r\n    HTTPError(response.status, response.message, response.headers, response.body))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\r\n    raise ex\r\nazure.common.AzureHttpError: This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\r\nRequestId:5488fc90-001e-0080-5d20-064ea8000000\r\nTime:2021-02-18T18:02:54.3372332Z<\/Message><\/Error>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 994, in emit\r\n    msg = self.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 840, in format\r\n    return fmt.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 577, in format\r\n    record.message = record.getMessage()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/traitlets\/config\/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelapp.py\", line 612, in start\r\n    self.io_loop.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/platform\/asyncio.py\", line 199, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 814, in inner\r\n    self.ctx_run(self.run)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 775, in run\r\n    yielded = self.gen.send(value)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 362, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 265, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2867, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2895, in _run_cell\r\n    return runner(coro)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3072, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3263, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3343, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-0f28dc9194af>\", line 4, in <module>\r\n    show_progress=True)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in upload_files\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 321, in _start_upload_task\r\n    tq.add_task(async_task)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 55, in __exit__\r\n    self.flush(self.identity)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in flush\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in <genexpr>\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/async_task.py\", line 58, in wait\r\n    res = self._handler(self._future, self._logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 340, in handler\r\n    exception_handler(e, logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 304, in exception_handler\r\n    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\r\nMessage: 'Upload failed, please make sure target_path does not start with invalid characters.'\r\nArguments: (AzureHttpError('This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\\nRequestId:5488fc90-001e-0080-5d20-064ea8000000\\nTime:2021-02-18T18:02:54.3372332Z<\/Message><\/Error>',),)\r\n$AZUREML_DATAREFERENCE_010e49b94ea645928f99f4a15d7b3a00\r\nfrom azurem",
        "Challenge_closed_time":1613973498000,
        "Challenge_created_time":1613675010000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1348",
        "Challenge_link_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":280.06,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":199,
        "Challenge_solved_time":82.9133333333,
        "Challenge_title":"ERROR:  Learning: Build AI solutions with Azure Machine Learning - 06 - Work with Data - Upload data to a datastore",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":1293,
        "Platform":"Github",
        "Solution_body":"@jb80016 can you provide more context on this issue? which example are you using? is it from this repository or elsewhere?  LEARNING PATH\r\nBuild AI solutions with Azure Machine Learning\r\nWork with Data in Azure Machine Learning  link:  https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/work-with-data-in-aml\/ \r\nCloned this repository to workspace within Azure using public key:  git@github.com:MicrosoftLearning\/mslearn-dp100.git \r\n\r\nLet me know if any other info would be helpful.   I figured it out using:  \r\n\r\nfrom azureml.core import Workspace\r\nws = Workspace.from_config()\r\ndatastore = ws.get_default_datastore()\r\ndatastore.upload(src_dir='.\/data',\r\n                 target_path='diabetes-data',\r\n                 overwrite=True)\r\n\r\nfrom azureml.core import Dataset\r\n\r\n# Get the default datastore\r\ndefault_ds = ws.get_default_datastore()\r\n\r\n#Create a tabular dataset from the path on the datastore (this may take a short while)\r\ntab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data\/*.csv'))\r\n\r\n# Display the first 20 rows as a Pandas dataframe\r\ntab_data_set.take(20).to_pandas_dataframe() \r\n We are closing this issue, but if you have any follow-ups, please reopen it!  #please-close",
        "Solution_link_count":1.0,
        "Solution_readability":10.5,
        "Solution_reading_time":14.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"\r\n[Enter feedback here]\r\n\r\nWe need details description of `azureml-defaults`. \r\n\r\nWe need this when deployment. In training, we usually use `azureml-core`. In deployment, `azureml-defaults` is necessary (only `azureml-core` is not enough to deploy). I heard `azureml-defaults` includes `azureml-core`. But it is not documented.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":null,
        "Challenge_created_time":1613523098000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1341",
        "Challenge_link_count":2,
        "Challenge_readability":13.1,
        "Challenge_reading_time":13.97,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"azureml-defaults not described ",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":92,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":8,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_created_time":1612300739000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2210.2472222222,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":134,
        "Platform":"Github",
        "Solution_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Solution_link_count":2.0,
        "Solution_readability":10.4,
        "Solution_reading_time":28.7,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":333.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"Azure's TabularDataset implementation introduces an index, \\_\\_index\\_level_0\\_\\_ when creating or reading parquet files that were originally written by Pandas\/Python.  This occurs when an index is unnamed but has been modified at some point; if an index is named we get an extra column with the same name as the index.\r\n\r\nWhen making changes to datasets, this additional field causes Azure errors if not handled.  Depending on what's been done to the index of the original dataset, you may or may not get that additional field.\r\n\r\nI have an example notebook that can be run to reproduce the issue.  It's here: https:\/\/github.com\/vla6\/Azure_notes\/blob\/main\/tabulardataset_parquet_index_di_issue.ipynb\r\n\r\nThe notebook requires an Azure Machine Learning workspace and a storage account to run",
        "Challenge_closed_time":null,
        "Challenge_created_time":1611344395000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1299",
        "Challenge_link_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":10.5,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML TabularDataSet via parquet and pandas index error",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"## Describe the issue\r\n\r\nversion 1.20.0 of python package azureml-contrib-pipeline-steps throws (works fine on version 1.19 or 1.18)\r\n\r\n File \"C:\/Users\/v-songshanli\/projects\/ashexplore\/object_identification\/obj_segmentation_azure_2_steps.py\", line 88, in run\r\n    pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\core\\_experiment_method.py\", line 97, in wrapper\r\n    return init_func(self, *args, **kwargs)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\pipeline.py\", line 177, in __init__\r\n    self._graph = self._graph_builder.build(self._name, steps, finalize=False)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1481, in build\r\n    graph = self.construct(name, steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1503, in construct\r\n    self.process_collection(steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1539, in process_collection\r\n    builder.process_collection(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1830, in process_collection\r\n    self._base_builder.process_collection(item)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1533, in process_collection\r\n    return self.process_step(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1577, in process_step\r\n    node = step.create_node(self._graph, self._default_datastore, self._context)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\steps\\python_script_step.py\", line 243, in create_node\r\n    return super(PythonScriptStep, self).create_node(\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 140, in create_node\r\n    self._set_compute_params_to_node(node,\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 229, in _set_compute_params_to_node\r\n    self._module_param_provider.set_params_to_node(\r\nTypeError: _set_params_to_node_hook() got an unexpected keyword argument 'command'\r\n\r\n## Minimal example\r\n\r\n```python\r\nfrom azureml.core import Workspace\r\n\r\nws = Workspace.from_config()\r\n\r\n\r\nsplit_step = PythonScriptStep(\r\n        name=\"Train Test Split\",\r\n        script_name=\"obj_segment_step_data_process.py\",\r\n        arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\r\n                   \"--train-split\", train_split_data, \"--test-split\", test_split_data,\r\n                   \"--test-size\", 50],\r\n        compute_target=compute_target,\r\n        runconfig=aml_run_config,\r\n        source_directory=source_directory,\r\n        allow_reuse=False\r\n    )\r\n\r\npipeline_steps = [split_step ]\r\n\r\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n```\r\n\r\n## Additional context\r\nI am using aml sdk 1.20. no type errors with version 1.19\/1.18 of azureml-contrib-pipeline-steps.\r\n-\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1611092820000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1417",
        "Challenge_link_count":0,
        "Challenge_readability":23.1,
        "Challenge_reading_time":44.03,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":null,
        "Challenge_title":"python package azureml-contrib-pipeline-steps 1.20.0 not working ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":179,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":7538.9758333333,
        "Challenge_answer_count":15,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097588000,
        "Challenge_created_time":1609957275000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":3.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7538.9758333333,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Solution_link_count":4.0,
        "Solution_readability":5.2,
        "Solution_reading_time":55.84,
        "Solution_score_count":16.0,
        "Solution_sentence_count":77.0,
        "Solution_word_count":608.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":94.4830555556,
        "Challenge_answer_count":5,
        "Challenge_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Challenge_closed_time":1598387113000,
        "Challenge_created_time":1598046974000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Challenge_link_count":0,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":94.4830555556,
        "Challenge_title":"error: azureml-train-automl-runtime is required however it is included",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"can you share the full stacktrace? and is the error happening when you submit the pipeline script? or is it happening in the logs of the `PythonScriptStep`? ```\r\n\"error\": {\r\n        \"code\": \"UserError\",\r\n        \"message\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"detailsUri\": \"https:\/\/aka.ms\/azureml-known-errors\",\r\n        \"details\": [],\r\n        \"debugInfo\": {\r\n            \"type\": \"UserScriptException\",\r\n            \"message\": \"UserScriptException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException OptionalDependencyMissingException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"ValidationError\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ScenarioNotSuported\\\",\\n                \\\"inner_error\\\": {\\n                    \\\"code\\\": \\\"OptionalDependencyMissing\\\"\\n                }\\n            }\\n        },\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\",\r\n            \"stackTrace\": \"  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 197, in execute_with_context\\n    raise UserScriptException(baseEx).with_traceback(exceptionInfo[2])\\n  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"run_models.py\\\", line 286, in \\n    main()\\n  File \\\"run_models.py\\\", line 197, in main\\n    run = experiment.submit(config=automl_config, tags=tags)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\\\", line 211, in submit\\n    run = submit_func(config, self.workspace, self.name, **kwargs)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 97, in _automl_static_submit\\n    show_output)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 255, in _start_execution\\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 121, in _default_execution\\n    return automl_estimator.fit(**fit_params)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\\\", line 349, in fit\\n    \\\"azureml-train-automl-runtime must be installed in the current environment to run local in \\\"\\n\"\r\n        },\r\n        \"messageFormat\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"messageParameters\": {}\r\n    },\r\n    \"time\": \"0001-01-01T00:00:00.000Z\"\r\n}\r\n``` Here is my stack trace from the 70_driver_log.txt:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_models.py\", line 286, in <module>\r\n    main()\r\n  File \"run_models.py\", line 197, in main\r\n    run = experiment.submit(config=automl_config, tags=tags)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\", line 211, in submit\r\n    run = submit_func(config, self.workspace, self.name, **kwargs)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 97, in _automl_static_submit\r\n    show_output)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 255, in _start_execution\r\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 121, in _default_execution\r\n    return automl_estimator.fit(**fit_params)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 349, in fit\r\n    \"azureml-train-automl-runtime must be installed in the current environment to run local in \"\r\nUserScriptException: UserScriptException:\r\n\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n``` @swatig007 this is an error, @BillmanH is experiencing when submitting an AutoML run from within a `PythonScriptStep` rather than using an `AutoMLStep`. This approach worked for over a year, but is now throwing an error about `azureml-train-automl-runtime` not being installed. upgraded to 1.12.0, which solved this problem and opened other issues. ",
        "Solution_link_count":1.0,
        "Solution_readability":16.5,
        "Solution_reading_time":83.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":486.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":4,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_created_time":1587712154000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3154.2575,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":55,
        "Platform":"Github",
        "Solution_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Solution_link_count":5.0,
        "Solution_readability":18.4,
        "Solution_reading_time":17.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":123.0280555556,
        "Challenge_answer_count":1,
        "Challenge_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Challenge_closed_time":1583847372000,
        "Challenge_created_time":1583404471000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Challenge_link_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":123.0280555556,
        "Challenge_title":"Internal server error when deploying from Azure ML to AKS",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":201,
        "Platform":"Github",
        "Solution_body":"@robinvdheijden \r\n\r\nThanks for reaching out to us. This is forum for Machine Learning Notebook only. Please open a new forum thread in [MSDN forum](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/home?forum=AzureMachineLearningService)as it could be better place to get help on your scenario. These forum community members could provide their expert guidance on your scenario based on their experience. Thanks.\r\n\r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion.",
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":0.5533333333,
        "Challenge_answer_count":2,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440044000,
        "Challenge_created_time":1581438052000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.5533333333,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":5.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":3914.1044444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Challenge_closed_time":1587086020000,
        "Challenge_created_time":1572995244000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Challenge_link_count":0,
        "Challenge_readability":17.7,
        "Challenge_reading_time":45.44,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":3914.1044444444,
        "Challenge_title":"Error trying to load azureml.train.automl",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":272,
        "Platform":"Github",
        "Solution_body":"Do you know which version of tensorflow you are using? \r\n\r\nThis SO question may be applicable: https:\/\/stackoverflow.com\/questions\/55318626\/module-tensorflow-has-no-attribute-logging Hello, Not sure about tensorflow.  This is a \"stock\" Notebook VM that was created last week, so no changes were made to the libraries. Hello,\r\n\r\nSorry for the inconvenience. This issue has been fixed since v1.0.72 but, it's related to the fact that tf==2.0. is installed by default on the notebook instance. It broke other things too as TF2.0 has many changes in its API. Your two options are to upgrade to v1.0.72+ or use the following code to downgrade tensorflow.\r\n\r\npip install -U tensorflow-gpu==1.14.0 \r\ntensorflow==estimator==1.14.0 \r\n\r\nThat should fix it for you.",
        "Solution_link_count":1.0,
        "Solution_readability":5.2,
        "Solution_reading_time":9.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":109.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":75.5002777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I've installed and updated the sdk yet when attempting to import the module for the ExplainationDashboard i keep getting the error that the interpret module does not exist.\r\ni am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb \r\nthe failing line in the sample notebook is:\r\n**from azureml.contrib.interpret.visualize import ExplanationDashboard**\r\n**\"ModuleNotFoundError: No module named 'azureml.contrib.interpret' \"**\r\nthis is the update command i ran:\r\npip install --upgrade azureml-sdk[explain,automl,contrib] \r\n(the install ran fine - no errors)\r\njim",
        "Challenge_closed_time":1573060395000,
        "Challenge_created_time":1572788594000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/639",
        "Challenge_link_count":0,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":75.5002777778,
        "Challenge_title":"azureml.contrib.interpret - ModuleNotFoundError - build 1.0.72",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":79,
        "Platform":"Github",
        "Solution_body":"azureml-contrib-interpret is not a part of azureml-sdk contrib extras. Please install it separately",
        "Solution_link_count":0.0,
        "Solution_readability":9.4,
        "Solution_reading_time":1.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":0.4416666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Challenge_closed_time":1565217619000,
        "Challenge_created_time":1565216029000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Challenge_link_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4416666667,
        "Challenge_title":"No name 'opendatasets' in module 'azureml' Error",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":71,
        "Platform":"Github",
        "Solution_body":"Find the solution, maybe because `opendatasets` is a preview module, so it is not included in azureml sdk yet. You can download through pip `pip install azureml-opendatasets` in your env. > pip install azureml-opendatasets\r\n\r\nThanks, was looking for the solution, this worked !! However, I had another error \" [WinError 5] Access is denied:\" This was solved by adding --user at the end of your command.",
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":9752.3283333333,
        "Challenge_answer_count":2,
        "Challenge_body":"> from azureml.core.model import Model\r\nmodel = Model.register(model_path = MODEL_FILENAME,\r\n                       model_name = \"MyONNXmodel\",\r\n                       tags = {\"onnx\":\"V0\"},\r\n                       description = \"test\",\r\n                       workspace = ws)\r\nthis is the python code I am using to register the model.",
        "Challenge_closed_time":1587148100000,
        "Challenge_created_time":1552039718000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/243",
        "Challenge_link_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":6.17,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9752.3283333333,
        "Challenge_title":"Unable to register the model using Jupyter Notebook with error message: \"HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET https:\/\/cert-westeurope.experiments.azureml.net\/rp\/workspaces'\"",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":50,
        "Platform":"Github",
        "Solution_body":"Thanks for your report. Are you still experiencing this issue? Have you posted on the forum https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/home?forum=AzureMachineLearningService? Thank you for reaching out to us.  We see our answer this issue was delayed. Our apologies. We did not receive a response to our response, so will close this issue for now. Should you need further assistance, please submit a post on this forum. We are happy to help.\r\n",
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":5.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? YES <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Create compute instance in azure ML (Tesla K80) and add schedule\r\n2. Connect with VSCode on day 2. \r\n\r\nAction: Resolver.resolve\r\nError type: ce\r\nError Message: Failed to connect to target. Make sure that it exists and is in a running state (Error: Invalid response: 502 Bad Gateway)\r\n\r\n\r\nVersion: 0.22.0\r\nOS: darwin\r\nOS Release: 22.1.0\r\nProduct: Visual Studio Code\r\nProduct Version: 1.74.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nce.NotAvailable extensionHostProcess.js:92:17513\r\nb.handle400BadRequestAnd502BadGateway extension.js:2:1949684\r\nb.handleError extension.js:2:1948588\r\no.value extension.js:2:1911817\r\nextension.js:2:1960308\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1671822949000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1850",
        "Challenge_link_count":0,
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.79,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"502 Bad Gateway error after running Tesla K80 compute instance on azure ML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":129,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.22.0\r\nOS: linux\r\nOS Release: 5.15.0-1022-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.68.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1976096\r\ns extension.js:2:1972783\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669213419000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1817",
        "Challenge_link_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to login in Azure ML extension in VSCode",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":88,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1668197837000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1797",
        "Challenge_link_count":0,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"connecting to VScode to AzureML",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":84,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"To reproduce the issue:\r\nRight click on ML default project in left hand bar.\r\nFrom the menu click \"View Properties\" and getting:\r\n\r\n[UriError]: If a URI does not contain an authority component, then the path cannot begin with two slash characters (\"\/\/\")",
        "Challenge_closed_time":null,
        "Challenge_created_time":1667838269000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1783",
        "Challenge_link_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":3.8,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Visual Studio Azure ML extension error on \"View Properties\" ",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":50,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: linux\r\nOS Release: 5.15.0-1017-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1665696667000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1754",
        "Challenge_link_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.33,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Mutliple consecutive sign-in requests from Azure ML plugin VS Code",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1664990324000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1745",
        "Challenge_link_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Problem signing into Azure ML using VSCode with latest version",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.18.0\r\nOS: darwin\r\nOS Release: 21.6.0\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1664468718000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1737",
        "Challenge_link_count":0,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Working with Azure ML Studio on VSCode",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":86,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Azure sign in\r\n2. Sign in using Azure portal. You get the sign in successful, you may close the window message, but Azure asks to sign in again.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.17.2022090809\r\nOS: win32\r\nOS Release: 10.0.19042\r\nProduct: Visual Studio Code - Insiders\r\nProduct Version: 1.72.0-insider\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1662929331000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1714",
        "Challenge_link_count":0,
        "Challenge_readability":7.8,
        "Challenge_reading_time":11.43,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"I keep on getting this error continuously for Azure Machine Learning extension",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":119,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":572.4141666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Challenge_closed_time":1663956142000,
        "Challenge_created_time":1661895451000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Challenge_link_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":572.4141666667,
        "Challenge_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":40,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":1032.455,
        "Challenge_answer_count":2,
        "Challenge_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Challenge_closed_time":1665527881000,
        "Challenge_created_time":1661811043000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Challenge_link_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.74,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1032.455,
        "Challenge_title":"Can't use Azure ML features when remotely connected to a compute",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"Seems to work when remotely connected via VS Code desktop, but it definitely doesn't work when connected via vscode.dev. The azure ml remote extension is disabled in this case. Seems like we should be able to support this. Yes, this is only on web platforms like vscode.dev or codespaces.",
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Expected Behavior\r\nIf the user is logged out, the AML extension should not prompt to login until the user specifically tries to run an AzureML command. Prompting when VS Code loads is disruptive and unnecessary, and no other extensions for AWS or Azure do this.\r\n\r\n## Actual Behavior\r\nIf you are signed out of the Azure ML extension and reload VS Code, you are prompted to login when it loads (Issue #1). If you click cancel, you are prompted again (#2). \r\n\r\n## Steps to Reproduce the Problem\r\n  1. Install the Azure ML Extension\r\n  2. Login\r\n  3. Logout\r\n  4. Reload VS Code\r\n  5. Click \"Cancel\" when prompted to login\r\n\r\n\r\n## Specifications\r\nAzure ML Extension Version 0.16.0\r\n \r\nVersion: 1.70.0-insider (Universal)\r\nCommit: da76f93349a72022ca4670c1b84860304616aaa2\r\nDate: 2022-08-03T05:55:27.651Z (1 day ago)\r\nElectron: 18.3.5\r\nChromium: 100.0.4896.160\r\nNode.js: 16.13.2\r\nV8: 10.0.139.17-electron.0\r\nOS: Darwin x64 21.6.0\r\n\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1659626460000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1627",
        "Challenge_link_count":0,
        "Challenge_readability":5.9,
        "Challenge_reading_time":11.77,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML Prompts twice to login when VS Code (Insiders) loads",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":6.2444444444,
        "Challenge_answer_count":1,
        "Challenge_body":"## Expected Behavior\r\nIt seems new version on AzureML extension to VS Code doesn't have this option in settings. I needed to downgrade to 0.6x.\r\n\r\n## Actual Behavior\r\nCurrent version 0.10.0 doesn't have the option. Cannot locally debug or documentation doesn't provide info about that.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.10.0\r\n  - Platform: VS Code, Windows\r\n",
        "Challenge_closed_time":1654701310000,
        "Challenge_created_time":1654678830000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.2444444444,
        "Challenge_title":"Run and debug experiments locally - azureML.CLI Compatibility Mode for CLI v1 - cannot find",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":63,
        "Platform":"Github",
        "Solution_body":"@michalmar We have completely deprecated the v1 CLI Compatibility mode settings from v0.8.0 onwards and v2 mode will be the way going forward :).",
        "Solution_link_count":0.0,
        "Solution_readability":9.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":430.5058333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the customer is shown an error message but also has the option to report an issue which is misleading, we should remove the report issue button for this scenario.",
        "Challenge_closed_time":1655835283000,
        "Challenge_created_time":1654285462000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1588",
        "Challenge_link_count":0,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":430.5058333333,
        "Challenge_title":"Improve the error message when trying to execute a YAML that is not Azure ML realted",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":45,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":659.0783333333,
        "Challenge_answer_count":4,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":1652117002000,
        "Challenge_created_time":1649744320000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Challenge_link_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":659.0783333333,
        "Challenge_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"@evakkuri thanks for filing this issue. Is this happening every time you open a remote connection? Are you connecting to Compute Instance through the ML Studio? Currently this happens every time I connect. I'm not connecting via ML Studio, instead through VS Code with the Azure Machine Learning extension. @sevillal Can you please follow up here :) ? @evakkuri we have published version v0.10.0 of the extension. Could you please upgrade and retry to check if you issue is still reproducible? Please reopen this issue if that's the case.",
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":243.16,
        "Challenge_answer_count":3,
        "Challenge_body":"Error when adding extetnion azureml\r\naz k8s-extension create --name azureml-extension --extension-type Microsoft.AzureML.Kubernetes --config enableTraining= cluster-type conneced--cluster-name <your-AKS-cluster-name> --resource-group <your-RG-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Creating\"}\r\ncli.azure.cli.core.sdk.policies: Request URL: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: Request method: 'GET'\r\ncli.azure.cli.core.sdk.policies: Request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'CommandName': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'ParameterSetName': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'User-Agent': 'AZURECLI\/2.36.0 (MSI) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 Python\/3.10.4 (Windows-10-10.0.19044-SP0)'\r\ncli.azure.cli.core.sdk.policies:     'Authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: Request body:\r\ncli.azure.cli.core.sdk.policies: This request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmanagement.azure.com%2F&data=05%7C01%7Cjohan.andolf%40microsoft.com%7C37b5d083c3d7447f133208da3e347a4a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637890692414003835%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=1kWOqV7FwAgqmYol4W7wfZRbf%2BCTKz9XucDBe%2FKgGKA%3D&reserved=0) \"GET \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01 HTTP\/1.1\" 200 None\r\ncli.azure.cli.core.sdk.policies: Response status: 200\r\ncli.azure.cli.core.sdk.policies: Response headers:\r\ncli.azure.cli.core.sdk.policies:     'Cache-Control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Transfer-Encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'Content-Type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'Content-Encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'Expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'Vary': 'Accept-Encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-Preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'X-Content-Type-Options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'SWEDENCENTRAL:20220525T095135Z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'Date': 'Wed, 25 May 2022 09:51:34 GMT'\r\ncli.azure.cli.core.sdk.policies: Response content:\r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Failed\",\"error\":{\"code\":\"ExtensionCreationFailed\",\"message\":\" error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: Traceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.OperationFailed: Operation failed or canceled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.HttpResponseError: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x0387C190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: Command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: Save telemetry record of length 3581 in cache\r\ntelemetry.check: Returns Positive.\r\ntelemetry.main: Begin creating telemetry upload process.\r\ntelemetry.process: Creating upload process: \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\python.exe C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\Lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc C:\\Users\\ropa04\\.azure\"\r\ntelemetry.process: Return from creating process\r\ntelemetry.main: Finish creating telemetry upload process.",
        "Challenge_closed_time":1654438640000,
        "Challenge_created_time":1653563264000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/1213",
        "Challenge_link_count":2,
        "Challenge_readability":19.4,
        "Challenge_reading_time":114.38,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":243.16,
        "Challenge_title":"Can not add AzureML extention on openshift cluster ",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":533,
        "Platform":"Github",
        "Solution_body":"Hey friend! Thanks for opening this issue. We appreciate your contribution and welcome you to our community! We are glad to have you here and to have your input on the Azure Arc Jumpstart.<p><\/p> Hello Johan, thanks for submitting feedback. Have you checked the [prerequisites specific to ARO](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-kubernetes-anywhere?tabs=studio#prerequisites) prior to attempting the extension installation?  Hello @rataxe , have you checked the pre-reqs above. If you already had and still facing a problem, we recommend you open a support case as this is not strictly related to the Jumpstart project but to the product itself. ",
        "Solution_link_count":1.0,
        "Solution_readability":9.0,
        "Solution_reading_time":8.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0396927017,
        "Challenge_watch_issue_ratio":0.0166453265
    },
    {
        "Challenge_adjusted_solved_time":41.2755555556,
        "Challenge_answer_count":3,
        "Challenge_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Challenge_closed_time":1631711922000,
        "Challenge_created_time":1631563330000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Challenge_link_count":5,
        "Challenge_readability":10.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":41.2755555556,
        "Challenge_title":"error when installing AZURE ML training model piece",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":477,
        "Platform":"Github",
        "Solution_body":"Hi @arturoqu77 - thanks for reaching out. We tried to repro this issue but couldn't.\r\n\r\nThis [line of code](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/AzureMLLogonScript.ps1#L266) leverages grep to parse the file name. `grep` should have been installed as part of the [bootstrap](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73). If  `grep` wasn't installed, this implies something must have interrupted the install before it got there.\r\n\r\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working. \r\n\r\nAre you seeing Postman installed - this happens [after `grep`](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73)? If not, this is probably what happened.\r\n\r\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above. Hello,\n\nThank you for your reply. I may have logged on before the bootstrap completed, I re-started the deployment to a new RG and seems to be working now.\n\nThanks for the help.\n\nRegards\n\n***@***.***\nArturo Quiroga\nSr. Cloud Solutions Architect (CSA)\nAzure Applications & Infrastructure\n***@***.******@***.***>\n[MSFT_logo_Gray DE sized SIG1.png]\n\n\nFrom: Raki ***@***.***>\nDate: Tuesday, September 14, 2021 at 6:22 PM\nTo: microsoft\/azure_arc ***@***.***>\nCc: Arturo Quiroga ***@***.***>, Mention ***@***.***>\nSubject: Re: [microsoft\/azure_arc] error when installing AZURE ML training model piece (#758)\n\nHi @arturoqu77<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Farturoqu77&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666347896%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r1kAuKxYlYhONjoSTk83SERggUvNcbP1Hr4vmNh29io%3D&reserved=0> - thanks for reaching out. We tried to repro this issue but couldn't.\n\nThis line of code<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FAzureMLLogonScript.ps1%23L266&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=oAjL%2BfBBF4QXfnwN9gcM9UqEB4OA0ZZrzMuKilatz5A%3D&reserved=0> leverages grep to parse the file name. grep should have been installed as part of the bootstrap<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=V8dzJxj3W5a6IL8T%2BvB0mijBm5Ng4G46bb%2Fcdo2uvz4%3D&reserved=0>. If grep wasn't installed, this implies something must have interrupted the install before it got there.\n\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working.\n\nAre you seeing Postman installed - this happens after grep<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=XvpeFo2T7Kjr4qrIZYKO7eM0khlOddES9O3DGaw1yZ4%3D&reserved=0>? If not, this is probably what happened.\n\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fissues%2F758%23issuecomment-919554382&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ZBGNkrDGFcqvrdWHXy5iEGluQiq2Ph%2BZnfosqC3qTTU%3D&reserved=0>, or unsubscribe<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHV4QUFA72NR7CEJ3UPS5NLUB7DLDANCNFSM5D6SSBHA&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ctEevpiqzC%2FQnTc6ho2hfr2PVGA%2FqwGJzj1pPUCEylY%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cMCZqYPB6q8c9n%2BgPTk9f3MCQr%2BlV4GsOW9iPFSZtgE%3D&reserved=0> or Android<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666387876%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6B5T09q%2Bx2Q2rWftui6b32lD1VLrCRMPiLSrTUS7xnI%3D&reserved=0>.\n Great!",
        "Solution_link_count":11.0,
        "Solution_readability":21.6,
        "Solution_reading_time":96.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":39.0,
        "Solution_word_count":416.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0396927017,
        "Challenge_watch_issue_ratio":0.0166453265
    },
    {
        "Challenge_adjusted_solved_time":1510.63,
        "Challenge_answer_count":1,
        "Challenge_body":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False\r\n\r\nThis behaviour is a bit confusing and I had to debug the code to understand what was happening. I would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what I want that is train locally.\r\n\r\nRepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.DeepSMILECrck \r\n\r\nAlso the histopathology.DeepSMILECrck is not trainable because it does not have a default encoder type. Should we flag base classes as not trainable and throw an error?",
        "Challenge_closed_time":1657547192000,
        "Challenge_created_time":1652108924000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/hi-ml\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":693.0,
        "Challenge_repo_star_count":111.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1510.63,
        "Challenge_title":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":120,
        "Platform":"Github",
        "Solution_body":"Resolved in #420",
        "Solution_link_count":0.0,
        "Solution_readability":0.9,
        "Solution_reading_time":0.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0245310245,
        "Challenge_watch_issue_ratio":0.0101010101
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n\r\nThe introduction section of the 11_exploring_hyperparameters_on_azureml notebook under object detection includes two broken links:\r\n` [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb)`\r\n`[03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb)`\r\n\r\nThe master branch of this repo (which I am working from...please tell me that was intended...) does not contain these notebooks. \r\n\r\n### In which platform does it happen?\r\nAll.\r\n\r\n### How do we replicate the issue?\r\nClick the links\r\n\r\n### Expected behavior (i.e. solution)\r\nNotebooks are present or links are removed\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573072566000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/410",
        "Challenge_link_count":0,
        "Challenge_readability":11.4,
        "Challenge_reading_time":9.05,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Some links to notebooks in introduction are broken in 11_exploring_hyperparameters_on_azureml notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":79,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n\r\nUsers need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. Some guidance within the notebook on how to obtain these values and fill in the strings would be helpful.\r\n\r\nIt would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to `get_or_create_workspace()` later on.\r\n\r\n### Expected behavior with the suggested feature\r\n\r\nUsers who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. Novice users receive some guidance on how to obtain their Azure subscription id without having to reference other notebooks.\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573072237000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/409",
        "Challenge_link_count":0,
        "Challenge_readability":11.8,
        "Challenge_reading_time":11.22,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"[FEATURE_REQUEST] Provide guidance on how to obtain a subscription id in 11_exploring_hyperparameters_on_azureml notebook",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":149,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n\r\nThe 11_exploring_hyperparameters_on_azureml notebook contains the following link in markdown:\r\n`[20_azure_workspace_setup.ipynb](..\/..\/classification\/notebooks\/20_azure_workspace_setup.ipynb)`\r\n\r\nThe link does not resolve properly -- it appears the relative location of the notebook has changed.\r\n\r\n### In which platform does it happen?\r\nAll\r\n\r\n### How do we replicate the issue?\r\nClick the link\r\n\r\n### Expected behavior (i.e. solution)\r\nLink works\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573071661000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/408",
        "Challenge_link_count":0,
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Link to 20_azure_workspace_setup.ipynb in 11_exploring_hyperparameters_on_azureml notebook is broken",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\nThe current version of AzureML is a little dated \r\nhttps:\/\/github.com\/microsoft\/ComputerVision\/blob\/3e0631e0dc7d5ddbfc6283b1e89b3ce51f0bd449\/environment.yml#L41\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573068707000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/404",
        "Challenge_link_count":1,
        "Challenge_readability":17.6,
        "Challenge_reading_time":3.25,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[FEATURE_REQUEST] AzureML may need to be updated 1.0.30->1.0.72?",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":19,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\nIn https:\/\/github.com\/microsoft\/ComputerVision\/blob\/master\/scenarios\/detection\/11_exploring_hyperparameters_on_azureml.ipynb\r\nyou copy the whole directory in order to make use of the utils_cv\r\nThis is a bit cumbersome and unecesarily copies things around. You can create a pip wheel package of your utils_cv and add it as a dependency see here https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#add-private-pip-wheel-workspace--file-path--exist-ok-false-\r\n\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573065169000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/396",
        "Challenge_link_count":2,
        "Challenge_readability":16.8,
        "Challenge_reading_time":7.8,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"[FEATURE_REQUEST] Install utils_cv as a pip wheel in AzureML",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":54,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nThis is the error, it looks it is related to the deployment of ACI and AKS resources. \r\n\r\n\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:35:17.380577', 'duration': 1113.334717, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [26]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-26-21aec20dbbb2> in <module>\r\nE                 1 # Deploy the web service\r\nE           ----> 2 service.wait_for_deployment(show_output=True)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<01:03,  1.01cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:44,  1.40cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:31,  1.92cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:04<01:13,  1.24s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<01:00,  1.07s\/cell]\r\nExecuting:  15%|\u2588\u258c        | 10\/65 [00:05<00:43,  1.27cell\/s]\r\nExecuting:  18%|\u2588\u258a        | 12\/65 [00:05<00:30,  1.72cell\/s]\r\nExecuting:  20%|\u2588\u2588        | 13\/65 [00:06<00:22,  2.26cell\/s]\r\nExecuting:  23%|\u2588\u2588\u258e       | 15\/65 [00:07<00:23,  2.15cell\/s]\r\nExecuting:  26%|\u2588\u2588\u258c       | 17\/65 [00:07<00:16,  2.87cell\/s]\r\nExecuting:  28%|\u2588\u2588\u258a       | 18\/65 [00:13<01:34,  2.00s\/cell]\r\nExecuting:  31%|\u2588\u2588\u2588       | 20\/65 [00:13<01:04,  1.43s\/cell]\r\nExecuting:  32%|\u2588\u2588\u2588\u258f      | 21\/65 [00:15<01:06,  1.51s\/cell]\r\nExecuting:  35%|\u2588\u2588\u2588\u258c      | 23\/65 [00:15<00:45,  1.08s\/cell]\r\nExecuting:  37%|\u2588\u2588\u2588\u258b      | 24\/65 [00:16<00:45,  1.11s\/cell]\r\nExecuting:  38%|\u2588\u2588\u2588\u258a      | 25\/65 [00:18<00:54,  1.37s\/cell]\r\nExecuting:  42%|\u2588\u2588\u2588\u2588\u258f     | 27\/65 [00:18<00:37,  1.01cell\/s]\r\nExecuting:  43%|\u2588\u2588\u2588\u2588\u258e     | 28\/65 [00:20<00:50,  1.37s\/cell]\r\nExecuting:  45%|\u2588\u2588\u2588\u2588\u258d     | 29\/65 [00:21<00:38,  1.07s\/cell]\r\nExecuting:  48%|\u2588\u2588\u2588\u2588\u258a     | 31\/65 [00:22<00:33,  1.01cell\/s]\r\nExecuting:  51%|\u2588\u2588\u2588\u2588\u2588     | 33\/65 [00:22<00:22,  1.39cell\/s]\r\nExecuting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 34\/65 [00:23<00:19,  1.61cell\/s]\r\nExecuting:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 35\/65 [00:23<00:14,  2.11cell\/s]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 37\/65 [00:23<00:10,  2.76cell\/s]\r\nExecuting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 38\/65 [00:23<00:07,  3.41cell\/s]\r\nExecuting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 40\/65 [00:24<00:05,  4.18cell\/s]\r\nExecuting:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 42\/65 [00:24<00:04,  5.02cell\/s]\r\nExecuting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 43\/65 [00:32<00:59,  2.70s\/cell]\r\nExecuting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 44\/65 [11:52<1:12:00, 205.75s\/cell]\r\nExecuting:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 45\/65 [11:52<48:01, 144.08s\/cell]  \r\nExecuting:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 46\/65 [11:53<32:00, 101.08s\/cell]\r\nExecuting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 47\/65 [11:53<21:14, 70.80s\/cell] \r\nExecuting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 48\/65 [11:53<14:03, 49.63s\/cell]\r\nExecuting:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 49\/65 [11:53<09:16, 34.79s\/cell]\r\nExecuting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 50\/65 [11:53<06:05, 24.40s\/cell]\r\nExecuting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 51\/65 [11:56<04:07, 17.70s\/cell]\r\nExecuting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 52\/65 [11:56<02:41, 12.44s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:32<25:30, 127.58s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:33<04:12, 21.01s\/cell] \r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:58:40.389449', 'duration': 1402.445046, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [12]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-12-ea5338712650> in <module>\r\nE                 8         deployment_target = aks_target\r\nE                 9     )\r\nE           ---> 10     aks_service.wait_for_deployment(show_output = True)\r\nE                11     print(f\"The web service is {aks_service.state}\")\r\nE                12 else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n```\r\n\r\nFYI @PatrickBue @jiata any idea of what could be happening?\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1569349581000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/332",
        "Challenge_link_count":12,
        "Challenge_readability":10.6,
        "Challenge_reading_time":263.67,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":217,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Error in o16n with AzureML  notebooks",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":2075,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":263.7483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:40.699401', 'duration': 5.033488, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [2]\":\r\nE           ---------------------------------------------------------------------------\r\nE           SSLError                                  Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1317                 h.request(req.get_method(), req.selector, req.data, headers,\r\nE           -> 1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\r\nE              1238         \"\"\"Send a complete request to the server.\"\"\"\r\nE           -> 1239         self._send_request(method, url, body, headers, encode_chunked)\r\nE              1240 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\nE              1284             body = _encode(body, 'body')\r\nE           -> 1285         self.endheaders(body, encode_chunked=encode_chunked)\r\nE              1286 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\r\nE              1233             raise CannotSendHeader()\r\nE           -> 1234         self._send_output(message_body, encode_chunked=encode_chunked)\r\nE              1235 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_output(self, message_body, encode_chunked)\r\nE              1025         del self._buffer[:]\r\nE           -> 1026         self.send(msg)\r\nE              1027 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in send(self, data)\r\nE               963             if self.auto_open:\r\nE           --> 964                 self.connect()\r\nE               965             else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in connect(self)\r\nE              1399             self.sock = self._context.wrap_socket(self.sock,\r\nE           -> 1400                                                   server_hostname=server_hostname)\r\nE              1401             if not self._context.check_hostname and self._check_hostname:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\nE               406                          server_hostname=server_hostname,\r\nE           --> 407                          _context=self, _session=session)\r\nE               408 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in __init__(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\r\nE               816                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\nE           --> 817                     self.do_handshake()\r\nE               818 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self, block)\r\nE              1076                 self.settimeout(None)\r\nE           -> 1077             self._sslobj.do_handshake()\r\nE              1078         finally:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self)\r\nE               688         \"\"\"Start the SSL\/TLS handshake.\"\"\"\r\nE           --> 689         self._sslobj.do_handshake()\r\nE               690         if self.context.check_hostname:\r\nE           \r\nE           SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           URLError                                  Traceback (most recent call last)\r\nE           <ipython-input-2-2e2a8adec5e2> in <module>\r\nE           ----> 1 learn = model_to_learner(models.resnet18(pretrained=True), IMAGENET_IM_SIZE)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in resnet18(pretrained, progress, **kwargs)\r\nE               229     \"\"\"\r\nE               230     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\nE           --> 231                    **kwargs)\r\nE               232 \r\nE               233 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)\r\nE               215     if pretrained:\r\nE               216         state_dict = load_state_dict_from_url(model_urls[arch],\r\nE           --> 217                                               progress=progress)\r\nE               218         model.load_state_dict(state_dict)\r\nE               219     return model\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)\r\nE               460         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\r\nE               461         hash_prefix = HASH_REGEX.search(filename).group(1)\r\nE           --> 462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\r\nE               463     return torch.load(cached_file, map_location=map_location)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in _download_url_to_file(url, dst, hash_prefix, progress)\r\nE               370 def _download_url_to_file(url, dst, hash_prefix, progress):\r\nE               371     file_size = None\r\nE           --> 372     u = urlopen(url)\r\nE               373     meta = u.info()\r\nE               374     if hasattr(meta, 'getheaders'):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)\r\nE               221     else:\r\nE               222         opener = _opener\r\nE           --> 223     return opener.open(url, data, timeout)\r\nE               224 \r\nE               225 def install_opener(opener):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in open(self, fullurl, data, timeout)\r\nE               524             req = meth(req)\r\nE               525 \r\nE           --> 526         response = self._open(req, data)\r\nE               527 \r\nE               528         # post-process response\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _open(self, req, data)\r\nE               542         protocol = req.type\r\nE               543         result = self._call_chain(self.handle_open, protocol, protocol +\r\nE           --> 544                                   '_open', req)\r\nE               545         if result:\r\nE               546             return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _call_chain(self, chain, kind, meth_name, *args)\r\nE               502         for handler in handlers:\r\nE               503             func = getattr(handler, meth_name)\r\nE           --> 504             result = func(*args)\r\nE               505             if result is not None:\r\nE               506                 return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in https_open(self, req)\r\nE              1359         def https_open(self, req):\r\nE              1360             return self.do_open(http.client.HTTPSConnection, req,\r\nE           -> 1361                 context=self._context, check_hostname=self._check_hostname)\r\nE              1362 \r\nE              1363         https_request = AbstractHTTPHandler.do_request_\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           -> 1320                 raise URLError(err)\r\nE              1321             r = h.getresponse()\r\nE              1322         except:\r\nE           \r\nE           URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)>\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<00:56,  1.14cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:39,  1.58cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:27,  2.16cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:03<01:00,  1.03s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:04<00:47,  1.19cell\/s]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<00:35,  1.59cell\/s]\r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:46.959285', 'duration': 5.817276, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-af5043783823> in <module>\r\nE           ----> 1 docker_image = ws.images[\"image-classif-resnet18-f48\"]\r\nE           \r\nE           KeyError: 'image-classif-resnet18-f48'\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/36 [00:00<?, ?cell\/s]\r\nExecuting:   3%|\u258e         | 1\/36 [00:00<00:30,  1.16cell\/s]\r\nExecuting:  11%|\u2588         | 4\/36 [00:02<00:24,  1.32cell\/s]\r\nExecuting:  19%|\u2588\u2589        | 7\/36 [00:02<00:15,  1.84cell\/s]\r\nExecuting:  25%|\u2588\u2588\u258c       | 9\/36 [00:02<00:10,  2.52cell\/s]\r\nExecuting:  31%|\u2588\u2588\u2588       | 11\/36 [00:03<00:10,  2.47cell\/s]\r\nExecuting:  33%|\u2588\u2588\u2588\u258e      | 12\/36 [00:04<00:16,  1.50cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:12,  1.81cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:09,  2.41cell\/s]\r\n_____________________________ test_23_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_23_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\"23_aci_aks_web_service_testing\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:106: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:53.061402', 'duration': 6.023939, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-883397ed965d> in <module>\r\nE                 1 # Retrieve the web services\r\nE           ----> 2 aci_service = ws.webservices['im-classif-websvc']\r\nE                 3 aks_service = ws.webservices['aks-cpu-image-classif-web-svc']\r\nE           \r\nE           KeyError: 'im-classif-websvc'\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1569234937000,
        "Challenge_created_time":1568285443000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/320",
        "Challenge_link_count":0,
        "Challenge_readability":13.1,
        "Challenge_reading_time":224.21,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":155,
        "Challenge_solved_time":263.7483333333,
        "Challenge_title":"[BUG] pipeline azureml-notebook-test-linux-cpu failing",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":1547,
        "Platform":"Github",
        "Solution_body":"fixed with new pipeline and test machines",
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":343.0208333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Run 2236 in experiment \"master\" in RadiomicsNN: \r\n- Only metrics for 3 out of the 4 GPUs are visible\r\n- The MemAllocated and MemReserved metrics are all zero and hence meaningless.",
        "Challenge_closed_time":1613669349000,
        "Challenge_created_time":1612434474000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/InnerEye-DeepLearning\/issues\/389",
        "Challenge_link_count":0,
        "Challenge_readability":12.9,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":27.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":676.0,
        "Challenge_repo_star_count":471.0,
        "Challenge_repo_watch_count":27.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":343.0208333333,
        "Challenge_title":"Memory utilization metrics are not correctly visible in AzureML",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":37,
        "Platform":"Github",
        "Solution_body":"Root cause: We are hitting the 50 metrics limit, https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-limits-quotas-capacity.\r\nRemoving the meaningless metrics should reduce impact.",
        "Solution_link_count":1.0,
        "Solution_readability":17.2,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0399408284,
        "Challenge_watch_issue_ratio":0.0399408284
    },
    {
        "Challenge_adjusted_solved_time":0.0888888889,
        "Challenge_answer_count":0,
        "Challenge_body":"A regression was introduced in https:\/\/github.com\/augerai\/a2ml\/commit\/c4f89d282fd951defe3e1d51d35386be2c55c7d9#diff-1cd4abe6fbca8804140fbb9b340e3cc8, where this import statement causes azureml.core.authentication to be loaded when it's not needed if you only have the default set of a2ml dependencies installed.\r\n\r\n```\r\n~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages\/a2ml\/api\/azure\/credentials.py\", line 4, in <module>\r\n    from azureml.core.authentication import ServicePrincipalAuthentication, InteractiveLoginAuthentication\r\nModuleNotFoundError: No module named 'azureml'\r\n```\r\n\r\nThe following import statement could be added around L34, right before `InteractiveLoginAuthentication` is called:\r\n\r\n```python\r\nfrom azureml.core.authentication import InteractiveLoginAuthentication\r\n```\r\n\r\nThen this could be removed from the top:\r\n\r\n```python\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication, InteractiveLoginAuthentication\r\n```",
        "Challenge_closed_time":1589931676000,
        "Challenge_created_time":1589931356000,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/175",
        "Challenge_link_count":1,
        "Challenge_readability":19.2,
        "Challenge_reading_time":13.34,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":614.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.0888888889,
        "Challenge_title":"azure credentials module should lazy-import any azureml.core modules",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":85,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0211726384,
        "Challenge_watch_issue_ratio":0.013029316
    },
    {
        "Challenge_adjusted_solved_time":1985.0091666667,
        "Challenge_answer_count":2,
        "Challenge_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Challenge_closed_time":1597072927000,
        "Challenge_created_time":1589926894000,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Challenge_link_count":0,
        "Challenge_readability":8.2,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":614.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1985.0091666667,
        "Challenge_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":"try again pls, I cannot reproduce it with latest azure ml Not able to reproduce now.",
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0211726384,
        "Challenge_watch_issue_ratio":0.013029316
    },
    {
        "Challenge_adjusted_solved_time":8.1033333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Challenge_closed_time":1630110931000,
        "Challenge_created_time":1630081759000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_readability":6.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":270.0,
        "Challenge_repo_star_count":13.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.1033333333,
        "Challenge_title":"Show lightgbm logs in the logs in AzureML",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":27,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0407407407,
        "Challenge_watch_issue_ratio":0.0222222222
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### Steps to reproduce\r\n\r\n1. Create a fresh RAPIDS conda environment <br\/> `conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.8 cudatoolkit=11.5`\r\n2. `conda activate rapids-22.06`\r\n3. `conda list | grep pyarrow` shows 7.0.0 installed\r\n4. Launch python\/ipython and `import cudf` should work\r\n5. `pip install azureml-sdk`\r\n6. Launch python\/ipython and `import cudf` fails\r\n7. `conda list | grep pyarrow` shows 3.0.0 installed\r\n\r\n#### Error:\r\n```\r\n$ python -m cudf\r\nTraceback (most recent call last):\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 144, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/__init__.py\", line 13, in <module>\r\n    from cudf import api, core, datasets, testing\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/datasets.py\", line 7, in <module>\r\n    from cudf._lib.transform import bools_to_mask\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/_lib\/__init__.py\", line 4, in <module>\r\n    from . import (\r\n  File \"cudf\/_lib\/avro.pyx\", line 1, in init cudf._lib.avro\r\n  File \"cudf\/_lib\/column.pyx\", line 1, in init cudf._lib.column\r\n  File \"cudf\/_lib\/scalar.pyx\", line 37, in init cudf._lib.scalar\r\n  File \"cudf\/_lib\/interop.pyx\", line 1, in init cudf._lib.interop\r\nAttributeError: module 'pyarrow.lib' has no attribute 'MonthDayNanoIntervalArray'\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1655998483000,
        "Challenge_link":"https:\/\/github.com\/rapidsai\/cloud-ml-examples\/issues\/165",
        "Challenge_link_count":0,
        "Challenge_readability":11.5,
        "Challenge_reading_time":25.18,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":68.0,
        "Challenge_repo_issue_count":201.0,
        "Challenge_repo_star_count":121.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"azureml-sdk downgrades pyarrow to 3.0.0 which breaks cudf",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":173,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.1094527363,
        "Challenge_watch_issue_ratio":0.0447761194
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"### Describe the issue\r\n\r\n I am trying to run onnxruntime-gpu on an Azure Machine Learning instance with this base image: [openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04](https:\/\/github.com\/Azure\/AzureML-Containers\/blob\/master\/base\/gpu\/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04\/Dockerfile) using CUDA. When trying to create an inference session I get this error:\r\n\r\n```\r\n  File \"\/azureml-envs\/azureml_4ab38fdb3b18635e56f7e63921a429e8\/lib\/python3.9\/site-packages\/onnxruntime\/capi\/onnxruntime_inference_collection.py\", line 347, in __init__\r\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n  File \"\/azureml-envs\/azureml_4ab38fdb3b18635e56f7e63921a429e8\/lib\/python3.9\/site-packages\/onnxruntime\/capi\/onnxruntime_inference_collection.py\", line 395, in _create_inference_session\r\n    sess.initialize_session(providers, provider_options, disabled_optimizers)\r\nRuntimeError: \/onnxruntime_src\/onnxruntime\/core\/providers\/cuda\/cuda_call.cc:122 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] \/onnxruntime_src\/onnxruntime\/core\/providers\/cuda\/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 46: all CUDA-capable devices are busy or unavailable ; GPU=0 ; hostname=c7b375a29d54407dad59b0dd621129a7000000 ; expr=cudaDeviceSynchronize(); \r\n```\r\n\r\nI previously had this working with near identical environments using earlier versions of CUDA (e.g. [11.3](https:\/\/github.com\/Azure\/AzureML-Containers\/blob\/master\/base\/gpu\/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04\/Dockerfile) with onnxruntime-gpu 1.12) which makes me think it is some error between CUDA versions and onnxruntime rather than a general configuration error.\r\n\r\nThe only potential difference I can see is that the docker image uses cudnn 8.4 while the compatibility list for onnxruntime says cudnn 8.2, though I think 8.4 should be back compatible? The error given doesn't really seem to match this case either\r\n\r\nAny advice would be much appreciated!\r\n\r\n### To reproduce\r\n\r\nInstall onnxruntime-gpu in a conda env with the above dockerfile and try to create an inference session\r\n\r\n### Urgency\r\n\r\n_No response_\r\n\r\n### Platform\r\n\r\nLinux\r\n\r\n### OS Version\r\n\r\n20.04\r\n\r\n### ONNX Runtime Installation\r\n\r\nReleased Package\r\n\r\n### ONNX Runtime Version or Commit ID\r\n\r\n1.13 and 1.12\r\n\r\n### ONNX Runtime API\r\n\r\nPython\r\n\r\n### Architecture\r\n\r\nX64\r\n\r\n### Execution Provider\r\n\r\nCUDA\r\n\r\n### Execution Provider Library Version\r\n\r\nCUDA 11.6",
        "Challenge_closed_time":null,
        "Challenge_created_time":1671527243000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/onnxruntime\/issues\/14030",
        "Challenge_link_count":2,
        "Challenge_readability":14.8,
        "Challenge_reading_time":33.45,
        "Challenge_repo_contributor_count":371.0,
        "Challenge_repo_fork_count":1858.0,
        "Challenge_repo_issue_count":13484.0,
        "Challenge_repo_star_count":7928.0,
        "Challenge_repo_watch_count":214.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"CUDA error 46 with CUDA 11.6 on Azure ML Linux image",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":251,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0275140908,
        "Challenge_watch_issue_ratio":0.0158706615
    },
    {
        "Challenge_adjusted_solved_time":12402.8375,
        "Challenge_answer_count":7,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517763000,
        "Challenge_created_time":1617867548000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171.0,
        "Challenge_repo_fork_count":1727.0,
        "Challenge_repo_issue_count":5102.0,
        "Challenge_repo_star_count":12323.0,
        "Challenge_repo_watch_count":282.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":12402.8375,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":1467,
        "Platform":"Github",
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0335162681,
        "Challenge_watch_issue_ratio":0.0552724422
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"If a parent model was trained with the Alignment Enhanced architecture and the dictionary on, preprocessing for the child model will look for the dict.*.txt files (dict.src.txt, dict.trg.txt, dict.vref.txt) from the parent model.  Those files are not currently being copied into the \/tmp directory on the AQUA server when the experiment is launched through ClearML, so preprocessing fails on the child model.\r\n\r\nSample [experiment ](https:\/\/app.pro.clear.ml\/projects\/2243ca6c76d642699db1f28951bbb78a\/experiments\/fc444552b21243149fd3c90a9a4c6c8d\/execution?columns=selected&columns=type&columns=name&columns=tags&columns=status&columns=project.name&columns=users&columns=started&columns=last_update&columns=last_iteration&columns=parent.name&order=-last_update&filter=)with this failure.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1644785881000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/125",
        "Challenge_link_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Child models need to copy the dict.*.txt files from the parent model when launching an experiment on ClearML",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":84,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":4565.0625,
        "Challenge_answer_count":3,
        "Challenge_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Challenge_closed_time":1657980432000,
        "Challenge_created_time":1641546207000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Challenge_link_count":0,
        "Challenge_readability":7.7,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4565.0625,
        "Challenge_title":"Execute translate script without creating ClearML task",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"I think that this error might be preventing me from using the Translate script locally.\r\n\r\nWhen I try I get the following error:\r\nInstalling the current project: silnlp (1.0.0)\r\n(silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate BT-English\/cba-en\/cba-en_cp01 --src-project cba --trg-iso en --books EXO --output-usfm BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT --checkpoint best\r\n2022-06-28 21:02:08,808 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-06-28 21:02:09,149 - silnlp.common.utils - INFO - Git commit: f46a63c3b3\r\nRetrying (Retry(total=239, connect=239, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4556fa0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\nRetrying (Retry(total=238, connect=238, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569190>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n^CRetrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569340>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n\r\nprint(args):\r\nNamespace(books=['EXO'], checkpoint='best', clearml_queue=None, eager_execution=False, end_seq=None, experiment='BT-English\/cba-en\/cba-en_cp01', memory_growth=False, output_usfm='BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT', src=None, src_prefix=None, src_project='cbaNT', start_seq=None, trg=None, trg_iso='en', trg_prefix=None)\r\n Tested this for translating and it worked fine.   (silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate --checkpoint last --src-project tl-TCB --src \/home\/david\/disk2\/gutenberg\/Paratext\/projects\/TCB\/091SAtlASD15.SFM --trg-iso blx --output-usfm \/home\/david\/disk2\/gutenberg\/BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\/results\/091SAAMIU_last.sfm BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\r\n2022-07-16 15:03:40,452 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-07-16 15:03:40,828 - silnlp.common.utils - INFO - Git commit: 8cd5b9c649\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 231, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 225, in main\r\n    translator.translate_text_file(args.src, args.trg_iso, args.trg)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 151, in translate_text_file\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{os.path.basename(src_file_path)}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 79, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 9, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml_connection.py\", line 24, in __post_init__\r\n    self.name = self.name.replace(\"\\\\\", \"\/\")\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":15.1,
        "Solution_reading_time":46.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":282.0,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":4.1744444444,
        "Challenge_answer_count":2,
        "Challenge_body":"I tried to translate with the following command line and trace.\r\nThe command is meant to run locally, but there is an error about ClearML credentials. The ClearML argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project GELA3_2021_11_22 --book OT --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/Gutenberg_new as per environment variable SIL_NLP_DATA_PATH.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - INFO - Git commit: 12aca87cab\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 8, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml.py\", line 27, in __post_init__\r\n    self.task = Task.init(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 164, in __init__\r\n    super(Task, self).__init__(**kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(Task, self).__init__(id=task_id, session=session, log=log)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 131, in __init__\r\n    super(IdObjectBase, self).__init__(session, log, **kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    InterfaceBase._default_session = Session(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise LoginError(\r\nclearml.backend_api.session.session.LoginError: Failed getting token (error 401 from https:\/\/api.pro.clear.ml): Unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1637601038000,
        "Challenge_created_time":1637586010000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/109",
        "Challenge_link_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":56.44,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":4.1744444444,
        "Challenge_title":"Translate is trying to use ClearML even though it was not requested. Preventing translation on local machine.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":275,
        "Platform":"Github",
        "Solution_body":"@davidbaines, Did that fix it? Yes! Thanks so much.",
        "Solution_link_count":0.0,
        "Solution_readability":-1.0,
        "Solution_reading_time":0.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":1943.8975,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130932000,
        "Challenge_created_time":1655132901000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17217.0,
        "Challenge_repo_issue_count":20687.0,
        "Challenge_repo_star_count":76119.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":1943.8975,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":298,
        "Platform":"Github",
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_link_count":5.0,
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":312.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0212210567,
        "Challenge_watch_issue_ratio":0.0415720017
    },
    {
        "Challenge_adjusted_solved_time":75.8813888889,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Challenge_closed_time":1650063297000,
        "Challenge_created_time":1649790124000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Challenge_link_count":1,
        "Challenge_readability":20.9,
        "Challenge_reading_time":56.91,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":75.8813888889,
        "Challenge_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":286,
        "Platform":"Github",
        "Solution_body":"Facing the same issue but with W and B. see https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12529 and https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12714 This was fixed and included in the 1.6.1 release. Could you try upgrading lightning? \r\n`pip install --upgrade pytorch-lightning` \ud83e\udd1f  @awaelchli not the same bug, but my colab link to reproduce the bug is now throwing another error.",
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":7593.3588888889,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRichProgressBar doesn't display progress bar when using Comet logger.\r\nI verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### To Reproduce\r\n```python\r\nimport comet_ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning.loggers import CometLogger\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nlogger = CometLogger(api_key=os.environ.get(\"COMET_API_TOKEN\"))\r\n\r\ntrainer = Trainer(logger=logger, max_epochs=100, callbacks=[RichProgressBar()])\r\n# trainer = Trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### Environment\r\n- PyTorch Lightning Version 1.5.5\r\n- PyTorch Version 1.10.0\r\n- Python version 3.8\r\n- OS Ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @SeanNaren",
        "Challenge_closed_time":1666742778000,
        "Challenge_created_time":1639406686000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/11043",
        "Challenge_link_count":0,
        "Challenge_readability":13.3,
        "Challenge_reading_time":36.09,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":6.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":7593.3588888889,
        "Challenge_title":"RichProgressBar doesn't display progress bar when using Comet logger.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":251,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Is there any update for this bug? Any update on this issue?\r\nI'm experiencing the same problem when using `comet` logger and `RichProgressBar` @ItamarKanter @JackLin-Authme I just tried this and can see the rich progress bar working fine. Is it possible that I am using a newer version of either rich or comet that now fixed the problem? Do you still have documentation of what version(s) you were using?\r\n\r\nI'm closing the issue now, but if you find any more issues related to this we can continue the investigation.  I still experience the issue. Adding more information on this, the progress bar DO show, however only after it has been completed. Moreover, any `rich.print` calls show no color, including the progress bar itself. The only solution I found is to stop using the Comet logger.\r\n\r\npackage versions:\r\npytorch-lightning     1.9.0\r\ncomet-ml                 3.32.0\r\nrich                          13.3.1\r\n\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":15.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":215.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":2330.3280555556,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181493000,
        "Challenge_created_time":1633792312000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":2330.3280555556,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":786,
        "Platform":"Github",
        "Solution_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_link_count":0.0,
        "Solution_readability":6.1,
        "Solution_reading_time":16.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":238.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":3846.9952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Challenge_closed_time":1636988013000,
        "Challenge_created_time":1623138830000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Challenge_link_count":4,
        "Challenge_readability":11.6,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3846.9952777778,
        "Challenge_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":172,
        "Platform":"Github",
        "Solution_body":"@tchaton Is this a lightning issue? Closing this issue as there is no progress nor manifestation from the Comet Team.",
        "Solution_link_count":0.0,
        "Solution_readability":6.0,
        "Solution_reading_time":1.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":2840.7811111111,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600832000,
        "Challenge_created_time":1621374020000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":2840.7811111111,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":325,
        "Platform":"Github",
        "Solution_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_link_count":5.0,
        "Solution_readability":10.5,
        "Solution_reading_time":28.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":260.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":10,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_created_time":1618430187000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":439,
        "Platform":"Github",
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_link_count":1.0,
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":296.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":755.2722222222,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Challenge_closed_time":1615221269000,
        "Challenge_created_time":1612502289000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Challenge_link_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":18.42,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":755.2722222222,
        "Challenge_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":157,
        "Platform":"Github",
        "Solution_body":"Thanks for the report! Mind sending a PR to fix this? cc @Borda  Sorry for the long delay in getting back to you on this issue. I tried to fix it by manually rearranging the imports, with the relevant annotations so that this manual placement would be ignored by `isort`. However, I can't seem to be able to make it work like it used to.\r\n\r\nIn the end, I think it might be better to solve this issue elsewhere for me, either in my own code or upstream with Comet to see if they can improve on their requirement of being imported first. Seems like a pain to solve this.\r\n@nathanpainchaud You can set a env variable `COMET_DISABLE_AUTO_LOGGING=1`, not sure how much it helps or what side effects it has. \r\nJust saw it in the docs [here](https:\/\/www.comet.ml\/docs\/python-sdk\/warnings-errors\/). @awaelchli Thanks for the link! I've not yet tried to disable Comet auto-logging, since I'm a bit fearful about the logging capabilities I might lose.\r\n\r\nI first created the issue here because I thought it might be solved easily by simply reordering the imports in Lightning, but I'm fully aware that would only cover up the symptoms, and not treat the underlying issue. I think the best solution, even if it's ugly IMO, is to manually import Comet at the very beginning of my main script.\r\n\r\nA more permanent resolution to the issue, if possible, should come from upstream. Therefore, I'm closing the issue here, but if anyone as a better idea on how to resolve this issue, they're welcome to re-open it :slightly_smiling_face:  So I have something to add to this which is very strange. I usually run my experiments on a slurm cluster, I just found that when I launch through sbatch I don't get this error, but when I use srun to get a terminal on a node to do some debugging I do get the error. I have no idea why they would be different.",
        "Solution_link_count":1.0,
        "Solution_readability":8.4,
        "Solution_reading_time":21.92,
        "Solution_score_count":2.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":326.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":195.695,
        "Challenge_answer_count":0,
        "Challenge_body":"After https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/2553  there is a changed logger behavior. It starts using `COMET_EXPERIMENT_KEY`. But it doesn't respect it if it is set already.\r\nSo the bug is in the following.\r\nI already set this variable \r\nThen logger overwrites my value here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L189\r\nThen it deletes this variable at all here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L215\r\nThis way it ignores my variable and deletes it at all later\r\nMoreover in version function it also ignores my set variable\r\nI will create a pull request to fix it ",
        "Challenge_closed_time":1603809056000,
        "Challenge_created_time":1603104554000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4229",
        "Challenge_link_count":3,
        "Challenge_readability":12.2,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":195.695,
        "Challenge_title":"Comet logger overrides COMET_EXPERIMENT_KEY env variable",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":86,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":0.8413888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCometmllogger with api key and  without save dir results in error.\r\nThis happens due to this if https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nThis can be fixed by setting _save_dir to None. I will supply PR in a moment\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\n    model = LightningModel({})\r\n    comet_logger = CometLogger(\r\n        api_key=KEY,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = Trainer(logger=comet_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\nTraceback (most recent call last):\r\ntrainer.fit(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/comet.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1599658056000,
        "Challenge_created_time":1599655027000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3417",
        "Challenge_link_count":3,
        "Challenge_readability":12.9,
        "Challenge_reading_time":31.44,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.8413888889,
        "Challenge_title":"CometLogger failing without save_dir",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":206,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue!",
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":0.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":719.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Challenge_closed_time":1591023634000,
        "Challenge_created_time":1588434434000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Challenge_link_count":0,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.8,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":6.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":719.2222222222,
        "Challenge_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":171,
        "Platform":"Github",
        "Solution_body":"@ceyzaguirre4 pls ^^",
        "Solution_link_count":0.0,
        "Solution_readability":8.8,
        "Solution_reading_time":0.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":755.505,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Challenge_closed_time":1591023635000,
        "Challenge_created_time":1588303817000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Challenge_link_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.81,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":755.505,
        "Challenge_title":"Comet logger cannot be pickled after creating an experiment",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":"@ceyzaguirre4 pls ^^ I don't know if it can help or if it is the right place, but a similar error occurswhen running in ddp mode with the WandB logger.\r\n\r\nWandB uses a lambda function at some point.\r\n\r\nDoes the logger have to pickled ? Couldn't it log only on rank 0 at epoch_end ?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"..\/train.py\", line 140, in <module>\r\n    main(args.gpus, args.nodes, args.fast_dev_run, args.mixed_precision, project_config, hparams)\r\n  File \"..\/train.py\", line 117, in main\r\n    trainer.fit(model)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n```\r\n\r\nalso related: \r\n#1704 I had the same error as @jeremyjordan  `can't pickle _thread.lock objects`. This happened when I added the  `logger` and additional `callbacks` in `from_argparse_args`, as explained here https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/hyperparameters.html\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams, logger=logger, callbacks=[PrinterCallback(), ])\r\n```\r\nI could make the problem go away by directly overwriting the members of `Trainer`\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams)\r\ntrainer.logger = logger\r\ntrainer.callbacks.append(PrinterCallback())\r\n``` Same issue as @F-Barto using a wandb logger across 2 nodes with `ddp`. same issue when using wandb logger with ddp same here.. @joseluisvaz your workaround doesn't solve the callback issue.. when I try to add a callback like this it is simply being ignored :\/ but adding it the Trainer init call normally works.. so I'm pretty sure the error is thrown by the logger (I'm using TB) not the callbacks. Same issue, using wandb logger with 8 gpus in an AWS p2.8xlarge machine  With CometLogger, I get this error only when the experiment name is declared. If it is not declared, I get no issue. I still have this error with 1.5.10 on macOS\r\n\r\n```\r\nError executing job with overrides: ['train.pl_trainer.fast_dev_run=False', 'train.pl_trainer.gpus=0', 'train.pl_trainer.precision=32', 'logging.wandb_arg.mode=offline']\r\nTraceback (most recent call last):\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 78, in main\r\n    train(conf)\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 70, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/plugins\/training_type\/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/epoch\/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 198, in __iter__\r\n    self._apply_patch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 133, in _apply_patch\r\n    apply_to_collections(self.loaders, self.loader_iters, (Iterator, DataLoader), _apply_patch_fn)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 181, in loader_iters\r\n    loader_iters = self.dataloader_iter.loader_iters\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 537, in loader_iters\r\n    self._loader_iters = self.create_loader_iters(self.loaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 577, in create_loader_iters\r\n    return apply_to_collection(loaders, Iterable, iter, wrong_dtype=(Sequence, Mapping))\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 104, in apply_to_collection\r\n    v = apply_to_collection(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 96, in apply_to_collection\r\n    return function(data, *args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 177, in __iter__\r\n    self._loader_iter = iter(self.loader)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n``` I still see this bug as well with WandB logger. Currently having this issue with wandbLogger.",
        "Solution_link_count":1.0,
        "Solution_readability":19.8,
        "Solution_reading_time":127.3,
        "Solution_score_count":23.0,
        "Solution_sentence_count":105.0,
        "Solution_word_count":638.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":73.1380555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Challenge_closed_time":1586910754000,
        "Challenge_created_time":1586647457000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Challenge_link_count":1,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":73.1380555556,
        "Challenge_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":95,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! @PyTorchLightning\/core-contributors or @alsrgv mind submitting a PR? good catch! Happy to, but I could use some pointers into what may be broken.  Does logging use aggregation with flush in the end, and that flush is somehow not called for the test pass?  @alexeykarnachev, any ideas? Shall be fixed in #1459 Sorry, guys, totally missed the messages.\r\n@Borda , is anything required from my end? I think it is fine, just if you have an idea why the Github Actions fails\/hangs...\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1459\/checks?check_run_id=584135478",
        "Solution_link_count":1.0,
        "Solution_readability":5.9,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":88.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":703.9719444444,
        "Challenge_answer_count":10,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760093000,
        "Challenge_created_time":1580225794000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":703.9719444444,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":277,
        "Platform":"Github",
        "Solution_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Solution_link_count":1.0,
        "Solution_readability":9.6,
        "Solution_reading_time":29.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":312.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":15.6752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Challenge_closed_time":1576023863000,
        "Challenge_created_time":1575967432000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Challenge_link_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.6752777778,
        "Challenge_title":"Comet PAPI Depreciated",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":44,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":121.7916666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Challenge_closed_time":1573531232000,
        "Challenge_created_time":1573092782000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Challenge_link_count":0,
        "Challenge_readability":22.1,
        "Challenge_reading_time":25.49,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":121.7916666667,
        "Challenge_title":"CometLogger does not implement name() and version() class methods",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":15,
        "Challenge_body":"### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https:\/\/github.com\/ultralytics\/yolov5\/issues) and [discussions](https:\/\/github.com\/ultralytics\/yolov5\/discussions) and found no similar questions.\n\n\n### Question\n\nI am unable to train alway the same error:\r\n\r\npython train.py --img 640 --batch 16 --epochs 5 --data dataset.yaml --weights yolov5s.pt\r\ntrain: weights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\ngithub: skipping check (not a git repository), for updates see https:\/\/github.com\/ultralytics\/yolov5\r\nYOLOv5  2022-11-26 Python-3.9.13 torch-1.13.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http:\/\/localhost:6006\/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Using 'C:\\\\Users\\\\telem\\\\Desktop\\\\Yolo\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nCOMET WARNING: Native output logging mode is not available, falling back to basic output logging\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 633, in <module>\r\n    main(opt)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 527, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 95, in train\r\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\__init__.py\", line 132, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 97, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 234, in check_dataset\r\n    if data_config['path'].startswith(COMET_PREFIX):\r\nKeyError: 'path'\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO: ----------------------------------\r\nCOMET WARNING: Experiment Name is generated at upload time for Offline Experiments unless set explicitly with Experiment.set_name\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Starting saving the offline archive\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload C:\\Users\\telem\\Desktop\\Yolo\\.cometml-runs\\5f05924ec89f489db0356c7c3201ce0f.zip\r\n\r\nI have tested many dataset and alway the same error any advice ?\r\n\n\n### Additional\n\n_No response_",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669476859000,
        "Challenge_link":"https:\/\/github.com\/ultralytics\/yolov5\/issues\/10301",
        "Challenge_link_count":4,
        "Challenge_readability":12.5,
        "Challenge_reading_time":59.08,
        "Challenge_repo_contributor_count":283.0,
        "Challenge_repo_fork_count":12204.0,
        "Challenge_repo_issue_count":9498.0,
        "Challenge_repo_star_count":33712.0,
        "Challenge_repo_watch_count":334.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"Comet Bug: Unable to train on Window 11",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":458,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0297957465,
        "Challenge_watch_issue_ratio":0.035165298
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"Hi! I've been trying the comet.ml integration and I must say this has been a great addition to the framework. \ud83d\ude4c\r\n\r\nI wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that I expected, especially when trying the learning_rate_warmup_epochs option, which I set to 6 as suggested. The learning rate that is plot on comet is the one set in learning_rate, and it's constant for the first epochs.\r\n\r\nCould this be related to this error?\r\n\r\n`COMET ERROR: Failed to extract parameters from Optimizer.init()\r\n`\r\n\r\n**To Reproduce**\r\n1. Setup comet\r\n2. Set  learning_rate_warmup_epochs option to 6\r\n\r\n**Expected behavior**\r\nI expected to see the lr increase in the first 6 epochs, reach the lr set in learning_rate, and eventually decrease, as I set also reduce_learning_rate_on_plateau .\r\n\r\n**Actual behavior**\r\nThe lr is equal to the set learning_rate in the first epochs, and eventually decreases due to reduce_learning_rate_on_plateau .\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1591889615000,
        "Challenge_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/733",
        "Challenge_link_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":123.0,
        "Challenge_repo_fork_count":1021.0,
        "Challenge_repo_issue_count":2798.0,
        "Challenge_repo_star_count":8658.0,
        "Challenge_repo_watch_count":181.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"The learning rate plot in Comet is not the expected one",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":163,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0439599714,
        "Challenge_watch_issue_ratio":0.0646890636
    },
    {
        "Challenge_adjusted_solved_time":347.7586111111,
        "Challenge_answer_count":8,
        "Challenge_body":"**Describe the bug**\r\n\r\nWhen activating the Comet contrib, most of Ludwig log message disappears.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nLaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --comet`\r\n\r\nYou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ V  V \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - Experiment\r\n\r\nExperiment name: experiment\r\nModel name: run\r\nOutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe log messages should be displayed when the Comet contrib is activated.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Fedora\r\n - Version 28\r\n- Python version: 3.6.8\r\n- Ludwig version: 0.1.2\r\n\r\n**Additional context**\r\n\r\nI think the issue is that ludwig is using the root-level logger configured through `logging.basicConfig`. The comet contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/comet.py#L56.\r\n\r\nThose calls happen before any `basicConfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#L461.\r\n\r\nThe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicConfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/Lib\/logging\/__init__.py#L2065. The direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a StreamHandler pointing to `\/dev\/stderr`.\r\n\r\nThe unfortunate side-effect is that calling `basicConfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\nI would recommend moving from using the root logger and configure the logger through `basicConfig` to using a `ludwig` logger and configure it manually, it's not that more complex. I can help if wanted.\r\n\r\nOne last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. That includes requests and is polluting the output. Using a separate logger would also solve this issue.\r\n",
        "Challenge_closed_time":1559077203000,
        "Challenge_created_time":1557825272000,
        "Challenge_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/340",
        "Challenge_link_count":3,
        "Challenge_readability":11.0,
        "Challenge_reading_time":29.59,
        "Challenge_repo_contributor_count":123.0,
        "Challenge_repo_fork_count":1021.0,
        "Challenge_repo_issue_count":2798.0,
        "Challenge_repo_star_count":8658.0,
        "Challenge_repo_watch_count":181.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":347.7586111111,
        "Challenge_title":"Logging issue when activating Comet contrib",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":315,
        "Platform":"Github",
        "Solution_body":"@Lothiraldan thanks for reporting this. It is indeed a bug to fix. Also notifying @dsblank as he is the main contributor of the comet integration.\r\nWould gladly accept your help offer on this. I'd like to avoid having a logger object that is passed around the whole codebase, but apart from that I'm open to suggestions.\r\n @w4nderlust Yes, and @Lothiraldan and I both are on the Comet team, so we have already been discussing this. @Lothiraldan has much expertise in loggers, so I look forward to his suggestions as well. Hi @w4nderlust, thank you for your prompt answer. I forgot to said that I'm working with @dsblank in the Comet team.\r\n\r\nHaving a non-global logger is ideal but not always feasible.\r\n\r\nThe approach I'm using in my projects is the following, use a logger per module: `LOGGER = logging.getLogger(__name__)`. As the __name__ often contains your project name, you get will get loggers like `dulwich`, `dulwich.experiment`, `dulwich.contribs.comet`. I think I have taken this idea from Django https:\/\/docs.djangoproject.com\/en\/2.1\/topics\/logging\/#using-logging.\r\n\r\nThis way you can configure the top-level logger for your project and every other loggers will just propagate the log messages to it and uses the configured handlers. This unlock having different log level on a module basis or even different handlers if needed.\r\n\r\nI would highly recommend having a central function where the top-level logger is configured, something like https:\/\/github.com\/Lothiraldan\/balto\/blob\/master\/balto\/_logging.py#L6, I found it that it really helps for maintaining a coherent logging configuration.\r\n\r\nApart from that, the Ludwig project seems to be only using a StreamHandler right now so there is no much expertise I can give you on the handlers subjects.\r\n\r\nDon't hesitate if you have some questions. Thanks for the detailed explanation @Lothiraldan . @msaisumanth Is on top of it. It looks pretty straightforward: have a single global logger setup function, add a logger in every module, use that logger instead of logging. I expect this to be solved pretty quickly.\r\nThank you again! @Lothiraldan please take a look at https:\/\/github.com\/uber\/ludwig\/pull\/352\r\nI was able to verify that the output is getting printed as expected. @w4nderlust if this makes sense, I'll modify the other modules as well.  @Lothiraldan it would be great if you could take a look at the PR, it should solve the issue, but wanted to doublecheck with you before merging it. We merged the PR as we believe it works fine, @Lothiraldan if you could take a look at it to confirm it's fine for you too, that owuld be great. I made some comments, sorry for the delay, I was busy with some other stuff.",
        "Solution_link_count":3.0,
        "Solution_readability":7.6,
        "Solution_reading_time":32.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":426.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0439599714,
        "Challenge_watch_issue_ratio":0.0646890636
    },
    {
        "Challenge_adjusted_solved_time":8082.0786111111,
        "Challenge_answer_count":3,
        "Challenge_body":"It fails at the \"apply patch\" stage",
        "Challenge_closed_time":1624956881000,
        "Challenge_created_time":1595861398000,
        "Challenge_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.94,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":12.0,
        "Challenge_repo_issue_count":219.0,
        "Challenge_repo_star_count":42.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8082.0786111111,
        "Challenge_title":"Comet \"Reproduce\" feature doesn't work",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":11,
        "Platform":"Github",
        "Solution_body":"is this still an issue @51N84D ? Yeah, this still doesn't work. I don't think anyone has tried to resolve it yet Ok ; should we in your opinion?",
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0410958904,
        "Challenge_watch_issue_ratio":0.0182648402
    },
    {
        "Challenge_adjusted_solved_time":1.09,
        "Challenge_answer_count":1,
        "Challenge_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600309841000,
        "Challenge_created_time":1600305917000,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Challenge_link_count":0,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1.09,
        "Challenge_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":96,
        "Platform":"Github",
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_link_count":3.0,
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.018957346,
        "Challenge_watch_issue_ratio":0.0142180095
    },
    {
        "Challenge_adjusted_solved_time":0.4752777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600153381000,
        "Challenge_created_time":1600151670000,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Challenge_link_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4752777778,
        "Challenge_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":109,
        "Platform":"Github",
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_link_count":3.0,
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.018957346,
        "Challenge_watch_issue_ratio":0.0142180095
    },
    {
        "Challenge_adjusted_solved_time":426.2658333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Challenge_closed_time":1626683431000,
        "Challenge_created_time":1625148874000,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Challenge_link_count":0,
        "Challenge_readability":13.8,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":426.2658333333,
        "Challenge_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":118,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0217391304,
        "Challenge_watch_issue_ratio":0.0093167702
    },
    {
        "Challenge_adjusted_solved_time":2.1180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Challenge_closed_time":1620393602000,
        "Challenge_created_time":1620385977000,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Challenge_link_count":2,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.1180555556,
        "Challenge_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":166,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0217391304,
        "Challenge_watch_issue_ratio":0.0093167702
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"- [ ] fix docstring\r\n- [ ] test with a Node that has `dvc.params` and `dvc.outs`\r\n\r\nhttps:\/\/github.com\/zincware\/ZnTrack\/blob\/cd2c4f05ad5abf2b23da80fe56558cef6c73e636\/zntrack\/zn\/nodes.py#L11-L28",
        "Challenge_closed_time":null,
        "Challenge_created_time":1658850596000,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/348",
        "Challenge_link_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"znNodes not working with `dvc.<...>`",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":17,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0065359477,
        "Challenge_watch_issue_ratio":0.0043572985
    },
    {
        "Challenge_adjusted_solved_time":2.0441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Challenge_closed_time":1643235530000,
        "Challenge_created_time":1643228171000,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Challenge_link_count":0,
        "Challenge_readability":2.8,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.0441666667,
        "Challenge_title":"zn.Method does not add params to `dvc.yaml`",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":24,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0065359477,
        "Challenge_watch_issue_ratio":0.0043572985
    },
    {
        "Challenge_adjusted_solved_time":473.2511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"One can either define a DVC option with default values in the init, which could be considered a constant, or change a DVC option that has no default values in the call method.\r\n\r\nIf a pre-intialized DVC option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Challenge_closed_time":1634716886000,
        "Challenge_created_time":1633013182000,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/76",
        "Challenge_link_count":0,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":473.2511111111,
        "Challenge_title":"raise Error if pre-initialized DVC option is being changed",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0065359477,
        "Challenge_watch_issue_ratio":0.0043572985
    },
    {
        "Challenge_adjusted_solved_time":503.9755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Challenge_closed_time":1621495987000,
        "Challenge_created_time":1619681675000,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Challenge_link_count":0,
        "Challenge_readability":5.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":503.9755555556,
        "Challenge_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":39,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.025,
        "Challenge_watch_issue_ratio":0.0425
    },
    {
        "Challenge_adjusted_solved_time":3357.2136111111,
        "Challenge_answer_count":2,
        "Challenge_body":"After https:\/\/github.com\/iterative\/dvc\/pull\/5265\r\nWe do not allow ignoring lockfile. `dvc-bench` is running currently on some older version of `dvc`, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Challenge_closed_time":1628758546000,
        "Challenge_created_time":1616672577000,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/244",
        "Challenge_link_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":3.07,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3357.2136111111,
        "Challenge_title":"requirements: update dvc",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":34,
        "Platform":"Github",
        "Solution_body":"@pared Sorry, not sure I understand what do we need to update here. Could you elaborate, please? Fixed by #267, forgot to close.",
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.56,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.025,
        "Challenge_watch_issue_ratio":0.0425
    },
    {
        "Challenge_adjusted_solved_time":64.6766666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In every run you can see:\r\n```\r\n               2020-07-03 23:24:19,549 DEBUG: Trying to spawn '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\r\n\/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               2020-07-03 23:24:19,550 DEBUG: Spawned '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef\r\niop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               Unknown mode daemon\r\n```\r\nwe clearly need to take more care on dvc-side, but a good enough workaround is to set CI or DVC_TEST env var to make dvc skip launching the updater.",
        "Challenge_closed_time":1594041670000,
        "Challenge_created_time":1593808834000,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/149",
        "Challenge_link_count":0,
        "Challenge_readability":10.9,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":64.6766666667,
        "Challenge_title":"dvc tries to launch updater using asv script",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":66,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.025,
        "Challenge_watch_issue_ratio":0.0425
    },
    {
        "Challenge_adjusted_solved_time":1034.8597222222,
        "Challenge_answer_count":11,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605521000,
        "Challenge_created_time":1638880026000,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1034.8597222222,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":176,
        "Platform":"Github",
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_link_count":5.0,
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":426.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1103896104,
        "Challenge_watch_issue_ratio":0.0909090909
    },
    {
        "Challenge_adjusted_solved_time":0.4736111111,
        "Challenge_answer_count":7,
        "Challenge_body":"> From https:\/\/github.com\/iterative\/dvc.org\/issues\/1743#issuecomment-730726776\r\n\r\n```console\r\n$ git@github.com:iterative\/example-get-started.git\r\n...\r\n$ cd example-get-started\r\n$ dvc fetch\r\nERROR: failed to fetch data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n```",
        "Challenge_closed_time":1606074573000,
        "Challenge_created_time":1606072868000,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/17",
        "Challenge_link_count":1,
        "Challenge_readability":16.4,
        "Challenge_reading_time":4.1,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":0.4736111111,
        "Challenge_title":"example-get-started is broken with latest DVC",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"What DVC version do you use? It should be be fixed in the most recent one. 1.9.1 on Windows (latest) the latest version is 1.10 something. if it's not updated on Windows then we have a problem with Win releases cc @efiop  Ah I was wrong, you're right. Works with 1.10.1 which I got from https:\/\/github.com\/iterative\/dvc\/releases\/\r\n\r\nThe problem is that the dvc.org home page download button is stuck at 1.9.1 for all platforms, it seems. Opened iterative\/dvc.org\/issues\/1964, resolving here. I use the latest dvc version [DVC version: 1.11.16 (pip)] and have got the same issue while following the [installation](https:\/\/github.com\/iterative\/example-get-started) steps:\r\nOS: Mac OS Mojave 10.14.6\r\n```\r\n$ dvc pull\r\nEverything is up to date.                                             \r\nERROR: failed to pull data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n``` @yakushechkin example-get-started is migrating to dvc 2.0, so it is no longer compatible with older dvc versions. We plan on releasing 2.0 on Wednesday. You could try `pip install --pre dvc` to install dvc pre-release. Sorry for the inconvenience.",
        "Solution_link_count":2.0,
        "Solution_readability":5.0,
        "Solution_reading_time":13.16,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":163.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1103896104,
        "Challenge_watch_issue_ratio":0.0909090909
    },
    {
        "Challenge_adjusted_solved_time":1618.0322222222,
        "Challenge_answer_count":2,
        "Challenge_body":"Experience is broken since every DVC command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Challenge_closed_time":1588739140000,
        "Challenge_created_time":1582914224000,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/12",
        "Challenge_link_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":2.2,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0322222222,
        "Challenge_title":"need to rebuild get-started with the latest DVC version",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"This will be done as part of iterative\/dvc.org\/issues\/599. Shall we close here? Yep.",
        "Solution_link_count":0.0,
        "Solution_readability":4.6,
        "Solution_reading_time":1.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1103896104,
        "Challenge_watch_issue_ratio":0.0909090909
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"When running pull command on DagsHub remote. I receive dvc pull failure, so I have to manually pull dvc again. \n\nThis issue permanent issue on windows. \n\n```bash\nfds clone <remote> \n\n```\n\nIt is not urgent issue, but in annoyance category. ",
        "Challenge_closed_time":null,
        "Challenge_created_time":1647838452000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/121",
        "Challenge_link_count":0,
        "Challenge_readability":5.6,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"fds fails to pull dvc on windows",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":45,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":109.7675,
        "Challenge_answer_count":0,
        "Challenge_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Challenge_closed_time":1630227884000,
        "Challenge_created_time":1629832721000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Challenge_link_count":2,
        "Challenge_readability":9.0,
        "Challenge_reading_time":5.02,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":109.7675,
        "Challenge_title":"DVC and Git services don't correctly detect the repo root directory",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":58,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":575.7216666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1630576282000,
        "Challenge_created_time":1628503684000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Challenge_link_count":0,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.28,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":575.7216666667,
        "Challenge_title":"fsd clone for non-DVC repos throws an error",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":232,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":5.0219444444,
        "Challenge_answer_count":0,
        "Challenge_body":"When running the `fds add` command for data files it tries to add them to DVC tracking but fails.\r\n\r\nIn my case I tried to add the raw-data directory that contains the following image files:\r\n```\r\n$ tree data\/raw-data\r\ndata\/raw-data\r\n\u251c\u2500\u2500 IM-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0003-0001.jpeg\r\n\u251c\u2500\u2500 IM-0005-0001.jpeg\r\n\u251c\u2500\u2500 IM-0006-0001.jpeg\r\n\u251c\u2500\u2500 IM-0007-0001.jpeg\r\n\u251c\u2500\u2500 IM-0009-0001.jpeg\r\n\u251c\u2500\u2500 IM-0010-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0002.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001.jpeg\r\n\u251c\u2500\u2500 IM-0013-0001.jpeg\r\n\u251c\u2500\u2500 IM-0015-0001.jpeg\r\n\u251c\u2500\u2500 IM-0016-0001.jpeg\r\n\u251c\u2500\u2500 IM-0017-0001.jpeg\r\n....\r\n```\r\nBut fds failed to execute the add command:\r\n```\r\n$ fds add data\/raw-data\r\n========== Make your selection, Press \"h\" for help ==========\r\n\r\nDVC add failed to execute\r\n```",
        "Challenge_closed_time":1622139051000,
        "Challenge_created_time":1622120972000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/39",
        "Challenge_link_count":0,
        "Challenge_readability":7.7,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":5.0219444444,
        "Challenge_title":"Fails to add files to DVC tracking",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":81,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":120.5566666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Challenge_closed_time":1622551859000,
        "Challenge_created_time":1622117855000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Challenge_link_count":1,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":120.5566666667,
        "Challenge_title":"Only display the DVC add prompt if there is anything to add",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":40,
        "Platform":"Github",
        "Solution_body":"Fixed in #46 ",
        "Solution_link_count":0.0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":3.7161111111,
        "Challenge_answer_count":0,
        "Challenge_body":"`Should we install dvc[https:\/\/dvc.org\/] (`pip install dvc <3`) for you right now?`\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#L27",
        "Challenge_closed_time":1621785253000,
        "Challenge_created_time":1621771875000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/13",
        "Challenge_link_count":2,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.7161111111,
        "Challenge_title":"Markdown in dvc install prompt isn't rendered as markdown",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":21,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":147.5366666667,
        "Challenge_answer_count":0,
        "Challenge_body":"The problem described in this issue es very similar to #77 .\r\n\r\nCurrently, the \"delete\" action just removes the base image file. This is not correct for some reasons:\r\n\r\n- The base images are under control by DVC. The right way to remove a file that has been previously added to DVC is using its remove command, which removes the file pointer.\r\n- The deletion of the base image is not needed because it is not actually in the repository: it is pushed to the DVC remote storage during the base image generation and does not persist after this finishes. In case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow.\r\n\r\nTo summarize: the right way to do the deletion would be using DVC _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.\r\n",
        "Challenge_closed_time":1643645434000,
        "Challenge_created_time":1643114302000,
        "Challenge_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/79",
        "Challenge_link_count":0,
        "Challenge_readability":9.7,
        "Challenge_reading_time":11.96,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":112.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":147.5366666667,
        "Challenge_title":"Use DVC remove instead of just removing the base image file",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":180,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0357142857,
        "Challenge_watch_issue_ratio":0.0267857143
    },
    {
        "Challenge_adjusted_solved_time":127.7283333333,
        "Challenge_answer_count":0,
        "Challenge_body":"The current implementation of the rename action (to be triggered when the \"rename\" section of the DVC diff contains elements) includes the actual rename of the base image using the shutils' mv command. \r\n\r\n```python\r\nguard_that_base_image_exists(base_filename_old)\r\ncreate_output_folder(base_filename_new)\r\nmove(f\"{base_filename_old}\", f\"{base_filename_new}\")\r\n```\r\n\r\nThis rename is to be committed afterwards so that the rename of the base image is applied to the main branch.\r\n\r\nHowever, this approach is invalid:\r\n\r\n- If only the actual file is renamed, when a DVC pull is performed, the file with the previous name will be pulled. We will get two identical files with different names.\r\n- Nor can we just rename the pointer (.dvc file), as the pointer file name is irrelevant to DVC. The _path_ property inside the pointer is what determines the filename of the pulled file.\r\n\r\nThe right, convenient way to implement the file rename action is using the **dvc rename** command that performs all these actions:\r\n\r\n- Rename the actual file\r\n- Rename the pointer\r\n- Update the _path_ property\r\n\r\nFor consistency, we should use our DVC wrapper. If the move command is not wrapped there, we can do it as part of this issue.",
        "Challenge_closed_time":1643122571000,
        "Challenge_created_time":1642662749000,
        "Challenge_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/77",
        "Challenge_link_count":0,
        "Challenge_readability":9.7,
        "Challenge_reading_time":15.35,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":112.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":127.7283333333,
        "Challenge_title":"Use DVC move instead of system's mv in rename action",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":194,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0357142857,
        "Challenge_watch_issue_ratio":0.0267857143
    },
    {
        "Challenge_adjusted_solved_time":94.8744444444,
        "Challenge_answer_count":1,
        "Challenge_body":"load_dataset function from hugging face can't access the dvc tracked data directory \r\n--> OSError: [Errno 30] Read-only file system: '\/data'",
        "Challenge_closed_time":1642070875000,
        "Challenge_created_time":1641729327000,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/11",
        "Challenge_link_count":0,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":94.8744444444,
        "Challenge_title":"data loading bug with dvc",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":23,
        "Platform":"Github",
        "Solution_body":"What command are you using? Note `\/data` is not same as `.\/data`",
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":0.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0210526316,
        "Challenge_watch_issue_ratio":0.0105263158
    },
    {
        "Challenge_adjusted_solved_time":0.4325,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1638706309000,
        "Challenge_created_time":1638704752000,
        "Challenge_link":"https:\/\/github.com\/se4ai2122-cs-uniba\/CT-COVID\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_readability":5.2,
        "Challenge_reading_time":0.66,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":66.0,
        "Challenge_repo_star_count":1.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":0.4325,
        "Challenge_title":"Missing params field for evaluate stage in dvc.yaml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":8,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0606060606,
        "Challenge_watch_issue_ratio":0.0151515152
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"CALL DVC-CC INIT just takes the first three letters of the repo name???\r\n\r\nHere you can enter the folder where you want to store the DVC files on the DVC Storage Server.\r\n\tThe remote DVC folder that you want use (default: ~\/*****\/***\/TES): \r\nThe username with that you can access the DVC storage server \"dt1.f4.htw-berlin.de\".\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1584006633000,
        "Challenge_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/28",
        "Challenge_link_count":0,
        "Challenge_readability":6.0,
        "Challenge_reading_time":4.69,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":30.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"\"dvc-cc init\" just take three letters for the dvc folder name?",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":64,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1333333333,
        "Challenge_watch_issue_ratio":0.0666666667
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n> Entsprechend dem Tutorial habe ich mit sshfs den data Ordner\r\n> gemountet, um externe Daten verwenden zu k\u00f6nnen, was soweit auch\r\n> funktioniert.\r\n> Wenn ich dann aber nach erfolgreich abgeschlossenem Job die Ergebnisse\r\n> ansehen will (git pull, git checkout rcc_00XX_ergebnis_branch, dvc\r\n> pull), bekomme ich eine Fehlermeldung:\r\n> \r\n> rmdir: data: Das Ger\u00e4t oder die Ressource ist belegt\r\n> \r\n> Wenn ich vorher mit fusermount -u data den Dataordner wieder unmounte,\r\n> funktioniert alles wie erwartet. Ist das das zu erwartende Verhalten?\r\n> Muss ich also \"data\" unmounten, um die Ergebnisse ansehen zu k\u00f6nnen?\r\n> Und dann erneut mounten, um einen neuen Job zu starten?\r\n\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66",
        "Challenge_closed_time":null,
        "Challenge_created_time":1583006053000,
        "Challenge_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_readability":6.2,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":30.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"\"dvc pull\" does not work in the result branch if a sshfs connection is mounted",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1333333333,
        "Challenge_watch_issue_ratio":0.0666666667
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nIf the dvc\/config file is created with whitespaces dvc-cc cann't read the config file.\r\n\r\n**To Reproduce**\r\nCreate a dvc\/config file like this:\r\n`[core]\r\n    remote = dvc_connection\r\n['remote \"dvc_connection\"']\r\n    url = ...............\r\n    ask_password = true\r\n\r\n**Additional context**\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1583005907000,
        "Challenge_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/26",
        "Challenge_link_count":0,
        "Challenge_readability":6.7,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":30.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"dvc servername and url not found by calling \"dvc-cc run\"",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1333333333,
        "Challenge_watch_issue_ratio":0.0666666667
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"UPDATE: Summary in https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20#issuecomment-1164570090\r\n\r\nI cloned https:\/\/github.com\/iterative\/dvc-checkpoints-mnist. I setup the IDE workspace so the extension is active.\r\n\r\nI haven't run any experiments:\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174509065-ac8f2c97-0d7f-4b1f-b6c4-e36603406c50.png)\r\n\r\nI check out the [`make_checkpoint`](https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/tree\/make_checkpoint) branch. The DVC view and Plots Dashboard never load.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174508899-c1e5788a-2ead-446d-bab6-0239cbc27519.png)\r\n\r\nThe Experiments Table says \"No Experiments to Display.\"\r\n\r\nOther components do load.\r\n\r\nDVC virtual env is loaded via MS Python extension.\r\n\r\n```console\r\n$ dvc version\r\nDVC version: 2.11.0 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.13 on macOS-12.4-arm64-arm-64bit\r\nSupports:\r\n        webhdfs (fsspec = 2022.5.0),\r\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\r\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)\r\nCache types: <https:\/\/error.dvc.org\/no-dvc-cache>\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk3s1s1\r\nRepo: dvc, git\r\n```\r\n\r\n---\r\n\r\n~~p.s. the same happens in the included `demo\/` project if I set up the extension with `\"dvc.dvcPath\": \"demo\/.env\/bin\/dvc\"` in .vscode\/settings.json (no MS Python extension).~~",
        "Challenge_closed_time":null,
        "Challenge_created_time":1655687938000,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20",
        "Challenge_link_count":6,
        "Challenge_readability":10.4,
        "Challenge_reading_time":18.61,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":20.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC View and Plots don't load in `vscode-dvc`",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":131,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.25,
        "Challenge_watch_issue_ratio":0.9
    },
    {
        "Challenge_adjusted_solved_time":9316.4808333333,
        "Challenge_answer_count":4,
        "Challenge_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Challenge_closed_time":1668173479000,
        "Challenge_created_time":1634634148000,
        "Challenge_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Challenge_link_count":0,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9316.4808333333,
        "Challenge_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":46,
        "Platform":"Github",
        "Solution_body":"So:\r\n1. You will need to have aws credentials set up with `aws configure`, having installed awscli (which is in the virtualenv)\r\n2. I'm having some issues getting the mantisnlp-public bucket to be accessible to dvc with a non mantis aws profile. I don't know if this is related but did you try `--acl public-read`? I had some problems with public buckets in grants tagger and for me it was resolved by adding this flag. example https:\/\/github.com\/wellcometrust\/grants_tagger\/blob\/970abbc63b448c4d14d7b70fa13ca29760a897ce\/Makefile#L94 I've done this at the bucket level, not at the individual object level, because they are added by dvc. It _should_ be working... btw this issue is probably badly named because:\r\n1. You only need to set `AWS_PROFILE` if you have more than one set of aws credentials\r\n2. You can also set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the folder to the same effect, and users can decide how best to do this.",
        "Solution_link_count":1.0,
        "Solution_readability":7.4,
        "Solution_reading_time":11.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0714285714,
        "Challenge_watch_issue_ratio":0.0714285714
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"If the DVC remote name is left blank, the post-gen hook shouldn't try to set one. Currently this raises a (non-fatal) error.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1623947751000,
        "Challenge_link":"https:\/\/github.com\/adamtupper\/cookiecutter-lvsn-workflow\/issues\/9",
        "Challenge_link_count":0,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":0.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":0.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Post-gen hook shouldn't configure a DVC remote if no name is provided",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":33,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0,
        "Challenge_watch_issue_ratio":0.1428571429
    },
    {
        "Challenge_adjusted_solved_time":0.9458333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Challenge_closed_time":1633697794000,
        "Challenge_created_time":1633694389000,
        "Challenge_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_readability":7.7,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":3.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.9458333333,
        "Challenge_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":191,
        "Platform":"Github",
        "Solution_body":"Thanks for the catch @mike0sv ! No trouble (literally, no trouble at all since it was @mnrozhkov :))",
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.6666666667,
        "Challenge_watch_issue_ratio":6.0
    },
    {
        "Challenge_adjusted_solved_time":68.5333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Challenge_closed_time":1664786016000,
        "Challenge_created_time":1664539296000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Challenge_link_count":0,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.5333333333,
        "Challenge_title":"kedro mlflow ui gets a FileNotFoundError",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI am sorry to see you are experiencing issues. this is not a dummy question, it sounds like a bug. \r\n\r\nI've just ran this: \r\n\r\n```bash\r\nconda create -n km-361 python=3.9 -y\r\nconda activate km-361\r\npip install kedro==0.18.3\r\npip install mlflow==1.29.0\r\npip install kedro-mlflow==0.11.3\r\nkedro new --starter=pandas-iris\r\ncd iris\r\nkedro mlflow init\r\nkedro mlflow ui\r\n```\r\n\r\nthen I opened ``http:\/\/127.0.0.1:5000`` and th UI opened as expected. \r\n\r\nCan you tell me: \r\n- your python version\r\n- your OS\r\n- your ``kedro`` \/ ``mlflow`` \/ ``kedro-mlflow`` version\r\n- the project using\r\n- the exact error message\r\n- check if you have a ``MLFLOW_TRACKING_URI`` environment set It turned out fine  after trying again! Sorry and thanks for your consideration!",
        "Solution_link_count":1.0,
        "Solution_readability":5.1,
        "Solution_reading_time":8.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_created_time":1656532075000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_created_time":1652380533000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":185,
        "Platform":"Github",
        "Solution_body":"Closed by #313 ",
        "Solution_link_count":0.0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1326.7408333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi @Galileo-Galilei\r\n\r\n## Description\r\nthe KedroPipelineModel has a `initial_catalog` property which causes some problems. This `initial_catalog` can contain some Kedro Datasets but it's not necessary to log them when you train your model. because of this property I can't load my model anymore. I have to train it again.\r\n\r\nI explain : when I trained my model I used a kedro home-made plugin to load a specific dataset (which has no impact for my model). After that, I updated this plugin independently of my ML project. Today, I want to load my model but I can't because the load function uses the old Kedro Catalog with my old plugin version which is not in my environnement anymore. \r\n\r\n## Context\r\nIt would be great if we can update the kedro-catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models.\r\n\r\n## Possible Implementation\r\nLog in Mlflow what is only necessary.\r\n\r\nI hope my issue is clear.\r\n\r\nthank you",
        "Challenge_closed_time":1644791409000,
        "Challenge_created_time":1640015142000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/273",
        "Challenge_link_count":0,
        "Challenge_readability":8.8,
        "Challenge_reading_time":12.57,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1326.7408333333,
        "Challenge_title":"KedroPipelineModel requires unnecessary pipeline input dependencies to be executed",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":168,
        "Platform":"Github",
        "Solution_body":"Hi, I can reproduce the issue, thank you very much for the feedback. To clarify, what happens here is the following: \r\n\r\n- the input of your inference pipeline is persisted in Kedro because you load it from the disk (e.g., pandas.ExcelDataSet)\r\n- after you log it in mlflow, it will be converted to a ``MemoryDataSet``, and you directly pass a pandas Dataframe when you want to reuse it. Mlflow complains that you need to have ``openpyxl`` installed, while you never use it in your pipeline, and you don't need it to predict.\r\n\r\nThis extra dependency is not useful as you mention. I will remove it in a patch release soon.\r\n\r\n For anyone having the same issue, notice that you can now export a pipeline as a mlflow model with the [``kedro mlflow modelify``](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/05_pipeline_serving\/03_cli_modelify.html) command.",
        "Solution_link_count":1.0,
        "Solution_readability":9.5,
        "Solution_reading_time":10.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":132.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_created_time":1619193727000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":273,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":7.0,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_created_time":1617627646000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_created_time":1610404594000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_created_time":1605983313000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":137,
        "Platform":"Github",
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_created_time":1605982845000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":0.6055555556,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Challenge_closed_time":1605720463000,
        "Challenge_created_time":1605718283000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Challenge_link_count":0,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.6055555556,
        "Challenge_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":218,
        "Platform":"Github",
        "Solution_body":"The issue is not confirmed and was due to adding a Pipeline and a PipelineML object.\r\nI close it.",
        "Solution_link_count":0.0,
        "Solution_readability":2.3,
        "Solution_reading_time":1.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":0.935,
        "Challenge_answer_count":1,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893558000,
        "Challenge_created_time":1601890192000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":0.935,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":212,
        "Platform":"Github",
        "Solution_body":"I see its fixed now so I'm closing this issue.",
        "Solution_link_count":0.0,
        "Solution_readability":2.5,
        "Solution_reading_time":0.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_created_time":1593379921000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Since kedro 0.17.7(?) there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled.\r\n\r\nUnless the function to create kfp artifacts is disabled in `kubeflow.yaml` config:\r\n```yaml\r\nstore_kedro_outputs_as_kfp_artifacts: False\r\n```\r\nIt causes issues like:\r\n```\r\nValueError: Only letters, numbers, spaces, \"_\", and \"-\" are allowed in name. Must begin with a letter. Got name: data_science.active_modelling_pipeline.X_train\r\n```\r\nwhen trying to run or update the pipeline.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1658927398000,
        "Challenge_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/160",
        "Challenge_link_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.99,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":207.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Add support for kedro namespaces in data catalog",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0531400966,
        "Challenge_watch_issue_ratio":0.0628019324
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # NOQA\r\n        from kedro_mlflow.framework.context import get_mlflow_config  # NOQA\r\n        return True\r\n    except ImportError:\r\n        return False\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Challenge_closed_time":null,
        "Challenge_created_time":1643989114000,
        "Challenge_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/102",
        "Challenge_link_count":0,
        "Challenge_readability":10.1,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":207.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Plugin only compatible with kedro-mlflow<0.8.0",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":34,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0531400966,
        "Challenge_watch_issue_ratio":0.0628019324
    },
    {
        "Challenge_adjusted_solved_time":213.8297222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nKedro Telemetry installed alongside a packaged and installed Kedro project breaks the project by assuming that the `pyproject.toml` file exists. The `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases.\r\n\r\nThe problem was introduced with https:\/\/github.com\/kedro-org\/kedro-plugins\/pull\/62\r\n\r\n## Context\r\nWhen deploying Kedro projects and if you have installed Kedro Telemetry, it breaks your project.\r\n\r\n## Steps to Reproduce\r\n1. Create a Kedro project\r\n2. Add a dependency on kedro-telemetry\r\n3. Package it through `kedro package`\r\n4. Install it in a different environment\r\n5. Run the project through `.\/<project>` in a folder where only the `conf\/` is\r\n\r\n## Expected Result\r\nThe project should run.\r\n\r\n## Actual Result\r\nAn exception is thrown.\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.18.x\r\n* Kedro plugin and kedro plugin version used (`pip show kedro-telemetry`): 0.2.2 \r\n* Python version used (`python -V`): Not relevant\r\n* Operating system and version: Not relevant\r\n",
        "Challenge_closed_time":1670415546000,
        "Challenge_created_time":1669645759000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/83",
        "Challenge_link_count":1,
        "Challenge_readability":7.5,
        "Challenge_reading_time":15.64,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":213.8297222222,
        "Challenge_title":"Kedro Telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":182,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":295.4369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\nInstalling `kedro-datasets[option]` installs a different set of dependencies than `kedro[option]`. It appears that `kedro-datasets[option]` is installing the superset of requirements for all datasets.\n\n## Context\nThis is currently blocking https:\/\/github.com\/kedro-org\/kedro\/issues\/1495\n\n## Steps to Reproduce\n1. `pip install \"kedro[pandas.CSVDataSet]\"; pip freeze > requirements-kedro.txt`\n2. `pip install \"kedro-datasets[pandas.CSVDataSet]\"; pip freeze > requirements-kedro-datasets.txt`\n3. Compare the requirements\n",
        "Challenge_closed_time":1667929584000,
        "Challenge_created_time":1666866011000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/64",
        "Challenge_link_count":1,
        "Challenge_readability":12.7,
        "Challenge_reading_time":8.27,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":295.4369444444,
        "Challenge_title":"pip installing kedro-datasets[option] causes different dependencies to installing kedro[option]",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":1225.01,
        "Challenge_answer_count":1,
        "Challenge_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1668830449000,
        "Challenge_created_time":1664420413000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Challenge_link_count":1,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.78,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1225.01,
        "Challenge_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":222,
        "Platform":"Github",
        "Solution_body":"I think you are missing the data from the catalog.\r\n\r\n```yml\r\nexample_iris_data:\r\n  type: pandas.CSVDataSet\r\n  filepath: data\/01_raw\/iris.csv\r\nexample_train_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_x.pkl\r\nexample_train_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_y.pkl\r\nexample_test_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_x.pkl\r\nexample_test_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_y.pkl\r\nexample_model:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/06_models\/example_model.pkl\r\nexample_predictions:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/07_model_output\/example_predictions.pkl\r\n```\r\n\r\nSee https:\/\/kedro.readthedocs.io\/en\/stable\/deployment\/airflow_astronomer.html?highlight=astro-airflow-iris\r\n\r\nCan you provide the steps to reproduce the issue? What versions of `kedro`, `kedro-airflow` are you using and what commands did you run?\r\n",
        "Solution_link_count":1.0,
        "Solution_readability":17.8,
        "Solution_reading_time":12.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":71.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":12,
        "Challenge_body":"Raised by @jweiss-ocurate:\r\n\r\n## Description\r\nI am trying to run a simple spaceflights example with Astrocloud. I wasn't sure if anyone has been able to get it to work. \r\n\r\nHere is the DockerFile:\r\nFROM quay.io\/astronomer\/astro-runtime:4.1.0\r\n\r\nRUN pip install --user new_kedro_project-0.1-py3-none-any.whl --ignore-requires-python\r\n\r\n## Context\r\nI am trying to use kedro-airflow with astrocloud.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Follow directions here https:\/\/kedro.readthedocs.io\/en\/latest\/10_deployment\/11_airflow_astronomer.html\r\n2. Replace the DockerFile with the above mentioned image.\r\n\r\n## Expected Result\r\nComplete Kedro Run on local Airflow image.\r\n\r\n## Actual Result\r\nFailure in local Airflow image.\r\n[2022-02-26, 16:43:26 UTC] {store.py:32} INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\r\n[2022-02-26, 16:43:26 UTC] {session.py:78} WARNING - Unable to git describe \/usr\/local\/airflow\r\n[2022-02-26, 16:43:29 UTC] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment you experienced the bug in:\r\n\r\n* Kedro-Airflow plugin version used (get it by running `pip show kedro-airflow`): 0.4.1\r\n* Airflow version (`airflow --version`):\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.17.7\r\n* Python version used (`python -V`): > 2.0.0\r\n* Operating system and version: Ubuntu Linux 20.04",
        "Challenge_closed_time":null,
        "Challenge_created_time":1648473272000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/13",
        "Challenge_link_count":1,
        "Challenge_readability":8.9,
        "Challenge_reading_time":18.22,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro-Airflow not working with Astrocloud",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":174,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nRunning the kedro-pipeline \"dp\" via kedro-cli with \"kedro run --pipeline dp\" results in the following error:\r\n```cmd\r\nkedro.io.core.DataSetError: Failed while loading data from data set TextDataSet(filepath=C:\/EEAA\/Repos\/QuaselDoku\/data\/01_raw\/Doku_v1\/Bedienung\/EasyInsert.html, protocol=file).\r\n'charmap' codec can't decode byte 0x81 in position 5899: character maps to <undefined>\r\n```\r\n\r\n### Steps to reproduce\r\ncatalog.yml:\r\n```yml\r\necu_test_doku:\r\n  type: PartitionedDataSet\r\n  path: data\/01_raw\/Doku_v1\r\n  dataset: text.TextDataSet\r\n  filename_suffix: html\r\n```\r\n\r\npython-function to parse documentation-data:\r\n```python\r\ndef filter_doku(partitioned_input: Dict[str, Callable[[], Any]], params: Dict) -> Dict[str, Callable[[], Any]]:\r\n    \"\"\"\r\n    flatten input where html files can occur on multiple levels, as well as filter out files that match certain string.\r\n    Return new Dictionary with filenames and load functions from which a PartioniedDataset can be created and persisted.\r\n\r\n    Args:\r\n        partitioned_input: A dictionary with partition ids (file path) as keys and load functions as values.\r\n\r\n    Returns:\r\n        Dictionary with the partitions to create.\r\n    \"\"\"\r\n\r\n    result = {}\r\n\r\n    print(\"filtering out relevant html files from doku ...\")\r\n    for partition_key, partition_load_func in tqdm(sorted(partitioned_input.items())):\r\n        \r\n        exclude = False\r\n        for string in params['exclude_docs']:\r\n            \r\n            if string in partition_key:\r\n                \r\n                exclude = True\r\n                break\r\n\r\n        if exclude:\r\n            continue\r\n\r\n        filename = partition_key.replace('\/', ' ')\r\n        filename += 'html'\r\n\r\n        # append new filename with load function to results dictionary\r\n        result[filename] = partition_load_func\r\n\r\n    return result\r\n```\r\n\r\nkedro  0.18.0\r\n\r\nThanks! :)\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1654682698000,
        "Challenge_link":"https:\/\/github.com\/quaseldoku\/QuaselDoku\/issues\/1",
        "Challenge_link_count":0,
        "Challenge_readability":12.5,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":1.0,
        "Challenge_repo_star_count":1.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Running kedro-pipeline \"dp\" results in \"character maps to <undefined>\"-error",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":188,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":4.0,
        "Challenge_watch_issue_ratio":1.0
    },
    {
        "Challenge_adjusted_solved_time":2083.0769444444,
        "Challenge_answer_count":1,
        "Challenge_body":"### Summary\r\n\r\nProfiling with mlflow and without an mlflow writer fails silently. \r\n\r\n### Steps to Reproduce it\r\n\r\nuse mlflow with get_or_create_session and no files are written.\r\n\r\n### Example\r\n\r\nThere are examples of how to configure mlflow writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_mlflow.yaml\r\n\r\nwhylogs should mention the missing mlflow writer in a warning. Maybe we can automatically add the mlflow writer (with a warning), so that it works and draws attention to where the behavior can be modified.\r\n\r\n## What is the current *bug* behavior?\r\n\r\nlogging with mlflow and default configuration appears to fail silently.\r\n\r\n### What is the expected *correct* behavior?\r\n\r\nmlflow integration should write to mlflow by default and warn if missing or inconsistent config is set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1655127386000,
        "Challenge_created_time":1647628309000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/480",
        "Challenge_link_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2083.0769444444,
        "Challenge_title":"using mlflow without an mlflow writer configured appears to fail silently",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":"This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_link_count":0.0,
        "Solution_readability":3.5,
        "Solution_reading_time":0.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":2491.9302777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Challenge_closed_time":1655127391000,
        "Challenge_created_time":1646156442000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Challenge_link_count":0,
        "Challenge_readability":12.1,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2491.9302777778,
        "Challenge_title":"Support writing out dataset profiles as json format with mlflow",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"How can i retrieve the profile while inside the start_run()? This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_link_count":0.0,
        "Solution_readability":2.8,
        "Solution_reading_time":1.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":3662.6730555556,
        "Challenge_answer_count":5,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127397000,
        "Challenge_created_time":1641941774000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3662.6730555556,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_link_count":2.0,
        "Solution_readability":11.3,
        "Solution_reading_time":25.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":261.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":23.5547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Challenge_closed_time":1603222865000,
        "Challenge_created_time":1603138068000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.1,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":23.5547222222,
        "Challenge_title":"MLFlow NameError",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":108,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":3836.5522222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the `.drone.yaml` is referencing the k8s secret `mlflow-server-secret` which doesn't exist by default.\r\n\r\nWe have noticed that `{{ .ProjectID }}-mlflow-secret` secret is created when a `kdlProject` resource is created.\r\n\r\nTo solve this issue the name of the `mlflow-server-secret` must be changed into `{{ .ProjectID }}-mlflow-secret`",
        "Challenge_closed_time":1664791991000,
        "Challenge_created_time":1650980403000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/810",
        "Challenge_link_count":0,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3836.5522222222,
        "Challenge_title":"Project template mlflow secret bad name",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":49,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":122.8697222222,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871931000,
        "Challenge_created_time":1635429600000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":122.8697222222,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":243.3152777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Only allow access to project members for the given MLflow.",
        "Challenge_closed_time":1620648494000,
        "Challenge_created_time":1619772559000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/404",
        "Challenge_link_count":0,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.2,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":243.3152777778,
        "Challenge_title":"Users can access to any MLflow project",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":16,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":1124.7483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Challenge_closed_time":1623230636000,
        "Challenge_created_time":1619181542000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Challenge_link_count":0,
        "Challenge_readability":10.1,
        "Challenge_reading_time":1.51,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1124.7483333333,
        "Challenge_title":"Bad MLflow artifact folder by default",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":17,
        "Platform":"Github",
        "Solution_body":"This problem has been solved adding the variable of `$ARTIFACTS_BUCKET` between `()` like this `$(ARTIFACTS_BUCKET)` in the deployment.yaml of the project-operator.",
        "Solution_link_count":0.0,
        "Solution_readability":11.9,
        "Solution_reading_time":2.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_created_time":1619181390000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":80,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":71.3516666667,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nI try to do multi-label classification with \"doc_classification_multilabel.py\". It worked at first. However when it came to `\"Train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n......\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n```\r\n\r\n  I have checked that the Internet connection was ok. So I was confused why this error occured ?\r\n  \r\n\r\n**Error message**\r\nError that was thrown (if available)\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n**System:**\r\n - OS: \r\n - GPU\/CPU:\r\n - FARM version:\r\n",
        "Challenge_closed_time":1580393757000,
        "Challenge_created_time":1580136891000,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/FARM\/issues\/217",
        "Challenge_link_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.8,
        "Challenge_repo_contributor_count":36.0,
        "Challenge_repo_fork_count":231.0,
        "Challenge_repo_issue_count":844.0,
        "Challenge_repo_star_count":1598.0,
        "Challenge_repo_watch_count":56.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":71.3516666667,
        "Challenge_title":"MLFlowLogger: \"Connection aborted.\" - RemoteDisconnected Error",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":177,
        "Platform":"Github",
        "Solution_body":"Hey @JiangYanting, \r\n\r\nAre you using our public mlflow server for logging (i.e. `ml_logger = MLFlowLogger(tracking_uri=\"https:\/\/public-mlflow.deepset.ai\/\")\r\n` in doc_classification_multilabel.py)? \r\n\r\nI would assume that your connection to that server was not available when the model tried to log the train_loss at step 17251. \r\n\r\nI see two solutions:\r\n- short term: you can log locally by setting `ml_logger = MLFlowLogger(tracking_uri=\"\")`\r\n- mid term: implementing a fix in FARM, so that we raise only a warning, if the logging doesn't succeed, but let the training continue. Let me know if you are interested in adding a PR for this. Otherwise, we can take care. It would be basically a try \/ catch block here: https:\/\/github.com\/deepset-ai\/FARM\/blob\/master\/farm\/utils.py#L126 @tholor By setting `ml_logger = MLFlowLogger(tracking_uri=\"\")` , it works. Thank you very much ! ^_^",
        "Solution_link_count":2.0,
        "Solution_readability":8.4,
        "Solution_reading_time":10.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":117.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0426540284,
        "Challenge_watch_issue_ratio":0.0663507109
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"```\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py\", line 188, in log\r\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 584, in log_artifacts\r\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 977, in log_artifacts\r\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 334, in log_artifacts\r\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py\", line 57, in log_artifacts\r\n    mkdir(artifact_dir)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/utils\/file_utils.py\", line 113, in mkdir\r\n    raise e\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/utils\/file_utils.py\", line 110, in mkdir\r\n    os.makedirs(target)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  [Previous line repeated 2 more times]\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 225, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/var\/lib\/mlflow'\r\n```\r\n\r\nEnvironment:\r\n* mlflow==1.2\r\n* macOS 12.1",
        "Challenge_closed_time":null,
        "Challenge_created_time":1642469367000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/499",
        "Challenge_link_count":0,
        "Challenge_readability":18.7,
        "Challenge_reading_time":25.24,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Permission denied when log models to mlflow on Mac",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":113,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":5.2394444444,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Challenge_closed_time":1642206718000,
        "Challenge_created_time":1642187856000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Challenge_link_count":0,
        "Challenge_readability":45.4,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.2394444444,
        "Challenge_title":"Can not create model in MLflowCatalog",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":41,
        "Platform":"Github",
        "Solution_body":"Duplicated to #496 ",
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":0.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":4.5558333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ML_predict(ssd, image) FROM coco limit 1;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Undefined function: '<anonymous>'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Register models into mlflow\r\n2. Start Spark thrift server\r\n3. Use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `SELECT ML_PREDICT(ssd, image) FROM coco`",
        "Challenge_closed_time":1642203169000,
        "Challenge_created_time":1642186768000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/492",
        "Challenge_link_count":0,
        "Challenge_readability":39.2,
        "Challenge_reading_time":31.45,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":4.5558333333,
        "Challenge_title":"MlflowCatalog anonymous function is not registered ",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":0.5105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import mlflow_logger as mlflow\r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/mlflow_logger.py:19: in <module>\r\n    import mlflow\r\nE   ModuleNotFoundError: No module named 'mlflow'\r\n```",
        "Challenge_closed_time":1617994925000,
        "Challenge_created_time":1617993087000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/207",
        "Challenge_link_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5105555556,
        "Challenge_title":"Leaking mlflow dependency",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":30,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Keep it in mind before mindlessly updating\r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/4118",
        "Challenge_closed_time":null,
        "Challenge_created_time":1613838919000,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/270",
        "Challenge_link_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":1.82,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Pytorch Lightning 1.2.0 requires new MLflow version",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":14,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0099337748,
        "Challenge_watch_issue_ratio":0.0033112583
    },
    {
        "Challenge_adjusted_solved_time":267.6955555556,
        "Challenge_answer_count":2,
        "Challenge_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Challenge_closed_time":1613342987000,
        "Challenge_created_time":1612379283000,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":267.6955555556,
        "Challenge_title":"Warning when training mlflow-pytorch 2.0.0",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":"Thats new I did not encountered this while I tested it.  Seems to be gone with my latest changes.\r\nPlease verify @Imipenem and close if not observed.",
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0099337748,
        "Challenge_watch_issue_ratio":0.0033112583
    },
    {
        "Challenge_adjusted_solved_time":1466.8491666667,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n* ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',`\r\n* ` f'mlflow.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'`\r\n\r\nThose two linting functions caused the template create WFs (and sometimes even local) to fail\r\n\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThey should pass. We should discuss why they fail and how to fix!\r\nSo currently they are outcommented!\r\n",
        "Challenge_closed_time":1613430703000,
        "Challenge_created_time":1608150046000,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/171",
        "Challenge_link_count":0,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.31,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1466.8491666667,
        "Challenge_title":"subprocess.call and mlflow.log_artifact checks inconsistent in linter",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":"@Emiller88 the linter should for all templates just check that these methods are called in the templates. Ideally you just need to add those two lines to the linter checks.\r\n\r\nI won't explain the original issue here since I just expect it to work :) If it still doesn't I will reassign @Imipenem and me.\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":3.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":54.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0099337748,
        "Challenge_watch_issue_ratio":0.0033112583
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"## Expected Behavior\r\nDeploy Jobs with --assets-only option\r\n## Current Behavior\r\nMLFlow API Request 409 Conflict \r\n## Steps to Reproduce (for bugs)\r\n[dbx][2022-11-03 12:30:40.370] \ud83d\udd0e Deployment file is not provided, searching in the conf directory\r\n[dbx][2022-11-03 12:30:40.375] \ud83d\udca1 Auto-discovery found deployment file conf\/deployment.json\r\n[dbx][2022-11-03 12:30:40.376] \ud83c\udd97 Deployment file conf\/deployment.json exists and will be used for deployment\r\n[dbx][2022-11-03 12:30:40.377] Starting new deployment for environment dev\r\n[dbx][2022-11-03 12:30:40.378] Using profile provided from the project file\r\n[dbx][2022-11-03 12:30:40.378] Found auth config from provider EnvironmentVariableConfigProvider, verifying it\r\n[dbx][2022-11-03 12:30:40.379] Found auth config from provider EnvironmentVariableConfigProvider, verification successful\r\n[dbx][2022-11-03 12:30:44.897] \r\n                Since v0.7.0 environment configurations should be nested under environments section.\r\n\r\n                Please nest environment configurations under this section to avoid potential issues while using \"build\"\r\n                configuration directive.\r\n            \r\n[dbx][2022-11-03 12:30:44.899] No build logic defined in the deployment file. Default pip-based build logic will be used.\r\n[dbx][2022-11-03 12:30:44.903] Usage of jobs keyword in deployment file is deprecated. Please use workflows instead (simply rename this section to workflows).\r\n[dbx][2022-11-03 12:30:44.906] Workflows ['add-on-chanel-AT', 'add-on-chanel-PL', 'add-on-PL'] were selected for further operations\r\n[dbx][2022-11-03 12:30:44.907] Following the provided build logic\r\n[dbx][2022-11-03 12:30:44.908] \ud83d\udc0d Building a Python-based project\r\n[dbx][2022-11-03 12:30:46.262] \u2705 Python-based project build finished\r\n[dbx][2022-11-03 12:30:46.264] Locating package file\r\n[dbx][2022-11-03 12:30:46.265] Package file located in: dist\/ds_recommenders-1.2.9-py3-none-any.whl\r\n[dbx][2022-11-03 12:30:47.221] Starting the traversal process\r\n[dbx][2022-11-03 12:30:47.222] Processing libraries for workflow add-on-chanel-AT\r\n[dbx][2022-11-03 12:30:47.223] \u2705 Processing libraries for workflow add-on-chanel-AT - done\r\n[dbx][2022-11-03 12:30:47.224] Processing libraries for workflow add-on-chanel-PL\r\n[dbx][2022-11-03 12:30:47.225] \u2705 Processing libraries for workflow add-on-chanel-PL - done\r\n[dbx][2022-11-03 12:30:47.225] Processing libraries for workflow add-on-PL\r\n[dbx][2022-11-03 12:30:47.226] \u2705 Processing libraries for workflow add-on-PL - done\r\n[dbx][2022-11-03 12:30:47.227] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.412] \u2705 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.414] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/comma \u2502\r\n\u2502 nds\/deploy.py:157 in deploy                                                  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, ele \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 157 \u2502   \u2502   \u2502   adjuster.traverse(deployable_workflows)                    \u2502\r\n\u2502   158 \u2502   \u2502   \u2502   if not _assets_only:                                       \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, dep \u2502\r\n\u2502   [16](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:17)0 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:185 in traverse                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   182 \u2502   def traverse(self, workflows: Union[WorkflowList, List[str]]):     \u2502\r\n\u2502   183 \u2502   \u2502   dbx_echo(\"Starting the traversal process\")                     \u2502\r\n\u2502   184 \u2502   \u2502   self.property_adjuster.library_traverse(workflows, self.additi \u2502\r\n\u2502 \u2771 185 \u2502   \u2502   self.property_adjuster.file_traverse(workflows, self.file_adju \u2502\r\n\u2502   186 \u2502   \u2502   self.property_adjuster.property_traverse(workflows)            \u2502\r\n\u2502   187 \u2502   \u2502   self.property_adjuster.cluster_policy_traverse(workflows)      \u2502\r\n\u2502   188 \u2502   \u2502   dbx_echo(\"Traversal process finished, all provided references  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:168 in file_traverse                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   165 \u2502   \u2502   for element, parent, index in self.traverse(workflows):        \u2502\r\n\u2502   166 \u2502   \u2502   \u2502   if isinstance(element, str):                               \u2502\r\n\u2502   167 \u2502   \u2502   \u2502   \u2502   if element.startswith(\"file:\/\/\") or element.startswith \u2502\r\n\u2502 \u2771 168 \u2502   \u2502   \u2502   \u2502   \u2502   file_adjuster.adjust_file_ref(element, parent, ind \u2502\r\n\u2502   169                                                                        \u2502\r\n\u2502   [17](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:18)0                                                                        \u2502\r\n\u2502   171 class Adjuster:                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/mixins\/file_reference.py:12 in adjust_file_ref                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    9 \u2502   \u2502   self._uploader = file_uploader                                  \u2502\r\n\u2502   10 \u2502                                                                       \u2502\r\n\u2502   11 \u2502   def adjust_file_ref(self, element: str, parent: Any, index: Any):   \u2502\r\n\u2502 \u2771 12 \u2502   \u2502   _uploaded = self._uploader.upload_and_provide_path(element)     \u2502\r\n\u2502   13 \u2502   \u2502   self.set_element_at_parent(_uploaded, parent, index)            \u2502\r\n\u2502   14                                                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/utils \u2502\r\n\u2502 \/file_uploader.py:59 in upload_and_provide_path                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   56 \u2502   \u2502   \u2502   self._verify_fuse_support()                                 \u2502\r\n\u2502   57 \u2502   \u2502                                                                   \u2502\r\n\u2502   58 \u2502   \u2502   dbx_echo(f\":arrow_up: Uploading local file {local_file_path}\")  \u2502\r\n\u2502 \u2771 59 \u2502   \u2502   self._upload_file(local_file_path)                              \u2502\r\n\u2502   60 \u2502   \u2502   dbx_echo(f\":white_check_mark: Uploading local file {local_file_ \u2502\r\n\u2502   61 \u2502   \u2502   return self._postprocess_path(local_file_path, as_fuse)         \u2502\r\n\u2502   62                                                                         \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[19](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:20)9 in http_request_safe                                   \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   196 \u2502   Wrapper around ``http_request`` that also verifies that the reques \u2502\r\n\u2502   197 \u2502   \"\"\"                                                                \u2502\r\n\u2502   198 \u2502   response = http_request(host_creds=host_creds, endpoint=endpoint,  \u2502\r\n\u2502 \u2771 199 \u2502   return verify_rest_response(response, endpoint)                    \u2502\r\n\u2502   [20](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:21)0                                                                        \u2502\r\n\u2502   201                                                                        \u2502\r\n\u2502   202 def verify_rest_response(response, endpoint):                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[21](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:22)2 in verify_rest_response                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   \u2502   endpoint,                                              \u2502\r\n\u2502   210 \u2502   \u2502   \u2502   \u2502   response.status_code,                                  \u2502\r\n\u2502   211 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502 \u2771 212 \u2502   \u2502   \u2502   raise MlflowException(                                     \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   \u2502   \"%s. Response body: '%s'\" % (base_msg, response.text), \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   \u2502   error_code=get_error_code(response.status_code),       \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nMlflowException: API request to endpoint \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f9[22](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:23)5de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n failed with error code 409 != 200. Response body: '<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text\/html;charset=ISO-8859-1\"\/>\r\n<title>Error 409 <\/title>\r\n<\/head>\r\n<body>\r\n<h2>HTTP ERROR: 409<\/h2>\r\n<p>Problem accessing \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f92[25](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:26)de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n. Reason:\r\n<pre>    File already exists, cannot overwrite: \r\n&apos;\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c908874\r\n[28](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:29)b97e6[37](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:38)1f[92](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:93)25de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.p\r\ny&apos;<\/pre><\/p>\r\n<hr \/>\r\n<\/body>\r\n<\/html>\r\n'\r\nError: Process completed with exit code 1.\r\n\r\n## Context\r\nUpdated few jobs today using the latest dbx version, and at the jobless deployment cicd step I get the error above.\r\nMLFlow is only used to define a specific experiment path. No path related updates or changes here!\r\n## Your Environment\r\n\r\n* dbx version used: 0.8.x\r\n* Databricks Runtime version:  10.4 LTS (standard or ML)\r\n* Python version: 3.8.11",
        "Challenge_closed_time":null,
        "Challenge_created_time":1667484803000,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/548",
        "Challenge_link_count":8,
        "Challenge_readability":16.8,
        "Challenge_reading_time":109.95,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":79.0,
        "Challenge_repo_issue_count":582.0,
        "Challenge_repo_star_count":246.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Error 409 when deploying --assets-only",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":570,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0481099656,
        "Challenge_watch_issue_ratio":0.0274914089
    },
    {
        "Challenge_adjusted_solved_time":316.8641666667,
        "Challenge_answer_count":8,
        "Challenge_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Challenge_closed_time":1661539227000,
        "Challenge_created_time":1660398516000,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Challenge_link_count":1,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":79.0,
        "Challenge_repo_issue_count":582.0,
        "Challenge_repo_star_count":246.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":316.8641666667,
        "Challenge_title":"dbx deploy fails due to mlflow experiment not found",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":"hi @zermelozf , \r\ncould you please provide full stack trace?  Sure, here it is:\r\n\r\n```\r\ndbx deploy --environment=default\r\n[dbx][2022-08-13 22:46:37.005] Starting new deployment for environment default\r\n[dbx][2022-08-13 22:46:37.006] Using profile provided from the project file\r\n[dbx][2022-08-13 22:46:37.006] Found auth config from provider ProfileEnvConfigProvider, verifying it\r\n[dbx][2022-08-13 22:46:37.007] Found auth config from provider ProfileEnvConfigProvider, verification successful\r\n[dbx][2022-08-13 22:46:37.007] Profile DEFAULT will be used for deployment\r\nTraceback (most recent call last):\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/bin\/dbx\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/commands\/deploy.py\", line 143, in deploy\r\n    api_client = prepare_environment(environment)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/utils\/common.py\", line 38, in prepare_environment\r\n    MlflowStorageConfigurationManager.prepare(info)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 42, in prepare\r\n    cls._setup_experiment(properties)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 53, in _setup_experiment\r\n    experiment: Optional[Experiment] = mlflow.get_experiment_by_name(properties.workspace_dir)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 1042, in get_experiment_by_name\r\n    return MlflowClient().get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 566, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 226, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 365, in get_experiment_by_name\r\n    raise e\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 351, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 57, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 274, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 200, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.\r\n``` hi @zermelozf , \r\nit seems to me that you're using an old version of `dbx`. Please upgrade to the latest 0.7.0 (or at least to 0.6.12).  hi @renardeinside I had the same issue mentioned here. I upgraded to dbx 0.7.0 and now the error looks like this:\r\nRestException: INVALID_PARAMETER_VALUE: Experiment with id '2624352622693299' does not exist.\r\nIt only happens if you deploy a job for the first time. Deploying changes to an existing job works fine. hi @frida-ah , \r\nwhat's the MLflow version you're using? I'm asking because I'm not running into this issue in any of the tests  could you please also verify that you have correct [databricks profile configured as in Step 3 point 4 of the public doc](https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#step-3-install-the-code-samples-dependencies)?\r\n\r\nif it's still the case, please provide the deploy command with `dbx deploy --debug` option (please feel free to omit the host url)? \r\nReally curious where is this coming from.\r\n Hi @renardeinside I don't have mlflow in my requirements.txt. I can also confirm that I have the correct databricks profile configured in the deployment.json file as such:\r\n\r\n{\r\n  \"environments\": {\r\n    \"default\": {\r\n      \"profile\": \"DEFAULT\",\r\n      \"workspace_dir\": \"\/Shared\/dbx\/projects\/<project_name>\/<...>\",\r\n      \"artifact_location\": \"dbfs:\/Shared\/dbx\/projects\/<project_name>\/<...>\"\r\n    }\r\n  }\r\n}\r\n\r\ndbx deploy --environment default --deployment-file=conf\/deployment.json --jobs=<job_name>\r\n\r\nI have fixed the issue using a workaround - sorry I didn't have more time to invest in this. I created an artifact manually through the UI in the location where the artifact should be. Then I deleted it. And then the artifact was created again through the IDE and GitHub Actions. \r\n\r\nI think the issue is with Databricks having a bug when creating an artifact for the first time.  hi @frida-ah , \r\nstill pretty strange behaviour, but thanks a lot anyways. We're going to change the mlflow client logic accordingly to fix this issue.",
        "Solution_link_count":1.0,
        "Solution_readability":14.8,
        "Solution_reading_time":77.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":61.0,
        "Solution_word_count":510.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0481099656,
        "Challenge_watch_issue_ratio":0.0274914089
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Version\r\n\r\n22.11\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\n_No response_\r\n\r\n### Describe the bug.\r\n\r\nai-engine fetch command at the 22.11 guide:\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-ai-engine-**22.09**.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-sdk-client-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-mlflow-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\n### Minimum reproducible example\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Full env printout\r\n\r\n_No response_\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Challenge_closed_time":null,
        "Challenge_created_time":1671535506000,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/576",
        "Challenge_link_count":4,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]: Helm fetch command for ai-engine,sdk-helper and mlflow includes the 22.09 release instead of 22.11",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":100,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0335820896,
        "Challenge_watch_issue_ratio":0.0186567164
    },
    {
        "Challenge_adjusted_solved_time":1.6669444444,
        "Challenge_answer_count":2,
        "Challenge_body":"### Version\r\n\r\n23.01\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\nDocker\r\n\r\n### Describe the bug.\r\n\r\nUnable to start the mlflow server when using `branch-22.11` but it works fine with `branch-22.09`\r\n\r\nDowngrading  mlflow version to `<1.29.0` works fine.\r\n\r\n\r\n### Minimum reproducible example\r\n\r\n```shell\r\n$ cd ~\/Morpheus\/examples\/digital_fingerprinting\/production\r\n\r\n$ docker-compose up mlflow\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[+] Running 3\/3                                                                                                                                               \r\n \u283f Network production_backend      Created                                                                                                               0.0s \r\n \u283f Network production_frontend     Created                                                                                                               0.0s\r\n \u283f Container mlflow_server  Created                                                                                                               0.1s\r\nAttaching to mlflow_server\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Error initializing backend store\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server  | Traceback (most recent call last):\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 392, in server\r\nmlflow_server  |     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 265, in initialize_backend_stores\r\nmlflow_server  |     _get_tracking_store(backend_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 244, in _get_tracking_store\r\nmlflow_server  |     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 39, in get_store\r\nmlflow_server  |     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri\r\nmlflow_server  |     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 112, in _get_sqlalchemy_store\r\nmlflow_server  |     return SqlAlchemyStore(store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__\r\nmlflow_server  |     mlflow.store.db.utils._verify_schema(self.engine)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 71, in _verify_schema\r\nmlflow_server  |     raise MlflowException(\r\nmlflow_server  | mlflow.exceptions.MlflowException: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server exited with code 1\r\n```\r\n\r\n\r\n### Full env printout\r\n\r\n```shell\r\n<details><summary>Click here to see environment details<\/summary><pre>\r\n     \r\n     **git***\r\n     commit 9619c0e3a5ddbdd476aba9331f288aac855da7cd (HEAD -> dfp-pipeline-module, origin\/dfp-pipeline-module)\r\n     Author: bsuryadevara <bhargavsuryadevara@gmail.com>\r\n     Date:   Wed Nov 30 17:13:05 2022 -0600\r\n     \r\n     used dill to persist source and preprocess schema\r\n     **git submodules***\r\n     -27efc4fd1c984332920db2a2d6ab1f84d3cb55cd external\/morpheus-visualizations\r\n     \r\n     ***OS Information***\r\n     DGX_NAME=\"DGX Server\"\r\n     DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\r\n     DGX_SWBUILD_DATE=\"2020-03-04\"\r\n     DGX_SWBUILD_VERSION=\"4.4.0\"\r\n     DGX_COMMIT_ID=\"ee09ebc\"\r\n     DGX_PLATFORM=\"DGX Server for DGX-1\"\r\n     DGX_SERIAL_NUMBER=\"QTFCOU7140058-R1\"\r\n     DISTRIB_ID=Ubuntu\r\n     DISTRIB_RELEASE=18.04\r\n     DISTRIB_CODENAME=bionic\r\n     DISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"\r\n     NAME=\"Ubuntu\"\r\n     VERSION=\"18.04.6 LTS (Bionic Beaver)\"\r\n     ID=ubuntu\r\n     ID_LIKE=debian\r\n     PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\r\n     VERSION_ID=\"18.04\"\r\n     HOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\n     SUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\n     BUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\n     PRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\n     VERSION_CODENAME=bionic\r\n     UBUNTU_CODENAME=bionic\r\n     Linux dgx04 4.15.0-162-generic #170-Ubuntu SMP Mon Oct 18 11:38:05 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\r\n     \r\n     ***GPU Information***\r\n     Thu Dec  1 17:37:05 2022\r\n     +-----------------------------------------------------------------------------+\r\n     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n     |-------------------------------+----------------------+----------------------+\r\n     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n     | Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n     |                               |                      |               MIG M. |\r\n     |===============================+======================+======================|\r\n     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    56W \/ 300W |  11763MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    43W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\r\n     | N\/A   28C    P0    41W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\r\n     | N\/A   29C    P0    44W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n     | N\/A   31C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     \r\n     +-----------------------------------------------------------------------------+\r\n     | Processes:                                                                  |\r\n     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n     |        ID   ID                                                   Usage      |\r\n     |=============================================================================|\r\n     |    0   N\/A  N\/A     31232      C   ...da\/envs\/rapids\/bin\/python      303MiB |\r\n     |    0   N\/A  N\/A     41206      C   ...da\/envs\/rapids\/bin\/python     7051MiB |\r\n     |    0   N\/A  N\/A     52497      C   ...nda3\/envs\/venv\/bin\/python     3137MiB |\r\n     |    0   N\/A  N\/A     55973      C   tritonserver                     1267MiB |\r\n     +-----------------------------------------------------------------------------+\r\n     \r\n     ***CPU***\r\n     Architecture:        x86_64\r\n     CPU op-mode(s):      32-bit, 64-bit\r\n     Byte Order:          Little Endian\r\n     CPU(s):              80\r\n     On-line CPU(s) list: 0-79\r\n     Thread(s) per core:  2\r\n     Core(s) per socket:  20\r\n     Socket(s):           2\r\n     NUMA node(s):        2\r\n     Vendor ID:           GenuineIntel\r\n     CPU family:          6\r\n     Model:               79\r\n     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\n     Stepping:            1\r\n     CPU MHz:             3267.078\r\n     CPU max MHz:         3600.0000\r\n     CPU min MHz:         1200.0000\r\n     BogoMIPS:            4390.17\r\n     Virtualization:      VT-x\r\n     L1d cache:           32K\r\n     L1i cache:           32K\r\n     L2 cache:            256K\r\n     L3 cache:            51200K\r\n     NUMA node0 CPU(s):   0-19,40-59\r\n     NUMA node1 CPU(s):   20-39,60-79\r\n     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\n     \r\n     ***CMake***\r\n     \/usr\/bin\/cmake\r\n     cmake version 3.10.2\r\n     \r\n     CMake suite maintained and supported by Kitware (kitware.com\/cmake).\r\n     \r\n     ***g++***\r\n     \/usr\/bin\/g++\r\n     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n     Copyright (C) 2017 Free Software Foundation, Inc.\r\n     This is free software; see the source for copying conditions.  There is NO\r\n     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n     \r\n     \r\n     ***nvcc***\r\n     \/usr\/local\/cuda\/bin\/nvcc\r\n     nvcc: NVIDIA (R) Cuda compiler driver\r\n     Copyright (c) 2005-2021 NVIDIA Corporation\r\n     Built on Thu_Nov_18_09:45:30_PST_2021\r\n     Cuda compilation tools, release 11.5, V11.5.119\r\n     Build cuda_11.5.r11.5\/compiler.30672275_0\r\n     \r\n     ***Python***\r\n     \/usr\/bin\/python\r\n     Python 2.7.17\r\n     \r\n     ***Environment Variables***\r\n     PATH                            : \/usr\/local\/cuda\/bin:\/opt\/bin\/:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/snap\/bin:\/home\/nfs\/bsuryadevara:\/home\/nfs\/bsuryadevara\r\n     LD_LIBRARY_PATH                 :\r\n     NUMBAPRO_NVVM                   :\r\n     NUMBAPRO_LIBDEVICE              :\r\n     CONDA_PREFIX                    :\r\n     PYTHON_PATH                     :\r\n     \r\n     conda not found\r\n     ***pip packages***\r\n     \/usr\/bin\/pip\r\n\/usr\/lib\/python2.7\/dist-packages\/OpenSSL\/crypto.py:12: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\r\n  from cryptography import x509\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\n     ansible (2.9.9)\r\n     asn1crypto (0.24.0)\r\n     backports.functools-lru-cache (1.6.4)\r\n     backports.shutil-get-terminal-size (1.0.0)\r\n     bcrypt (3.1.7)\r\n     beautifulsoup4 (4.9.3)\r\n     boto3 (1.17.112)\r\n     botocore (1.20.112)\r\n     bs4 (0.0.1)\r\n     certifi (2018.1.18)\r\n     cffi (1.11.5)\r\n     chardet (3.0.4)\r\n     click (7.1.2)\r\n     configparser (4.0.2)\r\n     contextlib2 (0.6.0.post1)\r\n     cryptography (3.3.2)\r\n     decorator (4.1.2)\r\n     defusedxml (0.6.0)\r\n     distro (1.6.0)\r\n     dnspython (1.15.0)\r\n     docker (4.4.4)\r\n     docopt (0.6.2)\r\n     enum34 (1.1.10)\r\n     fastrlock (0.8)\r\n     flake8 (3.9.2)\r\n     functools32 (3.2.3.post2)\r\n     futures (3.3.0)\r\n     gssapi (1.4.1)\r\n     gyp (0.1)\r\n     html-to-json (2.0.0)\r\n     html2text (2019.8.11)\r\n     html5lib (0.999999999)\r\n     http (0.2)\r\n     httplib2 (0.14.0)\r\n     httpserver (1.1.0)\r\n     idna (2.6)\r\n     importlib-metadata (2.1.3)\r\n     ipaclient (4.6.90rc1+git20180411)\r\n     ipaddress (1.0.17)\r\n     ipalib (4.6.90rc1+git20180411)\r\n     ipaplatform (4.6.90rc1+git20180411)\r\n     ipapython (4.6.90rc1+git20180411)\r\n     Jinja2 (2.10)\r\n     jmespath (0.10.0)\r\n     lxml (4.2.1)\r\n     MarkupSafe (1.0)\r\n     mccabe (0.6.1)\r\n     netaddr (0.7.19)\r\n     netifaces (0.10.4)\r\n     numpy (1.16.6)\r\n     ofed-le-utils (1.0.3)\r\n     olefile (0.45.1)\r\n     pandas (0.24.2)\r\n     paramiko (2.11.0)\r\n     pathlib2 (2.3.7.post1)\r\n     Pillow (5.1.0)\r\n     pip (9.0.1)\r\n     ply (3.11)\r\n     pyasn1 (0.4.2)\r\n     pyasn1-modules (0.2.1)\r\n     pycodestyle (2.7.0)\r\n     pycparser (2.18)\r\n     pycrypto (2.6.1)\r\n     pyflakes (2.3.1)\r\n     pygobject (3.26.1)\r\n     PyNaCl (1.4.0)\r\n     pyOpenSSL (17.5.0)\r\n     python-apt (1.6.5+ubuntu0.7)\r\n     python-augeas (0.5.0)\r\n     python-dateutil (2.8.2)\r\n     python-dotenv (0.18.0)\r\n     python-ldap (3.0.0)\r\n     python-yubico (1.3.2)\r\n     pytz (2022.4)\r\n     pyusb (1.0.0)\r\n     PyYAML (5.4.1)\r\n     qrcode (5.3)\r\n     requests (2.27.1)\r\n     s3fs (0.2.2)\r\n     s3transfer (0.4.2)\r\n     scandir (1.10.0)\r\n     setuptools (39.0.1)\r\n     six (1.16.0)\r\n     soupsieve (1.9.6)\r\n     splunk-sdk (1.7.2)\r\n     subprocess32 (3.5.4)\r\n     tqdm (4.60.0)\r\n     typing (3.10.0.0)\r\n     urllib3 (1.26.12)\r\n     webencodings (0.5)\r\n     yapf (0.32.0)\r\n     zipp (1.2.0)\r\n     \r\n<\/pre><\/details>\r\n```\r\n\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Challenge_closed_time":1669922495000,
        "Challenge_created_time":1669916494000,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/512",
        "Challenge_link_count":5,
        "Challenge_readability":8.3,
        "Challenge_reading_time":155.99,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":167,
        "Challenge_solved_time":1.6669444444,
        "Challenge_title":"[BUG]: Unable to Start DFP Production MLFlow Server",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":1151,
        "Platform":"Github",
        "Solution_body":"yeah, MLflow recently has started enforcing db schema checks at startup\r\n After removing docker volumes that are related to MLFlow and restarted `mlflow_server`. Now it's working as expected.\r\n```\r\ndocker volume rm production_mlflow_data\r\ndocker volume rm production_db_data\r\n```\r\n\r\n```\r\nmlflow_server        |   worker_int: <function WorkerInt.worker_int at 0x7faaa8f90dc0>\r\nmlflow_server        |   worker_abort: <function WorkerAbort.worker_abort at 0x7faaa8f90ee0>\r\nmlflow_server        |   pre_exec: <function PreExec.pre_exec at 0x7faaa8fa6040>\r\nmlflow_server        |   pre_request: <function PreRequest.pre_request at 0x7faaa8fa6160>\r\nmlflow_server        |   post_request: <function PostRequest.post_request at 0x7faaa8fa61f0>\r\nmlflow_server        |   child_exit: <function ChildExit.child_exit at 0x7faaa8fa6310>\r\nmlflow_server        |   worker_exit: <function WorkerExit.worker_exit at 0x7faaa8fa6430>\r\nmlflow_server        |   nworkers_changed: <function NumWorkersChanged.nworkers_changed at 0x7faaa8fa6550>\r\nmlflow_server        |   on_exit: <function OnExit.on_exit at 0x7faaa8fa6670>\r\nmlflow_server        |   proxy_protocol: False\r\nmlflow_server        |   proxy_allow_ips: ['127.0.0.1']\r\nmlflow_server        |   keyfile: None\r\nmlflow_server        |   certfile: None\r\nmlflow_server        |   ssl_version: 2\r\nmlflow_server        |   cert_reqs: 0\r\nmlflow_server        |   ca_certs: None\r\nmlflow_server        |   suppress_ragged_eofs: True\r\nmlflow_server        |   do_handshake_on_connect: False\r\nmlflow_server        |   ciphers: None\r\nmlflow_server        |   raw_paste_global_conf: []\r\nmlflow_server        |   strip_header_spaces: False\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Starting gunicorn 20.1.0\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [DEBUG] Arbiter booted\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Listening at: http:\/\/0.0.0.0:5000 (30)\r\nmlflow_server        | [2022-12-01 19:16:23 +0000] [30] [INFO] Using worker: sync\r\n```",
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":23.22,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0335820896,
        "Challenge_watch_issue_ratio":0.0186567164
    },
    {
        "Challenge_adjusted_solved_time":165.0966666667,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Challenge_closed_time":1654018977000,
        "Challenge_created_time":1653424629000,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_readability":14.9,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":165.0966666667,
        "Challenge_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":157,
        "Platform":"Github",
        "Solution_body":"Error in LaunchPad notebooks. There was a change in the triton upstream where you previously didn't need to specify the model suffix as the path. Now you do. ",
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0335820896,
        "Challenge_watch_issue_ratio":0.0186567164
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"When running hyperparameter tuning, MLflow expects an mlruns folder - which we don't create. If we stick with the standard we can ommit having to run `mlflow ui` with the backend store argument.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1621607539000,
        "Challenge_link":"https:\/\/github.com\/equinor\/flownet\/issues\/408",
        "Challenge_link_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":28.0,
        "Challenge_repo_issue_count":455.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow expecting mlruns folder",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":35,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0197802198,
        "Challenge_watch_issue_ratio":0.0065934066
    },
    {
        "Challenge_adjusted_solved_time":1.2725,
        "Challenge_answer_count":0,
        "Challenge_body":"If an ERT subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. This will then lead to a \"successful\" run in mlflow, whereas it should be registered as a failed run.",
        "Challenge_closed_time":1606475795000,
        "Challenge_created_time":1606471214000,
        "Challenge_link":"https:\/\/github.com\/equinor\/flownet\/issues\/269",
        "Challenge_link_count":0,
        "Challenge_readability":7.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":28.0,
        "Challenge_repo_issue_count":455.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.2725,
        "Challenge_title":"Failed ERT runs are not registered correctly in mlflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0197802198,
        "Challenge_watch_issue_ratio":0.0065934066
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Logging of parameters on Mlflow works as expected with default parameters set with Hydra; However hydra allows modification of parameters per experiment run, but modified parameters are not logged on Mlflow.  ",
        "Challenge_closed_time":null,
        "Challenge_created_time":1671594369000,
        "Challenge_link":"https:\/\/github.com\/NRCan\/geo-deep-learning\/issues\/440",
        "Challenge_link_count":0,
        "Challenge_readability":13.9,
        "Challenge_reading_time":3.06,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":42.0,
        "Challenge_repo_issue_count":426.0,
        "Challenge_repo_star_count":123.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Fix logging of parameters on Mlflow",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":36,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0375586854,
        "Challenge_watch_issue_ratio":0.044600939
    },
    {
        "Challenge_adjusted_solved_time":69.685,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nWhen you try to specify an artifact path and a run_id in an ``MlflowArtifactDataSet``, you get an error. \r\n\r\nThis works:\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=None,\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\n\r\nwhile this :\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=\"folder\", # this is the difference\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\nraises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:\r\n",
        "Challenge_closed_time":1665079955000,
        "Challenge_created_time":1664829089000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/362",
        "Challenge_link_count":0,
        "Challenge_readability":10.6,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":69.685,
        "Challenge_title":"MlflowArtifactDataset.load() fails if artifact_path is not None and run_id is specified",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":67,
        "Platform":"Github",
        "Solution_body":"The issue is still here when there is nested artifact_path: if the file does not exists, it is downloaded to ``self._filepath \/artifact_path\/filename.pkl`` and cannot be loaded (due to the ``artifact_path`` suffix)",
        "Solution_link_count":0.0,
        "Solution_readability":11.7,
        "Solution_reading_time":2.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":31.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":68.5333333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Challenge_closed_time":1664786016000,
        "Challenge_created_time":1664539296000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Challenge_link_count":0,
        "Challenge_readability":9.3,
        "Challenge_reading_time":7.26,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":68.5333333333,
        "Challenge_title":"kedro mlflow ui gets a FileNotFoundError",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI am sorry to see you are experiencing issues. this is not a dummy question, it sounds like a bug. \r\n\r\nI've just ran this: \r\n\r\n```bash\r\nconda create -n km-361 python=3.9 -y\r\nconda activate km-361\r\npip install kedro==0.18.3\r\npip install mlflow==1.29.0\r\npip install kedro-mlflow==0.11.3\r\nkedro new --starter=pandas-iris\r\ncd iris\r\nkedro mlflow init\r\nkedro mlflow ui\r\n```\r\n\r\nthen I opened ``http:\/\/127.0.0.1:5000`` and th UI opened as expected. \r\n\r\nCan you tell me: \r\n- your python version\r\n- your OS\r\n- your ``kedro`` \/ ``mlflow`` \/ ``kedro-mlflow`` version\r\n- the project using\r\n- the exact error message\r\n- check if you have a ``MLFLOW_TRACKING_URI`` environment set It turned out fine  after trying again! Sorry and thanks for your consideration!",
        "Solution_link_count":1.0,
        "Solution_readability":5.1,
        "Solution_reading_time":8.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":107.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":590.8691666667,
        "Challenge_answer_count":4,
        "Challenge_body":"## Description\r\n\r\nThis happens when i tried to configure my own metric functions. \r\n\r\n## Context\r\n\r\nI am trying to create a custom metric indicator, to be logged after each experimentation. When i run `kedro mlflow ui`, this is what I'm getting on the UI.\r\n![image](https:\/\/user-images.githubusercontent.com\/54475793\/184276876-57872dd2-3fb3-41c9-b3a3-edd6a4396aca.png)\r\n\r\n\r\n## Steps to Reproduce\r\n\r\nThis is my nodes.py\r\n```\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n```\r\n\r\n\r\n## Expected Result\r\n\r\nHow do i get the metric to be displayed when i use the Mlflow ui? Are there specific keywords that mlflow is tracking to be logged as metric?\r\n\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): **0.10.0**\r\n* Python version used (`python -V`):  **3.9.0** \r\n* Operating system and version: Windows 10\r\n",
        "Challenge_closed_time":1662408460000,
        "Challenge_created_time":1660281331000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/346",
        "Challenge_link_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":590.8691666667,
        "Challenge_title":"MlflowMetricsDataSet logs invalid metric which breaks mlflow UI",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":142,
        "Platform":"Github",
        "Solution_body":"Hi @xjlwi, sorry to see that you are facing issues with the plugins. There are two problems here:\r\n- kedro-mlflow logs an incorrect metric. We will solve the problem together. \r\n- mlflow does not complain when the incorrect metric is logged, but it breaks the database and hence the UI => we should open an issue in mlflow repo once we know what is going on. \r\n\r\nWould you mind give me some extra informations: \r\n- ``mlflow`` version\r\n- the catalog entry for ``avg_pnl`` (I guess it is a ``kedro_mlflow.io.metrics.MlflowMetricsDataSet``?) **If yes, check the documentation: [it should return something like ``{'trader1': {'step': 0, 'value': df.pnl.mean()}}``](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/04_experimentation_tracking\/05_version_metrics.html#how-to-return-metrics-from-a-node)**\r\n- the type of ``avg_metric`` and ``total_metric``: are they ``float`` instead of string?\r\n- can you check if ``df.pnl.mean()`` and ``df.pnl.sum()`` returns a float and not a single-row ``pandas.Series``?\r\n\r\nIf I can reproduce the bug, I will be able to give you a workaround. \r\n > Hi @xjlwi, sorry to see that you are facing issues with the plugins. There are two problems here:\r\n> \r\n> * kedro-mlflow logs an incorrect metric. We will solve the problem together.\r\n> * mlflow does not complain when the incorrect metric is logged, but it breaks the database and hence the UI => we should open an issue in mlflow repo once we know what is going on.\r\n> \r\n> Would you mind give me some extra informations:\r\n> \r\n> * `mlflow` version: <b> 1.26.1 <\/b>\r\n> * the catalog entry for `avg_pnl` (I guess it is a `kedro_mlflow.io.metrics.MlflowMetricsDataSet`?) **If yes, check the documentation: [it should return something like `{'trader1': {'step': 0, 'value': df.pnl.mean()}}`](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/04_experimentation_tracking\/05_version_metrics.html#how-to-return-metrics-from-a-node)** : \r\n\r\nYes it's a `kedro_mlflow.io.metrics.MlflowMetricsDataSet`. \r\n\r\ntype: kedro_mlflow.io.artifacts.MlflowArtifactDataSet \r\ndata_set:\r\n    type: pandas.CSVDataSet \r\n    filepath: \"${ml_model_output}PnL_summary_metrics_${current_date}_${model}.csv\" \r\n    save_args:\r\n      index: True\r\n\r\nMust the keywords for the output be specifically 'step'? This is my current node to return the output.\r\n`\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n`\r\n\r\n> * the type of `avg_metric` and `total_metric`: are they `float` instead of string? Definitely float, because in my local mlruns folder, I am able to see them from the mlruns>metrics folder.\r\n\r\n1660874133345 [{'ml_model_13_logit_pnl_total': 0.0}, {'ml_model_13_logit_pnl_avg': nan}] ml_model_13_logit\r\n1660874133347 [{'ml_model_14_rf_pnl_total': 0.0}, {'ml_model_14_rf_pnl_avg': nan}] ml_model_14_rf\r\n1660874133349 [{'ml_model_15_naive_clf_pnl_total': 0.0}, {'ml_model_15_naive_clf_pnl_avg': nan}] ml_model_15_naive_clf\r\n1660874133352 [{'ml_model_16_svc_pnl_total': 0.0}, {'ml_model_16_svc_pnl_avg': nan}] ml_model_16_svc\r\n1660874133354 [{'ml_model_17_decisison_tree_pnl_total': 0.0}, {'ml_model_17_decisison_tree_pnl_avg': nan}] ml_model_17_decisison_tree\r\n1660874133356 [{'ml_model_18_grad_boost_pnl_total': 0.0}, {'ml_model_18_grad_boost_pnl_avg': nan}] ml_model_18_grad_boost\r\n\r\n> * can you check if `df.pnl.mean()` and `df.pnl.sum()` returns a float and not a single-row `pandas.Series`?\r\n\r\n> If I can reproduce the bug, I will be able to give you a workaround.\r\n\r\n > Must the keywords for the output be specifically 'step'? This is my current node to return the output.\r\n\r\nYes exactly. That's for consistency between loading and saving metrics.\r\n\r\nReplace each entry ``df.pnl.mean()`` by  a dict``{'step': 0, 'value': df.pnl.mean()}``and you will be fine. This adds an extra nested dict level and is not ideal. I let the issue opened to improve the API in the future. Hi, I close the issue but feel free to reopen if needed. ",
        "Solution_link_count":2.0,
        "Solution_readability":9.5,
        "Solution_reading_time":50.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":47.0,
        "Solution_word_count":461.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_created_time":1656532075000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_created_time":1652380533000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":185,
        "Platform":"Github",
        "Solution_body":"Closed by #313 ",
        "Solution_link_count":0.0,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":2392.2144444444,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIt is not possible to store a ``PartitionedDataSet`` as an mlflow artifact with the ``MlflowArtifactDataSet``.\r\n\r\n## Context\r\n\r\nI had a use case where I need to save a dict with many small result tables to mlflow, and I tried to use ``PartitionedDataSet`` for this.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# catalog.yml\r\n\r\nmy_dataset:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: PartitionedDataSet  # or any valid kedro DataSet\r\n        path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"!\r\n        dataset: \"pandas.CSVDataSet\"\r\n```\r\n\r\nthen save a dict using this dataset:\r\n\r\n```\r\ncatalog.save(\"my_dataset\", dict(\"a\": pd.DataFrame(data=[1,2,3], columns=[\"a\"], \"b\": pd.DataFrame(data=[1,2,3], columns=[\"b\"])\r\n```\r\n## Expected Result\r\n\r\nThe 2 Dataframes should be logged as artifacts in the current mlflow run.\r\n\r\n## Actual Result\r\n\r\nAn error ``dataset has not attribute \"_filepath\"`` is raised.\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe error comes from this line:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py#L53\r\n\r\nmaybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Challenge_closed_time":1644674290000,
        "Challenge_created_time":1636062318000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/258",
        "Challenge_link_count":1,
        "Challenge_readability":7.6,
        "Challenge_reading_time":17.08,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2392.2144444444,
        "Challenge_title":"MlflowArtifactDataSet does not work with PartitionedDataSet",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":75.83,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIf I specify an experiment in `mlflow.yml`, and the set up the mlflow configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in mlflow \"Default\" (0) experiment. This works when running \"kedro run\" through the CLI.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# mlflow.yml\r\nexperiment:\r\n  name: my_awesome_experiment\r\n  create: True  # if the specified `name` does not exists, should it be created?\r\n```\r\n\r\n```python\r\n# test.py\r\n\r\nfrom kedro.framework.session import KedroSession\r\nfrom kedro.framework.startup import bootstrap_project\r\nfrom kedro_mlflow.config import get_mlflow_config\r\n\r\nbootstrap_project(r\"path\/to\/project\")\r\nwith KedroSession.create(project_path=r\"path\/to\/project\"):\r\n    config=get_mlflow_config()\r\n    config.setup()\r\n    \r\n    mlflow.log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"Default\".\r\n\r\n```\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe faulty line is: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L100\r\n\r\n[We should use mlflow ``mlflow.set_experiment`` method](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.set_experiment), but it does not restore deleted experiment. This wil replace part of the logic here: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L124-L132",
        "Challenge_closed_time":1636318265000,
        "Challenge_created_time":1636045277000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/256",
        "Challenge_link_count":3,
        "Challenge_readability":12.6,
        "Challenge_reading_time":20.71,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":75.83,
        "Challenge_title":"Setting the mlflow experiment does not work in interactive mode",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_created_time":1619193727000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":273,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_link_count":3.0,
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":7.0,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_created_time":1617627646000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_created_time":1610404594000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_link_count":0.0,
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_created_time":1605983313000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":137,
        "Platform":"Github",
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_link_count":0.0,
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_created_time":1605982845000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":291.4263888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\n`TypeError: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `MlflowArtifactDataSet` is used with `MlflowModelSaverDataSet`.\r\n\r\n## Context\r\n\r\nLogging locally and to MLflow in one step.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: kedro_mlflow.io.models.MlflowModelSaverDataSet\r\n        flavor: mlflow.sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## Expected Result\r\n\r\nThe model should be saved locally and in MLflow run at the same time.\r\n\r\n## Actual Result\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\nTypeError: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise DataSetError(message) from exc\r\nkedro.io.core.DataSetError: Failed while saving data to data set MlflowMlflowModelSaverDataSet(filepath=\/Users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=mlflow.sklearn, load_args={}, save_args={}, version=Version(load=None, save='2020-11-06T12.28.57.593Z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro-mlflow 0.4.0\r\n* Python 3.7.7\r\n* MacOS Catalina\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.",
        "Challenge_closed_time":1605715301000,
        "Challenge_created_time":1604666166000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_readability":16.6,
        "Challenge_reading_time":65.61,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":291.4263888889,
        "Challenge_title":"TypeError: unsupported operand type(s) for \/: 'str' and 'str' when using MlflowArtifactDataSet with MlflowModelSaverDataSet",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":337,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":49.3241666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nWhen `MlflowMetricsDataset` has no \"prefix\" specified, the name in the catalog is used instead. However, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a mlflow run interactively: \r\n```python\r\nmlflow.start_run()\r\nmlflow.end_run()\r\n```\r\nAnd browse the ui to retrieve the run_id\r\n\r\n2. Declare a `MlflowMetricsDataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_mlflow.io.MlflowMetricsDataSet\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. Launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## Expected Result\r\n\r\nA metric should be loggedin run \"1346579\".\r\n\r\n## Actual Result\r\n\r\nThe metric is logged is a new run.\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes",
        "Challenge_closed_time":1603665805000,
        "Challenge_created_time":1603488238000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/102",
        "Challenge_link_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":11.05,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":49.3241666667,
        "Challenge_title":"MlflowMetricsDataSet ignores run_id when prefix is not specified",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":126,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":179.7819444444,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nSince 0.16.5, kedro project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/439) instead of a `.kedro.yml` at the root of the projects. This breaks the `kedro mlflow init` command which is only compatible with `.kedro.yml` configuration file.\r\n\r\n## Context\r\nWe should remove the `_get_project_globals` util function in kedromlflow and use `kedro.framework.context import get_static_project_data` as suggested in #86. **Beware: this will break retrocompatibilty and work only with kedro>=0.16.5**\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch `kedro mlflow init` with no `.kedro.yml` config file in your project but a valid `pyproject.toml`.\r\n\r\n## Expected Result\r\nThe mlflow.yml file should be created\r\n\r\n## Actual Result\r\nAn error is raised.",
        "Challenge_closed_time":1603658563000,
        "Challenge_created_time":1603011348000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":179.7819444444,
        "Challenge_title":"Make mlflow init work when configuration is in pyproject.toml",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"Well spoted ! We can already solve this, by bundling kedro's `get_static_project_data` inside kedro_mlflow as long as we keep kedro < 0.16.5 retrocompatibility. We can switch then to `kedro.framework.context import get_static_project_data` when we drop this compatibility in the futur. I can add it to [Migrate 0.16.5 pull request](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/pull\/94) Ok let's do this!",
        "Solution_link_count":1.0,
        "Solution_readability":8.9,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":222.8511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Challenge_closed_time":1602278580000,
        "Challenge_created_time":1601476316000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Challenge_link_count":3,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":222.8511111111,
        "Challenge_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":106,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":426.9047222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Challenge_closed_time":1602948810000,
        "Challenge_created_time":1601411953000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":426.9047222222,
        "Challenge_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":64,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":2106.4869444444,
        "Challenge_answer_count":0,
        "Challenge_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Challenge_closed_time":1602948810000,
        "Challenge_created_time":1595365457000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2106.4869444444,
        "Challenge_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":31,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_created_time":1593379921000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1881.8983333333,
        "Challenge_answer_count":0,
        "Challenge_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Challenge_closed_time":1598336871000,
        "Challenge_created_time":1591562037000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Challenge_link_count":1,
        "Challenge_readability":10.6,
        "Challenge_reading_time":13.06,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1881.8983333333,
        "Challenge_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":126,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"*Currently*\n\n* since mlflow 1.28, running mlflow projects form remote sources causes\n  *mlflow: not found* issue on starting the project\n\n*Reproduce*\n\n* run TestMLFlowProjects.test_mlflow_gitproject_remote_https\n\n*Expected*\n\n* running remote-sourced mlflow project is supported as previously",
        "Challenge_closed_time":null,
        "Challenge_created_time":1663358103000,
        "Challenge_link":"https:\/\/github.com\/omegaml\/omegaml\/issues\/258",
        "Challenge_link_count":0,
        "Challenge_readability":9.4,
        "Challenge_reading_time":4.45,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":79.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"running mlflow>1.28 projects causes mlflow not found error",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0159744409,
        "Challenge_watch_issue_ratio":0.0127795527
    },
    {
        "Challenge_adjusted_solved_time":193.1908333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Execution get stuck if this case happens. It is necessary to manage this exception properly.",
        "Challenge_closed_time":1639743100000,
        "Challenge_created_time":1639047613000,
        "Challenge_link":"https:\/\/github.com\/ugr-sail\/sinergym\/issues\/101",
        "Challenge_link_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":255.0,
        "Challenge_repo_star_count":40.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":193.1908333333,
        "Challenge_title":"Experiments with mlflow don't work if MLFLOW_TRACKING_URI container variable specify an incorrect ip address",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0274509804,
        "Challenge_watch_issue_ratio":0.0156862745
    },
    {
        "Challenge_adjusted_solved_time":511.6538888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Challenge_closed_time":1610230183000,
        "Challenge_created_time":1608388229000,
        "Challenge_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Challenge_link_count":2,
        "Challenge_readability":23.7,
        "Challenge_reading_time":251.52,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":210.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":290,
        "Challenge_solved_time":511.6538888889,
        "Challenge_title":"Error when starting new experiment in mlflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":1131,
        "Platform":"Github",
        "Solution_body":"Already fixed in #79 by introducing delay between mlflow server start and starting experiments in mlflow",
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":1.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.019047619,
        "Challenge_watch_issue_ratio":0.0285714286
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # NOQA\r\n        from kedro_mlflow.framework.context import get_mlflow_config  # NOQA\r\n        return True\r\n    except ImportError:\r\n        return False\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Challenge_closed_time":null,
        "Challenge_created_time":1643989114000,
        "Challenge_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/102",
        "Challenge_link_count":0,
        "Challenge_readability":10.1,
        "Challenge_reading_time":3.98,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":207.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Plugin only compatible with kedro-mlflow<0.8.0",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":34,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0531400966,
        "Challenge_watch_issue_ratio":0.0628019324
    },
    {
        "Challenge_adjusted_solved_time":28.415,
        "Challenge_answer_count":0,
        "Challenge_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Challenge_closed_time":1580353817000,
        "Challenge_created_time":1580251523000,
        "Challenge_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Challenge_link_count":0,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.0,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":30.0,
        "Challenge_repo_issue_count":92.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.415,
        "Challenge_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":44,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0760869565,
        "Challenge_watch_issue_ratio":0.1304347826
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nProblem encountered by @ucsky. Running ```make one-click-mlflow``` is not working after ```make destroy``` because of the artifacts' bucket which still exists.\r\nGot the following error:\r\n````\r\n\r\nSetting up your GCP project...\r\n\u2577\r\n\u2502 Error: googleapi: Error 409: You already own this bucket. Please select another name., conflict\r\n\u2502 \r\n\u2502   with module.bucket_backend.google_storage_bucket.this,\r\n\u2502   on ..\/modules\/mlflow\/artifacts\/main.tf line 18, in resource \"google_storage_bucket\" \"this\":\r\n\u2502   18: resource \"google_storage_bucket\" \"this\" {\r\n\r\n````\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. run ```make one-click-mlflow``` and finish it\r\n2. run ```make destroy```\r\n3. run ```make one-click-mlflow```\r\n4. See error\r\n\r\n**Expected behavior**\r\nThe second command ```make one-click-mlflow``` should work \r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1633706491000,
        "Challenge_link":"https:\/\/github.com\/artefactory\/one-click-mlflow\/issues\/79",
        "Challenge_link_count":0,
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.3,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":17.0,
        "Challenge_repo_issue_count":80.0,
        "Challenge_repo_star_count":60.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"make one-click-mlflow not working after make destroy because of undeleted bucket",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":105,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0875,
        "Challenge_watch_issue_ratio":0.05
    },
    {
        "Challenge_adjusted_solved_time":338.3591666667,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Challenge_closed_time":1647350309000,
        "Challenge_created_time":1646132216000,
        "Challenge_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Challenge_link_count":1,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":79.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":338.3591666667,
        "Challenge_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":47,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1772151899,
        "Challenge_watch_issue_ratio":0.0759493671
    },
    {
        "Challenge_adjusted_solved_time":44.3683333333,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Challenge_closed_time":1660908206000,
        "Challenge_created_time":1660748480000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Challenge_link_count":1,
        "Challenge_readability":7.0,
        "Challenge_reading_time":27.79,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":44.3683333333,
        "Challenge_title":"[mlflow] Migration Job should run before upgrade",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":277,
        "Platform":"Github",
        "Solution_body":"Hi @faceless7171 \r\n\r\nThank you very much for reporting the issue. Yes, your suggestion can work. Let me create a PR and test it. Well, we can't use the pre-hook option because we need a configuration file for the DB connection. And I don't want to make secrets visible in the container.\r\n\r\n```console\r\n\u2502 Events:                                                                                                                                                          \u2502\r\n\u2502   Type     Reason       Age               From               Message                                                                                             \u2502\r\n\u2502   ----     ------       ----              ----               -------                                                                                             \u2502\r\n\u2502   Normal   Scheduled    22s               default-scheduler  Successfully assigned default\/mlflow-bzb8s to minikube                                              \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"migrations-config\" : configmap \"mlflow-migrations\" not found   \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"dbchecker\" : configmap \"mlflow-dbchecker\" not found            \u2502\r\n\u2502                                                                                                                                                                  \u2502\r\n```\r\n\r\nSo, we have another option. Maybe we can use the init container pattern for this purpose. Let me try. @all-contributors please add @faceless7171  for bug @burakince \n\nI've put up [a pull request](https:\/\/github.com\/community-charts\/helm-charts\/pull\/37) to add @faceless7171! :tada: Hi @faceless7171 \r\n\r\nCould you please try again with mlflow chart minimum 0.7.0 version? @burakince tested on 0.7.1 version. Everything is working now. Thanks for the fix.",
        "Solution_link_count":1.0,
        "Solution_readability":7.2,
        "Solution_reading_time":15.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":53.7558333333,
        "Challenge_answer_count":5,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Challenge_closed_time":1660345866000,
        "Challenge_created_time":1660152345000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Challenge_link_count":2,
        "Challenge_readability":8.7,
        "Challenge_reading_time":30.0,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":53.7558333333,
        "Challenge_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":261,
        "Platform":"Github",
        "Solution_body":"Hi @mohittalele ,\r\n\r\nThank you very much for reporting the error. Could you please share your mlflow pod and training pod logs with me?\r\n\r\nBest,\r\nBurak Hi @mohittalele \r\n\r\nI think you have a misconfiguration. I added a [full example to here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/bitnami-postgresql-and-bitnami-minio-sklearn-training-example). Simply, your `MLFLOW_S3_ENDPOINT_URL` configuration is wrong. URL must be `http:\/\/minio.airflow.svc.cluster.local:9000`. Could you please fix your configuration and try again?\r\n\r\nBest,\r\nBurak @burakince  Here is the log of the training container. Somehow the trining container does not \"know\" of the s3 endpoint and it is using the local path. \r\n\r\n```\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - The git executable must be specified in one of the following ways:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be included in your $PATH\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - explicitly set via git.refresh()\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - All git commands will error until this is rectified.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - This initial warning can be silenced or aggravated in the future by setting the\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - quiet|q|silence|s|none|n|0: for no warning or exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - warn|w|warning|1: for a printed warning\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - error|e|raise|r|2: for a raised exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Example:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     export GIT_PYTHON_REFRESH=quiet\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   RMSE: 0.793164022927685\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   MAE: 0.6271946374319586\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   R2: 0.10862644997792636\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_artifact_uri ::  .\/mlruns\/0\/3b376331bcaa4894a0723fe4b690658f\/artifacts\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_registry_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_tracking_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ElasticnetWineModel, version 11\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Created version '11' of model 'ElasticnetWineModel'.\r\n[2022-08-11, 13:53:30 CEST] {kubernetes_pod.py:453} INFO - Deleting pod: mlflow-example-1-7e89c5e6540645e3822fbf34410c6b99\r\n[2022-08-11, 13:53:30 CEST] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=mlflow, task_id=mlflow_example_1, execution_date=20220811T115225, start_date=20220811T115226, end_date=20220811T115330\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:156} INFO - Task exited with return code 0\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n```\r\n\r\n\r\n\r\nmlflow logs are quite, There is nothing logged there. I will try out the example. Thanks for the example. \r\n\r\nedit : with 9000 port number specifie, there is no improvement @burakince The setup now works. Actually there was problem with VPN setting since I was deploying mlflow behind mlflow. We can close the issue :) Hi @mohittalele,\r\n\r\nI'm glad to hear the problem was resolved. If you need anything else, please don't hesitate to open a new issue.\r\n\r\nBest,\r\nBurak",
        "Solution_link_count":4.0,
        "Solution_readability":7.5,
        "Solution_reading_time":60.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":66.0,
        "Solution_word_count":507.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":2.7277777778,
        "Challenge_answer_count":4,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1658854769000,
        "Challenge_created_time":1658844949000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_readability":9.3,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.7277777778,
        "Challenge_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Challenge_topic":"Kubernetes Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":286,
        "Platform":"Github",
        "Solution_body":"Hi @mikwieczorek \r\n\r\nThanks to inform us. Yes, probably it's my mistake. I changed it to the name some time ago. Let's write some tests and fix the problem. Well, it looks like your link refers to the port (service port) rather than targetPort (pod's port. This is currently what we use.). But we can even make it optional (port or targetPort selection) and use a name rather than a port number. It looks like it works with the latest version of Prometheus but I think we need to support all versions together.\r\n\r\nAnd [this is the full schema of endpoints field](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/api.md#monitoring.coreos.com\/v1.Endpoint).\r\n\r\nI will do some additional manual tests and send the PR. Hi @mikwieczorek \r\n\r\nChart version 0.3.0 should solve your problem. If it still accrues, feel free to reopen this issue. You can use the following command to update your deployment without the need for additional changes.\r\n\r\n```\r\nhelm repo update\r\nhelm upgrade --install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\r\n```\r\n\r\nBest,\r\nBurak Thank you @burakince for your prompt fix. It works correctly after the update. \r\nNext time, I will make an MR instead of just reporting the issue",
        "Solution_link_count":1.0,
        "Solution_readability":7.3,
        "Solution_reading_time":15.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":187.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":18.5819444444,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nThe new staticPrefix argument being under extraArgs breaks the chart for users that need to use the extraArgs\n\n### What's your helm version?\n\nversion.BuildInfo{Version:\"v3.8.1\", GitCommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", GitTreeState:\"clean\", GoVersion:\"go1.17.8\"}\n\n### What's your kubectl version?\n\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin\/arm64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/arm64\"}\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.7\n\n### What happened?\n\nThe newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesnt exist.\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f mlflow\/values.yaml mlflow .\/mlflow\/\n\n### Anything else we need to know?\n\nI am just creating a pull request to address this in a bit different way and havent tested it yet. Just wanted to create a request to highlight a solution.\r\n\r\nYou could also handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make this work smoother for a final user, but this solution should work as well.",
        "Challenge_closed_time":1657616964000,
        "Challenge_created_time":1657550069000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_readability":9.1,
        "Challenge_reading_time":23.2,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":18.5819444444,
        "Challenge_title":"[mlflow] Extra args broken",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":213,
        "Platform":"Github",
        "Solution_body":"Hi @subramaniam20jan \r\n\r\nCould you please share your values.yaml file with me? Do you have any additional change in it? Hi @subramaniam20jan \r\n\r\nI really didn't understand the problem. Static prefix is valid argument for mlflow server. You can find more information [here](https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-static-prefix).\r\n\r\nAlso, it tested with argument and without argument in [the unit tests](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/charts\/mlflow\/unittests\/deployment_test.yaml#L65). Also, it tested without argument in the integration test which we run it with [kind here](https:\/\/github.com\/community-charts\/helm-charts\/runs\/7283774204?check_suite_focus=true#step:12:175).\r\n\r\nI'm really not able to recreate the issue. Could you please share the error message? Well, if you use `mlflow ui` command, you must change it to `mlflow server` command for production usage. You can find same explanation from here: https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui Actually it was my bad. This wasnt really an issue but I appreciate the addition of the extra parameters to the readiness and liveness probe :) btw, @burakince great job on the chart and image! I was something I have made many times in individual assignments and missed having in open source somewhere. Came across your project when I was about to create one myself. Saves me a lot of work :) Thanks @subramaniam20jan :) I just added an example usage to [here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/liveness-probe-and-readiness-probe-example).\r\n\r\nBest",
        "Solution_link_count":5.0,
        "Solution_readability":11.2,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":191.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":1.4025,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1656583953000,
        "Challenge_created_time":1656578904000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Challenge_link_count":1,
        "Challenge_readability":7.8,
        "Challenge_reading_time":18.48,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4025,
        "Challenge_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":147,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":142.5483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## Instructions \r\nPage 133 of chapter 7 requires the reader to navigate to the following directory and enter the commands below:\r\n\r\n`cd Chapter07\/psystock-data-features-main`\r\n `mlflow run . --experiement-name=psystock_data_pipelines`\r\n\r\n## Problem\r\n\r\nThe following error message appears when running line of code specified above:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"feature_set_generation.py\", line 30, in <module>\r\n    raise Exception('x should not exceed 5. The value of x was: {}'.format(x))\r\nNameError: name 'x' is not defined\r\n```\r\n\r\n## Solution\r\nResolve this by deleting line 30 below in `feature_set_generation.py`\r\n\r\n`30         raise Exception('x should not exceed 5. The value of x was: {}'.format(x))`\r\nThe stray `raise` statement is referencing an undefined variable `x`.\r\n\r\nRemoving this line of code removed the reference to this point and lead to the successful deployment of the experiment. I would consider adding such assertions in the `check_verify_data.py` file instead.\r\n\r\n",
        "Challenge_closed_time":1637063606000,
        "Challenge_created_time":1636550432000,
        "Challenge_link":"https:\/\/github.com\/PacktPublishing\/Machine-Learning-Engineering-with-MLflow\/issues\/7",
        "Challenge_link_count":0,
        "Challenge_readability":9.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":59.0,
        "Challenge_repo_issue_count":18.0,
        "Challenge_repo_star_count":82.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":142.5483333333,
        "Challenge_title":"Chapter 7 `mlflow run . --experiement-name=psystock_data_pipelines` fails - BUGFIX",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":138,
        "Platform":"Github",
        "Solution_body":"Thanks, invaluable contributions. We will add this to the Errata!!!",
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":0.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.3333333333,
        "Challenge_watch_issue_ratio":0.5
    },
    {
        "Challenge_adjusted_solved_time":112.1258333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Challenge_closed_time":1601714116000,
        "Challenge_created_time":1601310463000,
        "Challenge_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Challenge_link_count":0,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":22.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":112.1258333333,
        "Challenge_title":"Improve mlflow logging for population",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":"0332ede5",
        "Solution_link_count":0.0,
        "Solution_readability":-3.5,
        "Solution_reading_time":0.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":1.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0909090909,
        "Challenge_watch_issue_ratio":0.1818181818
    },
    {
        "Challenge_adjusted_solved_time":1301.8425,
        "Challenge_answer_count":6,
        "Challenge_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Challenge_closed_time":1662562977000,
        "Challenge_created_time":1657876344000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Challenge_link_count":0,
        "Challenge_readability":16.2,
        "Challenge_reading_time":101.65,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":1301.8425,
        "Challenge_title":"MLflow fails to log to a tracking server",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":588,
        "Platform":"Github",
        "Solution_body":"cc @sgugger  I'm not the one who wrote or supports the ML Flow callback :-) @noise-field wrote the integration two years ago, do you have an idea of why it doesn't seem to work anymore @noise-field? @juliensimon, I had an error message similar (I think). I found that the issue was related to values with empty string values  (https:\/\/github.com\/mlflow\/mlflow\/issues\/6253), and it looks like there is a patch in the upcoming MLFLOW version 1.28 (not yet released)\r\n\r\nIn my case, I had to set `mp_parameters` to `None` instead of leaving it as an empty string (the default value), and I see your error message has `{'key': 'mp_parameters', 'value': ''}`.\r\n\r\nWhile later MLflow version fix will address this issue, I think setting the `mp_parameters` to `None` instead of an empty string is cleaner. However, I'm not sure about the extent of this change.\r\n\r\n OK, I'll give it a try and I'll let you know. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_link_count":2.0,
        "Solution_readability":8.5,
        "Solution_reading_time":15.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":195.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0212652587,
        "Challenge_watch_issue_ratio":0.0417554737
    },
    {
        "Challenge_adjusted_solved_time":43.4361111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\n- mlflow==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- Platform: Linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- Huggingface_hub version: 0.2.1\r\n- PyTorch version (GPU?): 1.10.2 (False)\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\nShould be fixed by #17067\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n1. Follow Training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. Change the training arguments to use `TrainingArguments(output_dir=\"test_trainer\", report_to=['mlflow'], run_name=\"run0\")`\r\n3. On `trainer.train()` the MLFlow UI should report a run with a Run Name of `run0` which is not currently the case.\r\n\r\nCause of the Issue:\r\n```\r\n>> import mlflow\r\n>> print(mlflow.active_run is None, mlflow.active_run() is None)\r\nFalse True\r\n```\r\n\r\nIn `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is None:` need to be replaced by `if self._ml_flow.active_run() is None:`\r\n\r\n### Expected behavior\r\n\r\nPR #14894 introduce support for run_name in the MLflowCallback. Though, this does not work as expected since the active run is checked using a method reference that always returns true. Bug introduced by #16131.\r\n",
        "Challenge_closed_time":1651758596000,
        "Challenge_created_time":1651602226000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17066",
        "Challenge_link_count":1,
        "Challenge_readability":6.6,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":43.4361111111,
        "Challenge_title":"Incorrect check for MLFlow active run in MLflowCallback",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":177,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0212652587,
        "Challenge_watch_issue_ratio":0.0417554737
    },
    {
        "Challenge_adjusted_solved_time":1209.0019444444,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1640642583000,
        "Challenge_created_time":1636290176000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Challenge_link_count":4,
        "Challenge_readability":12.9,
        "Challenge_reading_time":45.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":1209.0019444444,
        "Challenge_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":338,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":1106.8952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Challenge_closed_time":1635553585000,
        "Challenge_created_time":1631568762000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Challenge_link_count":2,
        "Challenge_readability":12.7,
        "Challenge_reading_time":26.53,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1106.8952777778,
        "Challenge_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":226,
        "Platform":"Github",
        "Solution_body":"`log_metrics` is part of the implementation of `LightningLoggerBase` yet using the experiment property returns the MlFlowClient which can be used to access methods specific to mlflow. So simply removing the experiment property from your calls should solve your problem.\r\n\r\nThe log_metric option of the mlflow client requires different args, see [here](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/mlflow.py#L226) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_link_count":1.0,
        "Solution_readability":12.5,
        "Solution_reading_time":8.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":14.3083333333,
        "Challenge_answer_count":3,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers.mlflow.html?highlight=logger#mlflow-logger) mentions there is an argument called run_name for the mlflow logger, where the run_name of a given experiment can be provided. Although,run_name is an unknown argument to the mlflow logger\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## Please reproduce using the BoringModel\r\nColab link:  https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n### To Reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nimport mlflow\r\n\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### Environment\r\nColab - https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n",
        "Challenge_closed_time":1626409391000,
        "Challenge_created_time":1626357881000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/8431",
        "Challenge_link_count":3,
        "Challenge_readability":18.4,
        "Challenge_reading_time":11.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.3083333333,
        "Challenge_title":"mlflow run_name unexpected argument error",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":69,
        "Platform":"Github",
        "Solution_body":"That's because you are looking at the docs under \"latest\" which is the development version. On the master branch, there is a run_name argument but 1.3.x does not have that. You are probably using Lightning 1.3.x. \r\nIf you want, you can install Lightning rc0 to test the features before the 1.4 release.   Here are the docs for the stable version (1.3.x) https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html Okay got it. Thanks for clarifying ",
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":6.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":210.9636111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Challenge_closed_time":1617875123000,
        "Challenge_created_time":1617115654000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Challenge_link_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":210.9636111111,
        "Challenge_title":"mlflow run context is not logged when using MLFlowLogger",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":936.1641666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1619815518000,
        "Challenge_created_time":1616445327000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Challenge_link_count":1,
        "Challenge_readability":10.0,
        "Challenge_reading_time":22.86,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":936.1641666667,
        "Challenge_title":"External MLFlow logging failures cause training job to fail",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":224,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nCurrently the `MLFlowLogger` creates a new run when resuming from an hpc checkpoint, e.g., after preemption by slurm and requeuing. Runs are an MLFlow concept that groups things in their UI, so when resuming after requeue, it should really be reusing the run ID. I think this can be patched into the hpc checkpoint using the logger which I believe exposes the run ID. This can also be seen on the `v_num` on the progress bar which changes after preemption (in general that `v_num` probably shouldnt be changing in this case). I'm happy to attempt to PR this if the owners agree that it's a bug.\r\n\r\n### To Reproduce\r\n\r\nUse `MLFlowLogger` on a slurm cluster and watch the mlflow UI when preemption happens, there will be a new run created.\r\n\r\n### Expected behavior\r\n\r\nRuns are grouped neatly on the MLFlow UI\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.20.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0\r\n        - tqdm:              4.57.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.1\r\n        - version:           #1 SMP Thu Jan 21 16:15:07 EST 2021\r\n\n\ncc @awaelchli @ananthsub @ninginthecloud @rohitgr7 @tchaton @akihironitta",
        "Challenge_closed_time":null,
        "Challenge_created_time":1614282696000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6205",
        "Challenge_link_count":0,
        "Challenge_readability":7.2,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Logger Makes a New Run When Resuming from hpc Checkpoint",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":195,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":162.0608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1613510526000,
        "Challenge_created_time":1612927107000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Challenge_link_count":3,
        "Challenge_readability":9.6,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":162.0608333333,
        "Challenge_title":"MlflowLogger fail when logging long parameters",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":"Dear @ducthienbui97,\n\nThanks for opening a PR.\n\nBest,\nT.C",
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":307.9769444444,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nUsing log_gpu_memory with MLFLow logger causes an error. It appears the name of the metric is not supported by MLFLow.\r\n\r\n    MlflowException: Invalid metric name: 'gpu_id: 0\/memory.used (MB)'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/).\r\n\r\n### To Reproduce\r\nI reproduced the bug with the BoringModel, in the link bellow:\r\nhttps:\/\/colab.research.google.com\/drive\/1P8uhSfjvYhKPMyRZH-QmfbOUOfnePy6G?usp=sharing\r\n\r\n### Expected behavior\r\nlog_gpu_memory should log gpu memory correctly when using an MLFlow logger.\r\n\r\n### Environment\r\nColab environment:\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cu101\r\n\t- pytorch-lightning: 1.0.3\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.9\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n",
        "Challenge_closed_time":1605009026000,
        "Challenge_created_time":1603900309000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4411",
        "Challenge_link_count":1,
        "Challenge_readability":8.7,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":307.9769444444,
        "Challenge_title":"Using log_gpu_memory with MLFLow logger causes an exception.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":125,
        "Platform":"Github",
        "Solution_body":"This could be fixed removing the parenthesis from the string (as in the linked pr), but requires discussion if you guys want to change this for all loggers. would [MB] work? or should MLFlowLogger sanitize keys automatically by removing the unsupported characters? Seems like MLFlow only wants: \"[...] Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/)\".\r\n\r\nMaybe make the GPU memory key different only if MLFlow is being used? Sanitizing mlflow metrics with a warning might also be an option.  I see sanitization directly in the MLFlowLogger as a better long term solution. We already do sanitization for other loggers (for hyperparameters). \r\n I changed the PR to remove parenthesis in the mlflow logger, however i don't know if it makes sense to give a warning, since the metric name for log_gpu_memory is not defined by the user. Should the removing of parenthesis be silent? Maybe do a PR on mlflow github too to support these or do sanitization process :joy: ",
        "Solution_link_count":0.0,
        "Solution_readability":8.2,
        "Solution_reading_time":12.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":163.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":6.9419444444,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Challenge_closed_time":1602141183000,
        "Challenge_created_time":1602116192000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Challenge_link_count":0,
        "Challenge_readability":18.0,
        "Challenge_reading_time":59.98,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":6.9419444444,
        "Challenge_title":"mlflow logger complains about missing run_id",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":304,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Here, mlflow logger is actually an MlflowClient object. so you ll need to use the function calls specified in this doc - https:\/\/www.mlflow.org\/docs\/latest\/_modules\/mlflow\/tracking\/client.html . These functions needs run_id as first argument which can be accessed as self.logger.run_id @nazim1021 thx for clarification! @qianyu-berkeley feel free to reopen if needed... Thanks! Same problem here, working with @nazim1021 suggestion. What about adding it to the doc? > What about adding it to the doc?\r\n\r\ngood idea, mind send a PR? :]",
        "Solution_link_count":1.0,
        "Solution_readability":5.0,
        "Solution_reading_time":7.33,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":1597.1266666667,
        "Challenge_answer_count":15,
        "Challenge_body":"## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nTrying to log model into mlflow using `mlflow.pytorch.log_model` in train end. Getting the above error only in multi gpu scenario. \r\n\r\n#### Code\r\n\r\n\r\nmnist script file - \r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom argparse import ArgumentParser\r\n#from mlflow.pytorch.pytorch_autolog import __MLflowPLCallback\r\nfrom pytorch_lightning.logging import MLFlowLogger\r\nfrom sklearn.metrics import accuracy_score\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        \"\"\"\r\n        Initializes the network\r\n        \"\"\"\r\n        super(LightningMNISTClassifier, self).__init__()\r\n\r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\r\n            \"--batch-size\",\r\n            type=int,\r\n            default=64,\r\n            metavar=\"N\",\r\n            help=\"input batch size for training (default: 64)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--num-workers\",\r\n            type=int,\r\n            default=0,\r\n            metavar=\"N\",\r\n            help=\"number of workers (default: 0)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--lr\",\r\n            type=float,\r\n            default=1e-3,\r\n            metavar=\"LR\",\r\n            help=\"learning rate (default: 1e-3)\",\r\n        )\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        Forward Function\r\n        \"\"\"\r\n        batch_size, channels, width, height = x.size()\r\n\r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n\r\n        # layer 1 (b, 1*28*28) -> (b, 128)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 2 (b, 128) -> (b, 256)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 3 (b, 256) -> (b, 10)\r\n        x = self.layer_3(x)\r\n\r\n        # probability distribution over labels\r\n        x = torch.log_softmax(x, dim=1)\r\n\r\n        return x\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        \"\"\"\r\n        Loss Fn to compute loss\r\n        \"\"\"\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        \"\"\"\r\n        training the data as batches and returns training loss on each batch\r\n        \"\"\"\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        \"\"\"\r\n        Performs validation of data in batches\r\n        \"\"\"\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average validation accuracy\r\n        \"\"\"\r\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs}\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Performs test and computes test accuracy\r\n        \"\"\"\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        a, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy_score(y_hat.cpu(), y.cpu())\r\n        return {\"test_acc\": torch.tensor(test_acc)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\r\n        return {\"avg_test_acc\": avg_test_acc}\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Preprocess the input data.\r\n        \"\"\"\r\n        return {}\r\n\r\n    def train_dataloader(self):\r\n        \"\"\"\r\n        Loading training data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_train,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        \"\"\"\r\n        Loading validation data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\r\n\r\n        return DataLoader(\r\n            mnist_val,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        \"\"\"\r\n        Loading test data as batches\r\n        \"\"\"\r\n        mnist_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_test,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Creates and returns Optimizer\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        self.scheduler = {\r\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n                self.optimizer,\r\n                mode=\"min\",\r\n                factor=0.2,\r\n                patience=2,\r\n                min_lr=1e-6,\r\n                verbose=True,\r\n            )\r\n        }\r\n        return [self.optimizer], [self.scheduler]\r\n\r\n    def optimizer_step(\r\n        self,\r\n        epoch,\r\n        batch_idx,\r\n        optimizer,\r\n        optimizer_idx,\r\n        second_order_closure=None,\r\n        on_tpu=False,\r\n        using_lbfgs=False,\r\n        using_native_amp=False,\r\n    ):\r\n        self.optimizer.step()\r\n        self.optimizer.zero_grad()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    from pytorch_autolog import autolog\r\n    autolog()\r\n    model = LightningMNISTClassifier()\r\n    mlflow_logger = MLFlowLogger(\r\n        experiment_name=\"Default\", tracking_uri=\"http:\/\/localhost:5000\/\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        logger=mlflow_logger,\r\n        gpus=2,\r\n        distributed_backend=\"ddp\",\r\n        max_epochs=1\r\n    )\r\n    trainer.fit(model)\r\n    trainer.test()\r\n\r\n```\r\n\r\nSample code from autolog - Callback class. \r\n\r\n```\r\n    class __MLflowPLCallback(pl.Callback):\r\n\r\n        def __init__(self):\r\n            super().__init__()\r\n\r\n        def on_train_end(self, trainer, pl_module):\r\n            \"\"\"\r\n            Logs the model checkpoint into mlflow - models folder on the training end\r\n            \"\"\"\r\n\r\n            mlflow.set_tracking_uri(trainer.logger._tracking_uri )\r\n            mlflow.set_experiment(trainer.logger._experiment_name)\r\n            mlflow.start_run(trainer.logger.run_id)\r\n            mlflow.pytorch.log_model(trainer.model, \"models\")\r\n            mlflow.end_run()\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\nStack Trace\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                          \r\n  File \"mnist.py\", line 231, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 218, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 209, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 992, in fit\r\n    results = self.spawn_ddp_children(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 462, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, q=None, model=model, is_master=True)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 560, in ddp_train\r\n    results = self.run_pretrain_routine(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1213, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 392, in train\r\n    self.run_training_teardown()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 872, in run_training_teardown\r\n    self.on_train_end()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 72, in on_train_end\r\n    callback.on_train_end(self, self.get_model())\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 120, in on_train_end\r\n    mlflow.pytorch.log_model(trainer.model, \"models\")\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 179, in log_model\r\n    signature=signature, input_example=input_example, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/models\/model.py\", line 154, in log\r\n    **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 300, in save_model\r\n    torch.save(pytorch_model, model_path, pickle_module=pickle_module, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 370, in save\r\n    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 443, in _legacy_save\r\n    pickler.dump(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 491, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 659, in save_reduce\r\n    self._batch_setitems(dictitems)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\n\r\n```\r\n\r\n\r\n\r\n#### What have you tried?\r\nTried out the possibilities mentioned in the similar thread - https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/2186\r\n\r\nTried  wrapping the code inside, `trainer.is_global_zero`  . And also tried `trainer.global_rank == 0`. Also tried decorating the method as `@rank_zero_only`. But no luck. Getting the same error. \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu\r\n - Packaging - torch, pytorch-lightning, torchvision, mlflow",
        "Challenge_closed_time":1605489870000,
        "Challenge_created_time":1599740214000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3444",
        "Challenge_link_count":2,
        "Challenge_readability":14.2,
        "Challenge_reading_time":152.56,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":140,
        "Challenge_solved_time":1597.1266666667,
        "Challenge_title":"TypeError: can't pickle _thread.lock objects - Error while logging model into mlflow in multi gpu scenario",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":935,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nThanks for the reply. Sure i will try and update here. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nI removed the scheduler part and re-ran the script. Still experiencing the same error.  @lucadiliello @williamFalcon Any suggestions here ? One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved. \r\n\r\nTested with 2 gpus and 0.9 version of pytorch lightning.  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I have the same error. Training with a single gpu works fine but with multiple the error is raised > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n\r\nTraining with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\nSomething like:\r\n```python\r\n>>> model = MyModel(...)\r\n>>>\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\n\r\nIn my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled. > One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved.\r\n> \r\n> Tested with 2 gpus and 0.9 version of pytorch lightning.\r\n\r\nPlease update to the latest version and let us know whether the error persist. > > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n> \r\n> Training with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\n> Something like:\r\n> \r\n> ```python\r\n> >>> model = MyModel(...)\r\n> >>>\r\n> >>> import pickle\r\n> >>> pickle.dump(model, \"tmp_file.pk\")\r\n> ```\r\n> \r\n> In my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled.\r\n\r\nI tried adding as suggested:\r\n```\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\nHowever, this raise the error: TypeError: file must have a 'write' attribute\r\nI then tried with:\r\n```\r\n>>> import pickle\r\n>>> with open(\"tmp_file.pk\",\"wb\") as f:\r\n>>>         pickle.dump(model, f)\r\n```\r\nThis then again raised TypeError: can't pickle _thread.lock objects\r\nI am on the latest version of pytorch lightning This suggests that the problem is in your model. Can you post it here?\n\nA complete log of the error would also be useful. Thanks I ran into this as well. I wrote a little function to iterate through the model.__dict__.items() and check which caused pickle errors. It looks like there's a model attribute pointing to the trainer, which has that _thread.lock on it. Maybe there's a step that's supposed to clean this up that's being missed somehow? My quick work-around was to `delattr(model, \"trainer\")` before pickling the model, but I haven't actually tried loading the model again, so I this could cause other problems. same problem same problem on 1.2.4, works fine with 1.1.0. any ideas what might be causing the difference between the versions?",
        "Solution_link_count":2.0,
        "Solution_readability":7.2,
        "Solution_reading_time":51.46,
        "Solution_score_count":3.0,
        "Solution_sentence_count":63.0,
        "Solution_word_count":630.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":27.1641666667,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Challenge_closed_time":1599644307000,
        "Challenge_created_time":1599546516000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Challenge_link_count":3,
        "Challenge_readability":10.7,
        "Challenge_reading_time":59.64,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":54,
        "Challenge_solved_time":27.1641666667,
        "Challenge_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":532,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\r\nI mean, your network is a single linear layer, you probably run through epochs super fast.\r\nI am not yet convinced it is a bug, but I'll try your example code hey @awaelchli, Thanks for replying!\r\nThe model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\r\n\r\nI've done some debugging and added a solution section to the issue. It appears to be in in the `experiment` property of the MLFlowLogger. Each time `.experiment` is accessed, `self._mlflow_client.get_experiment_by_name(self._experiment_name)` is called, which communicates with the MLFlow server.\r\n\r\nIt seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger. oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.  yeah sure, I'll link it here shortly. Did you encounter this #3392 problem as well?",
        "Solution_link_count":0.0,
        "Solution_readability":7.1,
        "Solution_reading_time":15.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":31.125,
        "Challenge_answer_count":8,
        "Challenge_body":"I think I'm logging correctly, this is my `training_step`\r\n\r\n        result = pl.TrainResult(loss)\r\n        result.log('loss\/train', loss)\r\n        return result\r\n\r\nand `validation_step`\r\n\r\n        result = pl.EvalResult(loss)\r\n        result.log('loss\/validation', loss)\r\n        return result\r\n\r\nThe validation loss is updated in mlflow each epoch, however the training loss isn't displayed until training has finished. Then it's available for every step. This may be a mlflow rather than pytorch-lighting issue - somewhere along the line it seems to be buffered?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/5028974\/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png)\r\n\r\nVersions:\r\n\r\npytorch-lightning==0.9.0\r\nmlflow==1.11.0\r\n\r\nEdit: logging TrainResult with on_epoch=True results in the metric appearing in mlflow during training, it's only the default train logging which gets delayed. i.e.\r\n\r\n        result.log('accuracy\/train', acc, on_epoch=True)\r\n\r\nis fine\r\n\r\n",
        "Challenge_closed_time":1599634019000,
        "Challenge_created_time":1599521969000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3392",
        "Challenge_link_count":1,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.12,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":31.125,
        "Challenge_title":"mlflow training loss not reported until end of run",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":107,
        "Platform":"Github",
        "Solution_body":"When using the minimal example provided in the linked issue, and using the default training logging shown above, I don't see the behaviour described. \r\nI can sometimes see a discrepancy between the reported steps for each metrics, but I suspect this is to do with MLFlow and not the PL logger.\r\n![newplot](https:\/\/user-images.githubusercontent.com\/17157991\/92454951-7120fe00-f204-11ea-99f3-7d2ac09b0f5e.png)\r\n\r\n@david-waterworth could you elaborate on a few points?\r\n1. When you set up the MLFLowLogger, if your `tracking_uri` over `http:` or using `file:`?\r\n2. If `http`, is the tracking server remote?\r\n3. How long does a model training run typically take? \r\n4. does this behaviour consistently happen even when refreshing the MLFlow page?\r\n @patrickorlando \r\n\r\nWhen you set up the MLFLowLogger, if your tracking_uri over http: or using file:?\r\n\r\nI'm using file i.e. `mlflow = loggers.MLFlowLogger(\"Transformer\")`\r\n\r\nHow long does a model training run typically take?\r\n\r\n10-20 minutes\r\n\r\ndoes this behavior consistently happen even when refreshing the MLFlow page?\r\n\r\nYes I've tried to reproduce this but cant seem to. I can confirm that the MLFlow logger is logging metrics at the end of each epoch and for me they show up in the MLFlow UI as I refresh the page. \r\nDo you have a working code sample that can reproduce the issue?\r\n So I can actually see the behaviour you've described, but not when using the minimal example in #3393. I'll try to work out why. So I _think_ this is because of the default behaviour of the `TrainResult` and the way `row_log_interval` works. And it only appears if the number of batches per epoch is less than `row_log_interval`\r\n\r\nBy default TrainResult logs on step and not on epoch.\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/aaf26d70c4658e961192ba4c408558f1cf39bb18\/pytorch_lightning\/core\/step_result.py#L510-L517\r\n\r\nWhen logging only per step, the logger connector only logs when the `batch_idx` is a multiple of `row_log_interval`. However if you don't have more than `row_log_interval` batches, the metrics are not logged.\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/aaf26d70c4658e961192ba4c408558f1cf39bb18\/pytorch_lightning\/trainer\/logger_connector.py#L229-L237\r\n\r\n@david-waterworth Do you have less than 50 batches per epoch in your model? can you try setting `row_log_interval` to be less than the number of train batches to confirm whether the issue is caused by this?\r\n\r\n @patrickorlando yes I have 38 batches per epoch. I set `row_log_interval=1` and now the training step metrics are being displayed as they're generated. > yes I have 38 batches per epoch. I set row_log_interval=1 and now the training step metrics are being displayed as they're generated.\r\n\r\nThat makes sense now :) \r\n@david-waterworth Should we close this? or is there something left unresolved? Thanks for the assistance, no nothing unresolved.",
        "Solution_link_count":3.0,
        "Solution_readability":8.8,
        "Solution_reading_time":35.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":405.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":9.3011111111,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1597848054000,
        "Challenge_created_time":1597814570000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Challenge_link_count":2,
        "Challenge_readability":16.5,
        "Challenge_reading_time":47.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":9.3011111111,
        "Challenge_title":"MLFlowLogger throws a JSONDecodeError",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":299,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Hi, thanks for submitting the bug. I don't know what's going on here. I cannot reproduce with your instructions. \r\n\r\nI'm running your sample code \r\n\r\n```python \r\n    mlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"http:\/\/127.0.0.1:5000\")\r\n    trainer = Trainer(logger=mlflow_logger)\r\n    trainer.logger.experiment_id\r\n```\r\nand the tracking uri I got from running \r\n```bash\r\nmlflow ui\r\n```\r\nThe experiment shows up in the UI and I get no errors. I verified this with the latest version of PL and mlflow, python 3.7.\r\n\r\nIs there any other information you can provide on the issue? Thanks for the quick response, @awaelchli! \r\n\r\nI did nothing different this morning and I am able to log metrics\/parameters to mlflow. I will close this now and in case I encounter this again, I will reopen this issue.",
        "Solution_link_count":1.0,
        "Solution_readability":7.0,
        "Solution_reading_time":10.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":124.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":59.9219444444,
        "Challenge_answer_count":3,
        "Challenge_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Challenge_closed_time":1597488847000,
        "Challenge_created_time":1597273128000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Challenge_link_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":59.9219444444,
        "Challenge_title":"mlflow checkpoints in the wrong location ",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":82,
        "Platform":"Github",
        "Solution_body":"@david-waterworth mind try the latest 0.9rc12? It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n Thanks @awaelchli  yes that's what I want - thanks!",
        "Solution_link_count":0.0,
        "Solution_readability":5.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":487.0688888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a `DictConfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#L177\r\n\r\n### To Reproduce\r\n\r\nUse Hydra and MLFlow together. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=None,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/utils\/validation.py\", line 120, in _validate_param_name\r\n    INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'\r\n```\r\n\r\n### Expected behavior\r\n\r\nCheck whether the instance if `dict` or `DictConfig` in the given line. \r\n",
        "Challenge_closed_time":1592925645000,
        "Challenge_created_time":1591172197000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2058",
        "Challenge_link_count":3,
        "Challenge_readability":19.8,
        "Challenge_reading_time":50.14,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":487.0688888889,
        "Challenge_title":"Hydra MLFlow Clash",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":248,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! > Check whether the instance if `dict` or `DictConfig` in the given line.\r\n\r\n@ssakhavi that sounds reasonable solution, mind sending a PR - fix and its test?",
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":2.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_created_time":1576464430000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":1965.6686111111,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":409,
        "Platform":"Github",
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_link_count":4.0,
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":72.0,
        "Solution_word_count":675.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":21.5194444444,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\nAttributeError: 'DatabaseServiceMetadataPipeline' object has no attribute 'mlModelFilterPattern'\r\n```\r\n\r\nWe need to review which configuration param is being sent here",
        "Challenge_closed_time":1662467440000,
        "Challenge_created_time":1662389970000,
        "Challenge_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/7232",
        "Challenge_link_count":0,
        "Challenge_readability":12.3,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":126.0,
        "Challenge_repo_fork_count":360.0,
        "Challenge_repo_issue_count":9147.0,
        "Challenge_repo_star_count":1692.0,
        "Challenge_repo_watch_count":21.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.5194444444,
        "Challenge_title":"Mlflow UI deployment error",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":22,
        "Platform":"Github",
        "Solution_body":"sourceConfig type missing to be sent from the UI",
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0137750082,
        "Challenge_watch_issue_ratio":0.0022958347
    },
    {
        "Challenge_adjusted_solved_time":229.9927777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Challenge_closed_time":1649449895000,
        "Challenge_created_time":1648621921000,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Challenge_link_count":0,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.1,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":229.9927777778,
        "Challenge_title":"error creating a triton deployment mlflow plugin",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":159,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0183128775,
        "Challenge_watch_issue_ratio":0.0225988701
    },
    {
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":2,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_created_time":1647959057000,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":745.5772222222,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":227,
        "Platform":"Github",
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_link_count":1.0,
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0183128775,
        "Challenge_watch_issue_ratio":0.0225988701
    },
    {
        "Challenge_adjusted_solved_time":2830.1302777778,
        "Challenge_answer_count":2,
        "Challenge_body":"At the moment, an mlflow byom predictor with arbitrary URLs can be created. We should first check whether an actual mlflow model is served at that URL before creating\/linking said model.",
        "Challenge_closed_time":1656947080000,
        "Challenge_created_time":1646758611000,
        "Challenge_link":"https:\/\/github.com\/mindsdb\/mindsdb\/issues\/2043",
        "Challenge_link_count":0,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":241.0,
        "Challenge_repo_fork_count":1404.0,
        "Challenge_repo_issue_count":4035.0,
        "Challenge_repo_star_count":12007.0,
        "Challenge_repo_watch_count":327.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2830.1302777778,
        "Challenge_title":"[ BYOM MLflow ] Check valid URL when creating predictor",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":"Can we close this @paxcema and @ea-rus  I think we need to merge the above PR after checking there are no conflicts (because it's a bit outdated by now), but once merged we can close this issue.",
        "Solution_link_count":0.0,
        "Solution_readability":13.0,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0597273854,
        "Challenge_watch_issue_ratio":0.0810408922
    },
    {
        "Challenge_adjusted_solved_time":1488.5780555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Challenge_closed_time":1651060598000,
        "Challenge_created_time":1645701717000,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Challenge_link_count":1,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":148.0,
        "Challenge_repo_fork_count":956.0,
        "Challenge_repo_issue_count":3383.0,
        "Challenge_repo_star_count":6165.0,
        "Challenge_repo_watch_count":89.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1488.5780555556,
        "Challenge_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":83,
        "Platform":"Github",
        "Solution_body":"fixed by https:\/\/github.com\/deepset-ai\/haystack\/pull\/2337",
        "Solution_link_count":1.0,
        "Solution_readability":21.0,
        "Solution_reading_time":0.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0437481525,
        "Challenge_watch_issue_ratio":0.0263080106
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":14,
        "Challenge_body":"Trying to integrate Mlflow with my current bentoml workflow and following this example\r\n`https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch`\r\nBut getting error when i try to deploy model with docker, \r\n\r\nwhen i run  `bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\n```Building docker image for Bento(tag=\"mlflow_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")...\r\nERROR: failed to solve: executor failed running [\/bin\/sh -c bash <<EOF\r\nset -euxo pipefail\r\n\r\nif [ -f \/home\/bentoml\/bento\/env\/conda\/environment.yml ]; then\r\n   set pip_interop_enabled to improve conda-pip interoperability. Conda can use\r\n   pip-installed packages to satisfy dependencies.\r\n  echo \"Updating conda base environment with environment.yml\"\r\n  \/opt\/conda\/bin\/conda config --set pip_interop_enabled True\r\n  \/opt\/conda\/bin\/conda env update -n base -f \/home\/bentoml\/bento\/env\/conda\/environment.yml\r\n  \/opt\/conda\/bin\/conda clean --all\r\nfi\r\nEOF]: exit code: 1\r\nFailed building docker image: Command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', 'mlflow_pytfile', 'env\\\\docker\\\\Dockerfile', '--load', '.']' returned non-zero exit status 1.\r\n```\r\n### To reproduce\r\n\r\nBug recreation steps:\r\nClone the repo `https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch` \r\nGoto the folder `examples\/mlflow\/pytorch`\r\n`python mnist.py`\r\n`bentoml build`\r\n`bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\nP.S. `bentoml serve service.py:svc`  works fine`\r\n\r\n\r\n### Environment\r\n\r\nbentoml version 1.0.7\r\nPython version  3.9.12\r\nDocker Engine 20.10.17",
        "Challenge_closed_time":null,
        "Challenge_created_time":1666803000000,
        "Challenge_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/3146",
        "Challenge_link_count":2,
        "Challenge_readability":12.8,
        "Challenge_reading_time":20.29,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":498.0,
        "Challenge_repo_issue_count":3155.0,
        "Challenge_repo_star_count":4302.0,
        "Challenge_repo_watch_count":60.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"bug: failed to containerize when using mlflow",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0427892235,
        "Challenge_watch_issue_ratio":0.0190174326
    },
    {
        "Challenge_adjusted_solved_time":341.4130555556,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nI can't load a *mlflow* model in the beta version of BentoML 1.0\r\n\r\n**To Reproduce**\r\n1. Train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport mlflow\r\n\r\n\r\n# Load training data\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Model Training\r\nclf = svm.SVC()\r\nclf.fit(X, y)\r\n\r\n# Wrap up as a custom pyfunc model\r\nclass ModelPyfunc(mlflow.pyfunc.PythonModel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# Log model\r\nwith mlflow.start_run() as run:\r\n    model = ModelPyfunc()\r\n    mlflow.pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. Load it into BentoML\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml.mlflow.import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml.mlflow.load(tag)\r\n```\r\n3. The model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml.mlflow.load(tag)` is failing to **AttributeError** `module 'mlflow.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**Expected behavior**\r\nPyfunc model should load without issues\r\n\r\n**Screenshots\/Logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"sandbox.py\", line 11, in <module>\r\n    bentoml.mlflow.load(tag)\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/mlflow.py\", line 85, in load\r\n    return loader_module.load_model(mlflow_folder)  # noqa\r\nAttributeError: module 'mlflow.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**Environment:**\r\n - OS: MacOS 11.6\r\n - Python Version Python 3.8.5\r\n - BentoML Version BentoML-1.0.0a1",
        "Challenge_closed_time":1642711286000,
        "Challenge_created_time":1641482199000,
        "Challenge_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/2160",
        "Challenge_link_count":0,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.44,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":498.0,
        "Challenge_repo_issue_count":3155.0,
        "Challenge_repo_star_count":4302.0,
        "Challenge_repo_watch_count":60.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":341.4130555556,
        "Challenge_title":"MLflow pyfunc model can't be loaded",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":187,
        "Platform":"Github",
        "Solution_body":"I think this version is not yet up-to-date with our 1.0 branch. cc @parano Line 85 is different from `main` branch @alexdivet @aarnphm the new 1.0.0a2 was just released, could you help confirm the issue has been resolved? ![Screenshot 2022-01-20 at 15 39 52](https:\/\/user-images.githubusercontent.com\/29749331\/150418600-d75d6b01-679d-4fa7-9a66-395262c13569.png)\r\nWorks just fine for me\r\nRunning on M1 Max, Python 3.9.10, with Rosetta 2. Please let me know if you still run into problems @alexdivet It works on my end too. Thanks for resolving it \ud83d\ude4f\ud83c\udffb",
        "Solution_link_count":1.0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":79.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0427892235,
        "Challenge_watch_issue_ratio":0.0190174326
    },
    {
        "Challenge_adjusted_solved_time":560.8433333333,
        "Challenge_answer_count":3,
        "Challenge_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Challenge_closed_time":1669124369000,
        "Challenge_created_time":1667105333000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Challenge_link_count":6,
        "Challenge_readability":8.4,
        "Challenge_reading_time":37.27,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":560.8433333333,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":296,
        "Platform":"Github",
        "Solution_body":"In addition, runs of `compare_models `and `create_model` get nested recursively as well in pycaret > 2.3.6.\r\nSee screenshot below where the red line shows the behaviour in pycaret==2.3.6 (which is the wanted and expected behaviour) and in orange the nested unwanted behaviour in pycaret 2.3.8 , 2.3.9 and 2.3.10\r\n![image](https:\/\/user-images.githubusercontent.com\/50994394\/200543740-0883c8ba-9f4a-4d1f-8abc-560ae7dbd54e.png)\r\n @nagamatz @tdekelver-bd This issue was recently fixed on last rc release. Can you try installing pycaret with `pip install --pre pycaret` and let me know if you still face the issue. It's not yet fixed with 3.0.0rc4. This is the results with the code in the first post. Now, mlflow is 2.0.1\r\n![\u7121\u984c](https:\/\/user-images.githubusercontent.com\/1991802\/206426156-4b19a4cc-b865-4af3-8281-1b89fc099f28.png)\r\n",
        "Solution_link_count":2.0,
        "Solution_readability":7.7,
        "Solution_reading_time":10.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":60.5666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Challenge_closed_time":1663775400000,
        "Challenge_created_time":1663557360000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Challenge_link_count":6,
        "Challenge_readability":14.4,
        "Challenge_reading_time":20.35,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":60.5666666667,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":145,
        "Platform":"Github",
        "Solution_body":"Cannot reproduce on master. Please try again with `pip install -U --pre pycaret` @nagamatz and reopen the issue if it persists.",
        "Solution_link_count":0.0,
        "Solution_readability":6.2,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":0.6102777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Challenge_closed_time":1660653306000,
        "Challenge_created_time":1660651109000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Challenge_link_count":3,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.6102777778,
        "Challenge_title":"MlFlow not logging metrics",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":161,
        "Platform":"Github",
        "Solution_body":"Update:\r\n\r\nWhen I write `exp.get_logs()` I can see some metrics there. But some runs still have the status \"RUNNING\", unsure why.\r\n\r\nAlso, all the runs that exist when I start the server using `!mlflow ui` are missing metrics. Edit: found issue, not on you! Sorry :)",
        "Solution_link_count":0.0,
        "Solution_readability":2.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":72.28,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nI have a problem saving xgboost run in mlflow server. The run has a status of UNFINISHED, no metrics or artifacts are created. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/101572186\/183577670-53398204-debf-428b-8b0c-3c7ca83f4785.png)\r\n\r\nWhen I use `mlflow ui` everything is fine, but when I run mlflow server with SQLite as backend store the problem occurs.\r\nCommand used to run mlflow server- `mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root \/mlflow\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/mlflow\/experiments\/mlflow.db`\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nimport pandas as pd\r\n\r\nmlflow.set_tracking_uri('http:\/\/localhost:5000')\r\n\r\ndata = pd.DataFrame({'V1': [-1.34419, -1.89211, 1.69421, 0.263328, 0.107918, 0.154241, 0.33468, 1.447778, -0.918269, 0.86319, -1.630049, 1.643798, 1.274341, -1.296742, -0.193585, 1.627422, -0.66805, -1.664491, -1.86911, 0.892885],\r\n                     'V2': [0.85556, -1.70503, -0.02896, 1.746258, -0.084151, 1.673185, 1.113326, -0.23231, 1.054817, -1.407584, 0.474997, 0.150687, -0.738246, -0.045513, 1.58637, 0.984249, 0.624333, 0.298866, 0.662204, 0.967942],\r\n                     'V3': [1.768638, -0.503169, -0.25622, -0.937752, -0.062189, -0.820652, -1.786942, -1.770495, 1.808681, -0.280286, -1.389736, 0.182212, -0.602959, -0.354683, -1.065631, 1.649264, 0.389538, -1.674815, 0.281824, -1.683662],\r\n                     'V4': [1.512828, 1.177697, -1.156862, -1.877876, 1.526013, 1.644001, -1.282481, -0.720543, 0.323963, -1.931616, 1.632839, 1.706752, 1.895627, 1.860705, -1.559702, 1.517466, 1.254323, 1.84415, -1.175013, -1.600652],\r\n                     'V5': [0.820483, -1.20923, -0.012221, 1.682836, 0.104248, 1.258085, 0.404062, 0.18019, 1.352545, -0.497071, 0.771277, 1.614052, -0.693854, 0.002655, 0.277743, -0.977744, -0.97259, -1.501586, -0.731194, -0.551264],\r\n                     'V6': [1.079115, -0.734152, -1.630816, -1.877664, 1.577477, -1.902078, 1.012828, -1.107726, 1.742781, -1.338595, 1.788969, -0.851507, 1.061596, -0.635559, -1.171469, -1.001642, 1.493507, 0.732088, 1.565327, -1.845441],\r\n                     'V7': [1.165929, 1.804607, 0.886589, -0.027458, -1.444197, -0.415643, 0.863924, -1.177661, 1.684514, 1.023797, -1.234116, -0.989024, 0.815575, -0.668453, 0.591911, -0.798925, 1.024032, -1.983963, 1.900752, 1.201001],\r\n                     'V8': [-0.536923, 0.641581, -0.585228, 1.061145, -0.303192, -0.652068, 0.858556, 0.11012, 1.839738, -1.51798, -0.942028, -0.736386, -0.098261, 0.699127, 0.173854, -1.16775, -0.417662, 0.021639, 1.745042, -1.119667],\r\n                     'V9': [0.643498, -1.090347, 0.120182, -0.819219, -1.296763, 0.530723, -1.367664, -0.708116, -1.304274, 1.486166, 1.656498, 1.645308, -0.257558, 0.400849, 1.356781, 1.693433, 0.42606, 0.370683, -0.239278, -0.541334],\r\n                     'V10': [-0.744989, 0.506658, 1.15586, 1.461127, 1.928769, -0.330472, 1.514159, -1.209056, -0.741453, -1.479674, 1.92057, -1.148481, 0.949433, 0.674107, -1.410627, 1.497083, -1.262624, -0.856706, -1.708155, 0.93153],\r\n                     'V11': [0.967242, 1.968385, -1.362337, -0.46194, 0.809224, 0.226177, 1.782128, -0.114595, 0.698243, -0.141743, -0.117251, 1.762656, -0.068839, 0.648945, -1.497037, -1.455443, -0.291242, 1.806048, -1.945438, 0.251282],\r\n                     'V12': [0.010432, -0.101522, -1.764095, 1.326967, -1.299122, -0.549148, 0.807092, -0.75387, 0.955056, 0.640369, -0.917832, 0.250338, 0.624729, 1.566922, 0.118619, 1.907585, -0.919995, 0.868393, -1.103909, 0.347108],\r\n                     'V13': [0.122315, -1.140017, -0.876424, -1.075771, 0.668814, 1.916654, -0.864906, 0.132892, 0.740058, 0.94469, -0.260381, 0.92833, -1.186423, -0.18321, 1.99266, -0.779091, -1.649025, -1.688821, 1.075145, -1.988603],\r\n                     'V14': [-1.494, 0.679776, 0.813194, 1.8687, -0.20273, -0.363265, 1.98902, 0.100025, 1.462866, 0.561017, 0.418922, 1.981837, -1.834009, -1.657952, 0.585069, -0.898764, 0.683234, 0.743215, -0.050289, -0.668302], \r\n                     'V15': [0.199787, 0.81829, 1.200156, -1.684249, 0.847466, 1.326102, 0.323103, -1.010648, -1.868355, -1.204467, 1.777393, 0.375692, -1.654002, 0.50357, -1.372448, -0.522425, 0.360716, 1.007605, 1.009369, -0.353638],\r\n                     'V16': [1.535552, -0.082278, -0.083154, 0.069432, 1.356735, -0.042527, -0.462543, 1.813852, -1.664882, 0.408013, -1.802172, -1.920202, 1.987332, -1.126771, 1.485496, 1.972345, -0.33345, 1.414685, -0.06674, 1.383197],\r\n                     'V17': [-0.249929, 1.668129, 0.860046, 0.013955, 0.085628, 1.285539, -0.754444, -0.306815, -1.244118, -0.61328, 0.711952, 1.384674, 1.710264, 1.337836, -0.029678, -1.382343, -1.963618, 0.088497, -0.110544, 0.954066],\r\n                     'V18': [0.665032, -1.214589, 0.486172, 1.184611, 1.152936, -0.192168, -1.096281, -0.762198, -0.338583, 0.170551, -0.045797, -0.897271, 0.433204, -0.986375, 0.430157, 1.846751, -0.905146, -1.398763, 1.790667, -1.580808],\r\n                     'V19': [1.347637, -0.356925, 0.414118, 0.277104, 0.41587, -1.237646, 0.580625, 1.468221, -0.254781, 0.245683, -1.25356, 0.241325, 1.15677, -1.74525, 1.970698, -0.038675, -0.314979, 0.114507, 1.378524, -0.139709],\r\n                     'V20': [-1.291686, -1.714475, 0.012188, 1.002238, -1.587334, 1.408967, 1.055095, -1.356865, 1.307388, 0.697003, -0.112676, 1.762375, 0.82697, 1.084934, 1.656421, 0.786079, -1.580991, 1.753751, -0.242525, 1.854008],\r\n                     'Class': [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]})\r\n\r\nsetup(data = data,\r\ntarget = 'Class', \r\nexperiment_name = 'xgb_test', \r\nfix_imbalance = True,\r\nlog_experiment = True, \r\nsilent=True, \r\nuse_gpu=True,\r\nfold=5,\r\npreprocess=False)\r\n\r\nmodels = ['xgboost','knn','rf']\r\ntop_models = compare_models(include = model)\r\ndd = pull()\n```\n\n\n### Expected Behavior\n\nArtifacts and metrics should be crated. \n\n### Actual Results\n\n```python-traceback\nError from logs.log:\r\n\r\n2022-08-09 06:11:05,384:ERROR:dashboard_logger.log_model() for XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=None, enable_categorical=False,\r\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:ERROR:Traceback (most recent call last):\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/mlflow_logger.py\", line 46, in log_params\r\n    mlflow.log_params(params)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'None'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'None'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'None'}, {'key': 'enable_categorical', 'value': 'False'}, {'key': 'eval_metric', 'value': 'None'}, {'key': 'feature_types', 'value': 'None'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'None'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n```\n\n\n### Installed Versions\n\n<details>\r\npycaret- Version: 2.3.10 <\/br>\r\nmlflow- Version: 1.27.0 <\/br>\r\nxgboost-  Version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Challenge_closed_time":1660286399000,
        "Challenge_created_time":1660026191000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2838",
        "Challenge_link_count":5,
        "Challenge_readability":8.8,
        "Challenge_reading_time":137.91,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":72.28,
        "Challenge_title":"[BUG]: MLflow server integration",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":925,
        "Platform":"Github",
        "Solution_body":"With new mlflow release-1.28.0- and **[Tracking \/ Model Registry] Fix an mlflow server bug that rejected parameters and tags with empty string values (https:\/\/github.com\/mlflow\/mlflow\/pull\/6179, @dbczumar)** bug fixed, the problem no longer occurs and artifacts are saved correctly",
        "Solution_link_count":1.0,
        "Solution_readability":8.0,
        "Solution_reading_time":3.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":3142.3483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nmlflow logs the name of both models \"Least Angle Regression\" and \"Lasso Least Angle Regression\" as \"Least Angle Regression\".\r\n\r\nWhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags.mlflow.runName`.\r\n\r\nPython Version: 3.9.5\r\nPyCaret Version: '3.0.0.rc3'\r\nPandas Version: 1.4.3\r\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nEXPERIMENT_NAME = 'diamond_experiment'\r\ns = setup(data=dataset, target='Price', log_experiment=True, experiment_name=EXPERIMENT_NAME, session_id=42, verbose=True)\r\n\r\nmodel = compare_models(verbose=False)\r\n\r\nprint(f\"Notice Least Angle Regression is not unique:\\n{get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'].value_counts()}\")\r\n\r\n# Loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# There should be a single unique value for each model\r\nfor model in pull().Model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == model])}\")\r\n\r\n# Further investigation: model Least Angle Regression has 2 instances (should be Lasso Least Angle Regression and Least Angle Regression)\r\nget_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == 'Least Angle Regression']\r\n```\n```\n\n\n### Expected Behavior\n\n`tags.mlflow.runName` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### Actual Results\n\n```python-traceback\nWhen looking into the `tags.mlflow.runName` you can see they are all unique but Least Angle Regression is there twice and Lasso Least Angle Regression isn't there at all. Could this be logged incorrectly?\r\n\r\nGradient Boosting Regressor - 1\r\nCatBoost Regressor - 1\r\nLight Gradient Boosting Machine - 1\r\nExtreme Gradient Boosting - 1\r\nLasso Regression - 1\r\nRidge Regression - 1\r\nLinear Regression - 1\r\nLasso Least Angle Regression - 0\r\nLeast Angle Regression - 2\r\nExtra Trees Regressor - 1\r\nRandom Forest Regressor - 1\r\nAdaBoost Regressor - 1\r\nDecision Tree Regressor - 1\r\nOrthogonal Matching Pursuit - 1\r\nElastic Net - 1\r\nHuber Regressor - 1\r\nBayesian Ridge - 1\r\nK Neighbors Regressor - 1\r\nDummy Regressor - 1\r\nPassive Aggressive Regressor - 1\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: PATH_TO_ENV\/venv\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1670522892000,
        "Challenge_created_time":1659210438000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2811",
        "Challenge_link_count":3,
        "Challenge_readability":11.4,
        "Challenge_reading_time":47.39,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":3142.3483333333,
        "Challenge_title":"[BUG]: mlflow incorrectly logging models \"Lasso Least Angle Regression\" and \"Least Angle Regression\"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":420,
        "Platform":"Github",
        "Solution_body":"@Yard1 Can you give me a hand here? I ended up spending a lot of time in figuring out where is it coming from. The names inside `containers\/regression.py` seems to be fine but even then the run name is wrong.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/204138706-db0d0cc3-9a08-46d3-8037-fbaee414876b.png)\r\n\r\nAny ideas?",
        "Solution_link_count":1.0,
        "Solution_readability":6.6,
        "Solution_reading_time":4.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":2889.2111111111,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nWe have been using pycaret 2.2 for the model training procedure and registration to the mlflow server. (python based) My company uses a managed version of this in Azure Databricks. After the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. We would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. This is not working in pycaret and appears to be a bug of some sort. It is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. \n\n### Reproducible Example\n\n```python\n# import mlflow.sklearn\r\n# model = mlflow.sklearn.load_model(production_algorithm)\r\n# model.predict_prob(X)\n```\n\n\n### Expected Behavior\n\nwe should see the probabilities model.predict_prob(X) but this code errors out. Another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.DataFrame(X))\r\n\n\n### Actual Results\n\n```python-traceback\nthe end result of the prediction should be a numeric value between 0 and 1. Ex. 0.4278\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python\r\n   machine: Linux-5.4.0-1083-azure-x86_64-with-glibc2.29\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.0.1\r\n          setuptools: 52.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 7.22.0\r\n          ipywidgets: 7.7.1\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.6.2\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.8.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.1\r\n            requests: 2.28.1\r\n          matplotlib: 3.4.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.12.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.4\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1669247416000,
        "Challenge_created_time":1658846256000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2801",
        "Challenge_link_count":3,
        "Challenge_readability":8.2,
        "Challenge_reading_time":32.33,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":2889.2111111111,
        "Challenge_title":"[BUG]: pycaret + mlflow integration does not allow probabilities for classification and binary response models",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":316,
        "Platform":"Github",
        "Solution_body":"@DerekKane We need more details than `code error out` to be able to help. Just tested this scenario with latest pycaret==3.0.0rc4 and it works fine.\r\n\r\nIf you are still facing issue, please feel free to open new ticket.",
        "Solution_link_count":0.0,
        "Solution_readability":3.5,
        "Solution_reading_time":2.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":28.55,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Challenge_closed_time":1653501835000,
        "Challenge_created_time":1653399055000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Challenge_link_count":5,
        "Challenge_readability":13.0,
        "Challenge_reading_time":21.61,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":28.55,
        "Challenge_title":"mlflow ui doesn't show any models",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"Creating a clean env and installing pycaret again solved the issue.",
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":0.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":35.6138888889,
        "Challenge_answer_count":5,
        "Challenge_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Challenge_closed_time":1650470329000,
        "Challenge_created_time":1650342119000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Challenge_link_count":0,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":35.6138888889,
        "Challenge_title":"plots are not saving through MLFLOW",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":18,
        "Platform":"Github",
        "Solution_body":"@akashg116414 We can't help you with this. Can you please describe a little bit more, show the code, show the complete error, etc. this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\nexample:\r\n\r\n```python\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('juice')\r\nfrom pycaret.classification import *\r\nclf1 = setup(data, target = 'Purchase', session_id=123, log_experiment=True, experiment_name='juice1',log_plots=True)\r\n``` > this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\n\r\nAre you installing both of them in the same environment? That should not be done as there may be conflicts at this time (until we officially release the 3.0.0 pycaret version which will have time-series integrated with the main pycaret package).\r\n\r\nTry installing one of them in a fresh (clean) environment and see if it works. @akashg116414 By the way, I am not able to recreate the issue with the information (code) you have provided. It works fine for me (see attached notebook below).\r\n\r\nhttps:\/\/gist.github.com\/ngupta23\/f7f33a5361928cac2f36f855f7398c88\r\n\r\nPlease provide a completely reproducible example that shows the steps to get to the error you are getting. Since we are unable to recreate this and information seems to be missing, we will close this for now. Feel free to create a new issue. We add a new Bug template that will guide you through the process of submitting a reproducible example. \r\n\r\nThanks!",
        "Solution_link_count":1.0,
        "Solution_readability":8.6,
        "Solution_reading_time":19.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":219.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":47.865,
        "Challenge_answer_count":6,
        "Challenge_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1650449956000,
        "Challenge_created_time":1650277642000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Challenge_link_count":0,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":47.865,
        "Challenge_title":"[BUG] mlflow ui never runs",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"@maverick-scientist there is not enough information here to recreate or debug the issue. Please provide a complete reproducible example so we can debug. Also, note that if the data is proprietary, you can create a fake dataset yourself to recreate the issue. Refer to the following for more details:\r\n\r\n\ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\r\n\ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\r\n\r\n- The do's and dont's have an example of how to create the fake data - minimal to reproduce the problem.\r\n- Alternately, you could try to reproduce the issue with a publicly available dataset or one available in pycaret itself.\r\n\r\nThanks!\r\n Hi Nikhil,\nCan we please discuss this over teams call? I can share a python file with\nyou for this issue but I feel if we can discuss this on a call it would\nsave me some time.\n\nWhat do you think?\n\nPlease advise.\n\nOn Mon, 18 Apr 2022 at 22:55, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> there is not\n> enough information here to recreate or debug the issue. Please provide a\n> complete reproducible example so we can debug. Also, note that if the data\n> is proprietary, you can create a fake dataset yourself to recreate the\n> issue. Refer to the following for more details:\n>\n> \ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\n> \ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\n>\n>    - The do's and dont's have an example of how to create the fake data -\n>    minimal to reproduce the problem.\n>    - Alternately, you could try to reproduce the issue with a publicly\n>    available dataset or one available in pycaret itself.\n>\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1101586641>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRACMIA55LOVAGIZZKPLVFWLKHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @maverick-scientist Honestly, I would prefer not to do that since it sets the wrong precedent for the open-source community and is not sustainable in the long run. The globally accepted best practice is to provide a minimal reproducible example and I would encourage you to do that.\r\n\r\nThanks! Hi Nikhil,\r\nAttaching the code file to reproduce this issue. Could you please check and tell me what's wrong with it or my machine?\r\n\r\nThanks & Regards,\r\nAbhinav\r\n\r\n[Simple MLflow.zip](https:\/\/github.com\/pycaret\/pycaret\/files\/8511837\/Simple.MLflow.zip)\r\n\r\n @maverick-scientist The example that you posted has no reference to pycaret. It is a generic MLFlow example. How is it related to pycaret and this repo? Hi Nikhil,\nThank you for your reply. However, if you read the issue carefully, I\u2019d\nclearly mentioned my dilemma whether it was the right forum to post this\nissue.\n\nAnyways, thanks for your support. I\u2019d try and see what\u2019s preventing the\nMLFlow to run properly on my machine.\n\nOn Wed, 20 Apr 2022 at 15:21, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> The example\n> that you posted has no reference to pycaret. It is a generic MLFlow\n> example. How is it related to pycaret and this repo?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1103727978>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRADHBVLOBH4SACXZHNLVF7HTHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "Solution_link_count":11.0,
        "Solution_readability":7.7,
        "Solution_reading_time":43.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":477.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":276.9580555556,
        "Challenge_answer_count":0,
        "Challenge_body":"if in setup log_plot set True then it is giving error in self._mlflow_log_model() as \r\nfor plot in log_plots:\r\nTypeError: 'bool' object is not iterable",
        "Challenge_closed_time":1635811087000,
        "Challenge_created_time":1634814038000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1736",
        "Challenge_link_count":0,
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":276.9580555556,
        "Challenge_title":"[BUG] Issue with Mlflow Timeseries_beta branch",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":29,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":375.8113888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405096000,
        "Challenge_created_time":1634052175000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":375.8113888889,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":268,
        "Platform":"Github",
        "Solution_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":1.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":740.8186111111,
        "Challenge_answer_count":2,
        "Challenge_body":"MLFlow logging issue in compare_models of time_series\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('airline')\r\n\r\nfrom pycaret.time_series import *\r\ns = setup(data, fold = 5, fh = 12, session_id = 123, log_experiment=True, experiment_name = 'airline')\r\n\r\nbest = compare_models()\r\n\r\n!mlflow ui\r\n```\r\n\r\nCheck localhost:5000:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992636-db293fe7-5461-43d9-baed-97d8790fd9bd.png)\r\n\r\nAll the runs fail in `compare_models`. Parameters are logged but metrics and artifacts didn't. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992655-e0d81e16-9a59-49a5-ace3-d22ea2ea10b8.png)\r\n\r\nI cannot reproduce this with `create_model` it means `create_model` works just fine! ",
        "Challenge_closed_time":1634125992000,
        "Challenge_created_time":1631459045000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1568",
        "Challenge_link_count":2,
        "Challenge_readability":13.6,
        "Challenge_reading_time":10.31,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":740.8186111111,
        "Challenge_title":"MLFlow logging issue in compare_models of time_series",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":67,
        "Platform":"Github",
        "Solution_body":"@moezali1 merged a fix, please recheck  Done.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/137127496-454bff0f-d3a4-410f-9d78-b8e4b1a3f547.png)",
        "Solution_link_count":1.0,
        "Solution_readability":18.2,
        "Solution_reading_time":2.19,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"I was exploring mlflow with pycaret today. And tried to store the artifacts in the Azure blob using the --default-artifact-root tag. It's working fine when I am not giving a experiment name to pycaret setup function. When a experiment name is given the artifacts are getting stored in the local directory.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1624984139000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1411",
        "Challenge_link_count":0,
        "Challenge_readability":8.4,
        "Challenge_reading_time":4.91,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG]Can we store mlflow artifacts in the Azure blob storage for new experiments in pycaret?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":65,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":3487.3783333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm using clustering module of pycaret and the integration with mlflow but I have problems because I think it doesn't save all artifacs and the status is always failed.\r\n![image](https:\/\/user-images.githubusercontent.com\/12554263\/101971863-66f70d00-3c02-11eb-9710-01cf228fca1b.png)\r\n\r\nThis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=True,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=True,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=True,\r\n                     log_plots=True,\r\n                     log_profile=True,\r\n                     log_data=True,\r\n                     profile=False,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# Create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\nMy logs are the following\r\n\r\n```\r\n2020-12-11 22:39:07,118:INFO:PyCaret Supervised Module\r\n2020-12-11 22:39:07,118:INFO:ML Usecase: clustering\r\n2020-12-11 22:39:07,118:INFO:version 2.2.0\r\n2020-12-11 22:39:07,118:INFO:Initializing setup()\r\n2020-12-11 22:39:07,119:INFO:setup(target=None, ml_usecase=clustering, available_plots={'cluster': 'Cluster PCA Plot (2d)', 'tsne': 'Cluster TSnE (3d)', 'elbow': 'Elbow', 'silhouette': 'Silhouette', 'distance': 'Distance', 'distribution': 'Distribution'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=mode, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=['avg_dias_bancos_3m', 'avg_dias_app_pagos_3m', 'avg_dias_viajes_3m', 'avg_dias_compras_3m', 'avg_dias_mb_total_3m', 'avg_mb_total_3m', 'avg_q_apps_3m', 'ate_wh_sum_dias_3m', 'LEADs_tot_3m', 'tot_dias_appmov_movil_3m', 'avg_days_out_voice_tot_3m', 'meses_pagodig_3m'], numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['periodo', 'telefono', 'anexo', 'tot_dias_appmov_fija_3m', 'avg_dias_vid_mus_3m'], normalize=True, normalize_method=robust, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=False, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=kfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=True, session_id=123, log_experiment=True, experiment_name=pospatid_segmentation, log_plots=['cluster', 'distribution', 'elbow'], log_profile=True, log_data=True, silent=False, verbose=True, profile=False, display=None)\r\n2020-12-11 22:39:07,119:INFO:Checking environment\r\n2020-12-11 22:39:07,119:INFO:python_version: 3.8.5\r\n2020-12-11 22:39:07,119:INFO:python_build: ('default', 'Aug  5 2020 09:44:06')\r\n2020-12-11 22:39:07,119:INFO:machine: AMD64\r\n2020-12-11 22:39:07,120:INFO:platform: Windows-10-10.0.18362-SP0\r\n2020-12-11 22:39:07,121:WARNING:cannot find psutil installation. memory not traceable. Install psutil using pip to enable memory logging.\r\n2020-12-11 22:39:07,122:INFO:Checking libraries\r\n2020-12-11 22:39:07,122:INFO:pd==1.1.4\r\n2020-12-11 22:39:07,122:INFO:numpy==1.19.4\r\n2020-12-11 22:39:07,122:INFO:sklearn==0.23.2\r\n2020-12-11 22:39:07,156:INFO:xgboost==1.2.0\r\n2020-12-11 22:39:07,156:INFO:lightgbm==3.0.0\r\n2020-12-11 22:39:07,170:INFO:catboost==0.24.1\r\n2020-12-11 22:39:07,901:INFO:mlflow==1.11.0\r\n2020-12-11 22:39:07,901:INFO:Checking Exceptions\r\n2020-12-11 22:39:07,901:INFO:Declaring global variables\r\n2020-12-11 22:39:07,901:INFO:USI: cd5c\r\n2020-12-11 22:39:07,901:INFO:pycaret_globals: {'_available_plots', 'master_model_container', 'display_container', 'imputation_classifier', 'logging_param', 'seed', 'transform_target_param', 'experiment__', 'transform_target_method_param', 'iterative_imputation_iters_param', 'fold_groups_param', 'fix_imbalance_param', 'prep_pipe', 'exp_name_log', '_all_metrics', 'html_param', '_ml_usecase', 'USI', 'imputation_regressor', 'stratify_param', 'fold_generator', 'fix_imbalance_method_param', '_all_models', 'gpu_param', 'target_param', '_gpu_n_jobs_param', 'log_plots_param', 'pycaret_globals', 'fold_shuffle_param', '_all_models_internal', 'fold_param', 'create_model_container', 'data_before_preprocess', '_internal_pipeline', 'X', 'n_jobs_param'}\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,914:INFO:Importing libraries\r\n2020-12-11 22:39:07,914:INFO:Copying data for preprocessing\r\n2020-12-11 22:39:07,927:INFO:Declaring preprocessing parameters\r\n2020-12-11 22:39:07,940:INFO:Creating preprocessing pipeline\r\n2020-12-11 22:39:08,059:INFO:Preprocessing pipeline created successfully\r\n2020-12-11 22:39:08,060:ERROR:(Process Exit): setup has been interupted with user command 'quit'. setup must rerun.\r\n2020-12-11 22:39:08,060:INFO:Creating global containers\r\n2020-12-11 22:39:08,061:INFO:Internal pipeline: Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\r\n2020-12-11 22:39:10,064:INFO:Creating grid variables\r\n2020-12-11 22:39:10,101:INFO:Logging experiment in MLFlow\r\n2020-12-11 22:39:10,108:WARNING:Couldn't create mlflow experiment. Exception:\r\n2020-12-11 22:39:10,185:WARNING:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 1668, in setup\r\n    mlflow.create_experiment(exp_name_log)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 365, in create_experiment\r\n    return MlflowClient().create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 184, in create_experiment\r\n    return self._tracking_client.create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 142, in create_experiment\r\n    return self.store.create_experiment(name=name, artifact_location=artifact_location,)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 288, in create_experiment\r\n    self._validate_experiment_name(name)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 281, in _validate_experiment_name\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Experiment 'pospatid_segmentation' already exists.\r\n\r\n2020-12-11 22:39:10,490:INFO:SubProcess save_model() called ==================================\r\n2020-12-11 22:39:10,501:INFO:Initializing save_model()\r\n2020-12-11 22:39:10,501:INFO:save_model(model=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), model_name=Transformation Pipeline, prep_pipe_=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), verbose=False)\r\n2020-12-11 22:39:10,501:INFO:Adding model into prep_pipe\r\n2020-12-11 22:39:10,506:WARNING:Only Model saved as it was a pipeline.\r\n2020-12-11 22:39:10,530:INFO:Transformation Pipeline.pkl saved in current working directory\r\n2020-12-11 22:39:10,535:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:39:10,535:INFO:save_model() succesfully completed......................................\r\n2020-12-11 22:39:10,536:INFO:SubProcess save_model() end ==================================\r\n2020-12-11 22:40:03,332:INFO:create_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:master_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:display_container: 0\r\n2020-12-11 22:40:03,336:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:40:03,336:INFO:setup() succesfully completed......................................\r\n2020-12-11 22:40:07,628:INFO:Initializing create_model()\r\n2020-12-11 22:40:07,628:INFO:create_model(estimator=kmeans, num_clusters=6, fraction=0.05, ground_truth=None, round=4, fit_kwargs=None, verbose=True, system=True, raise_num_clusters=False, display=None, kwargs={})\r\n2020-12-11 22:40:07,628:INFO:Checking exceptions\r\n2020-12-11 22:40:07,629:INFO:Preparing display monitor\r\n2020-12-11 22:40:07,645:INFO:Importing libraries\r\n2020-12-11 22:40:07,652:INFO:Importing untrained model\r\n2020-12-11 22:40:07,662:INFO:K-Means Clustering Imported succesfully\r\n2020-12-11 22:40:07,670:INFO:Fitting Model\r\n2020-12-11 22:42:30,467:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:42:30,467:INFO:create_models() succesfully completed......................................\r\n2020-12-11 22:42:30,467:INFO:Creating MLFlow logs\r\n2020-12-11 22:42:30,481:INFO:Model: K-Means Clustering\r\n2020-12-11 22:42:30,518:INFO:logged params: {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 6, 'n_init': 10, 'n_jobs': -1, 'precompute_distances': 'deprecated', 'random_state': 123, 'tol': 0.0001, 'verbose': 0}\r\n2020-12-11 22:42:30,557:INFO:SubProcess plot_model() called ==================================\r\n2020-12-11 22:42:30,557:INFO:Initializing plot_model()\r\n2020-12-11 22:42:30,557:INFO:plot_model(plot=cluster, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:30,557:INFO:Checking exceptions\r\n2020-12-11 22:42:30,558:INFO:Preloading libraries\r\n2020-12-11 22:42:30,558:INFO:Copying training dataset\r\n2020-12-11 22:42:30,560:INFO:Plot type: cluster\r\n2020-12-11 22:42:31,493:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:31,494:INFO:Initializing assign_model()\r\n2020-12-11 22:42:31,494:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=True, score=True, verbose=False)\r\n2020-12-11 22:42:31,494:INFO:Checking exceptions\r\n2020-12-11 22:42:31,495:INFO:Determining Trained Model\r\n2020-12-11 22:42:31,495:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:31,495:INFO:Copying data\r\n2020-12-11 22:42:31,496:INFO:Transformation param set to True. Assigned clusters are attached on transformed dataset.\r\n2020-12-11 22:42:31,529:INFO:(90000, 12)\r\n2020-12-11 22:42:31,529:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:31,530:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:31,541:INFO:Fitting PCA()\r\n2020-12-11 22:42:31,908:INFO:Sorting dataframe\r\n2020-12-11 22:42:31,974:INFO:Rendering Visual\r\n2020-12-11 22:42:41,765:INFO:Saving 'Cluster PCA Plot (2d).html' in current active directory\r\n2020-12-11 22:42:41,765:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:42,286:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:42,739:INFO:Initializing plot_model()\r\n2020-12-11 22:42:42,739:INFO:plot_model(plot=distribution, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:42,739:INFO:Checking exceptions\r\n2020-12-11 22:42:42,739:INFO:Preloading libraries\r\n2020-12-11 22:42:42,739:INFO:Copying training dataset\r\n2020-12-11 22:42:42,741:INFO:Plot type: distribution\r\n2020-12-11 22:42:42,741:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:42,742:INFO:Initializing assign_model()\r\n2020-12-11 22:42:42,742:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=False, score=True, verbose=False)\r\n2020-12-11 22:42:42,742:INFO:Checking exceptions\r\n2020-12-11 22:42:42,742:INFO:Determining Trained Model\r\n2020-12-11 22:42:42,742:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:42,742:INFO:Copying data\r\n2020-12-11 22:42:42,793:INFO:(90000, 18)\r\n2020-12-11 22:42:42,793:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:42,794:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:42,794:INFO:Sorting dataframe\r\n2020-12-11 22:42:42,925:INFO:Rendering Visual\r\n2020-12-11 22:42:48,837:INFO:Saving 'Distribution.html' in current active directory\r\n2020-12-11 22:42:48,837:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:48,979:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:49,583:INFO:Initializing plot_model()\r\n2020-12-11 22:42:49,584:INFO:plot_model(plot=elbow, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:49,584:INFO:Checking exceptions\r\n2020-12-11 22:42:49,584:INFO:Preloading libraries\r\n2020-12-11 22:42:49,584:INFO:Copying training dataset\r\n2020-12-11 22:42:49,586:INFO:Plot type: elbow\r\n2020-12-11 22:42:49,690:INFO:Fitting Model\r\n2020-12-11 22:43:12,604:INFO:Saving 'Elbow.png' in current active directory\r\n2020-12-11 22:43:13,207:INFO:Visual Rendered Successfully\r\n2020-12-11 22:43:13,325:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:43:13,340:INFO:SubProcess plot_model() end ==================================\r\n2020-12-11 22:43:13,341:WARNING:Couldn't infer MLFlow signature.\r\n2020-12-11 22:43:13,352:ERROR:_mlflow_log_model() for KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0) raised an exception:\r\n2020-12-11 22:43:13,431:ERROR:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 2631, in create_model_unsupervised\r\n    _mlflow_log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 9942, in _mlflow_log_model\r\n    mlflow.sklearn.log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 290, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\model.py\", line 160, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 171, in save_model\r\n    _save_example(mlflow_model, input_example, path)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 131, in _save_example\r\n    example = _Example(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 67, in __init__\r\n    input_example = pd.DataFrame.from_dict(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 1309, in from_dict\r\n    return cls(data, index=index, columns=columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 468, in __init__\r\n    mgr = init_dict(data, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 283, in init_dict\r\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 78, in arrays_to_mgr\r\n    index = extract_index(arrays)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 387, in extract_index\r\n    raise ValueError(\"If using all scalar values, you must pass an index\")\r\nValueError: If using all scalar values, you must pass an index\r\n\r\n2020-12-11 22:43:13,432:INFO:Uploading results into container\r\n2020-12-11 22:43:13,435:INFO:Uploading model into container now\r\n2020-12-11 22:43:13,440:INFO:create_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:master_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:display_container: 1\r\n2020-12-11 22:43:13,440:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:43:13,440:INFO:create_model() succesfully completed......................................\r\n\r\n```\r\n\r\nI'm using Pycaret version : 2.2.0",
        "Challenge_closed_time":1620299767000,
        "Challenge_created_time":1607745205000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/931",
        "Challenge_link_count":1,
        "Challenge_readability":25.9,
        "Challenge_reading_time":288.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":96,
        "Challenge_solved_time":3487.3783333333,
        "Challenge_title":"MLFlow doesn't save model artifact and some plots - Clustering",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":1212,
        "Platform":"Github",
        "Solution_body":"@DXcarlos I have tried to reproduce the error on `jewellery` dataset available on our GitHub repository and I couldn't reproduce this error.\r\n\r\nIs it possible for you to share the Notebook along with the dataset so we can reproduce the error and troubleshoot what is causing this?\r\n\r\nI am including @Yard1 in this thread to see if he can understand what's going on with the log file you shared above. Antoni, maybe something specific to the dataset.  @pycaret @Yard1 How can I share you privately? @DXcarlos You can private message on our Slack channel. If you are still not there, you can join using the following link:\r\n\r\nhttps:\/\/join.slack.com\/t\/pycaret\/shared_invite\/zt-kdoe7hee-yvNANPHXPM9VtK7R6Npx4Q\r\n\r\n Stale issue message @DXcarlos , we will close out this issue for now. Please feel free to reopen if you want.",
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":9.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":128.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":24.5525,
        "Challenge_answer_count":9,
        "Challenge_body":"Hi. I just upgraded to pycaret 2.1. When I ran the compare_models function with the Titanic dataset, I got the following error:\r\n\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)\r\n\r\nThe same code worked fine in pycaret 2.0.",
        "Challenge_closed_time":1598806653000,
        "Challenge_created_time":1598718264000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/566",
        "Challenge_link_count":0,
        "Challenge_readability":6.9,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.5525,
        "Challenge_title":"Compare models MLFlowException",
        "Challenge_topic":"TensorFlow Model Development",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":61,
        "Platform":"Github",
        "Solution_body":"@sagarnildass Hi, Thanks for reporting. This seems like an error from `MLFlow`. I tried to reproduce this out of `pycaret` and I was successful. See below code that throws an error:\r\n\r\n```\r\nimport pandas as pd\r\ndata = pd.read_csv('titanic.csv') #train data from Kaggle\r\nfrom mlflow.models.signature import infer_signature\r\ninfer_signature(data)\r\n```\r\nThis gives the following error:\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\r\n\r\nI will log an issue on MLFlow GitHub.\r\n\r\nHere is the issue I logged on MLFlow: https:\/\/github.com\/mlflow\/mlflow\/issues\/3362 Hey\r\n\r\nThanks for the quick reply. \r\n\r\nI found out that presence of null values are a problem. If the dataset contains null values, this error was raised. When I imputed the null values, this problem was solved. Can you also mention this when you log this issue?\r\n\r\nThanks! @sagarnildass Thanks. I have added that in my issue but I don't think so it's 100% True. I have worked with few missing datasets and it worked okay. For example, the `hepatitis` dataset on our repo works fine. Example code:\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('hepatitis')\r\nfrom pycaret.classification import *\r\ns = setup(data, target = 'Class', log_experiment=True, experiment_name = 'hepatitis1')\r\n```\r\n\r\nThis dataset has missing values but it just worked fine. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/58118658\/91642055-41991700-e9f6-11ea-9b6f-0f42a86401d9.png)\r\n\r\nCan you investigate more and add your comments on the original issue here: https:\/\/github.com\/mlflow\/mlflow\/issues\/3362\r\n\r\nThanks a lot for helping. @Yard1 I don't know how soon `MLFLow` will be able to fix this but in `2.2` we will have to create some kind of exceptions under `logging_param` chunks to not fail the process even when `infer_signature` fails, as it's not mandatory and has no impact other than the signature file that gets generated under `model` directory when `log_experiment` is set to `True`. @pycaret : I believe object datatypes are a problem. It clearly states: \"np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray), int, float)\". So do you think null values in a object datatype column might be the root  problem here? Because in hepatitis data, all the columns are numeric. @pycaret If we get MLFlow logging into a function then it will be easy to wrap it into a try except block.  @sagarnildass Thanks again for reporting. I am planning to do a bug fix release tomorrow `2.1.1`. For now, I have wrapped this inside `try` and `except` clause to avoid the error. I have tested it on the titanic dataset.\r\n\r\nCan you please sync the `master` and try to see if you can reproduce the error now?\r\n\r\nThanks Done...it's working as expected. @sagarnildass Thanks. I will publish the `2.1.1.` release today. @Yard1 FYI.\r\n\r\nThanks for your help @sagarnildass ",
        "Solution_link_count":3.0,
        "Solution_readability":6.6,
        "Solution_reading_time":36.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":39.0,
        "Solution_word_count":449.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":1114.6180555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939186000,
        "Challenge_created_time":1645926561000,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":47.0,
        "Challenge_repo_fork_count":181.0,
        "Challenge_repo_issue_count":2399.0,
        "Challenge_repo_star_count":2909.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1114.6180555556,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":81,
        "Platform":"Github",
        "Solution_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Solution_link_count":0.0,
        "Solution_readability":4.2,
        "Solution_reading_time":6.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0195914965,
        "Challenge_watch_issue_ratio":0.0145894123
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Operating System\n\nLinux\n\n### Version Information\n\nAzure cli v2\n\n### Steps to reproduce\n\nThe azureml-example batch endpoint nyc-taxi-mlflow-deployment.yml file, refers to a  .\/autolog_nyc_taxi folder that doesn't exist\n\n### Expected behavior\n\nIt looks like we need to re-add the folder?\n\n### Actual behavior\n\ncode fails because folder doesn't exist\n\n### Addition information\n\n_No response_",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669062340000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1899",
        "Challenge_link_count":0,
        "Challenge_readability":12.4,
        "Challenge_reading_time":5.73,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"nyc-taxi-mlflow-deployment.yml refers to a folder that doesn't exists",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":56,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0687372709,
        "Challenge_watch_issue_ratio":1.4042769857
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### Operating System\n\nWindows\n\n### Version Information\n\nlatest cli v2 \n\n### Steps to reproduce\n\nhttps:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/cli\/endpoints\/online\/mlflow\/sklearn-deployment.yaml\r\n\r\nThis yaml is out of date, the model yaml config is wrong. \"name\" is no longer required when specifying model.\n\n### Expected behavior\n\nThat the deployment works based on sklearn-deployment.yml using the cli command `az create deployment`, but it fails. \n\n### Actual behavior\n\nIt fails.\n\n### Addition information\n\n_No response_",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669043018000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1897",
        "Challenge_link_count":1,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.9,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow cli endpoint example out of date with new syntax from breaking changes in yaml (last updated May 11)",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":78,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0687372709,
        "Challenge_watch_issue_ratio":1.4042769857
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample: [CIFAR pytorch distributed](https:\/\/github.com\/Azure\/azureml-examples\/tree\/main\/cli\/jobs\/single-step\/pytorch\/cifar-distributed)\r\n\r\ndescription: model training shows completed, model is saved as well but driver logs (`70_driver_log..`.) for the model saving driver has:\r\n \r\n`ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: \/tmp\/tmpvthoxt0n\/model\/data, flavor: pytorch)\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/mlflow\/utils\/environment.py\", line 194, in infer_pip_requirements\r\n    return _infer_requirements(model_uri, flavor)\r\n  File \"\/azureml-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/mlflow\/utils\/requirements_utils.py\", line 306, in _infer_requirements\r\n    _MODULES_TO_PACKAGES = importlib_metadata.packages_distributions()\r\nAttributeError: module 'importlib_metadata' has no attribute 'packages_distributions'`\r\n\r\n## Additional context\r\n\r\nTried with variations to the environment in `job.yml: azureml:AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:11 and azureml:AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:6`. Same outcome. \r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1638959064000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/937",
        "Challenge_link_count":1,
        "Challenge_readability":18.8,
        "Challenge_reading_time":16.56,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Mlflow error on pytorch.log_model but model is saved",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":96,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0687372709,
        "Challenge_watch_issue_ratio":1.4042769857
    },
    {
        "Challenge_adjusted_solved_time":73.275,
        "Challenge_answer_count":3,
        "Challenge_body":"\r\n\r\nhttps:\/\/github.com\/Azure\/azureml-examples\/runs\/1618089261?check_suite_focus=true\r\n\r\n@trangevi \r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/3419\/files",
        "Challenge_closed_time":1609558021000,
        "Challenge_created_time":1609294231000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/318",
        "Challenge_link_count":2,
        "Challenge_readability":30.8,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":73.275,
        "Challenge_title":"MLflow 1.13 probably broke deployment",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":8,
        "Platform":"Github",
        "Solution_body":"yeah @trangevi the logging statement is guaranteed to bork out: https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/mlflow\/azureml\/__init__.py#L413\r\n\r\n@eedeleon fyi https:\/\/github.com\/mlflow\/mlflow\/pull\/3922\r\n @akshaya-a @eedeleon looks resolved, thanks for investigating! will close this issue ",
        "Solution_link_count":2.0,
        "Solution_readability":8.1,
        "Solution_reading_time":3.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":25.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0687372709,
        "Challenge_watch_issue_ratio":1.4042769857
    },
    {
        "Challenge_adjusted_solved_time":20.83,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658581000,
        "Challenge_created_time":1601583593000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":20.83,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":338,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0296176629,
        "Challenge_watch_issue_ratio":1.0936995153
    },
    {
        "Challenge_adjusted_solved_time":0.4194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Challenge_closed_time":1581065545000,
        "Challenge_created_time":1581064035000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Challenge_link_count":1,
        "Challenge_readability":18.1,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4194444444,
        "Challenge_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0296176629,
        "Challenge_watch_issue_ratio":1.0936995153
    },
    {
        "Challenge_adjusted_solved_time":558.1344444444,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718001000,
        "Challenge_created_time":1665708717000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":558.1344444444,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":583,
        "Platform":"Github",
        "Solution_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":6.36,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":89.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"when run workflow:\r\n```\r\nqrun ALSTM_workflow_config_alstm_Alpha158.yaml\r\n```  \r\nmlflow v1.27.0 work fine,but failed when with mlflow v1.28.0:\r\n```\r\nFile \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/pyqlib-0.8.6.99-py3.8-linux-x86_64.egg\/qlib\/workflow\/recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/client.py\", line 852, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/_tracking_service\/client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 778, which exceeded length limit of 500\r\n```\r\ni think the new mflow feature cause this bug.mlflow limit param valu lengh to 500,by read code ,it can not be overwrite.\r\nmaybe relate with this [issue](https:\/\/github.com\/mlflow\/mlflow\/commit\/d4109d00079355459a9a3df1821f0878877e42a8)\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1663557425000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1298",
        "Challenge_link_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.19,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"not compatible with mlflow v1.28.0",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":102,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\nwhen I run the code below in qlib-main\/examples\/workflow_by_code.ipynb\uff0cit caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' \r\n###################################\r\n# train model\r\n###################################\r\ndata_handler_config = {\r\n    \"start_time\": \"2008-01-01\",\r\n    \"end_time\": \"2020-08-01\",\r\n    \"fit_start_time\": \"2008-01-01\",\r\n    \"fit_end_time\": \"2014-12-31\",\r\n    \"instruments\": market,\r\n}\r\n\r\ntask = {\r\n    \"model\": {\r\n        \"class\": \"LGBModel\",\r\n        \"module_path\": \"qlib.contrib.model.gbdt\",\r\n        \"kwargs\": {\r\n            \"loss\": \"mse\",\r\n            \"colsample_bytree\": 0.8879,\r\n            \"learning_rate\": 0.0421,\r\n            \"subsample\": 0.8789,\r\n            \"lambda_l1\": 205.6999,\r\n            \"lambda_l2\": 580.9768,\r\n            \"max_depth\": 8,\r\n            \"num_leaves\": 210,\r\n            \"num_threads\": 20,\r\n        },\r\n    },\r\n    \"dataset\": {\r\n        \"class\": \"DatasetH\",\r\n        \"module_path\": \"qlib.data.dataset\",\r\n        \"kwargs\": {\r\n            \"handler\": {\r\n                \"class\": \"Alpha158\",\r\n                \"module_path\": \"qlib.contrib.data.handler\",\r\n                \"kwargs\": data_handler_config,\r\n            },\r\n            \"segments\": {\r\n                \"train\": (\"2008-01-01\", \"2014-12-31\"),\r\n                \"valid\": (\"2015-01-01\", \"2016-12-31\"),\r\n                \"test\": (\"2017-01-01\", \"2020-08-01\"),\r\n            },\r\n        },\r\n    },\r\n}\r\n\r\n# model initiaiton\r\nmodel = init_instance_by_config(task[\"model\"])\r\ndataset = init_instance_by_config(task[\"dataset\"])\r\n\r\n# start exp to train model\r\nwith R.start(experiment_name=\"train_model\"):\r\n    R.log_params(**flatten_dict(task))\r\n    model.fit(dataset)\r\n    R.save_objects(trained_model=model)\r\n    rid = R.get_recorder().id\r\n\r\n=====================\r\nThe whole error message is below\uff1a\r\n[2607:MainThread](2022-04-06 17:38:12,377) INFO - qlib.timer - [log.py:113] - Time cost: 18.919s | Loading data Done\r\n[2607:MainThread](2022-04-06 17:38:12,737) INFO - qlib.timer - [log.py:113] - Time cost: 0.147s | DropnaLabel Done\r\n\/Users\/yzwu\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/data\/dataset\/processor.py:310: SettingWithCopyWarning: \r\nA value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_indexer,col_indexer] = value instead\r\n\r\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\r\n  df[cols] = df[cols].groupby(\"datetime\").apply(self.zscore_func)\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 1.650s | CSZScoreNorm Done\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 2.010s | fit & process data Done\r\n[2607:MainThread](2022-04-06 17:38:14,388) INFO - qlib.timer - [log.py:113] - Time cost: 20.930s | Init data Done\r\n[2607:MainThread](2022-04-06 17:38:14,399) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x2859099a0>\r\n[2607:MainThread](2022-04-06 17:38:14,402) WARNING - qlib.workflow - [expm.py:195] - No valid experiment found. Create a new experiment with name train_model.\r\n---------------------------------------------------------------------------\r\nMlflowException                           Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:391, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    390 try:\r\n--> 391     exp = self.client.get_experiment_by_name(experiment_name)\r\n    392     if exp is None or exp.lifecycle_stage.upper() == \"DELETED\":\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:462, in MlflowClient.get_experiment_by_name(self, name)\r\n    432 \"\"\"\r\n    433 Retrieve an experiment by experiment name from the backend store\r\n    434 \r\n   (...)\r\n    460     Lifecycle_stage: active\r\n    461 \"\"\"\r\n--> 462 return self._tracking_client.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:167, in TrackingServiceClient.get_experiment_by_name(self, name)\r\n    163 \"\"\"\r\n    164 :param name: The experiment name.\r\n    165 :return: :py:class:`mlflow.entities.Experiment`\r\n    166 \"\"\"\r\n--> 167 return self.store.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     67 \"\"\"\r\n     68 Fetch the experiment by name from the backend store.\r\n     69 This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74 :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75 \"\"\"\r\n---> 76 for experiment in self.list_experiments(ViewType.ALL):\r\n     77     if experiment.name == experiment_name:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    259 try:\r\n    260     # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261     experiment = self._get_experiment(exp_id, view_type)\r\n    262     if experiment:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    336 self._check_root_dir()\r\n--> 337 _validate_experiment_id(experiment_id)\r\n    338 experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:189, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    187 try:\r\n    188     return (\r\n--> 189         self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),\r\n    190         False,\r\n    191     )\r\n    192 except ValueError:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:397, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    396 except MlflowException as e:\r\n--> 397     raise ValueError(\r\n    398         \"No valid experiment has been found, please make sure the input experiment name is correct.\"\r\n    399     ) from e\r\n\r\nValueError: No valid experiment has been found, please make sure the input experiment name is correct.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMlflowException                           Traceback (most recent call last)\r\nInput In [6], in <cell line: 51>()\r\n     48 dataset = init_instance_by_config(task[\"dataset\"])\r\n     50 # start exp to train model\r\n---> 51 with R.start(experiment_name=\"train_model\"):\r\n     52     R.log_params(**flatten_dict(task))\r\n     53     model.fit(dataset)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/contextlib.py:113, in _GeneratorContextManager.__enter__(self)\r\n    111 del self.args, self.kwds, self.func\r\n    112 try:\r\n--> 113     return next(self.gen)\r\n    114 except StopIteration:\r\n    115     raise RuntimeError(\"generator didn't yield\") from None\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:69, in QlibRecorder.start(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     25 @contextmanager\r\n     26 def start(\r\n     27     self,\r\n   (...)\r\n     34     resume: bool = False,\r\n     35 ):\r\n     36     \"\"\"\r\n     37     Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\r\n     38 \r\n   (...)\r\n     67         whether to resume the specific recorder with given name under the given experiment.\r\n     68     \"\"\"\r\n---> 69     run = self.start_exp(\r\n     70         experiment_id=experiment_id,\r\n     71         experiment_name=experiment_name,\r\n     72         recorder_id=recorder_id,\r\n     73         recorder_name=recorder_name,\r\n     74         uri=uri,\r\n     75         resume=resume,\r\n     76     )\r\n     77     try:\r\n     78         yield run\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:125, in QlibRecorder.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     84 def start_exp(\r\n     85     self,\r\n     86     *,\r\n   (...)\r\n     92     resume=False,\r\n     93 ):\r\n     94     \"\"\"\r\n     95     Lower level method for starting an experiment. When use this method, one should end the experiment manually\r\n     96     and the status of the recorder may not be handled properly. Here is the example code:\r\n   (...)\r\n    123     An experiment instance being started.\r\n    124     \"\"\"\r\n--> 125     return self.exp_manager.start_exp(\r\n    126         experiment_id=experiment_id,\r\n    127         experiment_name=experiment_name,\r\n    128         recorder_id=recorder_id,\r\n    129         recorder_name=recorder_name,\r\n    130         uri=uri,\r\n    131         resume=resume,\r\n    132     )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:339, in MLflowExpManager.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n    337 if experiment_name is None:\r\n    338     experiment_name = self._default_exp_name\r\n--> 339 experiment, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)\r\n    340 # Set up active experiment\r\n    341 self.active_experiment = experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:202, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    200 if pr.scheme == \"file\":\r\n    201     with FileLock(os.path.join(pr.netloc, pr.path, \"filelock\")):  # pylint: disable=E0110\r\n--> 202         return self.create_exp(experiment_name), True\r\n    203 # NOTE: for other schemes like http, we double check to avoid create exp conflicts\r\n    204 try:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:362, in MLflowExpManager.create_exp(self, experiment_name)\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n    361         raise ExpAlreadyExistError() from e\r\n--> 362     raise e\r\n    364 experiment = MLflowExperiment(experiment_id, experiment_name, self.uri)\r\n    365 experiment._default_name = self._default_exp_name\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:358, in MLflowExpManager.create_exp(self, experiment_name)\r\n    356 # init experiment\r\n    357 try:\r\n--> 358     experiment_id = self.client.create_experiment(experiment_name)\r\n    359 except MlflowException as e:\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:507, in MlflowClient.create_experiment(self, name, artifact_location, tags)\r\n    464 def create_experiment(\r\n    465     self,\r\n    466     name: str,\r\n    467     artifact_location: Optional[str] = None,\r\n    468     tags: Optional[Dict[str, Any]] = None,\r\n    469 ) -> str:\r\n    470     \"\"\"Create an experiment.\r\n    471 \r\n    472     :param name: The experiment name. Must be unique.\r\n   (...)\r\n    505         Lifecycle_stage: active\r\n    506     \"\"\"\r\n--> 507     return self._tracking_client.create_experiment(name, artifact_location, tags)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:182, in TrackingServiceClient.create_experiment(self, name, artifact_location, tags)\r\n    179 _validate_experiment_name(name)\r\n    180 _validate_experiment_artifact_location(artifact_location)\r\n--> 182 return self.store.create_experiment(\r\n    183     name=name,\r\n    184     artifact_location=artifact_location,\r\n    185     tags=[ExperimentTag(key, value) for (key, value) in tags.items()] if tags else [],\r\n    186 )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:321, in FileStore.create_experiment(self, name, artifact_location, tags)\r\n    319 def create_experiment(self, name, artifact_location=None, tags=None):\r\n    320     self._check_root_dir()\r\n--> 321     self._validate_experiment_name(name)\r\n    322     # Get all existing experiments and find the one with largest numerical ID.\r\n    323     # len(list_all(..)) would not work when experiments are deleted.\r\n    324     experiments_ids = [\r\n    325         int(e.experiment_id)\r\n    326         for e in self.list_experiments(ViewType.ALL)\r\n    327         if e.experiment_id.isdigit()\r\n    328     ]\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:303, in FileStore._validate_experiment_name(self, name)\r\n    299 if name is None or name == \"\":\r\n    300     raise MlflowException(\r\n    301         \"Invalid experiment name '%s'\" % name, databricks_pb2.INVALID_PARAMETER_VALUE\r\n    302     )\r\n--> 303 experiment = self.get_experiment_by_name(name)\r\n    304 if experiment is not None:\r\n    305     if experiment.lifecycle_stage == LifecycleStage.DELETED:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     66 def get_experiment_by_name(self, experiment_name):\r\n     67     \"\"\"\r\n     68     Fetch the experiment by name from the backend store.\r\n     69     This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74     :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75     \"\"\"\r\n---> 76     for experiment in self.list_experiments(ViewType.ALL):\r\n     77         if experiment.name == experiment_name:\r\n     78             return experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    258 for exp_id in rsl:\r\n    259     try:\r\n    260         # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261         experiment = self._get_experiment(exp_id, view_type)\r\n    262         if experiment:\r\n    263             experiments.append(experiment)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    335 def _get_experiment(self, experiment_id, view_type=ViewType.ALL):\r\n    336     self._check_root_dir()\r\n--> 337     _validate_experiment_id(experiment_id)\r\n    338     experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n    339     if experiment_dir is None:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    265 \"\"\"Check that `experiment_id`is a valid string or None, raise an exception if it isn't.\"\"\"\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. just rerun the code in my envirment\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\nDarwin\r\narm64\r\nmacOS-12.2.1-arm64-arm-64bit\r\nDarwin Kernel Version 21.3.0: Wed Jan  5 21:37:58 PST 2022; root:xnu-8019.80.24~20\/RELEASE_ARM64_T6000\r\n\r\nPython version: 3.8.11 (default, Jul 29 2021, 14:57:32)  [Clang 12.0.0 ]\r\n\r\nQlib version: 0.8.4.99\r\nnumpy==1.22.3\r\npandas==1.4.2\r\nscipy==1.8.0\r\nrequests==2.25.1\r\nsacred==0.8.2\r\npython-socketio==5.5.2\r\nredis==4.2.2\r\npython-redis-lock==3.7.0\r\nschedule==1.1.0\r\ncvxpy==1.1.18\r\nhyperopt==0.1.2\r\nfire==0.4.0\r\nstatsmodels==0.13.2\r\nxlrd==2.0.1\r\nplotly==5.6.0\r\nmatplotlib==3.5.1\r\ntables==3.7.0\r\npyyaml==6.0\r\nmlflow==1.24.0\r\ntqdm==4.61.2\r\nloguru==0.6.0\r\nlightgbm==3.3.2\r\ntornado==6.1\r\njoblib==1.1.0\r\nfire==0.4.0\r\nruamel.yaml==0.17.21\r\n\r\n\r\n## Additional Notes\r\n\r\nI installed qlib from source, and my conda env is the version for arm64\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1649238776000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1035",
        "Challenge_link_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":205.01,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":169,
        "Challenge_solved_time":null,
        "Challenge_title":"run the example workflow_by_code.ipynb, caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' ",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":1288,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via setup.py and uninstalling \/ reinstalling numpy. but this one i do not know how to solve:\r\nMlflowException: Got invalid value Series([], dtype: float64) for metric 'IC' (timestamp=1616595157552). Please specify value as a valid double (64-bit floating point)\r\n\r\nif i have only sh000300 in my instruments, it's gonna produce the following value error:\r\nValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]).\r\nYou can drop duplicate edges by setting the 'duplicates' kwarg\r\n\r\ni followed instructions on data collector's markdown page to download cn data up until 03\/01\/2021. my yaml file looks like this:\r\nqlib_init:\r\n    provider_uri: \"\/content\/gdrive\/MyDrive\/qlib\/qlib_data\/qlib_cn_1d\"\r\n    region: cn\r\nmarket: &market \r\nbenchmark: &benchmark SH000300\r\ndata_handler_config: &data_handler_config\r\n    start_time: 2008-01-01\r\n    end_time: 2021-03-01\r\n    fit_start_time: 2008-01-01\r\n    fit_end_time: 2018-12-31\r\n    instruments: ['SH000300', 'SH000903']\r\nport_analysis_config: &port_analysis_config\r\n    strategy:\r\n        class: TopkDropoutStrategy\r\n        module_path: qlib.contrib.strategy.strategy\r\n        kwargs:\r\n            topk: 50\r\n            n_drop: 5\r\n    backtest:\r\n        verbose: True\r\n        limit_threshold: 0.095\r\n        account: 50000\r\n        benchmark: *benchmark\r\n        deal_price: close\r\n        open_cost: 0.0005\r\n        close_cost: 0.0015\r\n        min_cost: 5\r\ntask:\r\n    model:\r\n        class: DEnsembleModel\r\n        module_path: qlib.contrib.model.double_ensemble\r\n        kwargs:\r\n            base_model: \"gbm\"\r\n            loss: mse\r\n            num_models: 6\r\n            enable_sr: True\r\n            enable_fs: True\r\n            alpha1: 1\r\n            alpha2: 1\r\n            bins_sr: 10\r\n            bins_fs: 5\r\n            decay: 0.5\r\n            sample_ratios:\r\n                - 0.8\r\n                - 0.7\r\n                - 0.6\r\n                - 0.5\r\n                - 0.4\r\n            sub_weights:\r\n                - 1\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n            epochs: 28\r\n            colsample_bytree: 0.8879\r\n            learning_rate: 0.2\r\n            subsample: 0.8789\r\n            lambda_l1: 205.6999\r\n            lambda_l2: 580.9768\r\n            max_depth: 8\r\n            num_leaves: 210\r\n            num_threads: 20\r\n            verbosity: -1\r\n    dataset:\r\n        class: DatasetH\r\n        module_path: qlib.data.dataset\r\n        kwargs:\r\n            handler:\r\n                class: Alpha158\r\n                module_path: qlib.contrib.data.handler\r\n                kwargs: *data_handler_config\r\n            segments:\r\n                train: [2008-01-01, 2018-12-31]\r\n                valid: [2019-01-01, 2020-07-31]\r\n                test: [2020-08-01, 2020-03-01]\r\n    record:\r\n        - class: SignalRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs: {}\r\n        - class: SigAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            ana_long_short: False\r\n            ann_scaler: 252\r\n        - class: PortAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            config: *port_analysis_config\r\n\r\nthanks for answering in advance.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1616595419000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/369",
        "Challenge_link_count":0,
        "Challenge_readability":8.9,
        "Challenge_reading_time":32.29,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Double Ensemble MlflowException",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":294,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":19.5733333333,
        "Challenge_answer_count":3,
        "Challenge_body":"## Expected Behavior\r\nThe metadata service (using Neptune) to start successfully.\r\n\r\n## Current Behavior\r\nFlask application startup fails due to an import error - `ImportError: module 'metadata_service.config' has no attribute 'NeptuneConfig'`\r\n\r\n## Possible Solution\r\nMake the NeptuneConfig discoverable by the service.\r\n\r\n## Steps to Reproduce\r\n1. Deploy a container based on the amundsen-metadata image (latest)\r\n2. Follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-neptune.md) to set up the service to use Neptune i.e. configure env vars\r\n3. Start container and the app is unable to start\r\n\r\n## Screenshots (if appropriate)\r\n![Screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png)\r\n\r\n## Context\r\nI cannot start an ECS task based on this image and therefore can't connect to the Neptune cluster.\r\n\r\n## Your Environment\r\n",
        "Challenge_closed_time":1666184788000,
        "Challenge_created_time":1666114324000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/2013",
        "Challenge_link_count":2,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.5733333333,
        "Challenge_title":"Bug Report: NeptuneConfig import failing - Flask",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":112,
        "Platform":"Github",
        "Solution_body":"Thanks for opening your first issue here!\n Any solution for this? > Any solution for this?\r\n\r\nThe error message was a bit of a red herring. The actual problem was amundsen-gremlin isn't installed as part of the base image creation `amundsendev\/amundsen-metadata`.\r\n\r\nSolution 1 - add the package to the requirements files and rebuild your own Amundsen image\r\n\r\nSolution 2 - build on the base image and add a `RUN pip install amundsen-gremlin` to your bespoke dockerfile. For my use case I've gone with the latter.",
        "Solution_link_count":0.0,
        "Solution_readability":6.7,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":1322.0291666667,
        "Challenge_answer_count":4,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Challenge_closed_time":1664069854000,
        "Challenge_created_time":1659310549000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Challenge_link_count":2,
        "Challenge_readability":10.1,
        "Challenge_reading_time":31.58,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":1322.0291666667,
        "Challenge_title":"Neptune MalformedQueryException",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":345,
        "Platform":"Github",
        "Solution_body":"Thanks for opening your first issue here!\n The PR that solves the issue in my case https:\/\/github.com\/amundsen-io\/amundsen\/pull\/1947\/files This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n This issue has been automatically closed for inactivity. If you still wish to make these changes, please open a new pull request or reopen this one.\n",
        "Solution_link_count":1.0,
        "Solution_readability":7.7,
        "Solution_reading_time":5.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":766.1991666667,
        "Challenge_answer_count":7,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Expected Behavior\r\n<WARNING:elasticsearch:PUT https:\/\/my aws ES endpoint\/table_a54a9a96-c246-4bcd-b417-2d8c005c3290 [status:400 request:0.069s]\r\nINFO:databuilder.callback.call_back:No callbacks to notify\r\nTraceback (most recent call last):\r\n  File \"example\/scripts\/sample_data_loader_neptune.py\", line 403, in <module>\r\n    job_es_table.launch()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 76, in launch\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 72, in launch\r\n    self.publisher.publish()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n    self.publish_impl()\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/elasticsearch_publisher.py\", line 93, in publish_impl\r\n    self.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/utils.py\", line 347, in _wrapped\r\n    return func(*args, params=params, headers=headers, **kwargs)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/indices.py\", line 146, in create\r\n    \"PUT\", _make_path(index), params=params, headers=headers, body=body\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 466, in perform_request\r\n    raise e\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 434, in perform_request\r\n    timeout=timeout,\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/http_requests.py\", line 216, in perform_request\r\n    self._raise_error(response.status_code, raw_data)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/base.py\", line 329, in _raise_error\r\n    status_code, error_message, additional_info\r\n\r\n\r\nelasticsearch.exceptions.RequestError: RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [schema : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [cluster : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [description : {analyzer=simple, type=text}] [display_name : {type=keyword}] [column_descriptions : {analyzer=simple, type=text}] [programmatic_descriptions : {analyzer=simple, type=text}] [tags : {type=keyword}] [badges : {type=keyword}] [database : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [total_usage : {type=long}] [name : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [last_updated_timestamp : {format=epoch_second, type=date}] [unique_usage : {type=long}] [column_names : {analyzer=simple, type=text, fields={raw={normalizer=column_names_normalizer, type=keyword}}}] [key : {type=keyword}]')->\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n* Amunsen version used: Databuilder: 6.7.1 Common 0.26.0 Amundsen-Gremlin 0.0.13 AWS ES : 6.8\r\n",
        "Challenge_closed_time":1649071896000,
        "Challenge_created_time":1646313579000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1748",
        "Challenge_link_count":1,
        "Challenge_readability":22.0,
        "Challenge_reading_time":43.86,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":766.1991666667,
        "Challenge_title":"Bug Report elasticsearch exception for sample_neptune_loader",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":216,
        "Platform":"Github",
        "Solution_body":"Thanks for opening your first issue here!\n This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n Same problem here, does you solved? @amandeep848 could you fix the problem? @amandeep848 could you fix the problem? Hello!\r\n\r\nI have been fixed the problem by putting the version of amundsen-common to 0.24.1\r\n\r\n- My `requirements.txt` file is setup as shown below:\r\n\r\n```text\r\namundsen-databuilder==6.5.2\r\namundsen-gremlin==0.0.13\r\ngremlinpython==3.4.10\r\nrequests-aws4auth==1.1.1\r\nboto3==1.21.23\r\nbotocore==1.24.23\r\ntyping-extensions==4.1.1\r\noverrides==6.1.0\r\namundsen-common==0.24.1\r\n```\r\n\r\n- My Glue databuilder script:\r\n\r\n```python\r\nimport logging\r\nimport os\r\nimport uuid\r\nimport boto3\r\nimport textwrap\r\nimport json\r\n\r\nfrom datetime import date\r\n\r\nfrom elasticsearch import Elasticsearch\r\nfrom pyhocon import ConfigFactory\r\n\r\nfrom databuilder.clients.neptune_client import NeptuneSessionClient\r\nfrom databuilder.extractor.es_last_updated_extractor import EsLastUpdatedExtractor\r\nfrom databuilder.extractor.neptune_search_data_extractor import NeptuneSearchDataExtractor\r\n\r\nfrom databuilder.job.job import DefaultJob\r\nfrom databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader\r\nfrom databuilder.loader.file_system_neptune_csv_loader import FSNeptuneCSVLoader\r\nfrom databuilder.publisher.elasticsearch_constants import (\r\n    DASHBOARD_ELASTICSEARCH_INDEX_MAPPING, USER_ELASTICSEARCH_INDEX_MAPPING,\r\n)\r\nfrom databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher\r\nfrom databuilder.publisher.neptune_csv_publisher import NeptuneCSVPublisher\r\nfrom databuilder.task.task import DefaultTask\r\nfrom databuilder.transformer.base_transformer import ChainedTransformer, NoopTransformer\r\nfrom databuilder.transformer.dict_to_model import MODEL_CLASS, DictToModel\r\nfrom databuilder.transformer.generic_transformer import (\r\n    CALLBACK_FUNCTION, FIELD_NAME, GenericTransformer,\r\n)\r\n\r\nfrom databuilder.extractor.glue_extractor import GlueExtractor\r\nfrom databuilder.task.neptune_staleness_removal_task import NeptuneStalenessRemovalTask\r\n\r\n\r\nes_host = os.getenv('ES_HOST')\r\n\r\nneptune_host = os.getenv('NEPTUNE_HOST')\r\nneptune_port = os.getenv('NEPTUNE_PORT', 8182)\r\nneptune_iam_role_name = os.getenv('NEPTUNE_IAM_ROLE')\r\n\r\nS3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\r\ntoday = date.today()\r\nS3_DATA_PATH = f'amundsen_data\/glue_extractor\/year={today.year}\/month={today.month}\/day={today.day}'\r\n\r\nAWS_REGION = os.getenv('AWS_REGION')\r\nGLUE_DATABASE_IDENTIFIER = os.getenv('GLUE_DATABASE_IDENTIFIER')\r\n\r\nes = Elasticsearch(\r\n    '{}'.format(es_host),\r\n    scheme=\"https\",\r\n    port=443,\r\n)\r\n\r\nNEPTUNE_ENDPOINT = '{}:{}'.format(neptune_host, neptune_port)\r\n\r\nLOGGER = logging.getLogger(__name__)\r\n\r\n\r\ndef run_glue_job(job_name):\r\n    \"\"\"Run Glue metadata extraction\r\n\r\n    Args:\r\n        job_name (string): job name\r\n    \"\"\"\r\n\r\n    tmp_folder = '\/var\/tmp\/amundsen\/{job_name}'.format(job_name=job_name)\r\n    node_files_folder = '{tmp_folder}\/nodes'.format(tmp_folder=tmp_folder)\r\n    relationship_files_folder = '{tmp_folder}\/relationships'.format(tmp_folder=tmp_folder)\r\n\r\n    loader = FSNeptuneCSVLoader()\r\n    publisher = NeptuneCSVPublisher()\r\n\r\n    with open(\"databases.json\") as jsonFile:\r\n\r\n        filters = json.load(jsonFile)\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        f'extractor.glue.{GlueExtractor.CLUSTER_KEY}': GLUE_DATABASE_IDENTIFIER,\r\n        f'extractor.glue.{GlueExtractor.FILTER_KEY}': filters,\r\n        loader.get_scope(): {\r\n            FSNeptuneCSVLoader.NODE_DIR_PATH: node_files_folder,\r\n            FSNeptuneCSVLoader.RELATION_DIR_PATH: relationship_files_folder,\r\n            FSNeptuneCSVLoader.SHOULD_DELETE_CREATED_DIR: True,\r\n            FSNeptuneCSVLoader.JOB_PUBLISHER_TAG: 'unique_tag'\r\n        },\r\n        publisher.get_scope(): {\r\n            NeptuneCSVPublisher.NODE_FILES_DIR: node_files_folder,\r\n            NeptuneCSVPublisher.RELATION_FILES_DIR: relationship_files_folder,\r\n            NeptuneCSVPublisher.AWS_S3_BUCKET_NAME: S3_BUCKET_NAME,\r\n            NeptuneCSVPublisher.AWS_BASE_S3_DATA_PATH: S3_DATA_PATH,\r\n            NeptuneCSVPublisher.NEPTUNE_HOST: NEPTUNE_ENDPOINT,\r\n            NeptuneCSVPublisher.AWS_IAM_ROLE_NAME: neptune_iam_role_name,\r\n            NeptuneCSVPublisher.AWS_REGION: AWS_REGION\r\n        },\r\n    })\r\n\r\n    DefaultJob(\r\n        conf=job_config,\r\n        task=DefaultTask(\r\n            extractor=GlueExtractor(),\r\n            loader=loader,\r\n            transformer=NoopTransformer()\r\n        ),\r\n        publisher=publisher\r\n    ).launch()\r\n\r\ndef create_remove_stale_data_job():\r\n    \"\"\"Run remove stale data from Neptune\r\n\r\n    Returns:\r\n        NeptuneStalenessRemovalTask: Neptune stateleness data job\r\n    \"\"\"\r\n\r\n    target_relations = ['DESCRIPTION', 'DESCRIPTION_OF', 'COLUMN', 'COLUMN_OF', 'TABLE', 'TABLE_OF']\r\n    target_nodes = ['Table', 'Column', 'Programmatic_Description', \"Schema\"]\r\n\r\n    staleness_max_pct = 5\r\n\r\n    while True:\r\n\r\n        try:\r\n\r\n            LOGGER.info(f'Delete stale data at threshold - {staleness_max_pct}%')\r\n\r\n            job_config = ConfigFactory.from_dict({\r\n                'task.remove_stale_data': {\r\n                    NeptuneStalenessRemovalTask.TARGET_RELATIONS: target_relations,\r\n                    NeptuneStalenessRemovalTask.TARGET_NODES: target_nodes,\r\n                    NeptuneStalenessRemovalTask.STALENESS_CUT_OFF_IN_SECONDS: 86400,  # 1 day\r\n                    NeptuneStalenessRemovalTask.STALENESS_MAX_PCT: staleness_max_pct,\r\n                    'neptune.client': {\r\n                        NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                        NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                    }\r\n                }\r\n            })\r\n\r\n            job = DefaultJob(\r\n                conf=job_config,\r\n                task=NeptuneStalenessRemovalTask()\r\n            )\r\n\r\n            job.launch()\r\n\r\n            break\r\n\r\n        except Exception as ex:\r\n\r\n            LOGGER.error(ex)\r\n            LOGGER.info(f'Increase stale data threshold')\r\n\r\n            staleness_max_pct += 5\r\n\r\n            if staleness_max_pct == 105:\r\n\r\n                break\r\n\r\n\r\ndef create_es_publisher_job(elasticsearch_index_alias='table_search_index',\r\n                            elasticsearch_doc_type_key='table',\r\n                            model_name='databuilder.models.table_elasticsearch_document.TableESDocument',\r\n                            entity_type='table',\r\n                            elasticsearch_mapping=None):\r\n    \"\"\"\r\n    :param elasticsearch_index_alias:  alias for Elasticsearch used in\r\n                                       amundsensearchlibrary\/search_service\/config.py as an index\r\n    :param elasticsearch_doc_type_key: name the ElasticSearch index is prepended with. Defaults to `table` resulting in\r\n                                       `table_{uuid}`\r\n    :param model_name:                 the Databuilder model class used in transporting between Extractor and Loader\r\n    :param entity_type:                Entity type handed to the `Neo4jSearchDataExtractor` class, used to determine\r\n                                       Cypher query to extract data from Neo4j. Defaults to `table`.\r\n    :param elasticsearch_mapping:      Elasticsearch field mapping \"DDL\" handed to the `ElasticsearchPublisher` class,\r\n                                       if None is given (default) it uses the `Table` query baked into the Publisher\r\n    \"\"\"\r\n    # loader saves data to this location and publisher reads it from here\r\n    extracted_search_data_path = '\/var\/tmp\/amundsen\/search_data.json'\r\n    loader = FSElasticsearchJSONLoader()\r\n    extractor = NeptuneSearchDataExtractor()\r\n\r\n    task = DefaultTask(\r\n        loader=loader,\r\n        extractor=extractor,\r\n        transformer=NoopTransformer()\r\n    )\r\n\r\n    # elastic search client instance\r\n    elasticsearch_client = es\r\n\r\n    # unique name of new index in Elasticsearch\r\n    elasticsearch_new_index_key = '{}_'.format(elasticsearch_doc_type_key) + str(uuid.uuid4())\r\n\r\n    publisher = ElasticsearchPublisher()\r\n\r\n    session = boto3.Session(region_name=AWS_REGION)\r\n\r\n    aws_creds = session.get_credentials()\r\n    aws_access_key = aws_creds.access_key\r\n    aws_access_secret = aws_creds.secret_key\r\n    aws_token = aws_creds.token\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        extractor.get_scope(): {\r\n            NeptuneSearchDataExtractor.ENTITY_TYPE_CONFIG_KEY: entity_type,\r\n            NeptuneSearchDataExtractor.MODEL_CLASS_CONFIG_KEY: model_name,\r\n            'neptune.client': {\r\n                NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                NeptuneSessionClient.AWS_ACCESS_KEY: aws_access_key,\r\n                NeptuneSessionClient.AWS_SECRET_ACCESS_KEY: aws_access_secret,\r\n                NeptuneSessionClient.AWS_SESSION_TOKEN: aws_token\r\n            }\r\n        },\r\n        'loader.filesystem.elasticsearch.file_path': extracted_search_data_path,\r\n        'loader.filesystem.elasticsearch.mode': 'w',\r\n        publisher.get_scope(): {\r\n            'file_path': extracted_search_data_path,\r\n            'mode': 'r',\r\n            'client': elasticsearch_client,\r\n            'new_index': elasticsearch_new_index_key,\r\n            'doc_type': elasticsearch_doc_type_key,\r\n            'alias': elasticsearch_index_alias\r\n        }\r\n    })\r\n\r\n    # only optionally add these keys, so need to dynamically `put` them\r\n    if elasticsearch_mapping:\r\n        job_config.put('publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_MAPPING_CONFIG_KEY),\r\n                       elasticsearch_mapping)\r\n\r\n    job = DefaultJob(\r\n        conf=job_config,\r\n        task=task,\r\n        publisher=ElasticsearchPublisher()\r\n    )\r\n\r\n    return job\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    LOGGER.info('ES Host: ' +  es_host)\r\n    LOGGER.info('Neptune Host: ' + neptune_host)\r\n    LOGGER.info('Neptune Port: ' + str(neptune_port))\r\n    LOGGER.info('Neptune IAM Role Name: ' + neptune_iam_role_name)\r\n    LOGGER.info('S3 Bucket Name: ' + S3_BUCKET_NAME)\r\n    LOGGER.info('S3 Data Path: ' + S3_DATA_PATH)\r\n    LOGGER.info('AWS Region: ' + AWS_REGION)\r\n\r\n    logging.info('>>> Running Remove Stale Data Job <<<')\r\n\r\n    create_remove_stale_data_job()\r\n\r\n    logging.info('>>> Running Glue Extractor <<<')\r\n\r\n    run_glue_job('amundsen_glue_extractor')\r\n\r\n    logging.info('>>> Running ES Publisher <<<')\r\n\r\n    job_es_table = create_es_publisher_job(\r\n        elasticsearch_index_alias='table_search_index',\r\n        elasticsearch_doc_type_key='table',\r\n        entity_type='table',\r\n        model_name='databuilder.models.table_elasticsearch_document.TableESDocument'\r\n    )\r\n    job_es_table.launch()\r\n```\r\n\r\n- databases.json\r\n\r\n```json\r\n[]\r\n```\r\n\r\n- .env\r\n\r\n```env\r\nES_HOST=<ES_HOST>\r\nNEPTUNE_HOST=<NEPTUNE_HOST>\r\nNEPTUNE_PORT=8182\r\nNEPTUNE_IAM_ROLE=<NEPTUNE_IAM_ROLE>\r\nS3_BUCKET_NAME=<S3_BUCKET_NAME>\r\nAWS_REGION=<AWS_REGION>\r\nSECRET_NAME=<SECRET_NAME>\r\nGLUE_DATABASE_IDENTIFIER=<GLUE_DATABASE_IDENTIFIER>\r\n```\r\n\r\n\r\nHope this help!\r\n\r\nBest Regards.\r\nBill\r\n I encountered this issue, too, as I installed data builder from codebase with `python setup.py install`, and after rebase with the main branch, the previous version was not clean up when we simply rerun `python setup.py install`, the way out was to do `pip uninstall amundsen-databuilder` and `pip uninstall amundsen-common` until non of packages existed(there could be multiple versions left, more than once per each package could be required).\r\n\r\nThen the expected elastic-related code is up to date w\/o this error anymore.",
        "Solution_link_count":0.0,
        "Solution_readability":20.1,
        "Solution_reading_time":132.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":103.0,
        "Solution_word_count":698.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":11799.0105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"For uploading data to AWS Neptune we use `NeptuneCSVPublisher`, which internally uses `NeptuneBulkLoaderApi`. The current configuration uses config key `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME`, which provides name of IAM role for the loader to be able to use S3 and Neptune. The issue is that `NeptuneBulkLoaderApi` constructs IAM role ARN from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['Account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of ARN aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -AWS Regions\r\n* `aws-cn` - China Regions\r\n* `aws-us-gov` - AWS GovCloud (US) Regions\r\n\r\nSince we use Amundsen also in AWS China, the above ARN is not valid. \r\n\r\n## Expected Behavior\r\n\r\nIAM role ARN either takes into account AWS partition or there is a possibility of passing IAM role ARN instead of name directly.\r\n\r\n## Current Behavior\r\n\r\nIAM role ARN is constructed incorrectly outside of AWS Global.\r\n\r\n## Possible Solutions\r\n\r\nIAM role ARN should take partition into account. There are two solutions:\r\n1. Add partition into current code\r\n2. Add option of passing IAM role ARN directly which supersedes IAM role name \r\n\r\n### Solution 1\r\n\r\nSince I didn't know or found any good way to get the AWS partition, we can use caller identity and ARN there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['Account']\r\npartition = identity['Arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nThis is smaller fix but it is a bit hacky and I'm not sure it'll work in all situation, but it should I guess.\r\n\r\n### Solution 2\r\n\r\nAdd config key `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN` which either supersedes `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nOr even replace `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` with `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN`, which is IMO cleaner, but would be not backward compatible. \r\n\r\n## Steps to Reproduce\r\nDeploy Amundsen in AWS China with Neptune and try to use Databuilder to upload CSV data from S3. \r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nCurrently we are unable to load data into Neptune as the IAM role ARN setting is hidden and we get an error:\r\n\r\n```\r\n[ERROR] Exception: Failed to load csv. Response: {'detailedMessage': \"Failed to start new load from the source s3:\/\/amundsenBucket\/amundsen\/2021_08_10_01_01_28. Couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/RoleForNeptune111111-2222\", 'code': 'InvalidParameterException', 'requestId': 'xxx'}\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to_neptune_job.launch()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/neptune_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(\"Failed to load csv. Response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## Your Environment\r\n* Amunsen version used: `amundsen-databuilder==4.3.1`\r\n* Data warehouse stores: AWS Neptune\r\n* Deployment (k8s or native): AWS Step Functions (k8s for backend but unrelated for now)\r\n* Link to your fork or repository:",
        "Challenge_closed_time":1671067447000,
        "Challenge_created_time":1628591009000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1430",
        "Challenge_link_count":1,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.7,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":11799.0105555556,
        "Challenge_title":"Databuilder `NeptuneBulkLoaderApi` constructs wrong IAM role ARN for AWS other than global",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":441,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n",
        "Solution_link_count":0.0,
        "Solution_readability":9.2,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":29.6355555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Challenge_closed_time":1635986654000,
        "Challenge_created_time":1635879966000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Challenge_link_count":4,
        "Challenge_readability":18.5,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":29.6355555556,
        "Challenge_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Challenge_topic":"Role Management",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":103,
        "Platform":"Github",
        "Solution_body":"Resolved as of release 3.0.8",
        "Solution_link_count":0.0,
        "Solution_readability":2.9,
        "Solution_reading_time":0.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":216.3194444444,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen using the Neptune ML widget to export data like the command below from the 01- Node Classification notebook:\r\n```\r\n%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nThe following error is thrown\r\n```\r\n{\r\n  \"message\": \"Credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**Expected behavior**\r\nThe export should run to completion\r\n\r\n\r\n",
        "Challenge_closed_time":1628716798000,
        "Challenge_created_time":1627938048000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/167",
        "Challenge_link_count":0,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":216.3194444444,
        "Challenge_title":"[BUG] Neptune ML Export widget throwing error",
        "Challenge_topic":"Dataset Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"This issue occurs on a cluster created using the default CFN script with IAM disabled\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":8.0,
        "Solution_reading_time":1.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":1679.0761111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Challenge_closed_time":1632957644000,
        "Challenge_created_time":1626912970000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Challenge_link_count":0,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1679.0761111111,
        "Challenge_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":76,
        "Platform":"Github",
        "Solution_body":"Hi @Roshin29, thank you for the bug report! \r\n\r\nThe machine learning sample notebooks received substantial revisions in [Release 3.0.1](https:\/\/github.com\/aws\/graph-notebook\/releases\/tag\/v3.0.1). This release also included a number of changes under the hood to support the general availability release of Amazon Neptune ML.\r\n\r\nThe Gremlin query listed is only seen in older versions of the `Neptune-ML-01-Introduction-to-Node-Classification-Gremlin` sample notebook, and is now replaced by the one below:\r\n```\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"${endpoint}\").\r\n  with(\"Neptune#ml.limit\",3).\r\n  V().has('title', 'Apollo 13 (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\n```\r\nI am not able to reproduce the listed exception when running this query using graph-notebook v3.0.6, so the issue appears to have been resolved with the latest changes.\r\n\r\nClosing this issue out, as there are no further action items at this time. Please feel free to re-open if you have any further questions.",
        "Solution_link_count":1.0,
        "Solution_readability":12.2,
        "Solution_reading_time":12.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"For the  01 notebooks for Neptune ML the text in the notebook incorrectly specifies that the genre returned for a node classification task on `Toy Story` is `Comedy` when it should be `Drama`",
        "Challenge_closed_time":null,
        "Challenge_created_time":1619195852000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Neptune ML notebooks have incorrect Genre stated in the text",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":43,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":1339.2047222222,
        "Challenge_answer_count":25,
        "Challenge_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Challenge_closed_time":1620330541000,
        "Challenge_created_time":1615509404000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Challenge_link_count":0,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.48,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1339.2047222222,
        "Challenge_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":"This appears to be an issue with the versions of `ipython` that SageMaker is using.  If you update the Lifecycle start script by putting the following code at the bottom (just before EOF) and stopping and starting the notebook.\r\n```\r\nsource activate JupyterSystemEnv\r\npip install --upgrade ipython==7.16.1\r\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\r\n``` Hi, i have updated the Lifecycle scripts as suggested and that works - but then it fails on the training:\r\n\r\n`\"status\": \"Failed\",\r\n    \"failureReason\": \"ClientError: Failed to download data`\r\n\r\n...\r\npreloading-2021-04-05-17-33-3910000\/preloading-output\/graph.bin has an illegal char sub-sequence '\/\/' in it\"`\r\n\r\ni just used the movie lens database and steps in the notebook. it adds an extra '\\' in the \"outputLocation\"...?\r\n\r\ncan you help?  \r\n Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?  > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\n\r\n<img width=\"1103\" alt=\"error_train_screen\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n > > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n> \r\n> <img alt=\"error_train_screen\" width=\"1103\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n\r\n<img width=\"1117\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705546-73cdce00-96d5-11eb-81fa-633c14942847.png\">\r\n > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\nhi @austinkline  - thanks for helping me out. So as you can see from the screenshots, it fails to download the data and seems to be adding an extra slash...\r\n\r\nso I changed the script: `--s3-processed-uri {str(s3_bucket_uri)}preloading \"\"\"` \r\nand it then ran fine.... perhaps you want to correct that in the notebook?\r\n\r\nbut when making the prediction I am getting:\r\n\r\n<img width=\"1120\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113713442-42f29680-96df-11eb-8dc8-131e8377fa4c.png\">\r\n\r\n\r\nso Toy Story comes up as 'Thriller\" and not 'Comedy' as  per the notebook\r\n\r\n\r\nhow can I see which actual model the classification is using? Is it a graph convolutional network, I recall seeing that in the notebooks in the repository. It would be good to see the actual DGL model & code. \r\n\r\nThanks!!\r\n Thanks for the info. I'll spend some time reproducing and get back to you I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n\r\n@Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore > I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n> \r\n> ![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n> \r\n> @Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore\r\n\r\nthank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?  > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n\r\nChecked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console. \r\n > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> \r\n> Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n\r\nyeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network > > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> > \r\n> > \r\n> > Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n> \r\n> yeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network\r\n\r\nyes. that's correct. Hi @Kristof-Neys and updates? Did recreating work for you? Hi @austinkline - thanks for reaching out. I have been caught up in another project but was just about to look at it. I'll update you guys probably tomorrow.  hi @austinkline & Team, i am finally getting around to this. I started everything new but now I cannot export the configuration any more, I get the following error:\r\n`{'error': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='none', port=443): Max retries exceeded with url: \/neptune-export (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f28f5e81748>: Failed to establish a new connection: [Errno -2] Name or service not known',))\",),)}`\r\n\r\nUPdate: when I re-started everything and used the notebook of last week... i get\r\n\r\n`403 \"Missing Authentication Token\" `\r\n\r\n\r\n\r\nany ideas? Thanks!!\r\n     Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n\r\nCan you provide your notebook version and configuration by running the following:\r\n\r\n1. What cell did you execute that gave you the above mentioned error?\r\n\r\n2. What version of `graph-notebook` are you running?\r\n```\r\n%graph_notebook_version\r\n```\r\n\r\n3. What is your configuration? Really we just care about the authentication setting\r\n**NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n\r\n```\r\n%graph_notebook_config\r\n```\r\n > Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n> \r\n> Can you provide your notebook version and configuration by running the following:\r\n> \r\n>     1. What cell did you execute that gave you the above mentioned error?\r\n> \r\n>     2. What version of `graph-notebook` are you running?\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_version\r\n> ```\r\n> \r\n>     1. What is your configuration? Really we just care about the authentication setting\r\n>        **NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_config\r\n> ```\r\n\r\n@austinkline thank you! Very much appreciate taking time & effort. Ok, so these are the detail:\r\n\r\ncell that I am running:\r\n`%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}`\r\n=> this gives me error: \r\n`{\r\n  \"message\": \"Missing Authentication Token\"\r\n}`\r\n\r\n\r\nVersion graph-notebook: 2.1.0\r\n\r\n%graph_notebook_config:\r\n`{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}`\r\n\r\nThe strange thing is that all worked well two weeks ago, altho I did get wrong predictions, but at least the export worked and I could train model and get predictions etc. Now I cannot get beyond the export.... \r\n\r\nthank you again\r\n\r\n @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n\r\n```\r\n%%graph_notebook_config\r\n{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"IAM\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}\r\n```\r\n\r\nNote that we're changing the auth mode to IAM > @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n> \r\n> ```\r\n> %%graph_notebook_config\r\n> {\r\n>   \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n>   \"port\": 8182,\r\n>   \"auth_mode\": \"IAM\",\r\n>   \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n>   \"ssl\": true,\r\n>   \"aws_region\": \"us-east-1\",\r\n>   \"sparql\": {\r\n>     \"path\": \"sparql\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> Note that we're changing the auth mode to IAM\r\nhey @austinkline  - that worked!, export and training went fine....but still predicting the wrong genre.... - how can this be??\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/115867858-82d1b180-a433-11eb-809c-a1aa733e5d90.png\">\r\n\r\n\r\n @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect.  Drama is what is coming back from the model that is generated .  I have created an issue to track this https:\/\/github.com\/aws\/graph-notebook\/issues\/116 and will address this with the additional feedback on those notebooks in the near future.  > @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect. Drama is what is coming back from the model that is generated . I have created an issue to track this #116 and will address this with the additional feedback on those notebooks in the near future.\r\n\r\nok understood - thank you\r\n Closing this out since we're tracking the reported issue of notebooks being out of date in #116. Please cut us a new ticket if you run into any further issues! Hi guys, I'm facing a similar issue, I applied your fix(setting \"auth_mode\": \"IAM\") but did not work, any suggestions? Hi @llealgt , is this referring to the same issue mentioned at https:\/\/github.com\/aws\/graph-notebook\/issues\/445#issuecomment-1426192856? Hi @michaelnchin, nope, it's not the same, this happens when running notebook \r\nNeptune-ML-01-Introduction-to-Node-Classification-Gremlin\r\nThe other errors happen in notebook \r\nNeptune-ML-00-Getting-Started-with-Neptune-ML-Gremlin\r\nI guess it is related but they are different errors in different notebooks.",
        "Solution_link_count":9.0,
        "Solution_readability":9.2,
        "Solution_reading_time":145.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":102.0,
        "Solution_word_count":1555.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":431.6069444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Challenge_closed_time":1608658157000,
        "Challenge_created_time":1607104372000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":431.6069444444,
        "Challenge_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":81,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":43.3186111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Challenge_closed_time":1607104425000,
        "Challenge_created_time":1606948478000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Challenge_link_count":1,
        "Challenge_readability":10.1,
        "Challenge_reading_time":20.85,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.3186111111,
        "Challenge_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"Looks like we have a missing piece in our walkthrough for connecting to Neptune:\r\n\r\nhttps:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune\r\n\r\nCould you set your hostname from localhost to your Neptune endpoint:\r\n\r\n```\r\n%graph_notebook_host <your endpoint here>\r\n```\r\n\r\nAnd give it a try? I  updated \/etc\/hosts on my Mac and added an alias for localhost as\r\n\r\n> _27.0.0.1       localhost    yourneptuneendpoint_\r\n\r\nFlushed DNS cache.\r\nSet the hostname in Jupyter notebook graph_notebook_config command to **yourneptuneendpoint**.\r\nRan sparql query and it successfully completed.\r\n\r\n\r\n Glad it worked! I have filed an issue for us to expand our documentation to cover the steps you had to take. Closing this but feel free to open or submit a new issue if you need further assistance",
        "Solution_link_count":1.0,
        "Solution_readability":6.9,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":110.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":38.1544444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030635000,
        "Challenge_created_time":1624893279000,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":160.0,
        "Challenge_repo_star_count":28.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":38.1544444444,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":22,
        "Platform":"Github",
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_link_count":0.0,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.06875,
        "Challenge_watch_issue_ratio":0.06875
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nNeptune guide's quicklaunch link points to the full launcher\r\n\r\n**Expected behavior**\r\nShould point to the minimal launcher\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1625774761000,
        "Challenge_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/57",
        "Challenge_link_count":0,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.54,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":16.0,
        "Challenge_repo_issue_count":82.0,
        "Challenge_repo_star_count":102.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] neptune minimal launcher link points to full launcher",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0487804878,
        "Challenge_watch_issue_ratio":0.1463414634
    },
    {
        "Challenge_adjusted_solved_time":4.2694444444,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\n\r\nCloud formation for neptune fails on a p3.2 and p3.16 yet succeeds on a g4dn\r\n\r\nReported by a Neptune user\r\n\r\n**To Reproduce**\r\n\r\nRun through Neptune tutorial and use a p3.16\r\n\r\n**Expected behavior**\r\nIt launches\r\n\r\n**Actual behavior**\r\nFormation template stalls out and auto-deletes\r\n\r\nWorking on getting logs. After 10min, GPU services (forge-etl-python + streamgl) failed to start. V100 issue?\r\n\r\n**Screenshots**\r\n\r\n**Browser environment (please complete the following information):**\r\nall\r\n\r\n**PyGraphistry environment**\r\nAll\r\n\r\n**Additional context**\r\nCurrent graph-app-kit",
        "Challenge_closed_time":1615109545000,
        "Challenge_created_time":1615094175000,
        "Challenge_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/45",
        "Challenge_link_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.89,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":16.0,
        "Challenge_repo_issue_count":82.0,
        "Challenge_repo_star_count":102.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4.2694444444,
        "Challenge_title":"[BUG] AWS neptune templates for p3.2, p3.16 fail to start",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":85,
        "Platform":"Github",
        "Solution_body":"Slow start, and wrong docker images load, with manual restart required to reach a healthy state:\r\n\r\n```\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS                             PORTS                                                NAMES\r\n0e32dfece83d        graphistry\/graphistry-nexus:v2.32.4      \"\/entrypoint \/bin\/ba\u2026\"   28 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_nexus_1\r\ndae68b0726e0        graphistry\/graphistry-pivot:v2.32.4      \"\/tini -- \/entrypoin\u2026\"   28 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_pivot_1\r\nffea7ceeb047        graphistry\/etl-server:v2.32.4            \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_forge-etl_1\r\nfbbb7924272e        graphistry\/streamgl-gpu:v2.32.4          \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 39 seconds (health: starting)   8080\/tcp                                             graphistry_streamgl-gpu_1\r\n6c1501fe0317        graphistry\/streamgl-sessions:v2.32.4     \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_streamgl-sessions_1\r\n485ef6374082        willfarrell\/autoheal:v0.7.0              \"\/docker-entrypoint \u2026\"   30 minutes ago      Up 28 minutes (healthy)            8080\/tcp                                             graphistry_autoheal_1\r\n24e6fd022e1e        graphistry\/graphistry-postgres:v2.32.4   \"docker-entrypoint.s\u2026\"   30 minutes ago      Up 28 minutes (healthy)            5432\/tcp, 8080\/tcp                                   graphistry_postgres_1\r\n6bd0cc9c7a58        graphistry\/streamgl-vgraph-etl:v2.32.4   \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_streamgl-vgraph-etl_1\r\n097054def985        graphistry\/etl-server-python:v2.32.4     \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_forge-etl-python_1\r\n2e516287f6e5        graphistry\/streamgl-viz:v2.32.4          \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_streamgl-viz_1\r\nf16cd890cb6b        graphistry\/caddy:v2.30.28                \"\/usr\/bin\/caddy --co\u2026\"   30 minutes ago      Up 40 seconds (health: starting)   0.0.0.0:80->80\/tcp, 0.0.0.0:443->443\/tcp, 2015\/tcp   graphistry_caddy_1\r\nd14e375955c8        redis:6.0.5                              \"docker-entrypoint.s\u2026\"   30 minutes ago      Up 28 minutes (healthy)            6379\/tcp, 8080\/tcp                                   graphistry_redis_1\r\nub\r\n```\r\n\r\nAMI:\r\n\r\n* ami-088aaa8746bde2e21\r\n* graphistry-standalone-2020-10-14T22-32-56Z-v2.32.4-062c9b47-e144-4bbe-8623-fbf14199f760-ami-0cbffaeee3c800e7a.4\r\n* aws-marketplace\/graphistry-standalone-2020-10-14T22-32-56Z-v2.32.4-062c9b47-e144-4bbe-8623-fbf14199f760-ami-0cbffaeee3c800e7a.4 Likely fixed by https:\/\/github.com\/graphistry\/graph-app-kit\/pull\/49 . Reopen if needed.",
        "Solution_link_count":1.0,
        "Solution_readability":10.4,
        "Solution_reading_time":30.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":195.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0487804878,
        "Challenge_watch_issue_ratio":0.1463414634
    },
    {
        "Challenge_adjusted_solved_time":1434.0155555556,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nWhen using `PyTorchLightningPruningCallback` to search best hyperparams, it reports `AttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'`\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom typing import List, Optional\r\n\r\nimport optuna\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchmetrics\r\nimport torchvision\r\nfrom optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback\r\nfrom torch.utils.data import random_split, DataLoader\r\n\r\n\r\nclass FashionDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str, batch_size: int):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        self.train_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=True, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.test_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=False, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.train_set, self.valid_set = random_split(self.train_set, [55000, 5000])\r\n\r\n    def train_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n    def test_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super(SimpleNet, self).__init__()\r\n\r\n        hidden_layers = []\r\n        d_inp = 28 * 28\r\n        for d_hid in d_hids:\r\n            hidden_layers.append(nn.Linear(d_inp, d_hid))\r\n            hidden_layers.append(nn.ReLU())\r\n            hidden_layers.append(nn.Dropout(p_drop))\r\n            d_inp = d_hid\r\n        hidden_layers.append(nn.Linear(d_inp, 10))\r\n\r\n        self.layers = nn.Sequential(*hidden_layers)\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(inputs)\r\n\r\n\r\nclass LitSimpleNet(pl.LightningModule):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super().__init__()\r\n        self.model = SimpleNet(d_hids, p_drop)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.accuracy = torchmetrics.Accuracy()\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.model(inputs.view(-1, 28 * 28))\r\n\r\n    def training_step(self, batch, batch_idx) -> torch.Tensor:\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        return self.criterion(outputs, targets)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        self.accuracy(outputs, targets)\r\n        self.log(\"valid_acc\", self.accuracy, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=3e-4, weight_decay=1e-5)\r\n\r\n\r\ndef objective(trial: optuna.trial.Trial) -> float:\r\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\r\n    p_drop = trial.suggest_float(\"p_drop\", 0.1, 0.5)\r\n    d_hids = [trial.suggest_int(f\"d_hid_{i}\", 16, 128, log=True) for i in range(n_layers)]\r\n\r\n    datamodule = FashionDataModule(\".\", 128)\r\n    model = LitSimpleNet(d_hids, p_drop)\r\n    trainer = pl.Trainer(\r\n        max_epochs=20,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        enable_checkpointing=False,\r\n        logger=True,\r\n        default_root_dir=\".\",\r\n        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"valid_acc\")]\r\n    )\r\n\r\n    hparams = dict(n_layers=n_layers, d_hids=d_hids, p_drop=p_drop)\r\n    trainer.logger.log_hyperparams(hparams)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    return trainer.callback_metrics[\"valid_acc\"].item()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    pruner = optuna.pruners.MedianPruner()\r\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\r\n    study.optimize(objective, n_trials=100, timeout=1000)\r\n\r\n    print(\"Number of Finished Trials:\", len(study.trials))\r\n\r\n    trial = study.best_trial\r\n    print(\"Best Trial:\")\r\n    print(\"\\tValue:\", trial.value)\r\n    print(\"\\tParams:\")\r\n    for key, value in trial.params.items():\r\n        print(f\"\\t\\t{key}: {value}\")\r\n\r\n```\r\n\r\n```bash\r\n[W 2022-09-08 20:14:45,294] Trial 0 failed because of the following error: AttributeError(\"'AcceleratorConnector' object has no attribute 'distributed_backend'\")\r\nTraceback (most recent call last):\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/study\/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"optuna_examples\/optuna_lightning_example.py\", line 89, in objective\r\n    trainer = pl.Trainer(\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/argparse.py\", line 345, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 497, in __init__\r\n    self._call_callback_hooks(\"on_init_start\")\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1585, in _call_callback_hooks\r\n    fn(self, *args, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/integration\/pytorch_lightning.py\", line 61, in on_init_start\r\n    trainer._accelerator_connector.distributed_backend is not None  # type: ignore\r\nAttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'\r\n```\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/github\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.ipynb\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n\r\n### Expected behavior\r\n\r\nShould not report any errors.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n\r\n```\r\n\r\n\r\n<details>\r\n  <summary>Details<\/summary>\r\n    Paste the output here and move this toggle outside of the comment block.\r\n<\/details>\r\n\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- Lightning Component:  Trainer\r\n- PyTorch Lightning Version:  1.7.5\r\n- PyTorch Version:  1.12.1\r\n- Python version: 3.8.13\r\n- OS: Linux (Ubuntu 20.04)\r\n- CUDA\/cuDNN version: 11.3.1\r\n- How you installed PyTorch: conda\r\n\r\n\n\ncc @akihironitta",
        "Challenge_closed_time":1667802669000,
        "Challenge_created_time":1662640213000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/14604",
        "Challenge_link_count":4,
        "Challenge_readability":17.8,
        "Challenge_reading_time":88.49,
        "Challenge_repo_contributor_count":447.0,
        "Challenge_repo_fork_count":2788.0,
        "Challenge_repo_issue_count":14589.0,
        "Challenge_repo_star_count":22027.0,
        "Challenge_repo_watch_count":231.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":1434.0155555556,
        "Challenge_title":"Optuna integration reports AttributeError",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":522,
        "Platform":"Github",
        "Solution_body":"Hey, @RegiusQuant. \r\n\r\nSide answer, you might be interested by Lightning HPO: https:\/\/github.com\/Lightning-AI\/lightning-hpo. This enables to run Optuna with PyTorch Lightning without friction and scalable in the cloud.\r\n\r\n Hey, @RegiusQuant - Thanks for the question. Can you please point me to the version of `optuna` that you are using?  For reference: https:\/\/github.com\/optuna\/optuna\/issues\/3978 @krshrimali Optuna version\uff1a3.0.0 I'm observing the same issue with Optuna 3.0.2 @hrzn Hi, I'm from the Optuna-dev team. Optuna's pytorch-lightning (PL) integration module doesn't support PL>=1.6 because it broke backwards-compatibility as investigated in https:\/\/github.com\/optuna\/optuna\/issues\/3418. Unfortunately, Optuna team doesn't have time to fix the module soon to support recent PL; we would like to wait for a PR from optuna and PL users.\r\n\r\n@tchaton I believe you can close this issue because the issue comes from Optuna... With Optuna==3.0.2 with lightning==1.5.10, I got \r\n`ValueError: optuna.integration.PyTorchLightningPruningCallback supports only optuna.storages.RDBStorage in DDP.`\r\nAfter downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.  @mikiotada Again, the error does not relate to PL. As the error message said, Optuna's integration does not support DDP without RDBStorage. \r\n\r\n> After downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.\r\n\r\nIn my understanding, Optuna 2.x didn't officially support DDP; it does not work as you expected, I'm afraid even though there was no error. Closing this issue as there seems nothing we can address from our side. Please refer to https:\/\/github.com\/optuna\/optuna\/issues\/3418.",
        "Solution_link_count":4.0,
        "Solution_readability":8.9,
        "Solution_reading_time":21.93,
        "Solution_score_count":5.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":232.0,
        "Tool":"Optuna",
        "Challenge_contributor_issue_ratio":0.0306395229,
        "Challenge_watch_issue_ratio":0.0158338474
    },
    {
        "Challenge_adjusted_solved_time":4.5216666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### What happened + What you expected to happen\n\nNotebook is broken due to missing permission first, maybe more issues down the road. @Yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### Versions \/ Dependencies\n\nmaster\n\n### Reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:sigopt_example`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Challenge_closed_time":1659051271000,
        "Challenge_created_time":1659034993000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/27203",
        "Challenge_link_count":0,
        "Challenge_readability":12.8,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5216666667,
        "Challenge_title":"[AIR] Fix  \/\/doc\/source\/tune\/examples:sigopt_example",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":61,
        "Platform":"Github",
        "Solution_body":"Duplicate of https:\/\/github.com\/ray-project\/ray\/issues\/26567",
        "Solution_link_count":1.0,
        "Solution_readability":29.2,
        "Solution_reading_time":0.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### What happened + What you expected to happen\n\nI tried to run a ray tune job using the Sigopt suggester on a remote cluster. The sigopt suggester object was later found to be unserialisable however the stack trace gave no indication of this.\r\n\r\nThe stack trace looks like this\r\n\r\ndiscussion around this issue can be found here https:\/\/ray-distributed.slack.com\/archives\/CNECXMW22\/p1652417782100299\r\n\r\nThanks to Matthew Deng for finding the issue on this one!\n\n### Versions \/ Dependencies\n\nPython 3.8.12\r\nray==1.12.0\n\n### Reproduction script\n\n```\r\nimport ray\r\nimport numpy as np\r\nimport os\r\nos.environ['SIGOPT_KEY'] = APIKEYHERE\r\n\r\nfrom ray.tune.suggest.sigopt import SigOptSearch\r\nfrom ray import tune\r\nWORKING_DIR = os.getcwd()\r\n\r\n\r\n\r\ndef main():\r\n\r\n\tray.init(\r\n\t\taddress = \"ray:\/\/127.0.0.1:10001\",\r\n\t\t# address = \"auto\",\r\n\t\truntime_env = {\r\n\t\t\t\"working_dir\": WORKING_DIR,\r\n\t\t\t\"pip\": [\"sigopt==5.7.0\"]\r\n\t\t}\r\n\t)\r\n\t\r\n\tn_observations = 20\r\n\r\n\thyperparameter_space = [\r\n          {\r\n              'name': 'learning_rate',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'max': np.log(0.01),\r\n                  'min': np.log(0.0001)\r\n              },\r\n          },\r\n          {\r\n              'name': 'momentum',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'min': 0.85,\r\n                  'max': 0.99\r\n              },\r\n          },\r\n      ]\r\n\t\r\n\tsigopt_search = SigOptSearch(\r\n\t\t# OmegaConf.to_container(config.search_space),\r\n        hyperparameter_space,\r\n\t\tname=\"Tune distributed\",\r\n\t\tmax_concurrent=2, \r\n\t\tobservation_budget=n_observations,\r\n\t\tproject=\"sigopt-ray-integration\",\r\n\t\tmetric=[\"val_loss\"],\r\n\t\tmode=[\"min\"]\r\n\t\t# metric=[\"val_loss\", \"training_loss\"],\r\n\t\t# mode=[\"max\", \"min\"]\r\n\t)\r\n\r\n\ttune_config = {\r\n\t\t# \"config\": config\r\n\t}\r\n\tanalysis = tune.run(\r\n\t\ttrain_model,\r\n\t\tmetric=\"val_loss\",\r\n\t\tmode=\"min\",\r\n\t\tconfig=tune_config,\r\n\t\tnum_samples=n_observations,\r\n\t\tname=\"Tune distributed\",\r\n\t\tresources_per_trial={'gpu': 1},\r\n\t\tsearch_alg=sigopt_search,\r\n\t\t# scheduler=FIFOScheduler(),\r\n\t)\r\n\r\n\r\n\r\n\r\ndef train_model(config):\r\n    pass\r\n\r\nmain()\r\n\r\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1652740461000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/24864",
        "Challenge_link_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":23.77,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"[tune] SigOptSearch suggester is not serialisable",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":183,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":5.2522222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498330000,
        "Challenge_created_time":1603479422000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.2522222222,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":102,
        "Platform":"Github",
        "Solution_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Solution_link_count":0.0,
        "Solution_readability":3.4,
        "Solution_reading_time":2.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":76.3483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Challenge_closed_time":1658150380000,
        "Challenge_created_time":1657875526000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Challenge_link_count":0,
        "Challenge_readability":6.1,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17206.0,
        "Challenge_repo_issue_count":20680.0,
        "Challenge_repo_star_count":76088.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":76.3483333333,
        "Challenge_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":153,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0212282398,
        "Challenge_watch_issue_ratio":0.0415860735
    },
    {
        "Challenge_adjusted_solved_time":341.7419444444,
        "Challenge_answer_count":8,
        "Challenge_body":"Hi,\r\n\r\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get **explainable** predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\r\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\r\n\r\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\r\n\r\n\r\n#### Environment details\r\n\r\n  - Google Cloud Notebook\r\n  - Python version: 3.7.12\r\n  - pip version: 21.3.1\r\n  - `google-cloud-aiplatform` version: 1.15.0\r\n\r\n#### Reference\r\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\r\n\r\n#### explanation-metadata.json\r\n(_Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key._)\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"A\": {},\r\n        \"B\": {}\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\n#### Model upload with explanation parameters and metadata\r\n```\r\n! gcloud ai models upload \\\r\n  --region=$REGION \\\r\n  --display-name=$MODEL_NAME \\\r\n  --container-image-uri=$PRED_IMAGE_URI \\\r\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\r\n  --explanation-method=sampled-shapley \\\r\n  --explanation-path-count=10 \\\r\n  --explanation-metadata-file=explanation-metadata.json\r\n```\r\n\r\n#### Prediction\/Explanation Input\r\n```\r\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\r\n\r\n# Another example\r\ninstances_2 = [[1.1,20], [2.2,21]]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances_2)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances_2) # Returns error\r\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\r\n```\r\n#### Prediction Server (Flask)\r\n```python\r\n# Custom Flask server to serve online predictions\r\n# Input for prediction\r\nraw_input = request.get_json()\r\ninput = raw_input['instances']\r\ndf = pd.DataFrame(input, columns = ['A', 'B'])\r\n# Prediction from model (loaded from GCP bucket)\r\npredictions = model.predict(df).tolist() # [0, 1]\r\nresponse = jsonify({\"predictions\": predictions})\r\nreturn response\r\n```\r\n\r\n#### Stack trace of error (1)\r\n```\r\n---------------------------------------------------------------------------\r\n_InactiveRpcError                         Traceback (most recent call last)\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     49         try:\r\n---> 50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\r\n    945                                       wait_for_ready, compression)\r\n--> 946         return _end_unary_response_blocking(state, call, False, None)\r\n    947 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\r\n    848     else:\r\n--> 849         raise _InactiveRpcError(state)\r\n    850 \r\n\r\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\r\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\r\n>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\r\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\r\n   1563             parameters=parameters,\r\n   1564             deployed_model_id=deployed_model_id,\r\n-> 1565             timeout=timeout,\r\n   1566         )\r\n   1567 \r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\r\n    917             retry=retry,\r\n    918             timeout=timeout,\r\n--> 919             metadata=metadata,\r\n    920         )\r\n    921 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\r\n    152             kwargs[\"metadata\"] = metadata\r\n    153 \r\n--> 154         return wrapped_func(*args, **kwargs)\r\n    155 \r\n    156 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n---> 52             raise exceptions.from_grpc_error(exc) from exc\r\n     53 \r\n     54     return error_remapped_callable\r\n\r\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\r\n---------------------------------------------------------------------------\r\n```",
        "Challenge_closed_time":1659550833000,
        "Challenge_created_time":1658320562000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Challenge_link_count":1,
        "Challenge_readability":14.2,
        "Challenge_reading_time":79.22,
        "Challenge_repo_contributor_count":75.0,
        "Challenge_repo_fork_count":188.0,
        "Challenge_repo_issue_count":1846.0,
        "Challenge_repo_star_count":283.0,
        "Challenge_repo_watch_count":53.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":341.7419444444,
        "Challenge_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI (Prediction without explanation works fine)",
        "Challenge_topic":"Docker Configuration",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":578,
        "Platform":"Github",
        "Solution_body":"Hi @jaycee-li,\r\nAny update on this? Would really appreciate your inputs! Hi @pankajrsingla, sorry for the late reply!\r\n\r\nSince instance_2 prediction works for your model, looks like your model takes unkeyed input. Could you please try this metadata setting:\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"X\": {},\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\nThen update the model, endpoint, and try:\r\n```\r\ninstances = [[1.1,20], [2.2,21]]\r\nendpoint.explain(instances=instances)\r\n```\r\n\r\nPlease let me know if this works for you. Hi @jaycee-li,\r\nThank you so much for your response.\r\nI tried your suggestion, but I got the same error as before.\r\n`Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"`\r\n\r\nIf you see the code for my prediction server, it can take both unkeyed as well as keyed input (prediction works fine for both cases), since it converts the input to a dataframe. The output is definitely unkeyed. However, I am still confused as to what should be the contents of the explanation-metadata.json file.\r\n\r\nAlso, just to be sure - the same API (predict) in the flask server is supposed to work for both predictions and explanations, right? Or do I need to create a separate API for 'explain'?\r\n\r\nIf you have any other suggestions, I would be more than happy to try them out. \r\n(If that would help, I can also send you the full contents of the Jupyter notebook - all code one place - if you share your email id.)\r\n\r\nPlease let me know!\r\n\r\nThank you! It would be helpful if you can share the notebook to jayceeli@google.com\r\n\r\nThank you very much! Done!\r\nThanks! :) Hi @pankajrsingla ,\r\n\r\nI got `AttributeError: 'Blob' object has no attribute 'open'` for `with blob.open(\"wb\") as f:` in your TRAIN_IMAGE_URI. So I was stuck here and didn't reproduce the error you got. \r\n\r\nYou mixed CLI, gapic API, and SDK in your code. Since I'm not familiar with CLI tool, I'm not very sure what the problem is. Maybe it's due to your PRED_IMAGE_URI? I would suggest you to try a pre-built container(`us-docker.pkg.dev\/vertex-ai\/prediction\/sklearn-cpu.1-0:latest`) when uploading the model.\r\n\r\nI drafted a notebook that used SDK only to train, upload, deploy a same model as yours. And it can successfully make predictions and explanations. I've shared the notebook with you for your reference.\r\n\r\nPlease let me know if you still get the error. Thanks! Hi @pankajrsingla ,\r\n\r\nPlease check this [notebook](https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage6\/get_started_with_xai_and_custom_server.ipynb) (Specifically **Create the model server** and **Build a FastAPI HTTP server** sections) for how to use XAI with a custom container. Thanks a lot, @jaycee-li! This is exactly what I was looking for!\r\nI will give this a try for my model, and will update you once I have the results. This should work.\r\n\r\nThank you!",
        "Solution_link_count":1.0,
        "Solution_readability":5.5,
        "Solution_reading_time":35.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0406283857,
        "Challenge_watch_issue_ratio":0.0287107259
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"## Expected Behavior\r\nI expect the notebook here https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/matching_engine\/sdk_matching_engine_for_indexing.ipynb to work\r\n\r\n\r\n\r\n## Actual Behavior\r\n\r\n, but for some reason whenever I try to import the module `aiplatform` inside a cell of the notebook\r\n```\r\nfrom google.cloud import aiplatform\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/protobuf\/descriptor.py in __new__(cls, name, index, number, type, options, serialized_options, create_key)\r\n    753                 type=None,  # pylint: disable=redefined-builtin\r\n    754                 options=None, serialized_options=None, create_key=None):\r\n--> 755       _message.Message._CheckCalledFromGeneratedFile()\r\n    756       # There is no way we can build a complete EnumValueDescriptor with the\r\n    757       # given parameters (the name of the Enum is not known, for example).\r\n\r\nTypeError: Descriptors cannot not be created directly.\r\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\r\n 1. Downgrade the protobuf package to 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\r\n\r\nMore information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\r\n```\r\n\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1. Clone the sample notebook\r\n1. Import it into a vertex AI Workbench running the Python3 image\r\n1. Try to run through the steps and get stuck in installation issues\r\n\r\n## Specifications\r\n\r\n- Version:\r\n- Platform:",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669999872000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/1315",
        "Challenge_link_count":2,
        "Challenge_readability":12.4,
        "Challenge_reading_time":22.23,
        "Challenge_repo_contributor_count":84.0,
        "Challenge_repo_fork_count":365.0,
        "Challenge_repo_issue_count":1349.0,
        "Challenge_repo_star_count":544.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to import aiplatform module when running Vertex AI Matching Engine sample notebook",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":203,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0622683469,
        "Challenge_watch_issue_ratio":0.0244625649
    },
    {
        "Challenge_adjusted_solved_time":170.435,
        "Challenge_answer_count":2,
        "Challenge_body":"I successfully trained and deployed a Tensorflow Recommender model on Vertex AI, Tensorflow 2.8.\r\n\r\nEverything is online and to predict the output. In the notebook I do:\r\n\r\n    loaded = tf.saved_model.load(path)\r\n    scores, titles = loaded([\"doctor\"])\r\n\r\nThat returns:\r\n\r\n    Recommendations: [b'Nelly & Monsieur Arnaud (1995)'\r\n     b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']\r\n\r\nThat is, the payload (input for the neural network) must be `[\"doctor\"]`\r\n\r\nThen I generate the JSON for payload (the error is here):\r\n\r\n    !echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"input_1\"\\\"\" : {[\"\\\"\"doctor\"\\\"\"]}}]} > instances0.json\r\n\r\nAnd submit to the endpoint:\r\n\r\n    !curl -X POST  \\\r\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\r\n    -H \"Content-Type: application\/json\" \\\r\n    https:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\\r\n    -d @instances0.json > results.json\r\n\r\n... as seen here: https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd\r\n\r\nHowever, when I use this payload, I get error 400:\r\n\r\n    code: 400\r\n    message: \"Invalid JSON payload received. Expected an object key or }. s\" : [{\"input_1\" : {[\"doctor\"]}}]} ^\"\r\n    status: \"INVALID_ARGUMENT\"\r\n\r\nThis below don't work either:\r\n\r\n    !echo {\"inputs\": {\"input_1\": [\"doctor\"]}} > instances0.json\r\n\r\nEven with validated JSON Lint, it does not return the proper prediction.\r\n\r\nRunning:\r\n\r\n    !saved_model_cli show --dir \/home\/jupyter\/model --all\r\n\r\nI get:\r\n\r\n    MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n    \r\n    signature_def['__saved_model_init_op']:\r\n      The given SavedModel SignatureDef contains the following input(s):\r\n      The given SavedModel SignatureDef contains the following output(s):\r\n        outputs['__saved_model_init_op'] tensor_info:\r\n            dtype: DT_INVALID\r\n            shape: unknown_rank\r\n            name: NoOp\r\n      Method name is: \r\n    \r\n    signature_def['serving_default']:\r\n      The given SavedModel SignatureDef contains the following input(s):\r\n        inputs['input_1'] tensor_info:\r\n            dtype: DT_STRING\r\n            shape: (-1)\r\n            name: serving_default_input_1:0\r\n      The given SavedModel SignatureDef contains the following output(s):\r\n        outputs['output_1'] tensor_info:\r\n            dtype: DT_FLOAT\r\n            shape: (-1, 10)\r\n            name: StatefulPartitionedCall_1:0\r\n        outputs['output_2'] tensor_info:\r\n            dtype: DT_STRING\r\n            shape: (-1, 10)\r\n            name: StatefulPartitionedCall_1:1\r\n      Method name is: tensorflow\/serving\/predict\r\n\r\n\r\n    Concrete Functions:\r\n      Function Name: '__call__'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #2\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #3\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #4\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n    \r\n      Function Name: '_default_save_signature'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n    \r\n      Function Name: 'call_and_return_all_conditional_losses'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #2\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #3\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #4\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n\r\nThe point is: I'm passing an array and I'm not sure if it must be in b64 format.\r\n\r\nThis Python code works, but returns a different result than expected:\r\n\r\n    import tensorflow as tf\r\n    import base64\r\n    from google.protobuf import json_format\r\n    from google.protobuf.struct_pb2 import Value\r\n    import numpy as np\r\n    from google.cloud import aiplatform\r\n    import os\r\n    vertex_model = tf.saved_model.load(\"gs:\/\/bucket\/model\")\r\n    \r\n    serving_input = list(\r\n        vertex_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\r\n    )[0]\r\n    \r\n    print(\"Serving input :\", serving_input)\r\n    \r\n    aip_endpoint_name = (\r\n        f\"projects\/my-project\/locations\/us-west1\/endpoints\/12345567\"\r\n    )\r\n    endpoint = aiplatform.Endpoint(aip_endpoint_name)\r\n    \r\n    def encode_input(input):\r\n        return base64.b64encode(np.array(input)).decode(\"utf-8\")\r\n    \r\n    instances_list = [{serving_input: {\"b64\": encode_input(np.array([\"doctor\"]))}}]\r\n    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\r\n    \r\n    results = endpoint.predict(instances=instances)\r\n    print(results.predictions[0][\"output_2\"])\r\n\r\n\r\n    ['8 1\/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de V\u00e9ronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']\r\n\r\nAny ideas on how to fix \/ encode the payload ?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1659564318000,
        "Challenge_created_time":1658950752000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/749",
        "Challenge_link_count":2,
        "Challenge_readability":14.4,
        "Challenge_reading_time":72.41,
        "Challenge_repo_contributor_count":84.0,
        "Challenge_repo_fork_count":365.0,
        "Challenge_repo_issue_count":1349.0,
        "Challenge_repo_star_count":544.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":170.435,
        "Challenge_title":"Vertex AI - Endpoint Call with JSON - Invalid JSON payload received",
        "Challenge_topic":"Lambda Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":591,
        "Platform":"Github",
        "Solution_body":"This works perfectly:\r\n\r\n    def encode_64(input):\r\n        message = input\r\n        message_bytes = message.encode('ascii')\r\n        base64_bytes = base64.b64encode(message_bytes)\r\n        base64_message = base64_bytes.decode('ascii')\r\n        return base64_message\r\n    \r\n    \r\n    instances_list = [{serving_input: {\"b64\": encode_64(\"doctor\")}}]\r\n    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\r\n    \r\n    results = endpoint.predict(instances=instances)\r\n    print(results.predictions[0][\"output_2\"][:3])\r\n\r\n    ['Nelly & Monsieur Arnaud (1995)', 'Three Lives and Only One Death (1996)', 'Critical Care (1997)']\r\n\r\nBut I still have doubts regarding the CURL method. You have extra curly braces around your input data. Try something like:\r\n\r\n```\r\n!echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"input_1\"\\\"\" : [\"\\\"\"doctor\"\\\"\"]}]} > instances0.json\r\n```\r\n\r\nAssuming you are using a [GCP prebuilt container](https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/pre-built-containers), this is endpoint launches the equivalent of a [TF Serving container](https:\/\/www.tensorflow.org\/tfx\/guide\/serving) and you can test your model locally with a container before pushing to an endpoint to make sure everything is working. Take a look at [this notebook](https:\/\/github.com\/northam-stp-team\/vertexai-apigee\/blob\/master\/notebooks\/Seq2SeqTranslation.ipynb).",
        "Solution_link_count":3.0,
        "Solution_readability":12.6,
        "Solution_reading_time":16.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":115.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0622683469,
        "Challenge_watch_issue_ratio":0.0244625649
    },
    {
        "Challenge_adjusted_solved_time":52.535,
        "Challenge_answer_count":6,
        "Challenge_body":"## Expected Behavior\r\nCode example  from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended.\r\n\r\n## Actual Behavior\r\nCode example below from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work\r\n\r\n```\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\n    from google.cloud import aiplatform\r\n    aiplatform.init(project=project, location=region)\r\n\r\n    # THIS IS THE METHOD THAT DOESN'T APPEAR TO WORK\r\n    model_upload_op = gcc_aip.ModelUploadOp(\r\n            project=project,\r\n            location=region,\r\n            display_name=model_display_name,\r\n            artifact_uri=model.uri,\r\n            serving_container_image_uri=serving_container_image_uri\r\n            )\r\n```\r\nOn the other hand, the method below worked:\r\n```\r\n # THIS METHOD DOES WORK\r\n    # aiplatform.Model.upload(\r\n    #     display_name=model_display_name,\r\n    #     artifact_uri=model.uri,\r\n    #     serving_container_image_uri=serving_container_image_uri,\r\n    # )\r\n```\r\n\r\nI'm currently using Vertex AI Pipelines to train a model and upload to Vertex AI. Currently in the pipeline, I'm attempting to use the ModelUploadOp class to upload a custom model to Vertex AI models. The logs show the job is succeeding, but the model never actually gets uploaded.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n## Specifications\r\n\r\nVersion: \r\n- Pipeline SDK (Kubeflow Pipelines\/TFX) Version: kfp\r\n- Pipelines Version: kfp==1.8.11\r\n- Platform: Google Cloud Vertex AI \r\n\r\n[1]: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Challenge_closed_time":1646371495000,
        "Challenge_created_time":1646182369000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/349",
        "Challenge_link_count":1,
        "Challenge_readability":15.2,
        "Challenge_reading_time":22.2,
        "Challenge_repo_contributor_count":84.0,
        "Challenge_repo_fork_count":365.0,
        "Challenge_repo_issue_count":1349.0,
        "Challenge_repo_star_count":544.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":52.535,
        "Challenge_title":"ModelUploadOp from \"Vertex AI Pipelines: model upload using google-cloud-pipeline-components\"  does not work",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":174,
        "Platform":"Github",
        "Solution_body":"There was a recent breaking change. Will update notebook accordingly. Notebook has been updated, tested and merged. @andrewferlitsch can we close this issue since the notebook is merged ? yes, I will close it. I thought I had. Hi all,\r\nI saw this thread and the updated notebook - thanks for fixing it. \r\n\r\nI can't help but think that using `artifact_uri=WORKING_DIR` in the Importer Node seems misaligned with Kubeflow's focus on Artifacts and Artifact management. Is it possible to set `artifact_uri` to the Artifact location without hardcoding `WORKING_DIR`?\r\n\r\n```\r\nimport_unmanaged_model_task = importer_node.importer(\r\n        artifact_uri=WORKING_DIR,        <--- Can we set this without hardcoding WORKING_DIR?\r\n        artifact_class=artifact_types.UnmanagedContainerModel,\r\n        metadata={\r\n            \"containerSpec\": {\r\n                \"imageUri\": \"us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest\",\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nTo make things simple, let's say that instead of putting the training code in CustomTrainingJobOp, you define a custom function (below). In this case wouldn't it be more Kubeflow-ish to replace save the file to `Output[Model].path` instead of hard coding `WORKING_DIR` , like the following? \r\n\r\n```\r\ndef train_model(dataset: Input[Dataset],  model: Output[Model]):\r\n      ...\r\n\r\n       # then save model\r\n       # model.save(WORKING_DIR)   <---- This is the way outlined in the notebook\r\n       model.save(model.path)         <--- This seems more aligned with KFP than above\r\n```\r\n\r\nThen, when you wanted to upload the model, you would again replace `WORKING_DIR` with the location of the Artifact set by Kubeflow.\r\n\r\n```\r\n@kfp.dsl.pipeline(...)\r\ndef pipeline(...):\r\n        ...\r\n\r\n        # train model\r\n        train_model_op = train_model(...)\r\n\r\n        import_unmanaged_model_task = importer_node.importer(\r\n                artifact_uri=train_model_op.outputs[\"model\"].uri,         <---- USING ARTIFACT LOCATION\r\n                artifact_class=artifact_types.UnmanagedContainerModel,\r\n                metadata={\r\n                     \"containerSpec\": {\r\n                      \"imageUri\": serving_container_image_uri,\r\n                 },\r\n               },\r\n           )\r\n```\r\nBut unfortunately you can't actually do this because you get an error: \"AttributeError: 'PipelinParam' object has no attribute uri'\". To avoid this error, you could also nest the Importer Node into a custom Component that has Input[Model] as one of the parameters. Then you could set `artifact_uri=model.uri`. \r\n\r\n```\r\n@component(...)\r\ndef custom_importer(trained_model: Input[Model], vertex_model: Output[Model]):\r\n     import_unmanaged_model_task = importer_node.importer(\r\n                artifact_uri=trained_model.uri,         <---- USING ARTIFACT LOCATION\r\n                artifact_class=artifact_types.UnmanagedContainerModel,\r\n                metadata={\r\n                     \"containerSpec\": {\r\n                      \"imageUri\": serving_container_image_uri,\r\n                 },\r\n               },\r\n           )\r\n```\r\nUnfortunately, you can't do this as you get \"TypeError: There are no registered serializers for type \"google.UnmanagedContainerModel\".\" @natetsang If you want an easy way to upload models to Vertex Model Registry, you can use my components: https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/KFPv2_hell\/components\/google-cloud\/Vertex_AI\/Models \r\nExample usage: https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/05b2f255f8ccd7d8588f8143a76536bf83c2c7c7\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI\/pipeline.py#L51\r\n\r\n```Python\r\nupload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https:\/\/raw.githubusercontent.com\/Ark-kun\/pipeline_components\/719783ef44c04348ea23e247a93021d91cfe602d\/components\/google-cloud\/Vertex_AI\/Models\/Upload_Tensorflow_model\/component.yaml\")\r\n\r\n...\r\n    vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\r\n        model=model,\r\n    ).outputs[\"model_name\"]\r\n```\r\nWhere `model` is a `TensorflowSavedModel` artifact that was produced by `model.save(model_path)`.\r\n\r\nPlease open an issue in my repo if you have any issues or feature requests for the components I've linked.",
        "Solution_link_count":3.0,
        "Solution_readability":15.8,
        "Solution_reading_time":49.27,
        "Solution_score_count":5.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":353.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0622683469,
        "Challenge_watch_issue_ratio":0.0244625649
    },
    {
        "Challenge_adjusted_solved_time":234.8477777778,
        "Challenge_answer_count":7,
        "Challenge_body":"### Contact Details [Optional]\n\nfrancogbocci@gmail.com\n\n### System Information\n\nZenML version: 0.20.5\r\nInstall path: \/Users\/f.bocci\/Library\/Caches\/pypoetry\/virtualenvs\/banana-bMSm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\nPython version: 3.9.6\r\nPlatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nEnvironment: native\r\nIntegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### What happened?\n\nTrying to follow the [guide to run a pipeline using Vertex AI](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because ZenML does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nStack Components:\r\n      alerter                 Commands to interact with alerters.\r\n      annotator               Commands to interact with annotators.\r\n      artifact-store          Commands to interact with artifact stores.\r\n      container-registry      Commands to interact with container registries.\r\n      data-validator          Commands to interact with data validators.\r\n      experiment-tracker      Commands to interact with experiment trackers.\r\n      feature-store           Commands to interact with feature stores.\r\n      model-deployer          Commands to interact with model deployers.\r\n      orchestrator            Commands to interact with orchestrators.\r\n      secrets-manager         Commands to interact with secrets managers.\r\n      step-operator           Commands to interact with step operators.\r\n$ zenml metadata-store\r\nError: No such command 'metadata-store'.\r\n```\n\n### Reproduction steps\n\n1. zenml metadata-store\r\n\r\nIf I don't add it and run the Vertex AI pipeline, it fails.\r\n\n\n### Relevant log output\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1667472145000,
        "Challenge_created_time":1666626693000,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/1001",
        "Challenge_link_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.77,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2571.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":234.8477777778,
        "Challenge_title":"[BUG]: Vertex AI blogpost is outdated after 0.20.0 release",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":186,
        "Platform":"Github",
        "Solution_body":"@francobocciDH Thanks for reporting the issue. We have recently undergone a [big architectural shift](https:\/\/blog.zenml.io\/zenml-revamped\/) and therefore a lot of the blog is a bit outdated! In particular, the metadata store is no longer a required stack component.\r\n\r\nIn order to make the vertex orchestrator work, I would suggest either taking a look at the [updated docs page](https:\/\/docs.zenml.io\/component-gallery\/orchestrators\/gcloud-vertexai), or taking a look at the [migration guide](https:\/\/docs.zenml.io\/guidelines\/migration-zero-twenty) that will help you update that blog's code to  the 0.20.5 world. Hey! Thanks for the quick reply. I followed the updated docs page. I checked the post as well to see if there is something different, but following the docs I'm still getting the error\r\n```\r\nMaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```\r\n\r\nFor what I saw following the traceback, it's something related to:\r\n`\/usr\/local\/lib\/python3.9\/site-packages\/zenml\/zen_stores\/base_zen_store.py:104`\r\nbut I haven't solved it yet.\r\n\r\nCould you follow the steps on the guide and make it work? I downloaded the image, got into the container and launch the entrypoint being used in Vertex AI:\r\n`python -m zenml.entrypoints.entrypoint --entrypoint_config_source zenml.integrations.gcp.orchestrators.vertex_entrypoint_configuration.VertexEntrypointConfiguration@zenml_0.20.5 --step_name importer --vertex_job_id test1234`\r\n\r\nAnd I got the same error. After that, I ran the ZenML Server (`zenml up`), and I got a different error (so apparently, something's missing?)\r\n\r\nThe error I'm getting now comes from `tfx` package and it's:\r\n```\r\nThe filesystem scheme 'gs:\/\/' is not available for use. For expanded filesystem scheme support, install the `tensorflow` package to enable additional filesystem plugins\r\n```\r\n\r\n I made it work locally. I had to:\r\n1) Register the `artifact-store` using GCS\r\n2) Set it as the artifact-store in the \"default\" stack\r\n3) Start zenml server\r\n\r\nShould this be done in some specific way by the user? @francobocciDH I think the main problem you are suffering from is that you have not deployed ZenML on Google before doing all this. Its our fault as I see that the Vertex orchestrator guide does not make this clear at all (only if you read the docs from the top, it does).\r\n\r\nPlease try [deploying ZenML](https:\/\/docs.zenml.io\/getting-started\/deploying-zenml) to google first. The easiest way to do it is to do:\r\n\r\n```\r\nzenml deploy\r\n```\r\n\r\nAfter you have done this, you can connect to the remote ZenML deployemnt, and re-register your stack as described in the Vertex AI docs, and then run your pipeline. It should work then! @francobocciDH Did this work out? Hey @htahir1 , yes, I deployed it and it worked. It could be clearer in the Vertex AI section of the docs, but it is clearly mentioned in other places of the documentation, so it's my fault for missing this. We can close this from my side. Let me know if there is anything I can help with \ud83d\udc4d ",
        "Solution_link_count":4.0,
        "Solution_readability":10.0,
        "Solution_reading_time":43.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":34.0,
        "Solution_word_count":480.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0482758621,
        "Challenge_watch_issue_ratio":0.0318965517
    },
    {
        "Challenge_adjusted_solved_time":5667.8238888889,
        "Challenge_answer_count":2,
        "Challenge_body":"Vertex AI Java documentation is incorrect.\r\n\r\nI used this code, [copied straight from the Javadocs](https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java), to delete a VertexAI Training Pipeline\r\n\r\n```\r\ntry (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\r\n  TrainingPipelineName name =\r\n      TrainingPipelineName.of(\"[PROJECT]\", \"[LOCATION]\", \"[TRAINING_PIPELINE]\");\r\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\r\n}\r\n```\r\n\r\nI get this error.\r\n\r\n```\r\nError in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \r\njava.util.concurrent.ExecutionException: \r\ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\r\nUNIMPLEMENTED: HTTP status code 404\r\n```\r\n\r\nAs discussed [here](https:\/\/stackoverflow.com\/questions\/69219230), you have to specify an endpoint -- and then it works.\r\n\r\nI suggest fixing   the documentation to produce working code.",
        "Challenge_closed_time":1652391878000,
        "Challenge_created_time":1631987712000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/java-aiplatform\/issues\/668",
        "Challenge_link_count":2,
        "Challenge_readability":22.4,
        "Challenge_reading_time":14.45,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":1138.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":41.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":5667.8238888889,
        "Challenge_title":"Bug in Vertex AI docs",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":"These are autogenerated samples, intended to be a guide in the right direction. They are a WIP. These generated samples are updated now to include comment that suggests they may require additional configuration to work (see #892) to reduce confusion. This isn't a Vertex AI bug (although you could add additional comments to the protos that explain this) but rather a feature request for gapic-generator, so I'll move this there. Closing - issue moved to generator https:\/\/github.com\/googleapis\/gapic-generator-java\/issues\/991",
        "Solution_link_count":1.0,
        "Solution_readability":11.6,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":76.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0175746924,
        "Challenge_watch_issue_ratio":0.0360281195
    },
    {
        "Challenge_adjusted_solved_time":0.3547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Challenge_closed_time":1664935217000,
        "Challenge_created_time":1664933940000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Challenge_link_count":1,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":558.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":42.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3547222222,
        "Challenge_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Challenge_topic":"CloudWatch Monitoring",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"> \r\n\r\nWhen I upgrade the nodejs to v16.17.1 and add a call_option\r\n`\r\n      const call_options = {\r\n        timeout: 200000 \/\/ millis\r\n      }\r\n`\r\nproblem solved.\r\n",
        "Solution_link_count":0.0,
        "Solution_readability":6.8,
        "Solution_reading_time":1.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0358422939,
        "Challenge_watch_issue_ratio":0.0752688172
    },
    {
        "Challenge_adjusted_solved_time":453.4755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nScikit Learn model in [`kubeflow_pipelines\/pipelines` directory](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/master\/notebooks\/kubeflow_pipelines\/pipelines\/solutions\/trainer_image\/train.py#L46) doesn't work in Vertex AI prediction environment, since it assumes the input as Pandas Dataframe and cannot handle JSON from Web API.\r\n\r\nAfter deploying the model following the labs, this issue can be reproduced with this code snippet.\r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [{'Elevation': [2841.0]},\r\n {'Aspect': [45, 0]},\r\n {'Slope': [0, 0]},\r\n {'Horizontal_Distance_To_Hydrology': [644.0]},\r\n {'Vertical_Distance_To_Hydrology': [282.0]},\r\n {'Horizontal_Distance_To_Roadways': [1376.0]},\r\n {'Hillshade_9am': [218.0]},\r\n {'Hillshade_Noon': [237.0]},\r\n {'Hillshade_3pm': [156.0]},\r\n {'Horizontal_Distance_To_Fire_Points': [1003.0]},\r\n {'Wilderness_Area': ['Commanche']},\r\n {'Soil_Type': ['C4758']}]\r\n\r\nendpoint.predict([instance])\r\n```\r\n\r\nreturns:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/6895245\/156965179-92e4e873-8f60-411c-86b7-df0685509e4c.png)\r\n\r\n## Approach\r\nRewrite feature definition part of `train.py` from:\r\nhttps:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/e87f3514dda440fb381a78f563bda177aa38ad80\/notebooks\/kubeflow_pipelines\/cicd\/solutions\/trainer_image_vertex\/train.py#L43-L63\r\n\r\nto:\r\n```python\r\n    numeric_feature_indexes = slice(0, 10)\r\n    categorical_feature_indexes = slice(10, 12)\r\n\r\n    preprocessor = ColumnTransformer(\r\n    transformers=[\r\n        ('num', StandardScaler(), numeric_feature_indexes),\r\n        ('cat', OneHotEncoder(), categorical_feature_indexes) \r\n    ])\r\n```\r\n\r\nAnd it should run with this \r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [\r\n    2841.0,\r\n    45.0,\r\n    0.0,\r\n    644.0,\r\n    282.0,\r\n    1376.0,\r\n    218.0,\r\n    237.0,\r\n    156.0,\r\n    1003.0,\r\n    \"Commanche\",\r\n    \"C4758\",\r\n]\r\nendpoint.predict([instance])\r\n```\r\n\r\nOutput:\r\n```\r\nPrediction(predictions=[1.0], deployed_model_id='4516996077043318784', explanations=None)\r\n```\r\n\r\n## Target Files\r\n[These 8 files ](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/search?q=numeric_features+%3D+%5B+++++++++%22Elevation%22%2C) should be update.",
        "Challenge_closed_time":1648259081000,
        "Challenge_created_time":1646626569000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/issues\/171",
        "Challenge_link_count":4,
        "Challenge_readability":17.8,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":220.0,
        "Challenge_repo_issue_count":286.0,
        "Challenge_repo_star_count":41.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":453.4755555556,
        "Challenge_title":"[Bug] scikit learn model feature definition doesn't work on Vertex AI Prediction.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.048951049,
        "Challenge_watch_issue_ratio":0.0384615385
    },
    {
        "Challenge_adjusted_solved_time":2751.6177777778,
        "Challenge_answer_count":2,
        "Challenge_body":"Synced astral-sweep-1: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/peg6pn8y\r\nRun peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: ERROR Run peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: Agent Starting Run: e8d1m877 with config:\r\nwandb: \tlayer_0-6: 2.581652533230976e-05\r\nwandb: \tlayer_12-18: 3.584294374584665e-05\r\nwandb: \tlayer_18-24: 4.488348372658677e-05\r\nwandb: \tlayer_6-12: 1.0161197251306803e-05\r\nwandb: \tnum_train_epochs: 40\r\nwandb: \tparams_classifier.dense.bias: 0.0005874506018709628\r\nwandb: \tparams_classifier.dense.weight: 0.0003389591868569285\r\nwandb: \tparams_classifier.out_proj.bias: 0.0003078179192499977\r\nwandb: \tparams_classifier.out_proj.weight: 0.0006868779346654171\r\nTracking run with wandb version 0.10.19\r\nSyncing run peach-sweep-2 to Weights & Biases (Documentation).\r\nProject page: https:\/\/wandb.ai\/sakrah\/humorize\r\nSweep page: https:\/\/wandb.ai\/sakrah\/humorize\/sweeps\/4sl6uygs\r\nRun page: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/e8d1m877\r\nRun data is saved locally in \/content\/wandb\/run-20210215_055312-e8d1m877",
        "Challenge_closed_time":1623275505000,
        "Challenge_created_time":1613369681000,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/993",
        "Challenge_link_count":4,
        "Challenge_readability":9.7,
        "Challenge_reading_time":16.88,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2751.6177777778,
        "Challenge_title":"Getting Errors with wandb sweeps ",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"I got this error when I tried to run wandb sweeps for a regressionn classifcation. It complained when I included the required num_labels. After removing it, the error is what I get. Are there some additional settings required besides setting regression=True. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n",
        "Solution_link_count":0.0,
        "Solution_readability":7.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0621468927,
        "Challenge_watch_issue_ratio":0.040960452
    },
    {
        "Challenge_adjusted_solved_time":1621.4025,
        "Challenge_answer_count":6,
        "Challenge_body":"When training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and wandb enabled:\r\n```\r\nFile \"train.py\", line 101, in <module>\r\n    rc=sklearn.metrics.recall_score)\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in \r\ntrain_model\r\n    **kwargs,\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train\r\n    scaled_loss.backward()\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 256, in <lambda>\r\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 254, in _callback\r\n    self.log_tensor_stats(grad.data, name)\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 165, in log_tensor_stats\r\n    flat = tensor.view(-1)\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\n",
        "Challenge_closed_time":1591178971000,
        "Challenge_created_time":1585341922000,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/287",
        "Challenge_link_count":0,
        "Challenge_readability":16.6,
        "Challenge_reading_time":18.04,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1621.4025,
        "Challenge_title":"wandb RuntimeError",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":104,
        "Platform":"Github",
        "Solution_body":"There does seem to be an issue with XLNet and ALBERT when using wandb. I haven't found the exact cause yet. I'll look into it again when I can. Thank you. Maybe it should be reported to wandb because it is from its `wandb.watch`?\r\nI temporarily fixed this with `wandb.watch(model, log=None)`. Does it still log the metrics when `log` is set to `None`? Yes, but without gradients. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n Was able to fix it for me with `pip install --upgrade wandb`",
        "Solution_link_count":0.0,
        "Solution_readability":4.8,
        "Solution_reading_time":7.42,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":108.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0621468927,
        "Challenge_watch_issue_ratio":0.040960452
    },
    {
        "Challenge_adjusted_solved_time":829.3705555556,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nAfter running a model evaluation suite and exprorint to wandb using \"to_wandb\" function, the confusion matrix appears in the w&b page without the values\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n\r\n**Expected behavior**\r\nThe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: linux\r\n - Python Version:3.7.1\r\n - Deepchecks Version:0.7.2\r\n\r\n",
        "Challenge_closed_time":1657703576000,
        "Challenge_created_time":1654717842000,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1592",
        "Challenge_link_count":1,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.52,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":829.3705555556,
        "Challenge_title":"[BUG] Weird behavior with \"to_wandb\" and confusion matrix",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"Hey @DL1992,\r\n\r\nFor me the export works fine, can you provide us with some more info about what you did?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/9868530\/173320868-74292589-5da2-4a15-9583-0855f592a602.png)\r\n hmmm, literally just result = suite.run follow by result.to_wandb.\r\nmy wandb version is 0.12.9\r\n what is the wandb and plotly version on the wandb server? This issue is stale and we couldn't reproduce it.\r\n@DL1992 feel free to reach out to us if this problem persists and we will try to help personally. Closing for now",
        "Solution_link_count":1.0,
        "Solution_readability":5.8,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0161216029,
        "Challenge_watch_issue_ratio":0.005988024
    },
    {
        "Challenge_adjusted_solved_time":72.2088888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\n to_wandb not sectioning by train\/test and overrides runs by checks\r\n\r\n**To Reproduce**\r\nrun a suite with train\/test checks and duplicate checks in suite\r\n\r\n**Expected behavior**\r\nsections for each dataset and being able to run a suite with a couple of checks\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n\r\n",
        "Challenge_closed_time":1649580744000,
        "Challenge_created_time":1649320792000,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1210",
        "Challenge_link_count":0,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":72.2088888889,
        "Challenge_title":"[BUG] to_wandb not sectioning by train\/test and overrides runs by checks",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":64,
        "Platform":"Github",
        "Solution_body":"Initial fix -  from 'name' to 'header', already applied in my local deepchecks environment (screenshots after that fix)\r\n\r\nSee example (note both DataDuplicates and CalibrationScore behavior):\r\n\r\nCode that ran:\r\n`custom_suite = Suite('Custom Evaluation',CalibrationScore(), CalibrationScore(),\r\n                     DataDuplicates(), DataDuplicates(columns=['Total Value']))\r\nsuite_res = custom_suite.run(train_ds, test_ds, rf_clf)\r\nsuite_res.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"my_run\")`\r\n\r\n### Suite Result\r\n#### Calibration Metric\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162167715-0db6398d-4822-4e35-a886-741753982ab5.png)\r\n\r\n#### Data Duplicates - \r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162167770-5e0450ce-a2ff-493f-8495-8779379d7a86.png)\r\n\r\n### W&B Logging - Suite Result\r\n#### Data Duplicates - appears 3 times (??)\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162168073-f79f6f3b-33fc-4453-8aff-4ae47652e979.png)\r\n\r\n\r\n#### Calibration Metric\r\nOnly one result (for each, after the above fix applied):\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162169354-b810fb42-2422-4ab5-8e5e-1bd377609fa0.png)\r\n\r\n\r\n\r\n",
        "Solution_link_count":4.0,
        "Solution_readability":21.0,
        "Solution_reading_time":15.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":73.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0161216029,
        "Challenge_watch_issue_ratio":0.005988024
    },
    {
        "Challenge_adjusted_solved_time":1.9119444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Challenge_closed_time":1583456108000,
        "Challenge_created_time":1583449225000,
        "Challenge_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Challenge_link_count":0,
        "Challenge_readability":11.2,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":88.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.9119444444,
        "Challenge_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":25,
        "Platform":"Github",
        "Solution_body":"@koustuvsinha Thanks for bringing this up. Could you try the new version?\r\n\r\nWhen constructing the logbook, pass an additional parameter:\r\n\r\n```\r\nfrom ml_logger import logbook as ml_logbook\r\nlogbook_config = ml_logbook.make_config(\r\n    logger_file_path = <path to write logs>,\r\n    wandb_config = <wandb config or None>,\r\n    wandb_prefix_key = \"mode\",\r\n)\r\n```",
        "Solution_link_count":0.0,
        "Solution_readability":10.7,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0113636364,
        "Challenge_watch_issue_ratio":0.0227272727
    },
    {
        "Challenge_adjusted_solved_time":20.8108333333,
        "Challenge_answer_count":0,
        "Challenge_body":"When using wandb, it shows step as X and not episode.\r\n\r\nHence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Challenge_closed_time":1605698611000,
        "Challenge_created_time":1605623692000,
        "Challenge_link":"https:\/\/github.com\/MathisFederico\/LearnRL\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_readability":7.1,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":137.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":20.8108333333,
        "Challenge_title":"Add episode to wandb",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":29,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0072992701,
        "Challenge_watch_issue_ratio":0.0145985401
    },
    {
        "Challenge_adjusted_solved_time":305.2888888889,
        "Challenge_answer_count":9,
        "Challenge_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Challenge_closed_time":1644017877000,
        "Challenge_created_time":1642918837000,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Challenge_link_count":0,
        "Challenge_readability":3.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":305.2888888889,
        "Challenge_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"This is expected. If, for example, you are checkpointing every 50 steps and your training run crashes after step 212, the last checkpoint will have been at step 200. So when you resume training, you start again from step 201, and W&B will warn you about logging duplicate steps until you get to step 213.  Why do we even try to log those earlier steps again? Wandb has an option for resuming runs. Because the W&B callback that was restored from the checkpoint at step 200 does not know that we actually got to step 212 before crashing.\r\n\r\nAnd we are using the `resume` option. Although after just rereading their docs just now, I think we should set `resume` to \"allow\" instead of \"auto\". https:\/\/github.com\/allenai\/tango\/pull\/155\r\n\r\nFrom their [docs](https:\/\/docs.wandb.ai\/ref\/python\/init):\r\n\r\n> \"auto\" (or True): if the preivous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. \r\n\r\n\"allow\" seems a little more robust for our use case, because maybe W&B won't always know when a run crashed (resulting in the \"auto\" option not working correctly). I'm assuming that what wandb wants is to have `resume=auto`, and then the next step we input into wandb is 201. But I think what happens now is that we resume with step 201 correctly, but we tell wandb that it's step 1 (because it's the first step we're actually running). > but we tell wandb that it's step 1\r\n\r\nNo, we tell W&B that it's step 201. W&B complains for the next 12 steps until we get to step 213. Ah, interesting. The documentation also says that new values will overwrite the old ones (which would be the right behavior), but the warning message clearly says it's dropping the new information. We could probably suppress those warnings though Is there a way we can make it actually overwrite the values? As it is, the values in the gap will be wrong (or at least might be wrong, if there is any non-determinism). > Is there a way we can make it actually overwrite the values?\r\n\r\nI don't think so \ud83d\ude15",
        "Solution_link_count":2.0,
        "Solution_readability":7.3,
        "Solution_reading_time":26.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":376.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0304878049,
        "Challenge_watch_issue_ratio":0.012195122
    },
    {
        "Challenge_adjusted_solved_time":2728.7016666667,
        "Challenge_answer_count":1,
        "Challenge_body":"",
        "Challenge_closed_time":1652740320000,
        "Challenge_created_time":1642916994000,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/151",
        "Challenge_link_count":0,
        "Challenge_readability":2.9,
        "Challenge_reading_time":1.03,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":2728.7016666667,
        "Challenge_title":"WandB callback changes the train step's unique ID, but does not change the results",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":14,
        "Platform":"Github",
        "Solution_body":"Actually, callbacks can change the result. So we'll close this.",
        "Solution_link_count":0.0,
        "Solution_readability":4.1,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0304878049,
        "Challenge_watch_issue_ratio":0.012195122
    },
    {
        "Challenge_adjusted_solved_time":123.815,
        "Challenge_answer_count":2,
        "Challenge_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Challenge_closed_time":1649731891000,
        "Challenge_created_time":1649286157000,
        "Challenge_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Challenge_link_count":1,
        "Challenge_readability":14.5,
        "Challenge_reading_time":51.74,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":41.0,
        "Challenge_repo_star_count":24.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":123.815,
        "Challenge_title":"Resume error in WandB.",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":326,
        "Platform":"Github",
        "Solution_body":"I will look into this soon. Crazily busy at the moment. This is not a bug, this happended because WandbCallbacks were used in the wrong way\r\n\r\nIn `pipeline.yaml`\r\n```python\r\n\"name\": \"WandbCallbacks\",\r\n\"args\": {\r\n    \"username\": \"lannguyen\",\r\n    \"project_name\": \"theseus_classification\",\r\n    \"resume\": true # <----- you didnt have to specify this\r\n}\r\n```\r\n\r\nThe repo havent been fully-well documented therefore it will be confusing sometimes.",
        "Solution_link_count":0.0,
        "Solution_readability":9.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0731707317,
        "Challenge_watch_issue_ratio":0.0731707317
    },
    {
        "Challenge_adjusted_solved_time":24.8108333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## What\r\n\r\nA clear and concise description of what the bug is.\r\n\r\n## How to reproduce\r\n\r\nReproduce by starting a non-dry run via a notebook\r\n\r\n1. Start the run\r\n2. Look at files on the wandb interface. There are no checkpoints\r\n\r\n## Expected\r\n\r\nCheckpoints should be uploaded to wandb whenever there is a better one available during training.\r\n\r\n## Additional context\r\n\r\nI thought I fixed wandb, but it seems that I don't understand the symlinking model of wandb. Apparently you need to have checkpoints under the project root? But this would mean that you can't run multiple experiements at the same time. ",
        "Challenge_closed_time":1624957138000,
        "Challenge_created_time":1624867819000,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/9",
        "Challenge_link_count":0,
        "Challenge_readability":5.5,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":24.8108333333,
        "Challenge_title":"Fix writing of checkpoints to wandb",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":104,
        "Platform":"Github",
        "Solution_body":"Fixed. See https:\/\/github.com\/feldberlin\/wavenet\/commit\/1125dcc5ce5004386160f3dfe0e1d1dc1e5aed98.",
        "Solution_link_count":1.0,
        "Solution_readability":44.6,
        "Solution_reading_time":1.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0285714286,
        "Challenge_watch_issue_ratio":0.0857142857
    },
    {
        "Challenge_adjusted_solved_time":52.5838888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Challenge_closed_time":1624287268000,
        "Challenge_created_time":1624097966000,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":52.5838888889,
        "Challenge_title":"Loading configs from wandb yields incorrect parameters",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0285714286,
        "Challenge_watch_issue_ratio":0.0857142857
    },
    {
        "Challenge_adjusted_solved_time":18.5230555556,
        "Challenge_answer_count":2,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948747000,
        "Challenge_created_time":1635882064000,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":208.0,
        "Challenge_repo_issue_count":1067.0,
        "Challenge_repo_star_count":1173.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.5230555556,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":87,
        "Platform":"Github",
        "Solution_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Solution_link_count":0.0,
        "Solution_readability":4.3,
        "Solution_reading_time":3.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0524835989,
        "Challenge_watch_issue_ratio":0.0281162137
    },
    {
        "Challenge_adjusted_solved_time":1.5752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/wandb.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Challenge_closed_time":1658480572000,
        "Challenge_created_time":1658474901000,
        "Challenge_link":"https:\/\/github.com\/alvarobartt\/wandbfsspec\/issues\/7",
        "Challenge_link_count":1,
        "Challenge_readability":35.5,
        "Challenge_reading_time":2.12,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":1.5752777778,
        "Challenge_title":"`WandbFileSystem.ls` not working fine with nested directories",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":7,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0909090909,
        "Challenge_watch_issue_ratio":0.0909090909
    },
    {
        "Challenge_adjusted_solved_time":97.4961111111,
        "Challenge_answer_count":0,
        "Challenge_body":"- [x] wandb index name modify\r\n\r\nwandb create index name error and  change name to \"modelname + save_folder_name\"",
        "Challenge_closed_time":1635155013000,
        "Challenge_created_time":1634804027000,
        "Challenge_link":"https:\/\/github.com\/boostcampaitech2\/semantic-segmentation-level2-cv-02\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":65.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":5.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":97.4961111111,
        "Challenge_title":"wandb create index name error",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":21,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0923076923,
        "Challenge_watch_issue_ratio":0.0769230769
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"```Problem Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.``` I'm running into this issue with a specific model (e.g. DA-RNN w\/meta-data sweep). If runs truly aren't cleared then sweeps could be corrupting subsequent runs. This behavior hasn't been observed previously however.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1600976683000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/162",
        "Challenge_link_count":0,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Weird memory problem with sweeps Colab Wandb",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":65,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":124.4336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"Wandb sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgH78bNb9A5JP6NcfFHB189TIjy5c#scrollTo=sTDGweZ0d0QP) advance instead they just stall after the first part of the sweep completes. This is causing problems.",
        "Challenge_closed_time":1600665871000,
        "Challenge_created_time":1600217910000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/154",
        "Challenge_link_count":1,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":124.4336111111,
        "Challenge_title":"Wandb Run stalling",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"So this appears to be a problem on the Weights and Biases end of things. https:\/\/github.com\/wandb\/client\/issues\/1243 This is fixed see original issue.",
        "Solution_link_count":1.0,
        "Solution_readability":7.6,
        "Solution_reading_time":1.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":45.9877777778,
        "Challenge_answer_count":3,
        "Challenge_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Challenge_closed_time":1578199794000,
        "Challenge_created_time":1578034238000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Challenge_link_count":2,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.42,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":45.9877777778,
        "Challenge_title":"Wandb bug when running train long",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"Currently working on this should be fixed. Just need to test it So it seems to not be doing steps anymore on the same model run chart, but the runs are still not terminating until the loop ends. Don't really know if this is a problem or not. Going to close this for now Wandb supports up to 50 concurrent runs. So as long as aren't training on more than 50 rivers at a time this shouldn't be an issue. If it becomes one will revert to the subprocess thing but don't want otherwise as with that it doesn't log debugging. ",
        "Solution_link_count":0.0,
        "Solution_readability":6.3,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":64.4975,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nAt model train completion, the test set loss is written as iteration 0 to the TensorBoard \/ W&B chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. As the validation loss and perplexity has already been written to this chart, this results in TensorBoard deleting all the validation metrics, overwriting them with the test loss and perplexity values. W&B refuses to add the test metrics to the charts at all, throwing a warning that looks like `wandb: WARNING Step must only increase in log calls.  Step 0 < 32000; dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Pip install and setup TensorBoard and W&B\r\n2. Begin training a model with a train, validation, and test set\r\n3. Observe in both TensorBoard and W&B that validation metrics are being logged\r\n4. Allow the model to train to completion\r\n5. Observe that the TensorBoard validation metrics are now gone, overwritten by the test set metrics\r\n6. Observe the W&B error in the text logs \/ program output\r\n\r\n**Expected behavior**\r\nTest metrics should be written to their own charts.\r\n\r\n**Proposed solution**\r\nTest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**Environment (please complete the following information):**\r\n - GPUs: 4x A100 80 GB\r\n- Configs: (configs that I used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# GPT-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"Adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": True,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": True,\r\n    \"reduce_scatter\": True,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": True,\r\n    \"cpu_offload\": False\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # Checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# Suggested data paths when using GPT-NeoX locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": False,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4TBNVME\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4TBNVME\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_wandb\": True,\r\n  \"wandb_host\": \"https:\/\/api.wandb.ai\",\r\n  \"wandb_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# Add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**Additional context**\r\n\r\nI have a bug fix ready, will follow up with it.",
        "Challenge_closed_time":1663248037000,
        "Challenge_created_time":1663015846000,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/669",
        "Challenge_link_count":3,
        "Challenge_readability":16.2,
        "Challenge_reading_time":63.87,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":64.4975,
        "Challenge_title":"Test set metrics overwrite validation set metrics in TensorBoard and are rejected for logging by Weights and Biases (W&B)",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":522,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0617977528,
        "Challenge_watch_issue_ratio":0.1053370787
    },
    {
        "Challenge_adjusted_solved_time":1618.0830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Challenge_closed_time":1624218172000,
        "Challenge_created_time":1618393073000,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0830555556,
        "Challenge_title":"Local wandb logging is borked",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":35,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0617977528,
        "Challenge_watch_issue_ratio":0.1053370787
    },
    {
        "Challenge_adjusted_solved_time":170.0166666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRefused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### To Reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"Screen Shot 2022-07-23 at 10 23 34 AM\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1659198312000,
        "Challenge_created_time":1658586252000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning-hpo\/issues\/17",
        "Challenge_link_count":3,
        "Challenge_readability":11.5,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":261.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":170.0166666667,
        "Challenge_title":"Refused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":62,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0574712644,
        "Challenge_watch_issue_ratio":0.0689655172
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"* IceNet version: 0.2.0.dev10\r\n\r\n`icenet\/model\/train.py` has the wandb.init entity hardcoded, oops\r\n\r\nMake this default to $USER, ICENET_WANDB_USER or be overridden by command line (whichever exists right to left... \ud83d\ude09 )\r\n\r\nDo the same for the project too",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669459637000,
        "Challenge_link":"https:\/\/github.com\/icenet-ai\/icenet\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_readability":6.3,
        "Challenge_reading_time":3.41,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":73.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"wandb entity is hardcoded",
        "Challenge_topic":"Model Persistence",
        "Challenge_topic_macro":"Model Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_link_count":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0273972603,
        "Challenge_watch_issue_ratio":0.0410958904
    },
    {
        "Challenge_adjusted_solved_time":50.4269444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Challenge_closed_time":1615408051000,
        "Challenge_created_time":1615226514000,
        "Challenge_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":47.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":50.4269444444,
        "Challenge_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":70,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0425531915,
        "Challenge_watch_issue_ratio":0.085106383
    },
    {
        "Challenge_adjusted_solved_time":1.3086111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## TL;DR\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## WHY?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## WHAT?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## TODOs\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Challenge_closed_time":1634915290000,
        "Challenge_created_time":1634910579000,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/90",
        "Challenge_link_count":0,
        "Challenge_readability":2.4,
        "Challenge_reading_time":3.53,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.3086111111,
        "Challenge_title":"wrong wandb dataset file name",
        "Challenge_topic":"Batch Transformation",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":49,
        "Platform":"Github",
        "Solution_body":"feature_68\uc5d0 \uc5c5\ub370\uc774\ud2b8\ub41c \uba54\uc778 \ube0c\ub79c\uce58\uac00 \uc801\uc6a9 \uc548\ub418\uc11c \uadf8\ub7f0\uac83.\r\nPR \uc0dd\uc131\ud574\uc11c \uc218\uc815\ud558\uc790",
        "Solution_link_count":0.0,
        "Solution_readability":-0.8,
        "Solution_reading_time":0.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0322580645,
        "Challenge_watch_issue_ratio":0.0161290323
    },
    {
        "Challenge_adjusted_solved_time":167.0766666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import Mapping, Sequence \r\nModuleNotFoundError: No module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**To Reproduce**\r\nRun on @ohsuz 's server.\r\n(Cannot reproduce on Intel i7 based local condition.)\r\n\r\n**Expected behavior**\r\nwandb should be properly imported.\r\n\r\n**Server (please complete the following information):**\r\n - OS: centOS\r\n",
        "Challenge_closed_time":1635333937000,
        "Challenge_created_time":1634732461000,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/89",
        "Challenge_link_count":0,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":167.0766666667,
        "Challenge_title":"wandb import failure",
        "Challenge_topic":"Environment Installation",
        "Challenge_topic_macro":"Infrastructure Management",
        "Challenge_word_count":45,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0322580645,
        "Challenge_watch_issue_ratio":0.0161290323
    },
    {
        "Challenge_adjusted_solved_time":6.8516666667,
        "Challenge_answer_count":1,
        "Challenge_body":"wandb api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Challenge_closed_time":1642173581000,
        "Challenge_created_time":1642148915000,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/51",
        "Challenge_link_count":1,
        "Challenge_readability":11.9,
        "Challenge_reading_time":2.13,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":6.8516666667,
        "Challenge_title":"wandb api key for github ci",
        "Challenge_topic":"Git Version Control",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":14,
        "Platform":"Github",
        "Solution_body":"\/settings\/secrets\r\n\r\n```\r\njobs:\r\n  weekday_job:\r\n    runs-on: ubuntu-latest\r\n    env:\r\n      DAY_OF_WEEK: Mon\r\n    steps:\r\n      - name: \"Hello world when it's Monday\"\r\n        if: ${{ env.DAY_OF_WEEK == 'Mon' }}\r\n        run: echo \"Hello $FIRST_NAME $middle_name $Last_Name, today is Monday!\"\r\n        env:\r\n          WANDB_API_KEY: $github.SECRETS.WANDB_API\r\n          WANDB_NAME: github_ci_tests\r\n          WANDB_NAME: Octocat\r\n```\r\n\r\n\r\nhttps:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables\r\n",
        "Solution_link_count":1.0,
        "Solution_readability":13.8,
        "Solution_reading_time":5.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":35.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0210526316,
        "Challenge_watch_issue_ratio":0.0105263158
    },
    {
        "Challenge_adjusted_solved_time":46.9786111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nWandbLogger throws error while import if etna[torch] is not installed.\n\n### Expected behavior\n\nWandb Logger should work no matter pytorch installation \n\n### How To Reproduce\n\n1. Create new env\r\n2. install etna and etna[wandb]\r\n3. import WandbLogger\r\n\n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version",
        "Challenge_closed_time":1638449992000,
        "Challenge_created_time":1638280869000,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_readability":8.6,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":46.9786111111,
        "Challenge_title":"[BUG] Wandb Logger does not work unless pytorch is installed ",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":63,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0173410405,
        "Challenge_watch_issue_ratio":0.0057803468
    },
    {
        "Challenge_adjusted_solved_time":360.0336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Challenge_closed_time":1635943713000,
        "Challenge_created_time":1634647592000,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Challenge_link_count":0,
        "Challenge_readability":11.8,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":360.0336111111,
        "Challenge_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":97,
        "Platform":"Github",
        "Solution_body":"The key to solve the bug can be [here](https:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/d99573326eb9acc3b4dd3148b9e63d2144acc917\/etna\/loggers\/wandb_logger.py#L149) lets discuss it  check that `fold_number` in df.column before drop\r\nhttps:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/master\/etna\/loggers\/wandb_logger.py#L175",
        "Solution_link_count":2.0,
        "Solution_readability":18.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0173410405,
        "Challenge_watch_issue_ratio":0.0057803468
    },
    {
        "Challenge_adjusted_solved_time":10471.2408333333,
        "Challenge_answer_count":5,
        "Challenge_body":"",
        "Challenge_closed_time":1668696973000,
        "Challenge_created_time":1631000506000,
        "Challenge_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Challenge_link_count":0,
        "Challenge_readability":8.0,
        "Challenge_reading_time":0.95,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":87.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":10471.2408333333,
        "Challenge_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":12,
        "Platform":"Github",
        "Solution_body":"I think it might be interesting to use tensorboard by default instead of wandb: It does not required external services and keep all data away from getting uploaded. Or at least using tensorboard as a fallback if wandb is not installed.\r\n\r\nWhat do you think @ragier ?  Yes, totally agree\r\nTensorboardX is also the default logger of pytorch lightning @thibo73800 We want to force everyone to change their script to `--log wandb` ? Not sure.  I don't",
        "Solution_link_count":0.0,
        "Solution_readability":8.3,
        "Solution_reading_time":5.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":75.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.03514377,
        "Challenge_watch_issue_ratio":0.0095846645
    },
    {
        "Challenge_adjusted_solved_time":43.0502777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello, \r\n\r\nI'm using `wandb` logger (and `csv` as well), I found recently `hydra` config no longer save to `wandb` 's`config.yaml` file.\r\nBefore:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.4\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.9.13\r\n    start_time: 1665409636.577166\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 13\r\n      - 23\r\n      4: 3.9.13\r\n      5: 0.13.4\r\n      8:\r\n      - 5\r\ncallbacks\/early_stopping\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.EarlyStopping\r\ncallbacks\/early_stopping\/check_finite:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/check_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/divergence_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/min_delta:\r\n  desc: null\r\n  value: 0.0\r\ncallbacks\/early_stopping\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/early_stopping\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/early_stopping\/patience:\r\n  desc: null\r\n  value: 100\r\ncallbacks\/early_stopping\/stopping_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/strict:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.ModelCheckpoint\r\ncallbacks\/model_checkpoint\/auto_insert_metric_name:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/dirpath:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints\r\ncallbacks\/model_checkpoint\/every_n_epochs:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/every_n_train_steps:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/filename:\r\n  desc: null\r\n  value: epoch_{epoch:03d}\r\ncallbacks\/model_checkpoint\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/model_checkpoint\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/model_checkpoint\/save_last:\r\n  desc: null\r\n  value: true\r\ncallbacks\/model_checkpoint\/save_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/save_top_k:\r\n  desc: null\r\n  value: 1\r\ncallbacks\/model_checkpoint\/save_weights_only:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/train_time_interval:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_summary\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichModelSummary\r\ncallbacks\/model_summary\/max_depth:\r\n  desc: null\r\n  value: -1\r\ncallbacks\/rich_progress_bar\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichProgressBar\r\nckpt_path:\r\n  desc: null\r\n  value: None\r\ndatamodule\/_target_:\r\n  desc: null\r\n  value: src.datamodules.mnist_datamodule.MNISTDataModule\r\ndatamodule\/batch_size:\r\n  desc: null\r\n  value: 128\r\ndatamodule\/data_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/data\/\r\ndatamodule\/num_workers:\r\n  desc: null\r\n  value: 0\r\ndatamodule\/pin_memory:\r\n  desc: null\r\n  value: false\r\ndatamodule\/train_val_test_split:\r\n  desc: null\r\n  value:\r\n  - 55000\r\n  - 5000\r\n  - 10000\r\nextras\/enforce_tags:\r\n  desc: null\r\n  value: true\r\nextras\/ignore_warnings:\r\n  desc: null\r\n  value: false\r\nextras\/print_config:\r\n  desc: null\r\n  value: true\r\nmodel\/_target_:\r\n  desc: null\r\n  value: src.models.mnist_module.MNISTLitModule\r\nmodel\/net\/_target_:\r\n  desc: null\r\n  value: src.models.components.simple_dense_net.SimpleDenseNet\r\nmodel\/net\/input_size:\r\n  desc: null\r\n  value: 784\r\nmodel\/net\/lin1_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/lin2_size:\r\n  desc: null\r\n  value: 128\r\nmodel\/net\/lin3_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/output_size:\r\n  desc: null\r\n  value: 10\r\nmodel\/optimizer\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/optimizer\/_target_:\r\n  desc: null\r\n  value: torch.optim.Adam\r\nmodel\/optimizer\/lr:\r\n  desc: null\r\n  value: 0.001\r\nmodel\/optimizer\/weight_decay:\r\n  desc: null\r\n  value: 0.0\r\nmodel\/params\/non_trainable:\r\n  desc: null\r\n  value: 0\r\nmodel\/params\/total:\r\n  desc: null\r\n  value: 67978\r\nmodel\/params\/trainable:\r\n  desc: null\r\n  value: 67978\r\nmodel\/scheduler\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/scheduler\/_target_:\r\n  desc: null\r\n  value: torch.optim.lr_scheduler.ReduceLROnPlateau\r\nmodel\/scheduler\/factor:\r\n  desc: null\r\n  value: 0.1\r\nmodel\/scheduler\/mode:\r\n  desc: null\r\n  value: min\r\nmodel\/scheduler\/patience:\r\n  desc: null\r\n  value: 10\r\nseed:\r\n  desc: null\r\n  value: 123\r\ntags:\r\n  desc: null\r\n  value:\r\n  - dev\r\ntask_name:\r\n  desc: null\r\n  value: train\r\ntrainer\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.Trainer\r\ntrainer\/accelerator:\r\n  desc: null\r\n  value: cpu\r\ntrainer\/check_val_every_n_epoch:\r\n  desc: null\r\n  value: 1\r\ntrainer\/default_root_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\r\ntrainer\/deterministic:\r\n  desc: null\r\n  value: false\r\ntrainer\/devices:\r\n  desc: null\r\n  value: 1\r\ntrainer\/max_epochs:\r\n  desc: null\r\n  value: 3\r\ntrainer\/min_epochs:\r\n  desc: null\r\n  value: 1\r\n```\r\nNow:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.6\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.8.15\r\n    start_time: 1670583155.275978\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 23\r\n      4: 3.8.15\r\n      5: 0.13.6\r\n      8:\r\n      - 5\r\n```\r\nThis may related to:\r\nhttps:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#L172\r\nAny idea how to restore to previous state?",
        "Challenge_closed_time":1670781696000,
        "Challenge_created_time":1670626715000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/478",
        "Challenge_link_count":1,
        "Challenge_readability":16.9,
        "Challenge_reading_time":71.23,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":43.0502777778,
        "Challenge_title":"Question: How to save hydra config to wandb config.yaml",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":551,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":367.1658333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Challenge_closed_time":1657912525000,
        "Challenge_created_time":1656590728000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Challenge_link_count":5,
        "Challenge_readability":17.7,
        "Challenge_reading_time":99.61,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":367.1658333333,
        "Challenge_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":523,
        "Platform":"Github",
        "Solution_body":"Hi @GillianGrayson \r\n\r\nFor **1. hydra-optuna-sweeper problem**, it has been modified in release_1.4. You can find [here](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/7e67c4692590550e7b703655845e59508eb071bb\/configs\/hparams_search\/mnist_optuna.yaml#L49)\r\n\r\n @GillianGrayson ty for reporting, the problems have been fixed on the current `main` branch.",
        "Solution_link_count":1.0,
        "Solution_readability":12.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":26.0741666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Challenge_closed_time":1654420353000,
        "Challenge_created_time":1654326486000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Challenge_link_count":0,
        "Challenge_readability":4.4,
        "Challenge_reading_time":61.14,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":210,
        "Challenge_solved_time":26.0741666667,
        "Challenge_title":"wandb logger not working",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":584,
        "Platform":"Github",
        "Solution_body":"`wandb-callbacks` haven't been maintained for a while and it might not work correctly with recent lightning and hydra releases. \r\n\r\nHave you trained using the `main` branch?\r\n\r\nI'm preparing new release and will fix the callbacks when it's ready https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/308\r\n So i managed to get it working using a fresh conda environment: \r\ntorch==1.10.0 with CUDA10.2\r\npytorch-lightning==1.6.4\r\nwandb == 0.12.17\r\n\r\nI doesnt check if all the callbacks work properly but my initial problem is solved. Thank you for your help! ",
        "Solution_link_count":1.0,
        "Solution_readability":7.0,
        "Solution_reading_time":6.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":77.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":1666.9041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Challenge_closed_time":1657910798000,
        "Challenge_created_time":1651909943000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Challenge_link_count":0,
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.94,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1666.9041666667,
        "Challenge_title":"wandb log only 1 run when using ddp and multirun",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":37,
        "Platform":"Github",
        "Solution_body":"Try adding `wandb.finish()` after testing to make sure it has closed properly",
        "Solution_link_count":0.0,
        "Solution_readability":3.3,
        "Solution_reading_time":0.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":1043.1541666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\r\n\r\nThere may be version conflict between wandb and PL 1.6.1\r\n\r\n**OS:** Ubuntu20.04\r\n**Python:** 3.8.13\r\n**Pytorch:**  1.11.0\r\n**PL:** 1.6.1\r\n**Wandb:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen I use the Hyperparameter Search, it produces the following error:\r\n\r\n```python\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/wandb\/offline-run-20*\/logs\/debug-internal.log'\r\nProblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py 357 experiment\r\n```\r\n",
        "Challenge_closed_time":1654689077000,
        "Challenge_created_time":1650933722000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/285",
        "Challenge_link_count":0,
        "Challenge_readability":10.0,
        "Challenge_reading_time":7.37,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1043.1541666667,
        "Challenge_title":"Wandb is not compatible with PL 1.6.1",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":55,
        "Platform":"Github",
        "Solution_body":"I have met the same problem. > I have met the same problem.\r\n\r\nInstall PL=1.5.10 for me it's working with 1.6.3 \r\nonly update wandb 0.12.16",
        "Solution_link_count":0.0,
        "Solution_readability":2.1,
        "Solution_reading_time":1.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":24.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":166.3613888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Challenge_closed_time":1666770135000,
        "Challenge_created_time":1666171234000,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Challenge_link_count":3,
        "Challenge_readability":18.9,
        "Challenge_reading_time":171.15,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":126,
        "Challenge_solved_time":166.3613888889,
        "Challenge_title":"WandB fails when config is too large",
        "Challenge_topic":"Compute Management",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":858,
        "Platform":"Github",
        "Solution_body":"Yeah, I wouldn't call this a bug _per se_. It's just that `WandbLogger` has some limitations that we need to navigate.\r\n\r\nI think your options are to:\r\n\r\n1. not log the training selection; \r\n2. log the test selection instead, as it should be considerably smaller; \r\n3. encode the selection in the data pipeline such that the train\/test label is a column in your database rather than a separate array, and then just log this column name; or \r\n4. implement and log the selection as a reproducible prescription (e.g., `test = event_no % 5 == 0` and `train = not test`) rather than as an explicit array of indices. \r\n\r\nI don't think (1) is a good option, but (2-4) could all work and I think they are all pretty straightforward to do.",
        "Solution_link_count":0.0,
        "Solution_readability":6.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0291777188,
        "Challenge_watch_issue_ratio":0.0159151194
    },
    {
        "Challenge_adjusted_solved_time":24.1527777778,
        "Challenge_answer_count":0,
        "Challenge_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Challenge_closed_time":1661948109000,
        "Challenge_created_time":1661861159000,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Challenge_link_count":0,
        "Challenge_readability":15.9,
        "Challenge_reading_time":59.04,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":24.1527777778,
        "Challenge_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":330,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0291777188,
        "Challenge_watch_issue_ratio":0.0159151194
    },
    {
        "Challenge_adjusted_solved_time":0.1094444444,
        "Challenge_answer_count":0,
        "Challenge_body":"Example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0\r\n\r\nAt the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job.\r\nHowever, we have:\r\n```\r\n...\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\nwandb: Waiting for W&B process to finish, PID 75\r\n...\r\n```\r\n\r\nWhile it should be:\r\n```\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nwandb: Waiting for W&B process to finish, PID 75\r\nwandb: Program ended successfully.\r\nwandb:                                                                                \r\n```\r\n\r\nOne line was overwritten by the `wandb: Waiting for W&B process to finish, PID 75`, which, apparently is running in a separate process (`wandb.Settings(start_method=\"fork\")`). ",
        "Challenge_closed_time":1625736868000,
        "Challenge_created_time":1625736474000,
        "Challenge_link":"https:\/\/github.com\/neuro-inc\/mlops-wandb-bucket-ref\/issues\/16",
        "Challenge_link_count":0,
        "Challenge_readability":15.0,
        "Challenge_reading_time":25.31,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":0.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":0.1094444444,
        "Challenge_title":"WandB output overwrites wabucketref's output in case of artifact upload",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":154,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_link_count":0.0,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0336134454,
        "Challenge_watch_issue_ratio":0.0504201681
    },
    {
        "Challenge_adjusted_solved_time":28.0155555556,
        "Challenge_answer_count":1,
        "Challenge_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Challenge_closed_time":1650034426000,
        "Challenge_created_time":1649933570000,
        "Challenge_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Challenge_link_count":0,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":102.0,
        "Challenge_repo_star_count":20.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.0155555556,
        "Challenge_title":"In random agent script wandb full episode data logging skips a few steps",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"This has been implemented and will be shipped with v0.4.4 \ud83d\ude80",
        "Solution_link_count":0.0,
        "Solution_readability":3.7,
        "Solution_reading_time":0.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0098039216,
        "Challenge_watch_issue_ratio":0.0196078431
    }
]