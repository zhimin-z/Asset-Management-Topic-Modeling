[
    {
        "Challenge_adjusted_solved_time":4170.7175,
        "Challenge_answer_count":3,
        "Challenge_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Challenge_closed_time":1600123381000,
        "Challenge_created_time":1585108798000,
        "Challenge_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.56,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":254.0,
        "Challenge_repo_issue_count":266.0,
        "Challenge_repo_star_count":224.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":4170.7175,
        "Challenge_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":146,
        "Platform":"Github",
        "Solution_body":"same with code cell [12]\r\nit calls for 5 child weights to be tried:\r\n\r\n`min_child_weights = [1, 2, 4, 8, 10]`\r\n\r\nbut the default number of instances across all training jobs in a new account is 4, and needs to be increased for the tour to work without errors.\r\n\r\nSuggestion:\r\n- add this to prerequsites section\r\n- change the notebook to only try 4 values for `min_child_weights`\r\n\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'Number of instances across all training jobs' is 4 Instances, with current utilization of 4 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.` Neither of these are doc issues. The notebook itself needs to be updated. https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html states that the default limit for ml.m4.xlarge is 20, so in a typical account, you should be able to run the notebook without failure. Your administrator could have changed this though. You can contact support for a limit increase to fix your account to be able to run this notebook.",
        "Solution_gpt_summary":"link servic limit increas page prerequisit section notebook instanc type endpoint default servic limit notebook valu child weight resourcelimitexceed account level servic limit xlarg endpoint usag instanc util instanc request delta instanc default instanc train job account increas tour notebook updat contact limit increas account run notebook",
        "Solution_link_count":1.0,
        "Solution_original_content":"cell call child weight tri child weight default instanc train job account increas tour add prerequsit section notebook valu child weight resourcelimitexceed resourcelimitexceed call createtrainingjob oper account level servic limit instanc train job instanc util instanc request delta instanc contact request increas limit doc notebook updat http doc com gener latest html state default limit xlarg typic account run notebook administr contact limit increas account run notebook",
        "Solution_preprocessed_content":"cell call child weight tri default instanc train job account increas tour add prerequsit section notebook valu doc notebook updat state default limit typic account run notebook administr contact limit increas account run notebook",
        "Solution_readability":9.8,
        "Solution_reading_time":14.06,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":176.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3421052632,
        "Challenge_watch_issue_ratio":0.1315789474
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [x] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [x] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nGetting this error, when invoking a MME on sagemaker setup using `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04` container image.\r\n\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=14448): Max retries exceeded with url: \/v1\/models\/d2295a7526f9df36354b8a2c4adc4f63 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70966dba50>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nTraceback (most recent call last):\r\n  File \"\/sagemaker\/python_service.py\", line 157, in _handle_load_model_post\r\n    self._wait_for_model(model_name)\r\n  File \"\/sagemaker\/python_service.py\", line 247, in _wait_for_model\r\n    response = session.get(url)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 546, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\r\n    raise ConnectionError(e, request=request)\r\n\r\n*DLC image\/dockerfile:*\r\n763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04\r\n*Current behavior:*\r\n\r\n*Expected behavior:*\r\nModel should load up and return prediction\r\n*Additional context:*\r\nI have setup a MME using the above mentioned container and invoking the endpoint using a lambda. The model files are in placed in S3 and are in the correct directory structure with a version number. ",
        "Challenge_closed_time":null,
        "Challenge_created_time":1602135852000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/170",
        "Challenge_link_count":1,
        "Challenge_open_time":18642.2633333333,
        "Challenge_readability":14.1,
        "Challenge_reading_time":30.6,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] : Model not loading while using existing container image to setup MME on sagemaker",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":238,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1048034934,
        "Challenge_watch_issue_ratio":0.1615720524
    },
    {
        "Challenge_adjusted_solved_time":1329.0686111111,
        "Challenge_answer_count":2,
        "Challenge_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Challenge_closed_time":1571957138000,
        "Challenge_created_time":1567172491000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":28.7,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":104.0,
        "Challenge_repo_issue_count":229.0,
        "Challenge_repo_star_count":160.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":1329.0686111111,
        "Challenge_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":283,
        "Platform":"Github",
        "Solution_body":"Hello @janismdhanbad,\r\n\r\nI believe your inference requests will have to follow the TensorFlow serving REST API specifications defined here: https:\/\/www.tensorflow.org\/tfx\/serving\/api_rest#request_format_2\r\n\r\n```\r\nThe request body for predict API must be JSON object formatted as follows:\r\n\r\n{\r\n  \/\/ (Optional) Serving signature to use.\r\n  \/\/ If unspecifed default serving signature is used.\r\n  \"signature_name\": <string>,\r\n\r\n  \/\/ Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\r\n  \/\/ A request can have either of them but NOT both.\r\n  \"instances\": <value>|<(nested)list>|<list-of-objects>\r\n  \"inputs\": <value>|<(nested)list>|<object>\r\n}\r\n``` closing due to inactivity. feel free to reopen if necessary.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"janismdhanbad believ infer request tensorflow serv rest api defin http tensorflow org tfx serv api rest request format request bodi predict api json object format option serv signatur unspecif default serv signatur signatur input tensor row instanc columnar input format request instanc input close inact free reopen",
        "Solution_preprocessed_content":"believ infer request tensorflow serv rest api defin close inact free reopen",
        "Solution_readability":11.3,
        "Solution_reading_time":8.78,
        "Solution_score_count":2.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1048034934,
        "Challenge_watch_issue_ratio":0.1615720524
    },
    {
        "Challenge_adjusted_solved_time":5.8786111111,
        "Challenge_answer_count":1,
        "Challenge_body":"",
        "Challenge_closed_time":1615314058000,
        "Challenge_created_time":1615292895000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/175",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.4,
        "Challenge_reading_time":1.84,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":5.8786111111,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":12,
        "Platform":"Github",
        "Solution_body":"Closing in favour of #174 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":0.5,
        "Solution_reading_time":0.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":4770.0555555556,
        "Challenge_answer_count":3,
        "Challenge_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_closed_time":1632465008000,
        "Challenge_created_time":1615292808000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.66,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4770.0555555556,
        "Challenge_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":23,
        "Platform":"Github",
        "Solution_body":"Thanks for using amazon-sagemaker-operator-for-k8s. Please help us with the steps to replicate the issue, especially the installation\r\n\r\nOfficial documentation for reference: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-operators-for-kubernetes.html I ran into this issue while myself and was resolved by making sure the SageMaker operator was applied and running by verifying with kubectl -n sagemaker-k8s-operator-system get pods Closing since there has been no activity in 90+ days",
        "Solution_gpt_summary":"oper appli run verifi kubectl oper pod",
        "Solution_link_count":1.0,
        "Solution_original_content":"oper step replic instal offici document http doc com latest oper kubernet html ran oper appli run verifi kubectl oper pod close activ dai",
        "Solution_preprocessed_content":"step replic instal offici document ran oper appli run verifi kubectl pod close activ dai",
        "Solution_readability":18.2,
        "Solution_reading_time":6.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":60.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":407.5572222222,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nSageMaker Operator Types, when included as part of KubeBuilder V2 custom CRD definition fail due to validation errors of unescaped regex patterns. \r\n\r\n```\r\n\/go\/bin\/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\".\/...\" output:crd:artifacts:config=config\/crd\/bases\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n```\r\n\r\n**What you expected to happen**:\r\nKubeBuilder should generate CRD specification which includes AWS SageMaker Operator Types\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n```\r\nimport (\r\n\tcommonv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\"\r\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\r\n)\r\n\r\n\/\/ GuestbookSpec defines the desired state of Guestbook\r\ntype GuestbookSpec struct {\r\n\t\/\/ INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\r\n\t\/\/ Important: Run \"make\" to regenerate code after modifying this file\r\n\r\n\tAlgorithmSpecification *commonv1.AlgorithmSpecification `json:\"algorithmSpecification\"`\r\n\r\n\tEnableInterContainerTrafficEncryption *bool `json:\"enableInterContainerTrafficEncryption,omitempty\"`\r\n\r\n\tEnableNetworkIsolation *bool `json:\"enableNetworkIsolation,omitempty\"`\r\n...\r\n\/\/Run make install with above  types in custom operator\r\nmake install \r\n```\r\n\r\n**Anything else we need to know?**:\r\nTried copying the above types and escaped the regex pattern with quotes (``\/\/ +kubebuilder:validation:Pattern='^(https|s3):\/\/([^\/]+)\/?(.*)$'``) and everything worked\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):Version: version.Version{KubeBuilderVersion:\"2.3.1\", KubernetesVendor:\"1.16.4\", GitCommit:\"8b53abeb4280186e494b726edf8f54ca7aa64a49\", BuildDate:\"2020-03-26T16:42:00Z\", GoOs:\"unknown\", GoArch:\"unknown\"}\r\n- Operator version (controller image tag):\tgithub.com\/aws\/amazon-sagemaker-operator-for-k8s v1.0.1-0.20200410212604-780c48ecb21a\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1596827786000,
        "Challenge_created_time":1595360580000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":23.6,
        "Challenge_reading_time":75.11,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":407.5572222222,
        "Challenge_title":"SageMaker Operator Types fails KubeBuilder Pattern validation check",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":316,
        "Platform":"Github",
        "Solution_body":"Hi Nagaraj, I'll contact you directly to discuss this. It appears as though you are attempting to build your project with a newer version of `controller-gen` than we have supported in our CRDs. We are using an older version [`v0.2.0-beta.2`](https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/blob\/283886dd7c66adfd8c491bf452796fea698ceab8\/Makefile#L98) for our builds. We will need to update our CRDs (and maybe some controller logic) and our build scripts to support the newest version ([`v0.3.0`](https:\/\/github.com\/kubernetes-sigs\/controller-tools\/releases\/tag\/v0.3.0)). ",
        "Solution_gpt_summary":"escap regex pattern quot older version control gen beta build oper type version crd build updat newest version control gen",
        "Solution_link_count":2.0,
        "Solution_original_content":"nagaraj contact directli build newer version control gen crd older version beta http github com oper blob ddcadfdcbffeaceab makefil build updat crd mayb control logic build newest version http github com kubernet sig control releas tag",
        "Solution_preprocessed_content":"nagaraj contact directli build newer version crd older version build updat crd build newest version",
        "Solution_readability":10.7,
        "Solution_reading_time":7.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":65.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":2039.8183333333,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Challenge_closed_time":1599678360000,
        "Challenge_created_time":1592335014000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2039.8183333333,
        "Challenge_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":180,
        "Platform":"Github",
        "Solution_body":"I believe this may be caused due to the generated deepcopy code not being checked in to the v1.1.0 branch. I attempted to backport this code previously but I don't think the go modules ever picked this up for some reason. I might suggest attempting this pinning to the `master` branch rather than `v1.1.0`. The APIs are backwardly compatible (while `master` is still pointed at a `v1.X`).",
        "Solution_gpt_summary":"pin master branch build type api backwardli compat master",
        "Solution_link_count":0.0,
        "Solution_original_content":"believ gener deepcopi branch backport previous modul pick reason pin master branch api backwardli compat master",
        "Solution_preprocessed_content":"believ gener deepcopi branch backport previous modul pick reason pin branch api backwardli compat",
        "Solution_readability":7.5,
        "Solution_reading_time":4.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":67.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":4417.9619444444,
        "Challenge_answer_count":15,
        "Challenge_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Challenge_closed_time":1599677796000,
        "Challenge_created_time":1583773133000,
        "Challenge_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.6,
        "Challenge_reading_time":23.71,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":49.0,
        "Challenge_repo_issue_count":205.0,
        "Challenge_repo_star_count":144.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4417.9619444444,
        "Challenge_title":"unable to kick off the sagemaker job",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":193,
        "Platform":"Github",
        "Solution_body":"@charlesa101  Thanks for trying out. I am assuming you have replaced input, output buckets and role Arn. \r\n\r\nWould you please run the following command provide the output ?\r\n\r\n```\r\nkubectl  get trainingjobs xgboost-mnist\r\nkubectl describe trainingjob xgboost-mnist\r\n``` @gautamkmr, here you go thank you! yeah i have my own bucket and sagemaker executor role\r\n\r\n```kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nxgboost-mnist                               2020-03-09T16:51:08Z ```\r\n\r\n```kubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400``` @charlesa101  Thanks for providing the output. It appears that operator is not running successfully on your k8s cluster.  you can verify that \r\n\r\n```\r\n kubectl get pods -A | grep -i sagemaker\r\n```\r\n\r\nYou can follow steps from [here](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#setup-and-operator-deployment) to install the operator, let us know if you face any issue. yeah that's what i noticed as well now\r\n\r\n```kubectl get pods -n sagemaker-k8s-operator-system\r\nNAME                                                         READY   STATUS    RESTARTS   AGE\r\nsagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8   0\/2     Pending   0          24h``` ```kubectl describe pod  -n sagemaker-k8s-operator-system                                             \r\nName:               sagemaker-k8s-operator-controller-manager-5858fd7b8d-h89s8\r\nNamespace:          sagemaker-k8s-operator-system\r\nPriority:           0\r\nPriorityClassName:  <none>\r\nNode:               <none>\r\nLabels:             control-plane=controller-manager\r\n                    pod-template-hash=5858fd7b8d\r\nAnnotations:        kubernetes.io\/psp: eks.privileged\r\nStatus:             Pending\r\nIP:                 \r\nControlled By:      ReplicaSet\/sagemaker-k8s-operator-controller-manager-5858fd7b8d\r\nContainers:\r\n  kube-rbac-proxy:\r\n    Image:      gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Port:       8443\/TCP\r\n    Host Port:  0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    Environment:\r\n      AWS_ROLE_ARN:                 arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\n  manager:\r\n    Image:      957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Port:       <none>\r\n    Host Port:  <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:  \r\n      AWS_ROLE_ARN:                    arn:aws:iam::123456789012:role\/DELETE_ME\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from sagemaker-k8s-operator-default-token-rwdkn (ro)\r\nConditions:\r\n  Type           Status\r\n  PodScheduled   False \r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  sagemaker-k8s-operator-default-token-rwdkn:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  sagemaker-k8s-operator-default-token-rwdkn\r\n    Optional:    false\r\nQoS Class:       Burstable\r\nNode-Selectors:  <none>\r\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\r\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\r\nEvents:\r\n  Type     Reason            Age                   From               Message\r\n  ----     ------            ----                  ----               -------\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n my eks\/ecr is on us-east2, but it seems all the crd artifacts are coming from us-east1 could that be the issue?\r\n EKS can pull the image from other region too. I think in your case it seems that you don't have any worker node associated to cluster?  At least thats what below message says.\r\n```\r\n  Warning  FailedScheduling  64s (x1378 over 34h)  default-scheduler  no nodes available to schedule pods\r\n```\r\n\r\nCan you run ?  \r\n```\r\nkubectl get node\r\n``` @charlesa101  did you get chance to review it again? ``` kubectl get nodes\r\nNAME                                           STATUS   ROLES    AGE     VERSION\r\nip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\nip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n yeah i did, recreated the cluster again but still the same issue\r\n @charlesa101   In previous describe output of `pod` it appears that cluster did not have any worker nodes available `(no nodes available to schedule pods)`.\r\n\r\nBut based on recent output it appears that you have three worker nodes available. \r\n\r\n> NAME                                           STATUS   ROLES    AGE     VERSION\r\n> ip-172-16-116-51.us-east-2.compute.internal    Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-121-255.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n> ip-172-16-137-197.us-east-2.compute.internal   Ready    <none>   5h47m   v1.14.8-eks-b8860f\r\n\r\n\r\nCould you please describe each of these nodes and operator pod ?\r\n\r\n```\r\n# Describe nodes , assuming the names of nodes are same as you mentioned in previous comment.\r\nkubectl describe node ip-172-16-116-51.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-121-255.us-east-2.compute.internal \r\nkubectl describe node ip-172-16-137-197.us-east-2.compute.internal \r\n```\r\n\r\n\r\n```\r\n#Get the operator pod name \r\nkubectl get pods -A | grep -i sagemaker\r\nkubectl describe pod <put the pod name here>  -n sagemaker-k8s-operator-system\r\n```\r\n\r\n\r\nIf operator has been deployed successfully and if trainingjob is still not yet running please attach the out put of describe trainingjob as well ? \r\n```\r\nkubectl describe trainingjob xgboost-mnist\r\n\r\n```\r\n\r\n i tried to look checked the operator pod, here is  the log @gautamkmr \r\n\r\n```\r\nkubectl logs -f sagemaker-k8s-operator-controller-manager-5858fd7b8d-2dk5c  -n sagemaker-k8s-operator-system manager\r\n2020-03-15T18:09:13.864Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.865Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2020-03-15T18:09:13.866Z        INFO    setup   starting manager\r\n2020-03-15T18:09:13.866Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"trainingjob\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"model\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"batchtransformjob\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hostingdeployment\"}\r\n2020-03-15T18:09:14.066Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"endpointconfig\"}\r\n2020-03-15T18:09:14.067Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"hyperparametertuningjob\"}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"trainingjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"model\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2020-03-15T18:09:14.167Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.962Z        INFO    controllers.TrainingJob Job status is empty, setting to intermediate status     {\"trainingjob\": \"default\/xgboost-mnist\", \"status\": \"SynchronizingK8sJobWithSageMaker\"}\r\n2020-03-15T19:09:19.963Z        INFO    controllers.TrainingJob Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"new-status\": {\"trainingJobStatus\":\"SynchronizingK8sJobWithSageMaker\",\"lastCheckTime\":\"2020-03-15T19:09:19Z\"}}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.976Z        INFO    controllers.TrainingJob Adding generated name to spec   {\"trainingjob\": \"default\/xgboost-mnist\", \"new-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}\r\n2020-03-15T19:09:19.982Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:19.983Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:09:20.916Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:09:20.916Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 01ea5be5-6bd5-4bae-b79e-2bc8d86338ee\",\"lastCheckTime\":\"2020-03-15T19:09:20Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:09:20.924Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Getting resource        {\"trainingjob\": \"default\/xgboost-mnist\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Loaded AWS config       {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:41.623Z        INFO    controllers.TrainingJob Calling SM API DescribeTrainingJob      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\"}\r\n2020-03-15T19:11:42.150Z        ERROR   controllers.TrainingJob.handleSageMakerApiError Handling unrecoverable sagemaker API error      {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\"}\r\ngithub.com\/go-logr\/zapr.(*zapLogger).Error\r\n        \/go\/pkg\/mod\/github.com\/go-logr\/zapr@v0.1.0\/zapr.go:128\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).handleSageMakerApiError\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:396\r\ngo.amzn.com\/sagemaker\/sagemaker-k8s-operator\/controllers\/trainingjob.(*TrainingJobReconciler).Reconcile\r\n        \/workspace\/controllers\/trainingjob\/trainingjob_controller.go:172\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).reconcileHandler\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:216\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).processNextWorkItem\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:192\r\nsigs.k8s.io\/controller-runtime\/pkg\/internal\/controller.(*Controller).worker\r\n        \/go\/pkg\/mod\/sigs.k8s.io\/controller-runtime@v0.2.0\/pkg\/internal\/controller\/controller.go:171\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil.func1\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:152\r\nk8s.io\/apimachinery\/pkg\/util\/wait.JitterUntil\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:153\r\nk8s.io\/apimachinery\/pkg\/util\/wait.Until\r\n        \/go\/pkg\/mod\/k8s.io\/apimachinery@v0.0.0-20190404173353-6a84e37a896d\/pkg\/util\/wait\/wait.go:88\r\n2020-03-15T19:11:42.150Z        INFO    controllers.TrainingJob.handleSageMakerApiError Updating job status     {\"trainingjob\": \"default\/xgboost-mnist\", \"training-job-name\": \"xgboost-mnist-792eb47166f011ea88d202c3652bf444\", \"aws-region\": \"us-east-2\", \"new-status\": {\"trainingJobStatus\":\"Failed\",\"additional\":\"UnrecognizedClientException: The security token included in the request is invalid.\\n\\tstatus code: 400, request id: 7145c885-b685-4663-8dd3-6c212ce574b2\",\"lastCheckTime\":\"2020-03-15T19:11:42Z\",\"cloudWatchLogUrl\":\"https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=xgboost-mnist-792eb47166f011ea88d202c3652bf444;streamFilter=typeLogStreamPrefix\",\"sageMakerTrainingJobName\":\"xgboost-mnist-792eb47166f011ea88d202c3652bf444\"}}\r\n2020-03-15T19:11:42.159Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"trainingjob\", \"request\": \"default\/xgboost-mnist\"}\r\n```\r\n @charlesa101  Thanks for sharing the log. You are on right track. I think the issue now is operator pod is unable to retrieve credentials from IAM service to talk to sagemaker. \r\n\r\n`\"error\": \"UnrecognizedClientException: The security token included in the request is invalid.\\n`\r\n\r\nCould you please check your [trust.json](https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_operators_for_kubernetes.html#create-an-iam-role) basically **trust policy have three places to update cluster region and OIDC ID and one place to add your AWS account number.** Hi @charlesa101\r\n\r\nClosing this issue since there has been no activity in 90 days. Please re-open if you still need help\r\n\r\nThanks Hi, I'm having the exact same issue except that my pod is running fine. I setup my k8s cluster using terraform with 1 master node and 1 worker node. When I submit the trainingjob, there is no status or job name or anything else. I tried all the commands above and it looks like the scheduler was able to assign the pods to the worker node. Any help would be appreciated! Please see outputs for commands below:\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get pods -A                                                                                                                                                                                                                                                    \r\nNAMESPACE        NAME                                                         READY   STATUS    RESTARTS   AGE                                                                                                                                                                                                                \r\nkube-system      aws-node-67tgx                                               1\/1     Running   0          2d18h\r\nkube-system      aws-node-k2q7z                                               1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-cwfvj                                     1\/1     Running   0          2d18h\r\nkube-system      coredns-85d5b4454c-x5ld9                                     1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-54vm5                                             1\/1     Running   0          2d18h\r\nkube-system      kube-proxy-r8j7j                                             1\/1     Running   0          2d18h\r\nkube-system      metrics-server-64cf6869bd-6nppx                              1\/1     Running   0          2d18h\r\nsagemaker-jobs   sagemaker-k8s-operator-controller-manager-855f498957-fhkvv   2\/2     Running   0          2d18h\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe pod sagemaker-k8s-operator-controller-manager-855f498957-fhkvv -n sagemaker-jobs\r\nName:         sagemaker-k8s-operator-controller-manager-855f498957-fhkvv\r\nNamespace:    sagemaker-jobs\r\nPriority:     0\r\nNode:         ip-10-0-1-245.us-west-2.compute.internal\/10.0.1.245\r\nStart Time:   Fri, 24 Jun 2022 22:26:03 +0000\r\nLabels:       control-plane=controller-manager\r\n              pod-template-hash=855f498957\r\nAnnotations:  kubernetes.io\/psp: eks.privileged\r\nStatus:       Running\r\nIP:           10.0.1.144\r\nIPs:\r\n  IP:           10.0.1.144\r\nControlled By:  ReplicaSet\/sagemaker-k8s-operator-controller-manager-855f498957\r\nContainers:\r\n  manager:\r\n    Container ID:  docker:\/\/d8fc52b3e20a050999d3f24ab914f1d865a84a168a8b038f3fa81ce59cccbced\r\n    Image:         957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s:v1\r\n    Image ID:      docker-pullable:\/\/957583890962.dkr.ecr.us-east-1.amazonaws.com\/amazon-sagemaker-operator-for-k8s@sha256:94ffbba68954249b1724fdb43f1e8ab13547114555b4a217849687d566191e23\r\n    Port:          <none>\r\n    Host Port:     <none>\r\n    Command:\r\n      \/manager\r\n    Args:\r\n      --metrics-addr=127.0.0.1:8080\r\n      --namespace=sagemaker-jobs\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:09 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Limits:\r\n      cpu:     100m\r\n      memory:  30Mi\r\n    Requests:\r\n      cpu:     100m\r\n      memory:  20Mi\r\n    Environment:\r\n      AWS_DEFAULT_SAGEMAKER_ENDPOINT:\r\n      AWS_DEFAULT_REGION:              us-west-2\r\n      AWS_REGION:                      us-west-2\r\n      AWS_ROLE_ARN:                    arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:     \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nkube-rbac-proxy:\r\n    Container ID:  docker:\/\/4ecdaa395fdc70d5cead609465dbf21f6e11771a80ad5db0a6125053ab08b9d3\r\n    Image:         gcr.io\/kubebuilder\/kube-rbac-proxy:v0.4.0\r\n    Image ID:      docker-pullable:\/\/gcr.io\/kubebuilder\/kube-rbac-proxy@sha256:297896d96b827bbcb1abd696da1b2d81cab88359ac34cce0e8281f266b4e08de\r\n    Port:          8443\/TCP\r\n    Host Port:     0\/TCP\r\n    Args:\r\n      --secure-listen-address=0.0.0.0:8443\r\n      --upstream=http:\/\/127.0.0.1:8080\/\r\n      --logtostderr=true\r\n      --v=10\r\n    State:          Running\r\n      Started:      Fri, 24 Jun 2022 22:26:11 +0000\r\n    Ready:          True\r\n    Restart Count:  0\r\n    Environment:\r\n      AWS_DEFAULT_REGION:           us-west-2\r\n      AWS_REGION:                   us-west-2\r\n      AWS_ROLE_ARN:                 arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n      AWS_WEB_IDENTITY_TOKEN_FILE:  \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount\/token\r\n    Mounts:\r\n      \/var\/run\/secrets\/eks.amazonaws.com\/serviceaccount from aws-iam-token (ro)\r\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from kube-api-access-6j8rt (ro)\r\nConditions:\r\n  Type              Status\r\n  Initialized       True\r\n  Ready             True\r\n  ContainersReady   True\r\n  PodScheduled      True\r\nVolumes:\r\n  aws-iam-token:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  86400\r\n  kube-api-access-6j8rt:\r\n    Type:                    Projected (a volume that contains injected data from multiple sources)\r\n    TokenExpirationSeconds:  3607\r\n    ConfigMapName:           kube-root-ca.crt\r\n    ConfigMapOptional:       <nil>\r\n    DownwardAPI:             true\r\nQoS Class:                   Burstable\r\nNode-Selectors:              <none>\r\nTolerations:                 node.kubernetes.io\/not-ready:NoExecute op=Exists for 300s\r\n                             node.kubernetes.io\/unreachable:NoExecute op=Exists for 300s\r\nEvents:                      <none>\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl logs sagemaker-k8s-operator-controller-manager-855f498957-fhkvv manager -n sagemaker-jobs\r\nI0624 22:26:11.339445       1 request.go:621] Throttling request took 1.046981399s, request: GET:https:\/\/172.20.0.1:443\/apis\/extensions\/v1beta1?timeout=32s\r\n2022-06-24T22:26:12.443Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \"127.0.0.1:8080\"}\r\n2022-06-24T22:26:12.443Z        INFO    Starting manager in the namespace:      sagemaker-jobs\r\n2022-06-24T22:26:12.443Z        INFO    setup   starting manager\r\n2022-06-24T22:26:12.444Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"\/metrics\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.444Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.445Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.446Z        INFO    controller      Starting EventSource    {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"source\": \"kind source: \/, Kind=\"}\r\n2022-06-24T22:26:12.665Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\"}\r\n2022-06-24T22:26:12.666Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\"}\r\n2022-06-24T22:26:12.746Z        INFO    controller      Starting Controller     {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\"}\r\n2022-06-24T22:26:12.747Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingDeployment\", \"controller\": \"hostingdeployment\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"Model\", \"controller\": \"model\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"EndpointConfig\", \"controller\": \"endpointconfig\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HostingAutoscalingPolicy\", \"controller\": \"hostingautoscalingpolicy\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"ProcessingJob\", \"controller\": \"processingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"BatchTransformJob\", \"controller\": \"batchtransformjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"TrainingJob\", \"controller\": \"trainingjob\", \"worker count\": 1}\r\n2022-06-24T22:26:12.766Z        INFO    controller      Starting workers        {\"reconcilerGroup\": \"sagemaker.aws.amazon.com\", \"reconcilerKind\": \"HyperparameterTuningJob\", \"controller\": \"hyperparametertuningjob\", \"worker count\": 1}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl get trainingjobs\r\nNAME            STATUS   SECONDARY-STATUS   CREATION-TIME          SAGEMAKER-JOB-NAME\r\nosic-test-run                               2022-06-24T22:38:13Z  \r\n```\r\n\r\n```\r\nubuntu@ip-172-31-35-229:\/imvaria\/repos\/model-training$ kubectl describe trainingjob osic-test-run                                                                                                                                                                                                                             \r\nName:         osic-test-run                                                                                                                                                                                                                                                                                                   \r\nNamespace:    default                                                                                                                                                                                                                                                                                                         \r\nLabels:       <none>                                                                                                                                                                                                                                                                                                          \r\nAnnotations:  <none>                                                                                                                                                                                                                                                                                                          \r\nAPI Version:  sagemaker.aws.amazon.com\/v1                                                                                                                                                                                                                                                                                     \r\nKind:         TrainingJob                                                                                                                                                                                                                                                                                                     \r\nMetadata:                                                                                                                                                                                                                                                                                                                     \r\n  Creation Timestamp:  2022-06-24T22:38:13Z                                                                                                                                                                                                                                                                                   \r\n  Generation:          1                                                                                                                                                                                                                                                                                                      \r\n  Managed Fields:\r\n    API Version:  sagemaker.aws.amazon.com\/v1\r\n    Fields Type:  FieldsV1\r\n    fieldsV1:\r\n      f:metadata:\r\n        f:annotations:\r\n          .:\r\n          f:kubectl.kubernetes.io\/last-applied-configuration:\r\n      f:spec:\r\n        .:\r\n        f:algorithmSpecification:\r\n          .:\r\n          f:trainingImage:\r\n          f:trainingInputMode:\r\n        f:inputDataConfig:\r\n        f:outputDataConfig:\r\n          .:\r\n          f:s3OutputPath:\r\n        f:region:\r\n        f:resourceConfig:\r\n          .:\r\n          f:instanceCount:\r\n          f:instanceType:\r\n          f:volumeSizeInGB:\r\n        f:roleArn:\r\n        f:stoppingCondition:\r\n          .:\r\n          f:maxRuntimeInSeconds:\r\n        f:trainingJobName:\r\n    Manager:         kubectl-client-side-apply\r\n    Operation:       Update\r\n    Time:            2022-06-24T22:38:13Z\r\n  Resource Version:  3182\r\n  UID:               0a0880c0-baf9-4f1a-8aa3-37480520c3e2\r\nSpec:\r\n  Algorithm Specification:\r\nTraining Image:       438029713005.dkr.ecr.us-west-2.amazonaws.com\/model-training:latest\r\n    Training Input Mode:  File\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Data Source:\r\n      s3DataSource:\r\n        s3DataDistributionType:  FullyReplicated\r\n        s3DataType:              S3Prefix\r\n        s3Uri:                   s3:\/\/osic-full-including-override\r\n  Output Data Config:\r\n    s3OutputPath:  s3:\/\/osic-full-including-override\/experiments\r\n  Region:          us-west-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.p3.2xlarge\r\n    Volume Size In GB:  500\r\n  Role Arn:             arn:aws:iam::438029713005:role\/model-training-sagemaker-role20220624222338450100000009\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  900\r\n  Training Job Name:         osic-test-run\r\nEvents:                      <none>\r\n```\r\n\r\nplease let me know if you need to see anything else!",
        "Solution_gpt_summary":"invok sampl mnist train job involv verifi oper run successfulli cluster step instal oper trust json cluster region oidc account updat",
        "Solution_link_count":7.0,
        "Solution_original_content":"charlesa replac input output bucket role arn run output kubectl trainingjob mnist kubectl trainingjob mnist gautamkmr yeah bucket executor role kubectl trainingjob statu secondari statu creation time job mnist kubectl trainingjob mnist namespac default label annot kubectl kubernet appli configur apivers com trainingjob metadata annot mnist namespac default api version com trainingjob metadata creation timestamp gener resourc version link api com namespac default trainingjob mnist uid efd spec algorithm train imag dkr ecr amazonaw com latest train input mode file hyper paramet depth valu eta valu gamma valu child weight valu silent valu object valu multi softmax num class valu num round valu input data config channel train compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist train channel compress type type text csv data sourc data sourc data distribut type fullyrepl data type sprefix uri mnist output data config output path mnist model region resourc config instanc count instanc type xlarg volum size role arn arn iam role execut role stop condit runtim charlesa output oper run successfulli cluster verifi kubectl pod grep step http readthedoc stabl oper kubernet html setup oper deploy instal oper yeah notic kubectl pod oper readi statu restart ag oper control fdbd pend kubectl pod oper oper control fdbd namespac oper prioriti priorityclassnam node label control plane control pod templat hash fdbd annot kubernet psp ek privileg statu pend control replicaset oper control fdbd kube rbac proxi imag gcr kubebuild kube rbac proxi port tcp host port tcp arg secur listen address upstream http logtostderr environ role arn arn iam role delet web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount oper default token rwdkn imag dkr ecr amazonaw com oper port host port arg metric addr limit cpu memori request cpu memori environ default endpoint role arn arn iam role delet web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount oper default token rwdkn condit type statu podschedul volum iam token type volum inject data multipl sourc tokenexpirationsecond oper default token rwdkn type secret volum popul secret secretnam oper default token rwdkn option qo class burstabl node selector toler node kubernet readi noexecut node kubernet unreach noexecut event type reason ag messag warn failedschedul default schedul node schedul pod ek ecr crd artifact come ek pull imag region worker node associ cluster that messag sai warn failedschedul default schedul node schedul pod run kubectl node charlesa chanc review kubectl node statu role ag version comput intern readi ek comput intern readi ek comput intern readi ek yeah recreat cluster charlesa previou output pod cluster worker node node schedul pod base output worker node statu role ag version comput intern readi ek comput intern readi ek comput intern readi ek node oper pod node node previou comment kubectl node comput intern kubectl node comput intern kubectl node comput intern oper pod kubectl pod grep kubectl pod oper oper deploi successfulli trainingjob run attach trainingjob kubectl trainingjob mnist tri oper pod log gautamkmr kubectl log oper control fdbd dkc oper control runtim metric metric server start listen addr control runtim control start eventsourc control trainingjob sourc sourc control runtim control start eventsourc control hyperparametertuningjob sourc sourc control runtim control start eventsourc control hostingdeploy sourc sourc control runtim control start eventsourc control model sourc sourc control runtim control start eventsourc control endpointconfig sourc sourc control runtim control start eventsourc control batchtransformjob sourc sourc setup start control runtim start metric server path metric control runtim control start control control trainingjob control runtim control start control control model control runtim control start control control batchtransformjob control runtim control start control control hostingdeploy control runtim control start control control endpointconfig control runtim control start control control hyperparametertuningjob control runtim control start worker control trainingjob worker count control runtim control start worker control model worker count control runtim control start worker control endpointconfig worker count control runtim control start worker control batchtransformjob worker count control runtim control start worker control hostingdeploy worker count control runtim control start worker control hyperparametertuningjob worker count control trainingjob resourc trainingjob default mnist control trainingjob job statu set intermedi statu trainingjob default mnist statu synchronizingksjobwith control trainingjob updat job statu trainingjob default mnist statu trainingjobstatu synchronizingksjobwith lastchecktim control trainingjob resourc trainingjob default mnist control trainingjob gener spec trainingjob default mnist mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist control trainingjob resourc trainingjob default mnist control trainingjob load config trainingjob default mnist train job mnist ebfeadcbf region control trainingjob call api describetrainingjob trainingjob default mnist train job mnist ebfeadcbf region control trainingjob handleapierror unrecover api trainingjob default mnist train job mnist ebfeadcbf region unrecognizedclientexcept secur token request tstatu request eab bae bcdee github com logr zapr zaplogg pkg mod github com logr zapr zapr amzn com oper control trainingjob trainingjobreconcil handleapierror workspac control trainingjob trainingjob control amzn com oper control trainingjob trainingjobreconcil reconcil workspac control trainingjob trainingjob control sig control runtim pkg intern control control reconcilehandl pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control processnextworkitem pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control worker pkg mod sig control runtim pkg intern control control apimachineri pkg util wait jitteruntil func pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait jitteruntil pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait pkg mod apimachineri aead pkg util wait wait control trainingjob handleapierror updat job statu trainingjob default mnist train job mnist ebfeadcbf region statu trainingjobstatu addit unrecognizedclientexcept secur token request tstatu request eab bae bcdee lastchecktim cloudwatchlogurl http consol com cloudwatch home region logstream group trainingjob prefix mnist ebfeadcbf streamfilt typelogstreamprefix trainingjobnam mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist control trainingjob resourc trainingjob default mnist control trainingjob load config trainingjob default mnist train job mnist ebfeadcbf region control trainingjob call api describetrainingjob trainingjob default mnist train job mnist ebfeadcbf region control trainingjob handleapierror unrecover api trainingjob default mnist train job mnist ebfeadcbf region unrecognizedclientexcept secur token request tstatu request cceb github com logr zapr zaplogg pkg mod github com logr zapr zapr amzn com oper control trainingjob trainingjobreconcil handleapierror workspac control trainingjob trainingjob control amzn com oper control trainingjob trainingjobreconcil reconcil workspac control trainingjob trainingjob control sig control runtim pkg intern control control reconcilehandl pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control processnextworkitem pkg mod sig control runtim pkg intern control control sig control runtim pkg intern control control worker pkg mod sig control runtim pkg intern control control apimachineri pkg util wait jitteruntil func pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait jitteruntil pkg mod apimachineri aead pkg util wait wait apimachineri pkg util wait pkg mod apimachineri aead pkg util wait wait control trainingjob handleapierror updat job statu trainingjob default mnist train job mnist ebfeadcbf region statu trainingjobstatu addit unrecognizedclientexcept secur token request tstatu request cceb lastchecktim cloudwatchlogurl http consol com cloudwatch home region logstream group trainingjob prefix mnist ebfeadcbf streamfilt typelogstreamprefix trainingjobnam mnist ebfeadcbf debug control runtim control successfulli reconcil control trainingjob request default mnist charlesa share log track oper pod retriev credenti iam servic unrecognizedclientexcept secur token request trust json http readthedoc stabl oper kubernet html creat iam role trust polici updat cluster region oidc add account charlesa close activ dai open exact pod run setup cluster terraform master node worker node submit trainingjob statu job tri schedul assign pod worker node output ubuntu imvaria repo model train kubectl pod namespac readi statu restart ag kube node tgx run kube node kqz run kube coredn dbc cwfvj run kube coredn dbc xld run kube kube proxi run kube kube proxi rjj run kube metric server cfbd nppx run job oper control fhkvv run ubuntu imvaria repo model train kubectl pod oper control fhkvv job oper control fhkvv namespac job prioriti node comput intern start time fri jun label control plane control pod templat hash annot kubernet psp ek privileg statu run ip control replicaset oper control docker dfcbeadfabfdaaabffacebc imag dkr ecr amazonaw com oper imag docker pullabl dkr ecr amazonaw com oper sha ffbbabfdbfeabbad port host port arg metric addr namespac job state run start fri jun readi restart count limit cpu memori request cpu memori environ default endpoint default region region role arn arn iam role model train role web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount kube api access jrt kube rbac proxi docker ecdaafdcdceaddbffeaaddbaabbd imag gcr kubebuild kube rbac proxi imag docker pullabl gcr kubebuild kube rbac proxi sha dbbbcbabddabdcabaccceefbed port tcp host port tcp arg secur listen address upstream http logtostderr state run start fri jun readi restart count environ default region region role arn arn iam role model train role web ident token file var run secret ek amazonaw com serviceaccount token mount var run secret ek amazonaw com serviceaccount iam token var run secret kubernet serviceaccount kube api access jrt condit type statu initi readi containersreadi podschedul volum iam token type volum inject data multipl sourc tokenexpirationsecond kube api access jrt type volum inject data multipl sourc tokenexpirationsecond configmapnam kube root crt configmapopt downwardapi qo class burstabl node selector toler node kubernet readi noexecut node kubernet unreach noexecut event ubuntu imvaria repo model train kubectl log oper control fhkvv job request throttl request took request http api extens vbeta timeout control runtim metric metric server start listen addr start namespac job setup start control runtim start metric server path metric control start eventsourc reconcilergroup com reconcilerkind endpointconfig control endpointconfig sourc sourc control start eventsourc reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici sourc sourc control start eventsourc reconcilergroup com reconcilerkind model control model sourc sourc control start eventsourc reconcilergroup com reconcilerkind trainingjob control trainingjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind processingjob control processingjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob sourc sourc control start eventsourc reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy sourc sourc control start control reconcilergroup com reconcilerkind model control model control start control reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici control start control reconcilergroup com reconcilerkind endpointconfig control endpointconfig control start control reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob control start control reconcilergroup com reconcilerkind processingjob control processingjob control start control reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob control start control reconcilergroup com reconcilerkind trainingjob control trainingjob control start control reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy control start worker reconcilergroup com reconcilerkind hostingdeploy control hostingdeploy worker count control start worker reconcilergroup com reconcilerkind model control model worker count control start worker reconcilergroup com reconcilerkind endpointconfig control endpointconfig worker count control start worker reconcilergroup com reconcilerkind hostingautoscalingpolici control hostingautoscalingpolici worker count control start worker reconcilergroup com reconcilerkind processingjob control processingjob worker count control start worker reconcilergroup com reconcilerkind batchtransformjob control batchtransformjob worker count control start worker reconcilergroup com reconcilerkind trainingjob control trainingjob worker count control start worker reconcilergroup com reconcilerkind hyperparametertuningjob control hyperparametertuningjob worker count ubuntu imvaria repo model train kubectl trainingjob statu secondari statu creation time job osic test run ubuntu imvaria repo model train kubectl trainingjob osic test run osic test run namespac default label annot api version com trainingjob metadata creation timestamp gener field api version com field type fieldsv fieldsv metadata annot kubectl kubernet appli configur spec algorithmspecif trainingimag traininginputmod inputdataconfig outputdataconfig soutputpath region resourceconfig instancecount instancetyp volumesizeingb rolearn stoppingcondit maxruntimeinsecond trainingjobnam kubectl client appli oper updat time resourc version uid baf spec algorithm train imag dkr ecr amazonaw com model train latest train input mode file input data config channel train compress type data sourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri osic overrid output data config soutputpath osic overrid region resourc config instanc count instanc type xlarg volum size role arn arn iam role model train role stop condit runtim train job osic test run event",
        "Solution_preprocessed_content":"replac input output bucket role arn run output yeah bucket executor role output oper run successfulli cluster verifi step instal oper yeah notic warn failedschedul node schedul pod kubectl node kubectl node statu role ag version readi readi readi yeah recreat cluster previou output cluster worker node base output worker node statu role ag version readi readi readi node oper pod oper deploi successfulli trainingjob run attach trainingjob tri oper pod log share log track oper pod retriev credenti iam servic trust polici updat cluster region oidc add account close activ dai exact pod run setup cluster terraform master node worker node submit trainingjob statu job tri schedul assign pod worker node output",
        "Solution_readability":18.8,
        "Solution_reading_time":403.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":227.0,
        "Solution_word_count":2188.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0682926829,
        "Challenge_watch_issue_ratio":0.0390243902
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### What did you do?\r\n\r\n<!--\r\n-->pip install stepfunctions fails in SageMaker Studio Notebook\r\n\r\nNotebook is using the Python3 (Data Science) kernel.\r\n\r\n\r\n\r\n\r\n### Reproduction Steps\r\n\r\n<!--\r\n--> pip install stepfunctions\r\n\r\n### What did you expect to happen?\r\n\r\n<!--\r\n-->I expected to be able to install AWS stepfunctions.\r\n\r\n### What actually happened?\r\n\r\n<!--\r\n-->\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\nCollecting stepfunctions\r\n  Using cached stepfunctions-2.3.0.tar.gz (67 kB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py egg_info did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [22 lines of output]\r\n      \/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py:760: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\r\n        % (opt, underscore_opt)\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 36, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"\/tmp\/pip-install-a9sl8pu9\/stepfunctions_fec8ededb6d5452993a38c0c5620f20d\/setup.py\", line 70, in <module>\r\n          \"IPython\",\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup\r\n          return distutils.core.setup(**attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup\r\n          _setup_distribution = dist = klass(attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__\r\n          for k, v in attrs.items()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__\r\n          self.finalize_options()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options\r\n          for ep in sorted(loaded, key=by_order):\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in <lambda>\r\n          loaded = map(lambda e: e.load(), filtered)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load\r\n          return functools.reduce(getattr, attrs, module)\r\n      AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\n\r\n### Environment\r\n\r\n  - **AWS Step Functions Data Science Python SDK version  : 2.3.0\r\n  - **Python Version:** <!-- Version of Python (run the command `python3 --version`) --> 3.7\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stack-traces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, slack, etc -->\r\n\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":null,
        "Challenge_created_time":1651588352000,
        "Challenge_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/188",
        "Challenge_link_count":0,
        "Challenge_open_time":4905.4577777778,
        "Challenge_readability":12.3,
        "Challenge_reading_time":42.53,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":78.0,
        "Challenge_repo_issue_count":190.0,
        "Challenge_repo_star_count":244.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":40,
        "Challenge_solved_time":null,
        "Challenge_title":"pip install stepfunctions fails in SageMaker Studio Notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":320,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1105263158,
        "Challenge_watch_issue_ratio":0.0947368421
    },
    {
        "Challenge_adjusted_solved_time":241.2261111111,
        "Challenge_answer_count":1,
        "Challenge_body":"TrainingPipeline needs to be updated to accommodate the `sagemaker.tensorflow.serving.Model` from Tensorflow package.\r\n\r\nRelated Thread: https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1201",
        "Challenge_closed_time":1579559963000,
        "Challenge_created_time":1578691549000,
        "Challenge_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/17",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":20.6,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":21.0,
        "Challenge_repo_fork_count":78.0,
        "Challenge_repo_issue_count":190.0,
        "Challenge_repo_star_count":244.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":241.2261111111,
        "Challenge_title":"Support Tensorflow with the new sagemaker.tensorflow.serving.Model",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":20,
        "Platform":"Github",
        "Solution_body":"This PR https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/1252 fixes the issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"http github com sdk pull",
        "Solution_preprocessed_content":null,
        "Solution_readability":18.6,
        "Solution_reading_time":1.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":6.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1105263158,
        "Challenge_watch_issue_ratio":0.0947368421
    },
    {
        "Challenge_adjusted_solved_time":0.1422222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Challenge_closed_time":1668738306000,
        "Challenge_created_time":1668737794000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":15.87,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":0.1422222222,
        "Challenge_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":160,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":"delet conda environ base environ delet subdirectori folder open fresh notebook space remov conda folder releas space restart runtim reimag studio lab instanc uninstal librari instal creat conda environ",
        "Solution_link_count":0.0,
        "Solution_original_content":"sorri haven research button reset instanc remov conda folder releas space servic glad reset featur vote step run herewith termin open termin stop dashboard open fresh notebook space space run row home studio lab note process free almsot space occupi space ont switch base conda environ conda activ base list conda environ account conda list env conda environ delet conda environ environ base conda remov env delet subdirectori folder home studio lab restart runtim respons yeasin arafat rafio close environ creat space suffici instal packag reproduc clone mlu cours github enabl option creat environ yml file univers acceler tab http github com sampl univers acceler tab yml file creat conda environ cours default cours titl page studio lab test safe bori korotkov reset instanc haven tri cuz figur restart instanc dashboard stop runtim start runtim",
        "Solution_preprocessed_content":"sorri haven research button reset instanc remov conda folder releas space glad reset featur vote step run herewith termin stop dashboard open fresh notebook space note process free almsot space occupi space ont respons close environ creat space suffici instal packag reproduc clone mlu cours github enabl option creat file yml file creat conda environ cours default cours titl page studio lab test safe reset instanc haven tri cuz figur restart instanc dashboard stop runtim start runtim",
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":22.5291666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThe banner message is shown on the top page of Studio Lab.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Studio Lab Top Page\r\n2. The message is shown.\r\n\r\n**Expected behavior**\r\nWe can use the Studio Lab as usual.\r\n\r\nI confirmed the following error.\r\n\r\n* We can start runtime but when clicking \"Open Project\", `ERR_EMPTY_RESPONSE` occurs in the browser.\r\n* When we click the start runtime, \"There was a problem when loading your project. This should be resolved shortly. Please try again later.\" occurred.\r\n\r\n**Screenshots**\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/202335625-de4d1505-97a7-4748-93d5-f0d6b0f5c597.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser Chrome\r\n\r\n**Additional context**\r\n\r\nAs the message suggests, we are working to restore the service. We apologize for any inconvenience.\r\nI'll announce after the service is back. \r\n",
        "Challenge_closed_time":1668731619000,
        "Challenge_created_time":1668650514000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/166",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.0,
        "Challenge_reading_time":13.81,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":22.5291666667,
        "Challenge_title":"Starting 16th Nov 2022 04:00 PM PST, we are experiencing elevated error starting runtimes. The SageMaker Studio Lab team is working to restore the service. We apologize for any inconvenience.",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":153,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":"studio lab team elev wait announc servic",
        "Solution_link_count":0.0,
        "Solution_original_content":"studio lab back patienc close",
        "Solution_preprocessed_content":"studio lab back patienc close",
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":52.6111111111,
        "Challenge_answer_count":4,
        "Challenge_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Challenge_closed_time":1667627477000,
        "Challenge_created_time":1667438077000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.29,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":52.6111111111,
        "Challenge_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":51,
        "Platform":"Github",
        "Solution_body":"up Ah finally btw, I can run the CPU but not GPU today\r\n @saleemmalik10835 Sorry for the long inconvenience of Studio Lab. As you know, the service was back and we confirmed that we can say it to you. I will close this issue because the mentioned problem is solved.\r\n\r\nBut as @aozorahime said, the GPU instance is sometime unavailable because of another instance allocation issue. Of course, we deal with this problem now. ",
        "Solution_gpt_summary":"start runtim api studio lab run cpu gpu runtim unavail gpu instanc instanc alloc address",
        "Solution_link_count":0.0,
        "Solution_original_content":"final btw run cpu gpu todai saleemmalik sorri inconveni studio lab servic close aozorahim said gpu instanc unavail instanc alloc cours deal",
        "Solution_preprocessed_content":"final btw run cpu gpu todai sorri inconveni studio lab servic close said gpu instanc unavail instanc alloc cours deal",
        "Solution_readability":6.7,
        "Solution_reading_time":5.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":74.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":320.9530555556,
        "Challenge_answer_count":5,
        "Challenge_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Challenge_closed_time":1655933766000,
        "Challenge_created_time":1654778335000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":14.06,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":320.9530555556,
        "Challenge_title":"How to get root access in SageMaker Studio Lab",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":"Thank you for trying Studio Lab. Now Studio Lab does not allow `sudo` (similar issue: https:\/\/github.com\/aws\/studio-lab-examples\/issues\/40#issuecomment-1005305538). We can use `pip` and `conda` instead. Please refer the following issue.\r\n\r\nWhat software do you try to install? Some libraries will be available in `conda-forge` . Here is the sample of search.\r\n\r\nhttps:\/\/anaconda.org\/search?q=gym Thanks for the reply, I will try to explain the issue.\r\n I am trying to run bipedal robot from Open-ai gym. \r\n```\r\n!pip install gym\r\n!apt-get update\r\n!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> \/dev\/null\r\n!apt-get install xvfb\r\n!pip install pyvirtualdisplay \r\n!pip -q install pyglet\r\n!pip -q install pyopengl\r\n!apt-get install swig\r\n!pip install box2d box2d-kengz\r\n!pip install pybullet\r\n```\r\nThese are the libraries which I need to install for the code to work. \r\nIt works fine on google-colab: (screenshot below)\r\n![colab_gym_ss](https:\/\/user-images.githubusercontent.com\/91401599\/173034039-1973ee00-6c0b-4f49-adad-9bb324c59b8c.png)\r\n\r\nBut it throws error when I run it on SageMaker Studio Lab: (screenshot below)\r\n![sagemaker1](https:\/\/user-images.githubusercontent.com\/91401599\/173034679-f49340dc-de46-42bb-be1b-7c7e3616dac4.png)\r\n\r\nError Log (I have made gym_install.sh file which installs everything described above, I am running it below.): \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ sh gym_install.sh\r\nRequirement already satisfied: gym in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (0.24.1)\r\nRequirement already satisfied: gym-notices>=0.0.4 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (0.0.7)\r\nRequirement already satisfied: numpy>=1.18.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (1.22.4)\r\nRequirement already satisfied: cloudpickle>=1.2.0 in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (from gym) (2.1.0)\r\nReading package lists... Done\r\nE: List directory \/var\/lib\/apt\/lists\/partial is missing. - Acquire (13: Permission denied)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nRequirement already satisfied: pyvirtualdisplay in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.0)\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\nCollecting box2d\r\n  Using cached Box2D-2.3.2.tar.gz (427 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting box2d-kengz\r\n  Using cached Box2D-kengz-2.3.3.tar.gz (425 kB)\r\n  Preparing metadata (setup.py) ... done\r\nBuilding wheels for collected packages: box2d, box2d-kengz\r\n  Building wheel for box2d (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d\r\n  Running setup.py clean for box2d\r\n  Building wheel for box2d-kengz (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py bdist_wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [16 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running bdist_wheel\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\n  ERROR: Failed building wheel for box2d-kengz\r\n  Running setup.py clean for box2d-kengz\r\nFailed to build box2d box2d-kengz\r\nInstalling collected packages: box2d-kengz, box2d\r\n  Running setup.py install for box2d-kengz ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 Running setup.py install for box2d-kengz did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [18 lines of output]\r\n      Using setuptools (version 62.3.3).\r\n      running install\r\n      \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\r\n        warnings.warn(\r\n      running build\r\n      running build_py\r\n      creating build\r\n      creating build\/lib.linux-x86_64-cpython-310\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      copying library\/Box2D\/Box2D.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\r\n      creating build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      copying library\/Box2D\/b2\/__init__.py -> build\/lib.linux-x86_64-cpython-310\/Box2D\/b2\r\n      running build_ext\r\n      building 'Box2D._Box2D' extension\r\n      swigging Box2D\/Box2D.i to Box2D\/Box2D_wrap.cpp\r\n      swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\/Box2D_wrap.cpp Box2D\/Box2D.i\r\n      error: command 'swig' failed: No such file or directory\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: legacy-install-failure\r\n\r\n\u00d7 Encountered error while trying to install package.\r\n\u2570\u2500> box2d-kengz\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for output from the failure.\r\nRequirement already satisfied: pybullet in \/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages (3.2.5)\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\n### To be specific I am getting error in this line:\r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ apt-get install xvfb\r\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\r\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\r\n```\r\nSo as mentioned by you I tried to find xvfb in conda-forge, but couldn't find it. \r\nThere are some wrapper xvfb on conda-forge but installing them didn't help with the error. \r\n```\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ python check.py\r\nTraceback (most recent call last):\r\n  File \"\/home\/studio-lab-user\/sagemaker-studiolab-notebooks\/GM_\/ARS_src\/check.py\", line 9, in <module>\r\n    display = Display(visible=0, size=(1024, 768))\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/display.py\", line 54, in __init__\r\n    self._obj = cls(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/xvfb.py\", line 44, in __init__\r\n    AbstractDisplay.__init__(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/abstractdisplay.py\", line 85, in __init__\r\n    helptext = get_helptext(program)\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/site-packages\/pyvirtualdisplay\/util.py\", line 13, in get_helptext\r\n    p = subprocess.Popen(\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 966, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"\/home\/studio-lab-user\/.conda\/envs\/AKR_env\/lib\/python3.10\/subprocess.py\", line 1842, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'\r\n(AKR_env) studio-lab-user@default:~\/sagemaker-studiolab-notebooks\/GM_\/ARS_src$ \r\n```\r\nTo reproduce above error run below code (check.py) :->\r\n```\r\nimport os\r\nimport numpy as np\r\nimport gym\r\nfrom gym import wrappers\r\nimport pyvirtualdisplay\r\nfrom pyvirtualdisplay import Display\r\n\r\nif __name__ == \"__main__\":\r\n    display = Display(visible=0, size=(1024, 768))\r\n```\r\n Thank you for sharing the error message. Studio Lab does not allow `apt install` so that the `gym_install.sh` does not work straightly. We have to prepare the environment by `conda` and `pip`.\r\n\r\nAt first we can install `swig` from `conda`. We can not install `xvfb` from `conda`, is this necessary to run the code?  Basically I have to capture the video output using `Display` of `pyvirtualdisplay` library. Everything else works. \r\nAs you can see `display = Display(visible=0, size=(1024, 768))` this line throws error  `FileNotFoundError: [Errno 2] No such file or directory: 'Xvfb'` , so I am trying to install `Xvfb`.\r\n\r\nThanks, @ar8372 Sorry for the late reply. I raised the #124 to work OpenAI Gym in Studio Lab. Please comment to #124 if you have additional information. We need your insight to solve the issue. If you do not mind, please close this issue to suppress the duplication of issues.",
        "Solution_gpt_summary":"studio lab allow sudo pip conda sudo instal librari librari conda forg search librari conda forg instal xvfb apt instal xvfb studio lab allow apt instal instal swig conda xvfb run comment openai gym studio lab addit",
        "Solution_link_count":4.0,
        "Solution_original_content":"studio lab studio lab allow sudo http github com studio lab issuecom pip conda softwar instal librari conda forg sampl search http anaconda org search gym repli explain run biped robot open gym pip instal gym apt updat apt instal xvfb freeglut dev ffmpeg dev null apt instal xvfb pip instal pyvirtualdisplai pip instal pyglet pip instal pyopengl apt instal swig pip instal boxd boxd kengz pip instal pybullet librari instal colab screenshot colab gym http imag githubusercont com adad bbcbc png throw run studio lab screenshot http imag githubusercont com fdc beb cedac png log gym instal file instal run akr env studio lab default studiolab notebook ar src gym instal gym home studio lab conda env akr env lib site packag gym notic home studio lab conda env akr env lib site packag gym numpi home studio lab conda env akr env lib site packag gym cloudpickl home studio lab conda env akr env lib site packag gym read packag list list directori var lib apt list partial miss acquir permiss open lock file var lib dpkg lock frontend open permiss acquir dpkg frontend lock var lib dpkg lock frontend root open lock file var lib dpkg lock frontend open permiss acquir dpkg frontend lock var lib dpkg lock frontend root pyvirtualdisplai home studio lab conda env akr env lib site packag open lock file var lib dpkg lock frontend open permiss acquir dpkg frontend lock var lib dpkg lock frontend root collect boxd cach boxd tar prepar metadata setup collect boxd kengz cach boxd kengz tar prepar metadata setup build wheel collect packag boxd boxd kengz build wheel boxd setup subprocess exit setup bdist wheel run successfulli exit line output setuptool version run bdist wheel run build run build creat build creat build lib linux cpython creat build lib linux cpython boxd copi librari boxd boxd build lib linux cpython boxd copi librari boxd init build lib linux cpython boxd creat build lib linux cpython boxd copi librari boxd init build lib linux cpython boxd run build ext build boxd boxd extens swig boxd boxd boxd boxd wrap cpp swig iboxd small includeal ignoremiss global bglobal outdir librari boxd keyword swig kwarg boxd boxd wrap cpp boxd boxd swig file directori end output note origin subprocess pip build wheel boxd run setup clean boxd build wheel boxd kengz setup subprocess exit setup bdist wheel run successfulli exit line output setuptool version run bdist wheel run build run build creat build creat build lib linux cpython creat build lib linux cpython boxd copi librari boxd init build lib linux cpython boxd copi librari boxd boxd build lib linux cpython boxd creat build lib linux cpython boxd copi librari boxd init build lib linux cpython boxd run build ext build boxd boxd extens swig boxd boxd boxd boxd wrap cpp swig iboxd small includeal ignoremiss global bglobal outdir librari boxd keyword swig kwarg boxd boxd wrap cpp boxd boxd swig file directori end output note origin subprocess pip build wheel boxd kengz run setup clean boxd kengz build boxd boxd kengz instal collect packag boxd kengz boxd run setup instal boxd kengz subprocess exit run setup instal boxd kengz run successfulli exit line output setuptool version run instal home studio lab conda env akr env lib site packag setuptool instal setuptoolsdeprecationwarn setup instal deprec build pip standard base warn warn run build run build creat build creat build lib linux cpython creat build lib linux cpython boxd copi librari boxd init build lib linux cpython boxd copi librari boxd boxd build lib linux cpython boxd creat build lib linux cpython boxd copi librari boxd init build lib linux cpython boxd run build ext build boxd boxd extens swig boxd boxd boxd boxd wrap cpp swig iboxd small includeal ignoremiss global bglobal outdir librari boxd keyword swig kwarg boxd boxd wrap cpp boxd boxd swig file directori end output note origin subprocess pip legaci instal instal packag boxd kengz note packag pip hint output pybullet home studio lab conda env akr env lib site packag akr env studio lab default studiolab notebook ar src line akr env studio lab default studiolab notebook ar src apt instal xvfb open lock file var lib dpkg lock frontend open permiss acquir dpkg frontend lock var lib dpkg lock frontend root tri xvfb conda forg wrapper xvfb conda forg instal akr env studio lab default studiolab notebook ar src traceback file home studio lab studiolab notebook ar src line displai displai visibl size file home studio lab conda env akr env lib site packag pyvirtualdisplai displai line init obj cl file home studio lab conda env akr env lib site packag pyvirtualdisplai xvfb line init abstractdisplai init file home studio lab conda env akr env lib site packag pyvirtualdisplai abstractdisplai line init helptext helptext program file home studio lab conda env akr env lib site packag pyvirtualdisplai util line helptext subprocess popen file home studio lab conda env akr env lib subprocess line init execut child arg execut preexec close fd file home studio lab conda env akr env lib subprocess line execut child rais child except type errno num err msg err filenam filenotfounderror errno file directori xvfb akr env studio lab default studiolab notebook ar src reproduc run import import numpi import gym gym import wrapper import pyvirtualdisplai pyvirtualdisplai import displai displai displai visibl size share messag studio lab allow apt instal gym instal straightli prepar environ conda pip instal swig conda instal xvfb conda run captur video output displai pyvirtualdisplai librari displai displai visibl size line throw filenotfounderror errno file directori xvfb instal xvfb sorri late repli rais openai gym studio lab comment addit close suppress duplic",
        "Solution_preprocessed_content":"studio lab studio lab allow softwar instal librari sampl search repli explain run biped robot gym librari instal throw run studio lab log line tri xvfb wrapper xvfb instal reproduc run share messag studio lab allow straightli prepar environ instal instal run captur video output librari line throw instal sorri late repli rais openai gym studio lab comment addit close suppress duplic",
        "Solution_readability":11.2,
        "Solution_reading_time":134.31,
        "Solution_score_count":2.0,
        "Solution_sentence_count":129.0,
        "Solution_word_count":1049.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":2124.5019444444,
        "Challenge_answer_count":9,
        "Challenge_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Challenge_closed_time":1655626980000,
        "Challenge_created_time":1647978773000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.9,
        "Challenge_reading_time":6.35,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2124.5019444444,
        "Challenge_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"Hey there, I\u2019m not one of the devs sorry\r\n\r\nBut i wanna ask if you can start up a GPU runtime? Kindly Try and let me know thanks @lorazabora  while launching the GPU instance I am getting this as there is no runtime environment available available right now \r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159628994-38b1c339-ac43-4f6a-8ac7-56f32a8174f9.png)\r\n So then indeed everyone is experiencing the same issue\r\n\r\nIt\u2019s been over a week now and not yet fixed, CPU runtimes aren\u2019t enough for my workloads neither that they even make much sense since there\u2019s already many free cloud CPU options out there\r\n\r\nI hope they see and fix this soon @someshfengde Thank you for reporting the problem. Would you please tell us how did you delete the notebooks? Because only delete the specific files does not affect the behavior of Jupyter Lab. We need to know the procedure to reproduce your problem.\r\n\r\nIf you need the Studio Lab as soon as possible, recreate the account is one of the option.\r\n Yes I tried to delete all notebooks from directory maybe because of that\n\nHow can I recreate account?\n\n\nOn Thu, Mar 24, 2022, 13:48 Takahiro Kubo ***@***.***> wrote:\n\n> @someshfengde <https:\/\/github.com\/someshfengde> Thank you for reporting\n> the problem. Would you please tell us how did you delete the notebooks?\n> Because only delete the specific files does not affect the behavior of\n> Jupyter Lab. We need to know the procedure to reproduce your problem.\n>\n> If you need the Studio Lab as soon as possible, recreate the account is\n> one of the option.\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94#issuecomment-1077354727>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AKBFX5JUOPOEUHKEZGXZF53VBQQN3ANCNFSM5RL44VJA>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @someshfengde You can delete the account from here.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/160840123-5f628318-f158-4e40-abba-29524ceb4248.png)\r\n Dear @someshfengde , to delete the account was worked for you? If you still have problem, please let us know. If you already solved the problem, please let us know by closing the this issue. Hi @icoxfog417  I resolved the issue by deleting and opening the account again .\r\n Thanks for your response :smile:  closing issue now :)  You are welcome! We are very glad if Studio Lab supports your data science learning.\r\n",
        "Solution_gpt_summary":"recreat account option delet open account report open databas file save file instal librari pip creat file start kernel",
        "Solution_link_count":5.0,
        "Solution_original_content":"dev sorri wanna start gpu runtim kindli lorazabora launch gpu instanc runtim environ imag http imag githubusercont com faf png experienc week cpu runtim arent workload sens there free cloud cpu option hope soon someshfengd report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option tri delet notebook directori mayb recreat account thu mar takahiro kubo wrote someshfengd report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option repli email directli github unsubscrib receiv messag someshfengd delet account imag http imag githubusercont com abba ceb png dear someshfengd delet account close icoxfog delet open account respons smile close welcom glad studio lab data scienc",
        "Solution_preprocessed_content":"dev sorri wanna start gpu runtim kindli launch gpu instanc runtim environ experienc week cpu runtim arent workload sens there free cloud cpu option hope soon report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option tri delet notebook directori mayb recreat account thu mar takahiro kubo wrote report delet notebook delet file affect jupyt lab procedur reproduc studio lab soon recreat account option repli email directli github unsubscrib receiv delet account dear delet account close delet open account respons smile close welcom glad studio lab data scienc",
        "Solution_readability":7.6,
        "Solution_reading_time":30.5,
        "Solution_score_count":2.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":349.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":6365.2066666667,
        "Challenge_answer_count":5,
        "Challenge_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Challenge_closed_time":1668499115000,
        "Challenge_created_time":1645584371000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":6.0,
        "Challenge_reading_time":8.11,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":6365.2066666667,
        "Challenge_title":"Can't open project on amazon sagemaker",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":91,
        "Platform":"Github",
        "Solution_body":"Not sure if your issue has been resolved or not.\r\nA quick fix is to delete your account and recreate. You will by pass the approval process if you use the same email that has already been approved. @bsaha205 do you still have the problem to open the project? Please let us know about your situation. I'll close the issue. If you have the trouble. please try @MicheleMonclova solution.  Hi @icoxfog417, yes the issue is resolved. Thanks. I am glad to hear that. Please enjoy your ML journey!",
        "Solution_gpt_summary":"delet recreat account bypass approv process",
        "Solution_link_count":0.0,
        "Solution_original_content":"quick delet account recreat pass approv process email approv bsaha open close troubl michelemonclova icoxfog glad enjoi journei",
        "Solution_preprocessed_content":"quick delet account recreat pass approv process email approv open close troubl glad enjoi journei",
        "Solution_readability":2.9,
        "Solution_reading_time":5.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":88.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":3.7888888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nCloning a single notebook using the \"Open In in Sagemaker Studio Lab\" fails.  Cloning the whole repo works.  \r\n\r\nUsing sagemaker's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, I get this error:\r\n```\r\nUnable to copy notebook to project.\r\nThe link to this notebook is broken or blocked. If this is a private GitHub notebook, sign in to GitHub before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb\r\n```\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create MD cell with `[![Open in SageMaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb)`  and run it\r\n2. Click on the button that appears once you run the cell.  Will open new tab in browser\r\n3. In the new pop up tab, click \"Copy to Project\".  Will open new tab in browser\r\n4. In the new pop up tab's modal, select \"Copy Notebook Only\"\r\n5. Error will now appear\r\n\r\n**Expected behavior**\r\nMy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [Windows 11]\r\n - Browser [Chrome]\r\n - Version [97.0.4692.71]\r\n\r\n",
        "Challenge_closed_time":1643319424000,
        "Challenge_created_time":1643305784000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/54",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":19.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":3.7888888889,
        "Challenge_title":"[BUG] \"Open In in Sagemaker Studio Lab\" button process fails when attempting to \"Copy Notebooks Only\"",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":177,
        "Platform":"Github",
        "Solution_body":"Interesting - this works fine on my end, but I'm using a Mac. I'll create a ticket for you. ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"end mac creat ticket",
        "Solution_preprocessed_content":"end mac creat ticket",
        "Solution_readability":3.3,
        "Solution_reading_time":1.07,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":139.3538888889,
        "Challenge_answer_count":5,
        "Challenge_body":"**Describe the bug**\r\nAmazon Sagemaker studio lab is not opening jupyter notebook. It is loading indefinitely at Preparing project run time after that i am getting `There was a problem when starting the project runtime. This should be resolved shortly.` Please try again later. It's been almost a week and it still hasn't been resolved. Even though I tried shifting the runtime from CPU to GPU but issue still persists. Any help would be appreciated.\r\n\r\n**The error i am getting is**\r\n![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1643050102000,
        "Challenge_created_time":1642548428000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/52",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.2,
        "Challenge_reading_time":8.17,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":139.3538888889,
        "Challenge_title":"couldn't open Project on amazon sagemaker studio lab",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":"Hi - sorry you're running into this issue, we're taking a look.  I have also encountered this problem, which still exists at present.\r\n![image](https:\/\/user-images.githubusercontent.com\/37031974\/150719178-b4b46fa6-d912-44e3-a412-f605435d664c.png)\r\n I deleted my account and re-created it, which took around 5 minutes for them to accept my request for the second time. If need instance then this is the only approach I could find. I had not received any updates from this issue after a week, so I recreated my instance.\r\n I see - really sorry you're running into that. I've created a ticket with the team, but I'll close this issue for now since there is a known workaround.  Hello, I can't open project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? Thanks!",
        "Solution_gpt_summary":"delet account creat took minut accept request time workaround creat ticket team workaround",
        "Solution_link_count":1.0,
        "Solution_original_content":"sorri run present imag http imag githubusercont com bbfa fdc png delet account creat took minut accept request time instanc receiv updat week recreat instanc sorri run creat ticket team close workaround open click open button load indefinit file restart browser laptop clear cach tri browser env gpu cpu account",
        "Solution_preprocessed_content":"sorri run present delet account took minut accept request time instanc receiv updat week recreat instanc sorri run creat ticket team close workaround open click open button load indefinit file restart browser laptop clear cach tri browser env gpu cpu account",
        "Solution_readability":6.9,
        "Solution_reading_time":12.88,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":168.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":2.6138888889,
        "Challenge_answer_count":1,
        "Challenge_body":"as the title suggests\r\n",
        "Challenge_closed_time":1641229646000,
        "Challenge_created_time":1641220236000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.0,
        "Challenge_reading_time":0.98,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2.6138888889,
        "Challenge_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":15,
        "Platform":"Github",
        "Solution_body":"Hello - if you have any specific technical issues please use our issue template here to describe it in detail. \r\n\r\nhttps:\/\/github.com\/aws\/studio-lab-examples\/issues\/new?assignees=&labels=bug&template=bug-report-for-sagemaker-studio-lab.md&title=\r\n",
        "Solution_gpt_summary":"direct templat report technic",
        "Solution_link_count":1.0,
        "Solution_original_content":"technic templat http github com studio lab assigne label templat report studio lab titl",
        "Solution_preprocessed_content":null,
        "Solution_readability":26.4,
        "Solution_reading_time":3.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":1.1852777778,
        "Challenge_answer_count":4,
        "Challenge_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Challenge_closed_time":1639666969000,
        "Challenge_created_time":1639662702000,
        "Challenge_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.16,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":88.0,
        "Challenge_repo_issue_count":182.0,
        "Challenge_repo_star_count":300.0,
        "Challenge_repo_watch_count":15.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":1.1852777778,
        "Challenge_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":72,
        "Platform":"Github",
        "Solution_body":"Hi! The use case you are describing is exactly why we have this example notebook:\r\n- https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/connect-to-aws\/Access_AWS_from_Studio_Lab.ipynb \r\n\r\nYour net net is:\r\n1\/ Ensure you have proper training and authorization to use your AWS access and secret keys. If you are an AWS account admin, you can find this in your IAM console. If you are not, work with your admin to determine if this pattern is appropriate for you. \r\n\r\n2\/ If you are qualified to manage your AWS keys, create a new file called `~\/.aws\/credentials`. There, copy and paste in your access and secret keys.\r\n\r\n3\/ Remove any cells you ran in a notebook to create or verify those files.\r\n\r\n4\/ Create a SageMaker execution role. The easiest way to do this is via the console - you can create a new SageMaker execution role when create a notebook instance or a Studio user profile. Once this is done, paste in the arn (Amazon Resource Name), for this execution role.\r\n\r\n5\/ Go forth and scale up on SageMaker! After that,  once you are using the SageMaker Python SDK, you should be able to use all code-based features within SageMaker, such as training, hosting, tuning, autopilot, pipelines, etc.   Hi @EmilyWebber. Might you also have an example that doesn't require AWS account information yet avoids \"ValueError: Must setup local AWS configuration with a region supported by SageMaker\" in the code below for \"role\"?\r\n\r\n```\r\n# create Hugging Face Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\ttransformers_version='4.6.1',\r\n\tpytorch_version='1.7.1',\r\n\tpy_version='py36',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\nCode source: https:\/\/huggingface.co\/distilbert-base-uncased-finetuned-sst-2-english, where Deploy is Amazon SageMaker rather than Accelerated Inference \u2014 the latter [currently returns an error](https:\/\/discuss.huggingface.co\/t\/distilbert-accelerated-inference-error\/15027) with or without SageMaker\r\n\r\nSince the SageMaker Studio Lab website emphasizes no costs and no need to sign up for an AWS account, a team and I are exploring whether to have students try. Thank you in advance. Hi @derekschan - thanks for the comment. This is actually expected behavior right now - Studio Lab defaults to not having access to any AWS API's unless explicitly granted. To date, that is solved only by the pattern mentioned above in this issue, explicitly installing AWS key permissions to utilize them. \r\n\r\nShould that ever change in the future we will be sure to let you know! I'll mark your comment as a feature enhancement.  Thank you, @EmilyWebber.",
        "Solution_gpt_summary":"step notebook configur profil cli built algorithm train author access secret kei qualifi kei creat file call credenti past access secret kei creat execut role past arn execut role sdk base featur train host tune autopilot pipelin account avoid valueerror setup local configur region role",
        "Solution_link_count":3.0,
        "Solution_original_content":"exactli notebook http github com studio lab blob connect access studio lab ipynb net net train author access secret kei account admin iam consol admin determin pattern qualifi kei creat file call credenti copi past access secret kei remov cell ran notebook creat verifi file creat execut role easiest consol creat execut role creat notebook instanc studio profil past arn resourc execut role forth scale sdk base featur train host tune autopilot pipelin emilywebb account avoid valueerror setup local configur region role creat hug model class huggingfac model huggingfacemodel transform version pytorch version version env hub role role sourc http huggingfac distilbert base uncas finetun sst english deploi acceler infer return http huggingfac distilbert acceler infer studio lab websit emphas cost sign account team explor student advanc derekschan comment studio lab default access api explicitli grant date pattern explicitli instal kei permiss util futur mark comment featur enhanc emilywebb",
        "Solution_preprocessed_content":"exactli notebook net net train author access secret kei account admin iam consol admin determin pattern qualifi kei creat file call copi past access secret kei remov cell ran notebook creat verifi file creat execut role easiest consol creat execut role creat notebook instanc studio profil past arn execut role forth scale sdk featur train host tune autopilot pipelin account avoid valueerror setup local configur region role sourc deploi acceler infer studio lab websit emphas cost sign account team explor student advanc comment studio lab default access api explicitli grant date pattern explicitli instal kei permiss util futur mark comment featur enhanc",
        "Solution_readability":9.3,
        "Solution_reading_time":31.28,
        "Solution_score_count":0.0,
        "Solution_sentence_count":24.0,
        "Solution_word_count":372.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0824175824,
        "Challenge_watch_issue_ratio":0.0824175824
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"In Pytorch images all the prints in stderr are not catched and are ignored:\r\n\r\n\r\n### Describe the problem\r\n\r\n### Minimal repro \/ logs\r\nEntrypoint.py:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    import sys\r\n    sys.stderr.write('Coucou stderr')\r\n    sys.stdout.write('Coucou stdout')\r\n```\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nestimator = PyTorch(entry_point='entrypoint.py',\r\n                    role=role,\r\n                    framework_version='1.1.0',\r\n                    train_instance_count=1,\r\n                    train_instance_type='local',\r\n                )\r\nestimator.fit({'config': 's3:\/\/sagemaker-eu-*************\/config\/test_sagemaker_1.json'})\r\n```\r\n\r\n<details><summary>LOGS<\/summary>\r\n<p>\r\n\r\nCreating tmpqp7i_4w3_algo-1-8gd7b_1 ... \r\nAttaching to tmpqp7i_4w3_algo-1-8gd7b_12mdone\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,345 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,349 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,363 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,365 sagemaker_pytorch_container.training INFO     Invoking user training script.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Module entrypoint does not provide a setup.py. \r\nalgo-1-8gd7b_1  | Generating setup.py\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating setup.cfg\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating MANIFEST.in\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,490 sagemaker-containers INFO     Installing module with the following command:\r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m pip install . \r\nalgo-1-8gd7b_1  | Processing \/opt\/ml\/code\r\nalgo-1-8gd7b_1  | Building wheels for collected packages: entrypoint\r\nalgo-1-8gd7b_1  |   Running setup.py bdist_wheel for entrypoint ... done\r\nalgo-1-8gd7b_1  |   Stored in directory: \/tmp\/pip-ephem-wheel-cache-44kbrxy0\/wheels\/35\/24\/16\/37574d11bf9bde50616c******356bc7164af8ca3\r\nalgo-1-8gd7b_1  | Successfully built entrypoint\r\nalgo-1-8gd7b_1  | Installing collected packages: entrypoint\r\nalgo-1-8gd7b_1  | Successfully installed entrypoint-1.0.0\r\nalgo-1-8gd7b_1  | You are using pip version 18.1, however version 19.3.1 is available.\r\nalgo-1-8gd7b_1  | You should consider upgrading via the 'pip install --upgrade pip' command.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,054 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,069 sagemaker-containers INFO     Invoking user script\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Training Env:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | {\r\nalgo-1-8gd7b_1  |     \"additional_framework_parameters\": {},\r\nalgo-1-8gd7b_1  |     \"channel_input_dirs\": {\r\nalgo-1-8gd7b_1  |         \"config\": \"\/opt\/ml\/input\/data\/config\"\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\r\nalgo-1-8gd7b_1  |     \"hosts\": [\r\nalgo-1-8gd7b_1  |         \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |     ],\r\nalgo-1-8gd7b_1  |     \"hyperparameters\": {},\r\nalgo-1-8gd7b_1  |     \"input_config_dir\": \"\/opt\/ml\/input\/config\",\r\nalgo-1-8gd7b_1  |     \"input_data_config\": {\r\nalgo-1-8gd7b_1  |         \"config\": {\r\nalgo-1-8gd7b_1  |             \"TrainingInputMode\": \"File\"\r\nalgo-1-8gd7b_1  |         }\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"input_dir\": \"\/opt\/ml\/input\",\r\nalgo-1-8gd7b_1  |     \"is_master\": true,\r\nalgo-1-8gd7b_1  |     \"job_name\": \"sagemaker-pytorch-2019-10-22-09-06-18-353\",\r\nalgo-1-8gd7b_1  |     \"log_level\": 20,\r\nalgo-1-8gd7b_1  |     \"master_hostname\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"model_dir\": \"\/opt\/ml\/model\",\r\nalgo-1-8gd7b_1  |     \"module_dir\": \"s3:\/\/sagemaker-eu-west-1-*********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\r\nalgo-1-8gd7b_1  |     \"module_name\": \"entrypoint\",\r\nalgo-1-8gd7b_1  |     \"network_interface_name\": \"eth0\",\r\nalgo-1-8gd7b_1  |     \"num_cpus\": 2,\r\nalgo-1-8gd7b_1  |     \"num_gpus\": 0,\r\nalgo-1-8gd7b_1  |     \"output_data_dir\": \"\/opt\/ml\/output\/data\",\r\nalgo-1-8gd7b_1  |     \"output_dir\": \"\/opt\/ml\/output\",\r\nalgo-1-8gd7b_1  |     \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\r\nalgo-1-8gd7b_1  |     \"resource_config\": {\r\nalgo-1-8gd7b_1  |         \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |         \"hosts\": [\r\nalgo-1-8gd7b_1  |             \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |         ]\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"user_entry_point\": \"entrypoint.py\"\r\nalgo-1-8gd7b_1  | }\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Environment variables:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | SM_HOSTS=[\"algo-1-8gd7b\"]\r\nalgo-1-8gd7b_1  | SM_NETWORK_INTERFACE_NAME=eth0\r\nalgo-1-8gd7b_1  | SM_HPS={}\r\nalgo-1-8gd7b_1  | SM_USER_ENTRY_POINT=entrypoint.py\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_PARAMS={}\r\nalgo-1-8gd7b_1  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]}\r\nalgo-1-8gd7b_1  | SM_INPUT_DATA_CONFIG={\"config\":{\"TrainingInputMode\":\"File\"}}\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\r\nalgo-1-8gd7b_1  | SM_CHANNELS=[\"config\"]\r\nalgo-1-8gd7b_1  | SM_CURRENT_HOST=algo-1-8gd7b\r\nalgo-1-8gd7b_1  | SM_MODULE_NAME=entrypoint\r\nalgo-1-8gd7b_1  | SM_LOG_LEVEL=20\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\r\nalgo-1-8gd7b_1  | SM_INPUT_DIR=\/opt\/ml\/input\r\nalgo-1-8gd7b_1  | SM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DIR=\/opt\/ml\/output\r\nalgo-1-8gd7b_1  | SM_NUM_CPUS=2\r\nalgo-1-8gd7b_1  | SM_NUM_GPUS=0\r\nalgo-1-8gd7b_1  | SM_MODEL_DIR=\/opt\/ml\/model\r\nalgo-1-8gd7b_1  | SM_MODULE_DIR=s3:\/\/sagemaker-eu-west-1-***********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\r\nalgo-1-8gd7b_1  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"\/opt\/ml\/input\/data\/config\"},\"current_host\":\"algo-1-8gd7b\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-8gd7b\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"config\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-10-22-09-06-18-353\",\"log_level\":20,\"master_hostname\":\"algo-1-8gd7b\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/sagemaker-eu-west-1-**********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]},\"user_entry_point\":\"entrypoint.py\"}\r\nalgo-1-8gd7b_1  | SM_USER_ARGS=[]\r\nalgo-1-8gd7b_1  | SM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\r\nalgo-1-8gd7b_1  | SM_CHANNEL_CONFIG=\/opt\/ml\/input\/data\/config\r\nalgo-1-8gd7b_1  | PYTHONPATH=\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Invoking script with the following command:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m entrypoint\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Coucou stdout2019-10-22 09:06:23,102 sagemaker-containers INFO     Reporting training SUCCESS\r\ntmpqp7i_4w3_algo-1-8gd7b_1 exited with code 0\r\nAborting on container exit...\r\n===== Job Complete =====\r\n<\/p>\r\n<\/details>\r\n\r\nAs you see the coucou stdout has been printed, stderr has been ignored. In distant mode same result.\r\n\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1571735353000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/issues\/37",
        "Challenge_link_count":0,
        "Challenge_open_time":27086.8463888889,
        "Challenge_readability":25.5,
        "Challenge_reading_time":98.29,
        "Challenge_repo_contributor_count":39.0,
        "Challenge_repo_fork_count":85.0,
        "Challenge_repo_issue_count":167.0,
        "Challenge_repo_star_count":340.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":37,
        "Challenge_solved_time":null,
        "Challenge_title":"Pytorch Sagemaker Container STDERR output",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":426,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.2335329341,
        "Challenge_watch_issue_ratio":0.2275449102
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n`sagemaker-inference` recently (10\/15) released v1.5.3, which included [this commit](https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/commit\/8efb1672798d747cd623e5dd2eb7919af87a1b80) updating the name of the model server artifact and command from `mxnet-model-server` to `multi-model-server`.\r\n\r\nall containers defined in this repository install `sagemaker-inference` as a dependency of this repo itself, on lines\r\n\r\n```dockerfile\r\nRUN pip install --no-cache-dir \"sagemaker-pytorch-inference<2\"\r\n```\r\n\r\nand this repo's `setup.py` has an `install_requires` which includes `sagemaker-inference>=1.3.1`. as a result, `sagemaker-inference=1.5.3` installed.\r\n\r\nso while the `Dockerfile`'s `CMD` value (which calls `mxnet-model-server` directly) will succeed, attempts to use the `ENTRYPOINT` with `serve` as a build arg will fail with message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 22, in <module>\r\n    serving.main()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 39, in main\r\n    _start_model_server()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 49, in wrapped_f\r\n    return Retrying(*dargs, **dkw).call(f, *args, **kw)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 206, in call\r\n    return attempt.get(self._wrap_exception)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 247, in get\r\n    six.reraise(self.value[0], self.value[1], self.value[2])\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 200, in call\r\n    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 35, in _start_model_server\r\n    model_server.start_model_server(handler_service=HANDLER_SERVICE)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/model_server.py\", line 94, in start_model_server\r\n    subprocess.Popen(multi_model_server_cmd)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 709, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 1344, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'multi-model-server': 'multi-model-server'\r\n\r\n```\r\n\r\n**To reproduce**\r\n1. build any container\r\n1. mount a model and `inference.py` (e.g. `half_plus_three`) into `\/opt\/ml\/model`\r\n1. `docker run [tag name] serve`\r\n\r\n**Expected behavior**\r\ntensorflow serving serves the mounted model \/ `inference.py`\r\n\r\n**System information**\r\nA description of your system. Please provide:\r\n- **Toolkit version**: 2.0.5, but should apply to all versions\r\n- **Framework version**: 1.4, but should apply to all versions\r\n- **Python version**: 3.7\r\n- **CPU or GPU**: cpu, but should apply to both\r\n- **Custom Docker image (Y\/N)**: N",
        "Challenge_closed_time":null,
        "Challenge_created_time":1607044885000,
        "Challenge_link":"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/88",
        "Challenge_link_count":1,
        "Challenge_open_time":17278.6430555556,
        "Challenge_readability":13.6,
        "Challenge_reading_time":40.59,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":134.0,
        "Challenge_repo_star_count":88.0,
        "Challenge_repo_watch_count":25.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"renaming of mxnet-model-server in sagemaker-inference package 1.5.3 causing entrypoint with command `serve` to fail",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":287,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.1492537313,
        "Challenge_watch_issue_ratio":0.1865671642
    },
    {
        "Challenge_adjusted_solved_time":790.3911111111,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1662449543000,
        "Challenge_created_time":1659604135000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.42,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":790.3911111111,
        "Challenge_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":98,
        "Platform":"Github",
        "Solution_body":"Already fixed",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.9,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":653.7947222222,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Deploy SWB in hongkong reigon\r\n2. Create a Sagemaker workspace\r\n3. Click \"Stop\" button.\r\n5. workspace status show \"UNKNOWN\"\r\n",
        "Challenge_closed_time":1659958133000,
        "Challenge_created_time":1657604472000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/45",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":653.7947222222,
        "Challenge_title":"[Bug] In HongKong region, After user stop sagemaker workspace manually, web console show \"UNKNOWN\" status",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":54,
        "Platform":"Github",
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/cd99bd2a4f39291e3149b0abbc78ab5b3d650454)",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"commit http github com awslab servic workbench commit cdbdafebabbcabbd",
        "Solution_preprocessed_content":null,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":69.9002777778,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1657702977000,
        "Challenge_created_time":1657451336000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.0,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":38.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":119.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":69.9002777778,
        "Challenge_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":116,
        "Platform":"Github",
        "Solution_body":"Fixed, refer [commit](https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/commit\/2339efe78d0f4705ef0cd6d2b1c5f06a810e6730) ",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"commit http github com awslab servic workbench commit efedfefcddbcfa",
        "Solution_preprocessed_content":null,
        "Solution_readability":52.8,
        "Solution_reading_time":1.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.3193277311,
        "Challenge_watch_issue_ratio":0.1008403361
    },
    {
        "Challenge_adjusted_solved_time":95.0875,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\r\n\r\nGood day.\r\n\r\nCould you add to the Sagemaker section?:\r\n```\r\npip install urllib3==1.24.3\r\npip install PyYAML==3.13\r\npip install ipython\r\n```\r\nFrom https:\/\/medium.com\/@jonathantse\/train-deepracer-model-locally-with-gpu-support-29cce0bdb0f9. \r\n\r\nKeep up the good work!",
        "Challenge_closed_time":1564216116000,
        "Challenge_created_time":1563873801000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/36",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.9,
        "Challenge_reading_time":3.69,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":95.0875,
        "Challenge_title":"Sagemaker dependencies",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":27,
        "Platform":"Github",
        "Solution_body":"I know ipython used to be used because of a couple lines in the python launch file, however it's been removed. Are you able to launch it without ipython?\n\nI'll double check the other two as they should come in through other dependencies. They used to be there but threw errors because of other dependencies requiring different versions What I did was go through your steps line by line on a fresh Ubuntu 18 install. And try and run sagemaker. Might be environment specific to Ubuntu? I\u2019m not too clued up on python and pip. But for sure I had to install those to get it to run.\n\n\n________________________________\nFrom: Chris Rhodes <notifications@github.com>\nSent: Wednesday, July 24, 2019 3:42 AM\nTo: crr0004\/deepracer\nCc: jarrett jordaan; Author\nSubject: Re: [crr0004\/deepracer] Sagemaker dependencies (#36)\n\n\nI know ipython used to be used because of a couple lines in the python launch file, however it's been removed. Are you able to launch it without ipython?\n\nI'll double check the other two as they should come in through other dependencies. They used to be there but threw errors because of other dependencies requiring different versions\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https:\/\/github.com\/crr0004\/deepracer\/issues\/36?email_source=notifications&email_token=AAF52S6N33S3XYV4OLVLAR3QA6XRLA5CNFSM4IGB77KKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2U47FQ#issuecomment-514445206>, or mute the thread<https:\/\/github.com\/notifications\/unsubscribe-auth\/AAF52S3V6KVWPFHAOM5ODA3QA6XRLANCNFSM4IGB77KA>.\n I have updated sagemaker python sdk setup.py file for the dependencies. The errors won't actually do any harm. Just some conflicting version constraints.",
        "Solution_gpt_summary":"instal packag run ipython previous remov doubl packag come depend author thread later updat sdk setup file depend",
        "Solution_link_count":2.0,
        "Solution_original_content":"ipython coupl line launch file remov launch ipython doubl come depend threw depend version step line line fresh ubuntu instal run environ ubuntu clu pip instal run chri rhode sent wednesdai juli crr deeprac jarrett jordaan author subject crr deeprac depend ipython coupl line launch file remov launch ipython doubl come depend threw depend version receiv author thread repli email directli github mute thread updat sdk setup file depend harm conflict version constraint",
        "Solution_preprocessed_content":"ipython coupl line launch file remov launch ipython doubl come depend threw depend version step line line fresh ubuntu instal run environ ubuntu clu pip instal run chri rhode sent wednesdai juli jarrett jordaan author subject depend ipython coupl line launch file remov launch ipython doubl come depend threw depend version receiv author thread repli email directli mute updat sdk file depend harm conflict version constraint",
        "Solution_readability":9.3,
        "Solution_reading_time":22.11,
        "Solution_score_count":0.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":235.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":278.3016666667,
        "Challenge_answer_count":4,
        "Challenge_body":"I mistakenly put my model in pretrained folder but outside the model subfolder. In such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nWhile I did not fix the issue, I added logging to make it verbose. I will try to upload a patch.",
        "Challenge_closed_time":1562359564000,
        "Challenge_created_time":1561357678000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.1,
        "Challenge_reading_time":4.39,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":278.3016666667,
        "Challenge_title":"When pretrained model is not found, sagemaker falls into an infinite silent loop",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":66,
        "Platform":"Github",
        "Solution_body":"```\r\nIndex: rl_coach\/src\/markov\/s3_client.py\r\nIDEA additional info:\r\nSubsystem: com.intellij.openapi.diff.impl.patch.CharsetEP\r\n<+>UTF-8\r\n===================================================================\r\n--- rl_coach\/src\/markov\/s3_client.py\t(revision 789d7553bdfda74d10dcfbcc0c0286fdb0b5b57f)\r\n+++ rl_coach\/src\/markov\/s3_client.py\t(date 1561357411000)\r\n@@ -60,10 +60,12 @@\r\n \r\n     def download_model(self, checkpoint_dir):\r\n         s3_client = self.get_client()\r\n+        logger.info(\"Downloading pretrained model from %s\/%s %s\" % (self.bucket, self.model_checkpoints_prefix, checkpoint_dir))\r\n         filename = \"None\"\r\n         try:\r\n             filename = os.path.abspath(os.path.join(checkpoint_dir, \"checkpoint\"))\r\n             if not os.path.exists(checkpoint_dir):\r\n+                logger.info(\"Model folder %s does not exist, creating\" % filename)\r\n                 os.makedirs(checkpoint_dir)\r\n \r\n             while True:\r\n@@ -73,13 +75,17 @@\r\n                 if \"Contents\" not in response:\r\n                     # If no lock is found, try getting the checkpoint\r\n                     try:\r\n+                        key = self._get_s3_key(\"checkpoint\")\r\n+                        logger.info(\"Downloading %s\" % key)\r\n                         s3_client.download_file(Bucket=self.bucket,\r\n-                                                Key=self._get_s3_key(\"checkpoint\"),\r\n+                                                Key=key,\r\n                                                 Filename=filename)\r\n                     except Exception as e:\r\n+                        logger.info(\"Something went wrong, will retry in 2 seconds %s\" % e)\r\n                         time.sleep(2)\r\n                         continue\r\n                 else:\r\n+                    logger.info(\"Found a lock file, waiting\")\r\n                     time.sleep(2)\r\n                     continue\r\n \r\n@@ -98,6 +104,8 @@\r\n                             filename = os.path.abspath(os.path.join(checkpoint_dir,\r\n                                                                     obj[\"Key\"].replace(self.model_checkpoints_prefix,\r\n                                                                                        \"\")))\r\n+\r\n+                            logger.info(\"Downloading model file %s\" % filename)\r\n                             s3_client.download_file(Bucket=self.bucket,\r\n                                                     Key=obj[\"Key\"],\r\n                                                     Filename=filename)\r\n\r\n``` Sadly, no file upload is available in issues You can open a pull request if you like, do you need help in doing so? It's definitely an issue and I've encountered it myself. It also happens when the checkpoint file references files that don't exist. Thank you @bhannebipro , I lost track :)",
        "Solution_gpt_summary":"log verbos plan upload patch open pull request",
        "Solution_link_count":0.0,
        "Solution_original_content":"index coach src markov client idea addit subsystem com intellij openapi diff impl patch charsetep utf coach src markov client revis dbdfdaddcfbcccfdbbbf coach src markov client date download model checkpoint dir client client logger download pretrain model bucket model checkpoint prefix checkpoint dir filenam filenam path abspath path checkpoint dir checkpoint path checkpoint dir logger model folder creat filenam makedir checkpoint dir respons lock checkpoint kei kei checkpoint logger download kei client download file bucket bucket kei kei checkpoint kei kei filenam filenam except logger went retri time sleep logger lock file wait time sleep filenam path abspath path checkpoint dir obj kei replac model checkpoint prefix logger download model file filenam client download file bucket bucket kei obj kei filenam filenam sadli file upload open pull request definit checkpoint file file bhannebipro lost track",
        "Solution_preprocessed_content":"sadli file upload open pull request definit checkpoint file file lost track",
        "Solution_readability":12.4,
        "Solution_reading_time":24.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":164.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":945.8408333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Steps to reproduce:\r\nI followed instructions in the readme, but instead of `docker pull nabcrr\/sagemaker-rl-tensorflow:console` I did `docker pull nabcrr\/sagemaker-rl-tensorflow:nvidia` and then tagged it as instructed. Before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` I went to that file and commented out the line that Lonon mentioned in #17 \r\n\r\nExpected result:\r\nWhen running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nActual result:\r\n```\r\nalgo-1-vrm2i_1  | ERROR: ld.so: object '\/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | Reporting training FAILURE\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | Traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | ModuleNotFoundError: No module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | No module named 'tensorflow'\r\n```\r\n\r\nSystem info:\r\nUbuntu 18.04.2 LTS\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nMon Jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660M    Off  | 00000000:01:00.0 N\/A |                  N\/A |\r\n| N\/A   46C    P8    N\/A \/  N\/A |    266MiB \/  1999MiB |     N\/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Challenge_closed_time":1564215404000,
        "Challenge_created_time":1560810377000,
        "Challenge_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.7,
        "Challenge_reading_time":41.03,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":105.0,
        "Challenge_repo_issue_count":108.0,
        "Challenge_repo_star_count":236.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":945.8408333333,
        "Challenge_title":"No tensorflow reported when trying to run nvidia image for sagemaker",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":266,
        "Platform":"Github",
        "Solution_body":"Note: adding tensorflow-gpu==1.11.0 and rebuilding the image solves the issue Image has been updated for this",
        "Solution_gpt_summary":"add tensorflow gpu rebuild imag modul tensorflow imag updat accordingli",
        "Solution_link_count":0.0,
        "Solution_original_content":"note tensorflow gpu rebuild imag imag updat",
        "Solution_preprocessed_content":"note rebuild imag imag updat",
        "Solution_readability":6.4,
        "Solution_reading_time":1.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0925925926,
        "Challenge_watch_issue_ratio":0.1481481481
    },
    {
        "Challenge_adjusted_solved_time":4348.4297222222,
        "Challenge_answer_count":6,
        "Challenge_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Challenge_closed_time":1586730089000,
        "Challenge_created_time":1571075742000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":16.4,
        "Challenge_reading_time":3.32,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":54.0,
        "Challenge_repo_issue_count":91.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":4348.4297222222,
        "Challenge_title":"Can not compile SageMaker examples",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":19,
        "Platform":"Github",
        "Solution_body":"This is reported by user and the problem is kubeflow pipeline has some breaking changes on parameters but we always install latest KFP pipeline which is not compatible. \r\n\r\nShort term. use lower kfp version\r\n```\r\n!pip install https:\/\/storage.googleapis.com\/ml-pipeline\/release\/0.1.29\/kfp.tar.gz --upgrade\r\n```\r\n\r\nLong term, update examples and make sure it leverages latest features of KFP.  Will check on the [SageMaker example](https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/blob\/01438d181f502504056eac89bfc0eb091733e9a8\/notebooks\/05_Kubeflow_Pipeline\/05_04_Pipeline_SageMaker.ipynb) and file a PR to make it leverage the latest features of KFP. And the master example of [SageMaker Kubeflow Pipeline](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/samples\/contrib\/aws-samples\/mnist-kmeans-sagemaker), will try to use [master yaml file](https:\/\/github.com\/kubeflow\/pipelines\/tree\/master\/components\/aws\/sagemaker). After so, will try to use latest version 2.05 of kfp to make it compatible. Potential SageMaker example issues with users: [1st](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1401) and [2nd](https:\/\/github.com\/kubeflow\/pipelines\/issues\/1642). But the issue description is not that informative. Will talk with users if necessary. Let's not put time on this one. I will ask SM team to fix Op issue and we can concentrate on others. Since the updated SageMaker example has been merged, let's close this issue.",
        "Solution_gpt_summary":"short term lower version kfp term updat leverag latest featur kfp plan file compat latest version kfp potenti plan updat merg close",
        "Solution_link_count":6.0,
        "Solution_original_content":"report kubeflow pipelin break paramet instal latest kfp pipelin compat short term lower kfp version pip instal http storag googleapi com pipelin releas kfp tar upgrad term updat leverag latest featur kfp http github com sampl ek kubeflow workshop blob dfeacbfcebea notebook kubeflow pipelin pipelin ipynb file leverag latest featur kfp master kubeflow pipelin http github com kubeflow pipelin tree master sampl contrib sampl mnist kmean master yaml file http github com kubeflow pipelin tree master compon latest version kfp compat potenti http github com kubeflow pipelin http github com kubeflow pipelin descript time team concentr updat merg close",
        "Solution_preprocessed_content":"report kubeflow pipelin break paramet instal latest kfp pipelin compat short term lower kfp version term updat leverag latest featur kfp file leverag latest featur kfp master latest version kfp compat potenti descript time team concentr updat merg close",
        "Solution_readability":10.4,
        "Solution_reading_time":18.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":157.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0769230769,
        "Challenge_watch_issue_ratio":0.1098901099
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/blob\/master\/hpo_pytorch_mnist\/hpo_pytorch_mnist.ipynb\r\n\r\nNeed to add \r\n```\r\n!pip install ipywidgets\r\n!jupyter nbextension enable --py widgetsnbextension\r\n```\r\n\r\nWith the command below, SageMaker local mode still show some errors: \r\n```\r\n!pip install docker-compose\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1590122594000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/issues\/32",
        "Challenge_link_count":1,
        "Challenge_open_time":21979.2794444444,
        "Challenge_readability":15.4,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":48.0,
        "Challenge_repo_issue_count":74.0,
        "Challenge_repo_star_count":103.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Error in PyTorch MNIST Notebook with SageMaker Studio ",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":34,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.2027027027,
        "Challenge_watch_issue_ratio":0.2567567568
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb\r\n\r\nResolve by adding `!apt-get install unzip -y`\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1590122435000,
        "Challenge_link":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/issues\/31",
        "Challenge_link_count":1,
        "Challenge_open_time":21979.3236111111,
        "Challenge_readability":22.9,
        "Challenge_reading_time":2.86,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":48.0,
        "Challenge_repo_issue_count":74.0,
        "Challenge_repo_star_count":103.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Error in Autopilot Notebook with SageMaker Studio ",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":15,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.2027027027,
        "Challenge_watch_issue_ratio":0.2567567568
    },
    {
        "Challenge_adjusted_solved_time":122.7663888889,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nWhen making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createModel\" when I run the pipeline from scratch. The error is:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png)\r\n\r\n**To Reproduce**\r\nThe only steps I took was to unfold it as it naturally comes. This bug prevented me from creating a sagemaker model for both the batch and realtime pipelines.\r\n\r\n**Expected behavior**\r\nThe ideal and expected behavior is that this error does not occur and you can create the model.\r\n\r\n**Solution to that moment**\r\nTry to fix the numpy versions issue by re-creating the `sagemaker_layer` layer, via pip installation of the libraries. However, there were conflicts with other modified libraries at the time of `pip install numpy`. For this reason, I had to choose to use the default AWS library that comes with numpy \"AWSLambda-Python38-SciPy1x-v29\". For this, I had to modify the code as follows:\r\n\r\nin deploy_actions.py \/ create_sagemaker_model - I add the layer:\r\n\"arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python38-SciPy1x:29\u201d\r\n\r\nWith this, I stop throwing that error at me. I think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. Please check if it still exists in the new versions.\r\n\r\n**Please complete the following information about the solution:**\r\n- [ ] Version: [e.g. v1.1.0]\r\n\r\n\r\nTo get the version of the solution, you can look at the description of the created CloudFormation stack. For example, \"(SO0136) - AWS MLOps Framework. Version v1.1.0\".\r\n\r\n- [ ] Region: [e.g. us-east-1]\r\n- [ ] Was the solution modified from the version published on this repository? No\r\n- [ ] If the answer to the previous question was yes, are the changes available on GitHub? -\r\n- [ ] Have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses?\r\n- [ ] Were there any errors in the CloudWatch Logs? Yes\r\n\r\n**Additional context**\r\nI am a Solution Architect of an advanced AWS partner company, and we are running a proof of concept with a real client.",
        "Challenge_closed_time":1620839615000,
        "Challenge_created_time":1620397656000,
        "Challenge_link":"https:\/\/github.com\/aws-solutions\/mlops-workload-orchestrator\/issues\/6",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":10.2,
        "Challenge_reading_time":28.62,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":45.0,
        "Challenge_repo_issue_count":23.0,
        "Challenge_repo_star_count":115.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":122.7663888889,
        "Challenge_title":"Error with sagemaker_layer in lambda \"create_sagemakermodel\"",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":321,
        "Platform":"Github",
        "Solution_body":"Please mention if this continues to support you in correcting this error with a pull request. Also to know if it is something from my environment or if it is really a bug in the layer. Greetings to all! Hi @CthompsonCL , thank you for raising this issue. In the newly released version (v1.2.0), the createmodel lambda does not exist anymore. The model is created within a CloudFromation template. So, this issue is resolved in the new release. We will investigate\/fix the build of the sagemaker layer in the previous release.     @CthompsonCL, one more note. if you build the solution locally (i.e, custom build), the sagemaker layer must be built using an Amazon Linux image. Otherwise, you will have the reported error. Perfect! thanks for all. ",
        "Solution_gpt_summary":"newli releas version createmodel lambda anymor model creat cloudform templat team build layer previou releas built local layer built linux imag avoid report",
        "Solution_link_count":0.0,
        "Solution_original_content":"pull request environ layer greet cthompsoncl rais newli releas version createmodel lambda anymor model creat cloudfrom templat releas build layer previou releas cthompsoncl note build local build layer built linux imag report perfect",
        "Solution_preprocessed_content":"pull request environ layer greet rais newli releas version createmodel lambda anymor model creat cloudfrom templat releas build layer previou releas note build local layer built linux imag report perfect",
        "Solution_readability":6.8,
        "Solution_reading_time":9.06,
        "Solution_score_count":1.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":125.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.347826087,
        "Challenge_watch_issue_ratio":0.6956521739
    },
    {
        "Challenge_adjusted_solved_time":1017.9547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Challenge_closed_time":1623053107000,
        "Challenge_created_time":1619388470000,
        "Challenge_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":18.94,
        "Challenge_repo_contributor_count":8.0,
        "Challenge_repo_fork_count":428.0,
        "Challenge_repo_issue_count":16.0,
        "Challenge_repo_star_count":350.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1017.9547222222,
        "Challenge_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":136,
        "Platform":"Github",
        "Solution_body":"Resolved by fixing Sagemaker's version to 1.72.0.",
        "Solution_gpt_summary":"specifi version downgrad version",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":7.2,
        "Solution_reading_time":0.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.5,
        "Challenge_watch_issue_ratio":1.125
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"### What steps did you take\r\n\r\nIf node scales\/up down, the sagemaker component tries to create the same job which fails. Since sagemaker does not let create the same name job. Component controller should be able to detect this and resume the job from existing state. \r\n\r\n### What happened:\r\nthe job hangs\/fail \r\n\r\n### What did you expect to happen:\r\nI expect the job to resume from previous state. \r\n\r\n### Environment:\r\nkfp-1.6\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1639174458000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7040",
        "Challenge_link_count":0,
        "Challenge_open_time":8353.7616666667,
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.62,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] Idempotency in kubeflow pipeline sagemaker component. ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"### What steps did you take\r\n\r\nCode gets stuck in infinite loop is SageMaker training job gets stopped (unhandled use case)\r\n\r\n### What happened:\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/sagemaker_training_component.py#L57-L66\r\n\r\nAbove code only caters for training job status `Completed` or `Failed`, so if the training job status is marked as `Stopped`, it causes an infinite loop in below code\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/sagemaker\/common\/sagemaker_component.py#L197-L201\r\n\r\n### What did you expect to happen:\r\n\r\nTraining job status `stopped` to be catered for\r\n\r\n### Environment:\r\n\r\n### Anything else you would like to add:\r\n\r\n\r\n### Labels\r\n<!-- Please include labels below by uncommenting them to help us better triage issues -->\r\n\r\n<!-- \/area frontend -->\r\n<!-- \/area backend -->\r\n<!-- \/area sdk -->\r\n<!-- \/area testing -->\r\n<!-- \/area samples -->\r\n<!-- \/area components -->\r\n\r\n\r\n---\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1630204381000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6465",
        "Challenge_link_count":2,
        "Challenge_open_time":10845.4497222222,
        "Challenge_readability":11.5,
        "Challenge_reading_time":15.25,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"[bug] Unhandled SageMaker training job status 'stopped' causing infinite loop",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":136,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":947.3408333333,
        "Challenge_answer_count":7,
        "Challenge_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1611093472000,
        "Challenge_created_time":1607683045000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":14.3,
        "Challenge_reading_time":43.94,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":947.3408333333,
        "Challenge_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":304,
        "Platform":"Github",
        "Solution_body":"\/assign @mameshini \r\n\/assign @PatrickXYS \r\n\r\nDo you mind taking a look? Thanks @numerology Thanks!\r\n\r\n@akartsky @RedbackThomson Can you take a look?  Hi @jchaudari, \r\nThanks for reporting the issue, we are taking a look at it. \r\n\r\nThanks,\r\nMeghna Hi @jchaudari, \r\nAre you certain you are using the latest version of the components ? The attached yaml files show that you are using version 0.3.0 of the image which is very old. This feature was added more recently in version 0.9.0 - \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/Changelog.md. \r\n\r\nCould you please try with the newer version and let us know if that fixes your issue ?\r\nThanks,\r\nMeghna Baijal If there aren't any further issues, we'll close this by the end of the week. Otherwise, let us know. \/close @akartsky: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888#issuecomment-763167821):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_gpt_summary":"latest version compon version imag featur updat endpoint version newer version",
        "Solution_link_count":4.0,
        "Solution_original_content":"assign mameshini assign patrickxi numerolog akartski redbackthomson jchaudari report meghna jchaudari latest version compon attach yaml file version imag featur version http github com kubeflow pipelin blob master compon changelog newer version meghna baijal aren close end week close akartski close respons http github com kubeflow pipelin issuecom close instruct interact comment http git commun contributor guid pull request relat file kubernet test infra http github com kubernet test infra titl prow repositori",
        "Solution_preprocessed_content":"assign assign report meghna latest version compon attach yaml file version imag featur version newer version meghna baijal aren close end week close close respons close instruct interact comment relat file repositori",
        "Solution_readability":9.7,
        "Solution_reading_time":16.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":158.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":9822.3969444444,
        "Challenge_answer_count":8,
        "Challenge_body":"### What steps did you take: Removed the HPO and Training Jobs only creating the model and batch transforming in SageMaker \r\n[Automatically taking the HPO and Training on SageMaker facing some issue while kfp compile]\r\n\r\n### What happened: Getting output properly in Kubefow. But I want to to see custom  model (ANY) output without HPO and Model training in Sagemaker\r\n\r\n### What did you expect to happen: Without HPO and batch job in SageMaker\r\n\r\n\r\n\r\n\r\n### Anything else you would like to add:\r\nAny open source loan data model using KF would be appriciated \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\/\/compile(kfp.compile ) \r\n\/\r\n",
        "Challenge_closed_time":1632453568000,
        "Challenge_created_time":1597092939000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.3,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9822.3969444444,
        "Challenge_title":"Want to create ANY model and do batch transform in without Training in Amazon SageMaker ",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"\/assign @Jeffwan @PatrickXYS \r\nlooks like aws specific SageMaker KFP should works fine as regular KFP installed on EKS.\r\n\r\n@RedbackThomson @akartsky Any idea you can share here?  Hi @swarnaditya \r\nThank you for using SageMaker Components for Kubeflow Pipelines. Could you provide more details about the issue:\r\n- are you bringing your own model? Is it uploaded to S3?\r\n- are you bringing your own container or using one of the [pre-built SageMaker algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html)?\r\n- sample reproducible code\r\n\r\nHere is a sample pipeline with only model and batch transform job - https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/tests\/integration_tests\/resources\/definition\/transform_job_pipeline.py\r\n\r\nand here are the sample inputs for this pipeline - https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/tests\/integration_tests\/resources\/config\/kmeans-mnist-batch-transform\/config.yaml#L6-L23. Note: the variables enclosed in `(( ))` are replaced at runtime by our integration tests so you will not be able to run with these inputs directly but we can help you with your use-case\r\n\r\nIn case this helps, here is a customer blog who created a pipeline with model+Hosting - https:\/\/aws.amazon.com\/blogs\/machine-learning\/cisco-uses-amazon-sagemaker-and-kubeflow-to-create-a-hybrid-machine-learning-workflow\/ This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \/assign @surajkota  @RedbackThomson  This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n \/close\r\n\r\nsince there is no updates on the issue @surajkota: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352#issuecomment-926312361):\n\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_gpt_summary":"model upload pre built algorithm sampl pipelin model batch transform job share sampl input pipelin blog share pipelin model host",
        "Solution_link_count":7.0,
        "Solution_original_content":"assign jeffwan patrickxi kfp regular kfp instal ek redbackthomson akartski idea share swarnaditya compon kubeflow pipelin model upload pre built algorithm http doc com latest algo html sampl reproduc sampl pipelin model batch transform job http github com kubeflow pipelin blob master compon test integr test resourc definit transform job pipelin sampl input pipelin http github com kubeflow pipelin blob master compon test integr test resourc config kmean mnist batch transform config yaml note variabl enclos replac runtim integr test run input directli blog creat pipelin model host http com blog cisco kubeflow creat hybrid workflow automat mark stale activ close activ contribut assign surajkota redbackthomson automat mark stale activ close activ contribut close updat surajkota close respons http github com kubeflow pipelin issuecom close instruct interact comment http git commun contributor guid pull request relat file kubernet test infra http github com kubernet test infra titl prow repositori",
        "Solution_preprocessed_content":"assign kfp regular kfp instal ek idea share compon kubeflow pipelin model upload sampl reproduc sampl pipelin model batch transform job sampl input pipelin note variabl enclos replac runtim integr test run input directli blog creat pipelin model host automat mark stale activ close activ contribut assign automat mark stale activ close activ contribut close updat close respons close instruct interact comment relat file repositori",
        "Solution_readability":14.4,
        "Solution_reading_time":30.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":19.0,
        "Solution_word_count":255.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":147.3688888889,
        "Challenge_answer_count":4,
        "Challenge_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Challenge_closed_time":1589522860000,
        "Challenge_created_time":1588992332000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.5,
        "Challenge_reading_time":10.43,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":147.3688888889,
        "Challenge_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":141,
        "Platform":"Github",
        "Solution_body":"Thank you @marwan116  for trying out the operator. Currently SageMaker has support for images hosted in ECR only. \r\nSageMaker has support for various frameworks like TensorFlow, XGBoost, PyTorch etc as well as some [in-built algorithms](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html).\r\n\r\nIf you have custom image, [here](https:\/\/docs.aws.amazon.com\/AmazonECR\/latest\/userguide\/docker-push-ecr-image.html) is the instruction to put them into ECR.\r\n  https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670 @marwan116 looks like question answered, I'm going to close this issue.\r\nBut feel free to reopen with `\/reopen` comment.\r\n\r\n\/close @Bobgy: Closing this issue.\n\n<details>\n\nIn response to [this](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728#issuecomment-629047584):\n\n>@marwan116 looks like question answered, I'm going to close this issue.\r\n>But feel free to reopen with `\/reopen` comment.\r\n>\r\n>\/close\n\n\nInstructions for interacting with me using PR comments are available [here](https:\/\/git.k8s.io\/community\/contributors\/guide\/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes\/test-infra](https:\/\/github.com\/kubernetes\/test-infra\/issues\/new?title=Prow%20issue:) repository.\n<\/details>",
        "Solution_gpt_summary":"imag host ecr imag instruct link ecr",
        "Solution_link_count":6.0,
        "Solution_original_content":"marwan oper imag host ecr framework tensorflow pytorch built algorithm http doc com latest algo html imag http doc com amazonecr latest userguid docker push ecr imag html instruct ecr http github com kubeflow pipelin marwan close free reopen reopen comment close bobgi close respons http github com kubeflow pipelin issuecom marwan close free reopen reopen comment close instruct interact comment http git commun contributor guid pull request relat file kubernet test infra http github com kubernet test infra titl prow repositori",
        "Solution_preprocessed_content":"oper imag host ecr framework tensorflow pytorch imag instruct ecr close free reopen comment close close respons close free reopen comment close instruct interact comment relat file repositori",
        "Solution_readability":13.1,
        "Solution_reading_time":16.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":129.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":280.5022222222,
        "Challenge_answer_count":6,
        "Challenge_body":"### What steps did you take:\r\nI run a custom image using the sagemaker training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/sagemaker\/train\/component.yaml) and it ran fine. I am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nThe problem arises however if inside the custom script I use boto3 to manually download an object from s3 - then I get an error: Unable to locate credentials ...  \r\n\r\n### What happened:\r\nBelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\nINFO:root:Submitting Training Job to SageMaker...\r\nINFO:root:Created Training Job with name: TrainingJob-20200430232331-LPHY\r\nINFO:root:Training job in SageMaker: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/sagemaker\/home?region=us-west-2#\/jobs\/TrainingJob-20200430232331-LPHY\r\nINFO:root:CloudWatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=TrainingJob-20200430232331-LPHY;streamFilter=typeLogStreamPrefix\r\nINFO:root:Job request submitted. Waiting for completion...\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training failed with the following error: AlgorithmError: Exception during training: Unable to locate credentials\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  File \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packag\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  File \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise Exception('Training job failed')\r\nException: Training job failed\r\n```\r\n\r\n### What did you expect to happen:\r\nI would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### Environment:\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nI deployed kubeflow pipelines as part of my kubeflow deployment on AWS EKS:\r\n\r\nKFP version: \r\nBuild commit: 743746b\r\n\r\nKFP SDK version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Challenge_closed_time":1589303399000,
        "Challenge_created_time":1588293591000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.2,
        "Challenge_reading_time":47.45,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":280.5022222222,
        "Challenge_title":"Sagemaker Custom Training Job Error: Unable to locate botocore.credentials",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":390,
        "Platform":"Github",
        "Solution_body":"Thanks Marwan for trying out the component. \r\nI believe your script was buried inside your custom image, if so that custom image runs inside sagemaker which does not inherit the `use_aws_secret` values. So either you need to add permission to the role https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L10 or read it from AWS secret manager. \r\n\r\nWould you mind sharing your script or minimal reproducible code ?  @gautamkmr  - thank you for taking the time to respond to this issue\r\n\r\nPlease find below a very simplified version of the script I'd like to run but hopefully should be good enough to show where the issue is - please note the comments in the script\r\n```\r\nimport pathlib\r\nimport os\r\nimport boto3\r\nimport sys\r\n\r\n\r\ndef main():\r\n    try:\r\n        # Reading data that sagemaker has copied from s3\r\n        # works fine\r\n        prefix = pathlib.Path('\/opt\/ml\/')\r\n        input_path = prefix \/ 'input\/data\/train\/'\r\n\r\n        with open(input_path \/ 'test.txt', 'r') as f:\r\n            content = f.read()\r\n\r\n        assert 'hello world' in content\r\n\r\n        # the below portion is trying to read data from s3\r\n        # using boto3 but it fails\r\n        bucket_name = os.environ['AWS_BUCKET']\r\n        object_name = 'dummy_input\/test.txt'\r\n        file_name = 'test.txt'\r\n\r\n        s3 = boto3.client('s3')\r\n\r\n        # specifically the below line fails:\r\n        # botocore.exceptions.NoCredentialsError: Unable to locate credentials\r\n        s3.download_file(bucket_name, object_name, file_name)\r\n\r\n    except Exception:\r\n        sys.exit(255)\r\n```\r\n\r\nHere is the script for compiling the pipeline just in case you need it\r\n```\r\nimport kfp.compiler as compiler\r\nimport json\r\nimport os\r\nfrom kfp import components, dsl\r\nfrom kfp.aws import use_aws_secret\r\n\r\n\r\n@dsl.pipeline(\r\n    name=\"sm_kfp_example\",\r\n    description=\"sample sagemaker training job\"\r\n)\r\ndef sm_kfp_example():\r\n    bucket = os.environ.get('AWS_BUCKET')\r\n    role = os.environ.get('SAGEMAKER_ROLE_ARN')\r\n\r\n    train_channels = json.dumps([{\r\n        'ChannelName': 'train',\r\n        'DataSource': {\r\n            'S3DataSource': {\r\n                'S3Uri': f's3:\/\/{bucket}\/dummy_input\/',\r\n                'S3DataType': 'S3Prefix',\r\n                'S3DataDistributionType': 'FullyReplicated'\r\n            }\r\n        },\r\n        'ContentType': '',\r\n        'CompressionType': 'None',\r\n        'RecordWrapperType': 'None',\r\n        'InputMode': 'File'\r\n    }])\r\n\r\n    # use the sagemaker training operator defined by aws\r\n    # [a wrapper around Sagemaker CreateTrainingJob]\r\n    repo_path = 'https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines'\r\n    # commit hash of current version of kfp that we are using\r\n    commit = 'master'\r\n    suffix = 'components\/aws\/sagemaker\/train\/component.yaml'\r\n    path = f'{repo_path}\/{commit}\/{suffix}'\r\n\r\n    sagemaker_train_op = components.load_component_from_url(path)\r\n    output_path = f's3:\/\/{bucket}\/output'\r\n    account_id = os.environ.get('AWS_ACCOUNT_ID')\r\n    region = os.environ.get('AWS_REGION')\r\n    image = f'{account_id}.dkr.ecr.{region}.amazonaws.com\/sm_kfp_example:latest'\r\n\r\n    _ = sagemaker_train_op(\r\n        region=region,\r\n        endpoint_url='',\r\n        image=image,\r\n        training_input_mode='File',\r\n        hyperparameters='{}',\r\n        channels=train_channels,\r\n        instance_type='ml.m5.xlarge',\r\n        instance_count='1',\r\n        volume_size='20',\r\n        max_run_time='3600',\r\n        model_artifact_path=output_path,\r\n        output_encryption_key='',\r\n        network_isolation='True',\r\n        traffic_encryption='False',\r\n        spot_instance='False',\r\n        max_wait_time='3600',\r\n        checkpoint_config='{}',\r\n        role=role,\r\n    ).apply(\r\n        use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY')\r\n    )\r\n\r\n\r\ndef compile(pipeline_func):\r\n    pipeline_filename = pipeline_func.__name__ + \".pipeline.tar.gz\"\r\n    compiler.Compiler().compile(pipeline_func, pipeline_filename)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # compile the pipeline\r\n    compile(sm_kfp_example)\r\n```\r\n\r\nThe very strange thing is if I try to create the training job using the sagemaker python sdk (i.e. not using sagemaker's k8s training operator) - the script runs fine - i.e. the credentials are passed down to the container - below is the script in case you need it\r\n\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.estimator import Estimator\r\nimport os\r\n\r\naws_region = os.environ['AWS_REGION']\r\nalgorithm_name = \"sm_kfp_example\"\r\ns3_bucket = os.environ['AWS_BUCKET']\r\n\r\n# use the security token service to verify the account identity\r\nclient = boto3.client('sts')\r\naccount = client.get_caller_identity()['Account']\r\n\r\n# set the sagemaker role\r\nrole = os.environ['SAGEMAKER_ROLE_ARN']\r\n\r\n# create a boto_session\r\nboto_session = boto3.session.Session(\r\n    region_name=aws_region,\r\n)\r\n\r\n# get full training image url\r\ntraining_image = f\"{account}.dkr.ecr.{aws_region}.amazonaws.com\/{algorithm_name}:latest\"\r\n\r\n\r\n# specify location on s3_bucket to output the results\r\ns3_output_location = f's3:\/\/{s3_bucket}\/output'\r\n\r\n# create a sagemaker_session\r\nsagemaker_session = sagemaker.session.Session(boto_session=boto_session)\r\n\r\n# create an estimator\r\nestimator = Estimator(\r\n    training_image,\r\n    role,\r\n    train_instance_count=1,\r\n    train_instance_type='ml.m5.xlarge',\r\n    train_volume_size=10,  # 10 GB\r\n    train_max_run=600,  # 10 minutes = 600seconds\r\n    input_mode='File',\r\n    output_path=s3_output_location,\r\n    sagemaker_session=sagemaker_session,\r\n    hyperparameters={},\r\n    base_job_name=\"sagemaker-sample\",\r\n)\r\n\r\nestimator.fit(\r\n    inputs={\r\n        'train': f's3:\/\/{s3_bucket}\/dummy_input\/'\r\n    },\r\n    logs=True\r\n)\r\n```\r\n Note, to avoid opening other Sagemaker issues, I will list out some of the pain points I have faced trying to integrate sagemaker with Kubeflow here and let me know if there are any solutions to these - excuse me for not following protocol - but if these are deemed as valid issues -I would be glad to open up the relevant issues:\r\n\r\n- I can't seem to pass an image to sagemaker_training_op that is not hosted on ECR and if the image is hosted on ECR - it has to be in the same region as that specified for sagemaker_training_op ... \r\n\r\n- Currently, the sagemaker logs are being output to cloudwatch not to kubeflow (would be much easier if they can be forwarded to kubeflow)\r\n  There has been some undocumented change to the `load_component_*` functions. It used to return a `ContainerOp`, now it returns a `TaskSpec` instead.\r\n\r\nCurrently there are 2 possible workaround:\r\n\r\n1.  use the private func `_create_container_op_from_component_and_arguments` to generate ur containerop from taskspec\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\n\r\ncomponent_op = components.load_component_from_url(...)\r\ntaskspec = component_op(...)\r\ncontainerop = _create_container_op_from_component_and_arguments(\r\n  taskspec.component_ref.spec, \r\n  taskspec.arguments, \r\n  taskspec.component_ref\r\n)\r\n```\r\n\r\n2. overwrite the default `_default_container_task_constructor`\r\n```py\r\nfrom kfp.dsl._component_bridge import _create_container_op_from_component_and_arguments\r\nimport kfp.components._components as _components\r\n\r\n_components._default_container_task_constructor = _create_container_op_from_component_and_arguments\r\n\r\n# now load_component will return a containerop\r\ncontainerop = components.load_component_from_url(...)\r\n```\r\n\r\nPS: @Ark-kun this is not the first time I seen this issue\/qns - what do u think? Hi @marwan116, I was able to reproduce the failure and the root cause is that the default value for `network_isolation` parameter is set to [False in python sdk](https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/bf48fb1219bd8ea22e78a913bfa091e544c57cc3\/src\/sagemaker\/estimator.py#L1112)  whereas in the pipeline definition you provided it is set to True, which is also the [default value](https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/component.yaml#L73-L75) in training component\r\n\r\nCan you try set it to False and let us know if your issue has been resolved ?\r\n\r\n\r\n----\r\n\r\nHere are some clarifications based on posts on this thread: \r\n\r\n- The logs you posted in the issue initially [under whats happened section](https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670#issue-610481941) (except the exception) is from the component pod and NOT from the training job itself. As you have already observed, for the training job logs you need to go to cloudwatch.\r\n  - the first log line which you see `INFO:botocore.credentials:Found credentials in environment variables.` is from boto session which is created by the component backend to call create_training_job API. It uses the credentials are from `aws-secret` that you would have created. These credentials are only used to invoke the job and are not passed to the instance in SageMaker\r\n\r\n- The SageMaker instance which runs in AWS assumes the credentials from the role ARN you provide in`SAGEMAKER_ROLE_ARN` not not from the secret\r\n\r\nLet us know if you have more questions.\r\n @surajkota  - thank you so much for taking the time to reproduce this - yes you are right it is because I had `network_isolation` set to `True` - (sorry I should have taken the time to understand what `network_isolation` does)\r\n\r\nAlso thank you for the clarifications!\r\n\r\nI saw @gautamkmr graciously took the time to open an issue concerning the logs - thank you @gautamkmr !\r\n\r\nI am closing this now as this particular issue is now resolved",
        "Solution_gpt_summary":"add permiss role read secret pass credenti imag train oper run set network isol privat function creat compon argument gener containerop taskspec overwrit default default task constructor return containerop load compon url log initi except compon pod train job train job log cloudwatch instanc run credenti role arn role arn secret",
        "Solution_link_count":5.0,
        "Solution_original_content":"marwan compon believ buri insid imag imag run insid inherit secret valu add permiss role http github com kubeflow pipelin blob master compon train compon yaml read secret share minim reproduc gautamkmr time simplifi version run hopefulli note comment import pathlib import import boto import sy read data copi prefix pathlib path opt input path prefix input data train open input path test txt read assert world portion read data boto bucket environ bucket object dummi input test txt file test txt boto client line botocor except nocredentialserror locat credenti download file bucket object file except sy exit compil pipelin import kfp compil compil import json import kfp import compon dsl kfp import secret dsl pipelin kfp descript sampl train job kfp bucket environ bucket role environ role arn train channel json dump channelnam train datasourc sdatasourc suri bucket dummi input sdatatyp sprefix sdatadistributiontyp fullyrepl contenttyp compressiontyp recordwrappertyp inputmod file train oper defin wrapper createtrainingjob repo path http raw githubusercont com kubeflow pipelin commit hash version kfp commit master suffix compon train compon yaml path repo path commit suffix train compon load compon url path output path bucket output account environ account region environ region imag account dkr ecr region amazonaw com kfp latest train region region endpoint url imag imag train input mode file hyperparamet channel train channel instanc type xlarg instanc count volum size run time model artifact path output path output encrypt kei network isol traffic encrypt spot instanc wait time checkpoint config role role appli secret secret access kei secret access kei compil pipelin func pipelin filenam pipelin func pipelin tar compil compil compil pipelin func pipelin filenam compil pipelin compil kfp creat train job sdk train oper run credenti pass import boto import estim import estim import region environ region algorithm kfp bucket environ bucket secur token servic verifi account ident client boto client st account client caller ident account set role role environ role arn creat boto session boto session boto session session region region train imag url train imag account dkr ecr region amazonaw com algorithm latest specifi locat bucket output output locat bucket output creat session session session session boto session boto session creat estim estim estim train imag role train instanc count train instanc type xlarg train volum size train run minut input mode file output path output locat session session hyperparamet base job sampl estim fit input train bucket dummi input log note avoid open list pain integr kubeflow excus protocol deem glad open relev pass imag train host ecr imag host ecr region specifi train log output cloudwatch kubeflow easier forward kubeflow undocu load compon function return containerop return taskspec workaround privat func creat compon argument gener containerop taskspec kfp dsl compon bridg import creat compon argument compon compon load compon url taskspec compon containerop creat compon argument taskspec compon ref spec taskspec argument taskspec compon ref overwrit default default task constructor kfp dsl compon bridg import creat compon argument import kfp compon compon compon compon default task constructor creat compon argument load compon return containerop containerop compon load compon url ark kun time qn marwan reproduc root default valu network isol paramet set sdk http github com sdk blob bffbbdeaeabfaeccc src estim pipelin definit set default valu http github com kubeflow pipelin blob master compon train compon yaml train compon set clarif base thread log initi what section http github com kubeflow pipelin except compon pod train job observ train job log cloudwatch log line botocor credenti credenti environ variabl boto session creat compon backend creat train job api credenti secret creat credenti invok job pass instanc instanc run credenti role arn role arn secret surajkota time reproduc network isol set sorri taken time network isol clarif gautamkmr gracious took time open log gautamkmr close",
        "Solution_preprocessed_content":"marwan compon believ buri insid imag imag run insid inherit valu add permiss role read secret share minim reproduc time simplifi version run hopefulli note comment compil pipelin creat train job sdk run credenti pass note avoid open list pain integr kubeflow excus protocol deem glad open relev pass imag host ecr imag host ecr region specifi log output cloudwatch kubeflow undocu function return return workaround privat func gener containerop taskspec overwrit default time reproduc root default valu paramet set pipelin definit set train compon set clarif base thread log initi compon pod train job observ train job log cloudwatch log line boto session creat compon backend api credenti creat credenti invok job pass instanc instanc run credenti role arn secret time reproduc set clarif gracious took time open log close",
        "Solution_readability":13.4,
        "Solution_reading_time":112.26,
        "Solution_score_count":2.0,
        "Solution_sentence_count":69.0,
        "Solution_word_count":954.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":1317.2311111111,
        "Challenge_answer_count":13,
        "Challenge_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Challenge_closed_time":1563269566000,
        "Challenge_created_time":1558527534000,
        "Challenge_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":21.6,
        "Challenge_reading_time":18.74,
        "Challenge_repo_contributor_count":326.0,
        "Challenge_repo_fork_count":1350.0,
        "Challenge_repo_issue_count":8555.0,
        "Challenge_repo_star_count":3062.0,
        "Challenge_repo_watch_count":103.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1317.2311111111,
        "Challenge_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":116,
        "Platform":"Github",
        "Solution_body":"Hi @Jeffwan ,\r\n\r\nneed your support on this.\r\n\r\nI am using training image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" and it is throwing an error for mising values for parameters K and feature_dim. Although we are not using these parameters anywhere in pipeline.\r\n\r\nCan you please provide the solution ?\r\n\r\nRegards,\r\nVarun em. I may delete the configuration fields in clean up. Let me double check and come back to you @vackysh  I can reproduce this issue. \r\n\r\n`HyperParameters` was removed by me in this commit\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/commit\/26f2719c28a731d8925ae2ce96252be1df2562aa\r\n\r\nAdd it back will solve this problem Image has been rebuilt and it should be good now.  Hi @Jeffwan ,\r\n\r\nThanks for your response.\r\n\r\nI again executed the pipeline using image \"382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1\" , but getting the same issue\r\n\r\n\"Training failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\"\r\n\r\nThe pipeline parameters are:\r\n\r\n@dsl.pipeline(\r\nname='MNIST Classification pipeline',\r\ndescription='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\nimage='382416733822.dkr.ecr.us-east-1.amazonaws.com\/kmeans:1',\r\ndataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\ninstance_type='ml.c4.8xlarge',\r\ninstance_count='2',\r\nvolume_size='50',\r\nmodel_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\nbatch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\nbatch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\nrole_arn=''\r\n):\r\n\r\nPlease suggest how to get through it if issue has already fixed at your end.\r\n @vackysh I think the problem is your machine already has this image. could you go to the machine and do a force pull? \r\n```\r\nseedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05\r\n``` HI @Jeffwan ,\r\n\r\nI refreshed the image It is now working fine.\r\nThank you so much.\r\n\r\nRegards,\r\nVarun Hi @Jeffwan,\r\n\r\nWhere can i get the actual source code (ML code ) reading from the image seedjeffwan\/kubeflow-pipeline-aws-sm:20190501-05 (not a docker file, but actual logic for train, predction) ?\r\n\r\nI actually working on similar automation and want to analyse the source code.\r\n\r\nRegards,\r\nVarun @vackysh This is a component example for training. \r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/train.py\r\n\r\nNot sure if your work is internal or public. It would be perfect if you can make some contribution! Feel free to ping me on Slack or shoot me an email. Hi @Jeffwan  ,\r\n\r\nThanks for information. But i am looking for the main Kmean algorithms code that is used for training the model. I couldn't find that anywhere on path.\r\n\r\nI have a requirement where Scikit SVM model to get deploy on kubeflow pipeline using aws sagemaker services and S3. So i want to have a look on source ML code that has been passed through image as an input to pipeline.\r\n\r\nRegards,\r\nVarun\r\n\r\n @vackysh Now I get your point, in the example, I am using the container images from SageMaker. I think KMEANS one is first-party models and you might don't have access to it. What I suggest you to do is bring your own training image if you have customization request. This issue is resolved. Now closing > This issue is resolved. Now closing\r\n\r\nHave you figured out a way to make it? Did you try bring your own container? ",
        "Solution_gpt_summary":"remov hyperparamet field commit refresh imag sourc kmean algorithm train model path train imag request",
        "Solution_link_count":2.0,
        "Solution_original_content":"jeffwan train imag dkr ecr amazonaw com kmean throw mise valu paramet featur dim paramet pipelin varun delet configur field clean doubl come vackysh reproduc hyperparamet remov commit http github com kubeflow pipelin commit fcadaecebedfaa add imag rebuilt jeffwan respons execut pipelin imag dkr ecr amazonaw com kmean train clienterror valu specifi featur dim hyperparamet validationerror pipelin paramet dsl pipelin mnist classif pipelin descript mnist classif kmean mnist classif region imag dkr ecr amazonaw com kmean dataset path mnist kmean data instanc type xlarg instanc count volum size model output path mnist kmean model batch transform input mnist kmean input batch transform ouput mnist kmean output role arn end vackysh imag forc pull seedjeffwan kubeflow pipelin jeffwan refresh imag varun jeffwan sourc read imag seedjeffwan kubeflow pipelin docker file logic train predction autom analys sourc varun vackysh compon train http github com kubeflow pipelin blob master compon train src train intern public perfect contribut free ping slack shoot email jeffwan kmean algorithm train model path scikit svm model deploi kubeflow pipelin servic sourc pass imag input pipelin varun vackysh imag kmean parti model access train imag request close close figur",
        "Solution_preprocessed_content":"train imag throw mise valu paramet paramet pipelin varun delet configur field clean doubl come reproduc remov commit add imag rebuilt respons execut pipelin imag train clienterror valu specifi hyperparamet pipelin paramet mnist classif pipelin descript mnist classif kmean end imag forc pull refresh imag varun sourc read imag autom analys sourc varun compon train intern public perfect contribut free ping slack shoot email kmean algorithm train model path scikit svm model deploi kubeflow pipelin servic sourc pass imag input pipelin varun imag kmean model access train imag request close close figur",
        "Solution_readability":9.1,
        "Solution_reading_time":43.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0381063705,
        "Challenge_watch_issue_ratio":0.0120397428
    },
    {
        "Challenge_adjusted_solved_time":7267.8997222222,
        "Challenge_answer_count":3,
        "Challenge_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Challenge_closed_time":1626207887000,
        "Challenge_created_time":1600043448000,
        "Challenge_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.38,
        "Challenge_repo_contributor_count":100.0,
        "Challenge_repo_fork_count":316.0,
        "Challenge_repo_issue_count":2511.0,
        "Challenge_repo_star_count":579.0,
        "Challenge_repo_watch_count":38.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":7267.8997222222,
        "Challenge_title":"[bug] Sagemaker Remote Test reporting issues",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":244,
        "Platform":"Github",
        "Solution_body":"@saimidu mentioned that codebuild runs have a timeout of 90min. However, \r\n- codebuild should have shown status as timed out instead of Failed\r\n- PR commit status should have been failed instead of pending.\r\nSo that's still an open issue. Depends on #444 It appears this issue has been resolved by the PR mentioned above. Closing this ticket out.",
        "Solution_gpt_summary":"remot test log report commit statu codebuild log sai codebuild log print termin abruptli",
        "Solution_link_count":0.0,
        "Solution_original_content":"sidu codebuild run timeout codebuild shown statu time commit statu pend open depend close ticket",
        "Solution_preprocessed_content":"codebuild run timeout codebuild shown statu time commit statu pend open depend close ticket",
        "Solution_readability":5.4,
        "Solution_reading_time":4.17,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":57.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.039824771,
        "Challenge_watch_issue_ratio":0.015133413
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"- [ X] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\nAutogluon 0.4.0 TextPredictor training on p3.8xl 4-GPU instance in Sagemaker Notebook terminal, with `env.num_gpus: 4` setting.  I get an error in spawning multiprocessing.  When I train with everything the same, but only on a single GPU within the same instance and setup, it trains without a problem.\r\n\r\n**Expected behavior**\r\nTrain across all 4 GPUs in the p3.8xl instance with no errors.\r\n\r\n**To Reproduce**\r\n* SageMaker Notebook p3.8xl instance\r\n* python 3.7.12.  \r\n* pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0\r\n* python train.py\r\n\r\nCode:\r\nin `train.py` file\r\n```from argparse import Namespace\r\n\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import TextPredictor\r\n\r\nargs = Namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\npredictor = TextPredictor(\r\n    label='label',\r\n    eval_metric=model_config['eval_metric']\r\n)\r\n\r\npredictor.fit(\r\n    train_data=train_df[model_config['features']],\r\n    hyperparameters=hyperparameters,\r\n    time_limit=model_config['time_limit']\r\n)\r\n\r\n```\r\n\r\n**Screenshots**\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/ec2-user\/SageMaker\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in <module>\r\n    time_limit=model_config['time_limit']\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit\r\n    seed=seed,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit\r\n    enable_progress_bar=self._enable_progress_bar,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit\r\n    ckpt_path=self._ckpt_path,  # this is to resume training that was broken accidentally\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\nIf you are using 0.4.0 and newer, please run the following code snippet:\r\n<details>\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-06\r\ntime                 : 15:22:16.975165\r\npython               : 3.7.12.final.0\r\nOS                   : Linux\r\nOS-release           : 4.14.252-131.483.amzn1.x86_64\r\nVersion              : #1 SMP Mon Nov 1 20:48:11 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 32\r\ncpu_ram_mb           : 245845\r\ncuda version         : 11.450.142.00\r\nnum_gpus             : 4\r\ngpu_ram_mb           : [8404, 8476, 16160, 16160]\r\navail_disk_size_mb   : 11391\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.34\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nmatplotlib           : 3.5.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.21.5\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : None\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorchmetrics         : 0.7.3\r\ntqdm                 : 4.64.0\r\ntransformers         : 4.16.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1649258575000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1650",
        "Challenge_link_count":0,
        "Challenge_open_time":5552.6180555556,
        "Challenge_readability":15.2,
        "Challenge_reading_time":104.31,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Unable to train on multiple GPUs in Sagemaker Notebook Terminal",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":636,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"- [x] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\n```python\r\n...\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\nIt appears that the SageMaker endpoint isn't able to find \/ open the model file. I was able to use the example code in the tutorial [Deploying AutoGluon Models with AWS SageMaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html) and managed to deploy an endpoint to SageMaker. But I get this error when I go to make predictions with test data. I wonder if this might be related to transition from MXNet to Pytorch and how their artifacts are typically stored? I'm using `v0.4.0` but the predictor object is `sagemaker.mxnet.model.MXNetPredictor`. This discrepancy in framework seems supported be a related error that I found in a GitHub issue [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1238).\r\n\r\nNote also that I am attempting to adapt the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset), because I'm ultimately try to deploy a multi-modal model and figure out how to pass image_paths to the SageMaker endpoint. \r\n\r\nHere's the full traceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nModelError                                Traceback (most recent call last)\r\n<ipython-input-51-abf97eb84e0a> in <module>\r\n----> 1 predictions = predictor.predict(test_data[:1].values)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\n    159             data, initial_args, target_model, target_variant, inference_id\r\n    160         )\r\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\r\n    162         return self._handle_response(response)\r\n    163 \r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\r\n    389                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    390             # The \"self\" in this scope is referring to the BaseClient.\r\n--> 391             return self._make_api_call(operation_name, kwargs)\r\n    392 \r\n    393         _api_call.__name__ = str(py_operation_name)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\r\n    717             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\r\n    718             error_class = self.exceptions.from_code(error_code)\r\n--> 719             raise error_class(parsed_response, operation_name)\r\n    720         else:\r\n    721             return parsed_response\r\n\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 110, in transform\r\n    self.validate_and_initialize(model_dir=model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 158, in validate_and_initialize\r\n    self._model = self._model_fn(model_dir)\r\n  File \"\/opt\/ml\/model\/code\/tabular_serve.py\", line 11, in model_fn\r\n    model = TabularPredictor.load(model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2816, in load\r\n    predictor = cls._load(path=path)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2772, in _load\r\n    predictor: TabularPredictor = load_pkl.load(path=path + cls.predictor_file_name)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/common\/loaders\/load_pkl.py\", line 37, in load\r\n    with compression_fn_map[compression_fn]['open'](validated_path, 'rb', **compression_fn_kwargs) as fin:\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**To Reproduce**\r\n\r\n1. Train a multi modal model, using code adapted from [Multimodal Data Tables: Tabular, Text, and Image Tutorial](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html): [petfinder_train.py](https:\/\/gist.github.com\/ijmiller2\/f5837977077674fe741fee031d2bad2a)\r\n2. Deploy the pet finder model: [petfinder_deploy.py](https:\/\/gist.github.com\/ijmiller2\/f6b21c2b0b40211161d1fb0252542189)\r\n\r\n**Screenshots**\r\nNA\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\n`0.4.0`\r\n<details>\r\n\r\n```python\r\n# Replace this code with the output of the following:\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-02\r\ntime                 : 19:45:52.692253\r\npython               : 3.9.7.final.0\r\nOS                   : Linux\r\nOS-release           : 5.4.0-66-generic\r\nVersion              : #74~18.04.2-Ubuntu SMP Fri Feb 5 11:17:31 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 12\r\ncpu_ram_mb           : 64324\r\ncuda version         : None\r\nnum_gpus             : 0\r\ngpu_ram_mb           : []\r\navail_disk_size_mb   : 289302\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.tabular    : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon.vision     : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.21\r\ncatboost             : 1.0.4\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nfastai               : 2.5.3\r\ngluoncv              : 0.11.0\r\nlightgbm             : 3.3.2\r\nmatplotlib           : 3.5.1\r\nnetworkx             : 2.7.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.22.3\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : 1.8.0\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorch                : 1.10.1+cpu\r\ntorchmetrics         : 0.7.2\r\ntqdm                 : 4.63.0\r\ntransformers         : 4.16.2\r\nxgboost              : 1.4.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\n\r\nI am attempting to follow the [tutorial to deploy a model via Sagemaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html), however, adapting to use the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset).\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1648949150000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1644",
        "Challenge_link_count":8,
        "Challenge_open_time":5638.5694444444,
        "Challenge_readability":14.6,
        "Challenge_reading_time":85.63,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":80,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] SageMaker endpoint appears unable to load model file \/ use image paths as features",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":618,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"Has anyone figured out an easy way to make plot_ensemble_model() work in jupyter based environments? I'm having a lot of difficulty installing pygraphviz (think it might be related to pygraphviz not able to see where graphviz is being installed? but not sure)\r\n\r\nI've tried the following code without success: \r\n%pip install python3-dev\r\n%pip install graphviz\r\n%pip install libgraphviz-dev\r\n%pip install pkg-config\r\n\r\n%pip install pygraphviz\r\n\r\n\r\nThanks for the help!",
        "Challenge_closed_time":null,
        "Challenge_created_time":1645398079000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1551",
        "Challenge_link_count":0,
        "Challenge_open_time":6624.9780555556,
        "Challenge_readability":9.8,
        "Challenge_reading_time":6.64,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to make plot_ensemble_model() work in sagemaker (or any jupyter based env)",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":79,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":8976.4125,
        "Challenge_answer_count":2,
        "Challenge_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1613263024000,
        "Challenge_created_time":1580947939000,
        "Challenge_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.4,
        "Challenge_reading_time":192.92,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":675.0,
        "Challenge_repo_issue_count":2354.0,
        "Challenge_repo_star_count":5152.0,
        "Challenge_repo_watch_count":96.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":230,
        "Challenge_solved_time":8976.4125,
        "Challenge_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":999,
        "Platform":"Github",
        "Solution_body":"Thanks for submitting this issue!\r\n\r\nWe haven't looked closely at which boto versions are functional with AutoGluon, but I would suspect using the newer version wouldn't cause issues.\r\n\r\nWe will take a look.\r\n @zhuwenzhen In the latest mainline of AutoGluon, the boto version limitation has been removed. This may resolve your issue if you install AutoGluon from source, or wait for AutoGluon 0.1 to be released.",
        "Solution_gpt_summary":"upgrad newer version boto instal autogluon sourc wait autogluon releas",
        "Solution_link_count":0.0,
        "Solution_original_content":"submit haven close boto version function autogluon newer version zhuwenzhen latest mainlin autogluon boto version limit remov instal autogluon sourc wait autogluon releas",
        "Solution_preprocessed_content":"submit haven close boto version function autogluon newer version latest mainlin autogluon boto version limit remov instal autogluon sourc wait autogluon releas",
        "Solution_readability":7.6,
        "Solution_reading_time":4.98,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":66.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0386576041,
        "Challenge_watch_issue_ratio":0.0407816483
    },
    {
        "Challenge_adjusted_solved_time":10.1325,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Challenge_closed_time":1600271697000,
        "Challenge_created_time":1600235220000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":38.77,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":651.0,
        "Challenge_repo_issue_count":2147.0,
        "Challenge_repo_star_count":3215.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":10.1325,
        "Challenge_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":254,
        "Platform":"Github",
        "Solution_body":"According to [this](https:\/\/github.com\/iterative\/dvc\/issues\/1995), it could be that `enum34` is installed.\r\n\r\nCan you check whether this is also the case here? Thanks @jaheba ! That was the issue and by running `!pip uninstall -y enum34` it is resolved.",
        "Solution_gpt_summary":"instal enum run pip uninstal enum",
        "Solution_link_count":1.0,
        "Solution_original_content":"accord http github com iter enum instal jaheba run pip uninstal enum",
        "Solution_preprocessed_content":null,
        "Solution_readability":4.9,
        "Solution_reading_time":3.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":36.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0423847229,
        "Challenge_watch_issue_ratio":0.032603633
    },
    {
        "Challenge_adjusted_solved_time":11.7994444444,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nUsing Amazon SageMaker on an AWS GPU-instance \"ml.p2.xlarge\", I was not able to run the example `benchmark_m4.py` script (copy\/pasted in SageMaker) on GPU. \r\n\r\n## To Reproduce\r\nAfter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nNext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        DeepAREstimator,\r\n        trainer=Trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on CPU)\r\n\r\n## Error Message\r\n\r\n```\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_quarterly.\r\nINFO:root:Start model training\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_yearly.\r\nINFO:root:Start model training\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[24000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"3M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=8, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='3M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='24000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=8>, train=<gluonts.dataset.common.FileDataset object at 0x7f9377c9e748>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x7f937812ce48>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"MASE\",\r\n     89             \"sMAPE\",\r\n---> 90             \"MSIS\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # When setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": True if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise KeyError(\r\n   1176                     \"None of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nKeyError: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n## Other\r\nIn addition, before installing gluonts (from https:\/\/beta.mxnet.io\/guide\/crash-course\/6-use_gpus.html): \r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n[[1. 1. 1. 1.]\r\n [1. 1. 1. 1.]\r\n [1. 1. 1. 1.]]\r\n<NDArray 3x4 @gpu(0)>\r\n```\r\n\r\nAfter installing gluonts: \r\n\r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nMXNetError                                Traceback (most recent call last)\r\n<ipython-input-16-749bd657d613> in <module>()\r\n      5 \r\n      6 \r\n----> 7 x = nd.ones((3,4), ctx=gpu())\r\n      8 x\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/ndarray.py in ones(shape, ctx, dtype, **kwargs)\r\n   2419     dtype = mx_real_t if dtype is None else dtype\r\n   2420     # pylint: disable= no-member, protected-access\r\n-> 2421     return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)\r\n   2422     # pylint: enable= no-member, protected-access\r\n   2423 \r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/register.py in _ones(shape, ctx, dtype, out, name, **kwargs)\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/_ctypes\/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/base.py in check_call(ret)\r\n    250     \"\"\"\r\n    251     if ret != 0:\r\n--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    253 \r\n    254 \r\n\r\nMXNetError: [22:29:51] src\/imperative\/imperative.cc:79: Operator _ones is not implemented for GPU.\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x9fb) [0x7f9397bb2abb]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f93d5b04e2e]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x12865) [0x7f93d5b05865]\r\n```\r\n\r\n## Environment\r\n\r\n- Amazon SageMaker, running on AWS instance \"ml.p2.xlarge\". \r\n- GluonTS version: 0.3.3 installed using pip.\r\n- Kernel: conda_mxnet_p36 \r\n\r\n",
        "Challenge_closed_time":1573208758000,
        "Challenge_created_time":1573166280000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/426",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":24.8,
        "Challenge_reading_time":190.21,
        "Challenge_repo_contributor_count":91.0,
        "Challenge_repo_fork_count":651.0,
        "Challenge_repo_issue_count":2147.0,
        "Challenge_repo_star_count":3215.0,
        "Challenge_repo_watch_count":70.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":81,
        "Challenge_solved_time":11.7994444444,
        "Challenge_title":"Problems using GPU with Amazon SageMaker on AWS-instance \"ml.p2.xlarge\".",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":785,
        "Platform":"Github",
        "Solution_body":"After installing GluonTS, could you try running in a cell \r\n\r\n```\r\n!pip show mxnet\r\n```\r\n\r\nand report the result? `!pip show mxnet` results in:\r\n\r\n```python\r\nName: mxnet\r\nVersion: 1.4.1\r\nSummary: MXNet is an ultra-scalable deep learning framework. This version uses openblas.\r\nHome-page: https:\/\/github.com\/apache\/incubator-mxnet\r\nAuthor: UNKNOWN\r\nAuthor-email: UNKNOWN\r\nLicense: Apache 2.0\r\nLocation: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\r\nRequires: numpy, graphviz, requests\r\nRequired-by: gluonts\r\nYou are using pip version 10.0.1, however version 19.3.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```\r\n\r\n`!pip install mxnet --upgrade` led to conflicts with gluonts and numpy and the same error message with `GPU is not enabled`. \r\n @tm1611 hopefully this is solved with the upcoming `0.4` release (to be relased very soon), since #428 was merged. I think the problem is that with `gluonts<0.4` the vanilla mxnet package gets installed and replaces the one with built-in cuda for GPU processing. @tm1611 can you check if the problem is gone now when you install GluonTS from scratch on your instance? Yes, the problem with mxnet and dependencies was removed. Thanks a lot for the quick fix. \r\n\r\nUnrelated to this issue, but related to `m4_benchmark.py` and the sake of completeness: Using gluonts-version 0.4., one has to change `num_eval_samples` to `num_sampels` (see #421 ). ",
        "Solution_gpt_summary":"upgrad gluont version mxnet depend complet benchmark gluont version num eval sampl num sampl",
        "Solution_link_count":1.0,
        "Solution_original_content":"instal gluont run cell pip mxnet report pip mxnet mxnet version summari mxnet ultra scalabl framework version openbla home page http github com apach incub mxnet author unknown author email unknown licens apach locat home anaconda env mxnet lib site packag numpi graphviz request gluont pip version version upgrad pip instal upgrad pip pip instal mxnet upgrad led conflict gluont numpi messag gpu enabl hopefulli releas relas soon merg gluont vanilla mxnet packag instal replac built cuda gpu process gone instal gluont scratch instanc mxnet depend remov quick unrel relat benchmark sake complet gluont version num eval sampl num sampel",
        "Solution_preprocessed_content":"instal gluont run cell report led conflict gluont numpi messag hopefulli releas merg vanilla mxnet packag instal replac cuda gpu process gone instal gluont scratch instanc mxnet depend remov quick unrel relat sake complet",
        "Solution_readability":5.7,
        "Solution_reading_time":17.88,
        "Solution_score_count":2.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":202.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0423847229,
        "Challenge_watch_issue_ratio":0.032603633
    },
    {
        "Challenge_adjusted_solved_time":21.5513888889,
        "Challenge_answer_count":5,
        "Challenge_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Challenge_closed_time":1585791347000,
        "Challenge_created_time":1585713762000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Challenge_link_count":35,
        "Challenge_open_time":null,
        "Challenge_readability":22.7,
        "Challenge_reading_time":580.3,
        "Challenge_repo_contributor_count":164.0,
        "Challenge_repo_fork_count":740.0,
        "Challenge_repo_issue_count":1942.0,
        "Challenge_repo_star_count":7884.0,
        "Challenge_repo_watch_count":102.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":434,
        "Challenge_solved_time":21.5513888889,
        "Challenge_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":1973,
        "Platform":"Github",
        "Solution_body":"Kedro aside there are a couple of things that you can do to ensure that your environments match from the terminal vs notebook.  I am not familiar with the new `pandas.CSVDataSet` as I am just now starting with my first `0.15.8` myself.  We have struggled to get package installs correct through our notebooks, I make sure my team is all using their own environment, created from the terminal.\r\n\r\n## activate python3 from the terminal before install\r\n\r\nNote that the file browser on the left hand side of a SageMaker notebook is really mounted at `~\/SageMaker`.\r\n\r\n``` bash\r\nsource activate python3\r\n# may also be - conda activate python3\r\n# unrelated on windows it was - activate python 3\r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n## install ipykernel in your terminal env\r\n\r\nFor conda environments to show up in the notebook dropdown selection you will need `ipykernel` installed. see [docs](https:\/\/ipython.readthedocs.io\/en\/stable\/install\/kernel_install.html)\r\n\r\n```\r\nconda create -n testing python=3.6\r\npip install ipykernel\r\n# I typically don't have to go this far, but installing ipykernel is recommended by the docs\r\nipykernel install --user \r\ncd ~\/SageMaker\/testing\/notebooks # this appears to be where your project is\r\nkedro install\r\n```\r\n\r\n\r\nDo note that if you shut down your SageMaker notebook you will loose your packages and environments by default.\r\n\r\nI also noticed that you have a difference between pandas.  I have no idea if that changes things, but might be a simple fix. Your second idea worked @WaylonWalker. I slightly adapted it as it didn't work straight up:\r\n```\r\nconda create --yes --name kedroenv python=3.6 ipykernel\r\nsource activate kedroenv\r\npython -m ipykernel install --user --name kedroenv --display-name \"Kedro py3.6\"\r\n\r\ncd ~\/Sagemaker\r\nkedro new # Name testing and example pipeline\r\ncd testing\/\r\nkedro run\r\n```\r\nWith a reasonable solution, I'll call this issue closed. Massive thank you @WaylonWalker for pointing me in the right direction.\r\n\r\nCheers,\r\nTom @tjcuddihy We're working with the AWS team to produce a knowledge document on using Kedro and Sagemaker. Would we be able to talk to you about how you used them together? I'd be keen on learning more about how to make Sagemaker play nicely with kedro so I can still access everything I need from my kedro context. @yetudada I have an alpha version of a kedro plugin that plays nicely with sagemaker and allows you to run processing jobs. @uwaisiqbal then you might be interested in this knowledge article that was just published on AWS: https:\/\/aws.amazon.com\/blogs\/opensource\/using-kedro-pipelines-to-train-amazon-sagemaker-models\/ \ud83d\ude80 ",
        "Solution_gpt_summary":"activ termin instal packag instal ipykernel termin environ creat conda environ ipykernel instal run alpha version plugin",
        "Solution_link_count":2.0,
        "Solution_original_content":"asid coupl environ match termin notebook familiar panda csvdataset start packag instal notebook team environ creat termin activ termin instal note file browser left hand notebook mount bash sourc activ conda activ unrel window activ test notebook instal instal ipykernel termin env conda environ notebook dropdown select ipykernel instal doc http ipython readthedoc stabl instal kernel instal html conda creat test pip instal ipykernel typic instal ipykernel doc ipykernel instal test notebook instal note shut notebook loos packag environ default notic panda idea idea waylonwalk slightli adapt straight conda creat env ipykernel sourc activ env ipykernel instal env displai test pipelin test run reason close massiv waylonwalk direct cheer tom tjcuddihi team produc knowledg document keen plai nice access context yetudada alpha version plugin plai nice allow run process job uwaisiqb knowledg articl publish http com blog opensourc pipelin train model",
        "Solution_preprocessed_content":"asid coupl environ match termin notebook familiar start packag instal notebook team environ creat termin activ termin instal note file browser left hand notebook mount instal ipykernel termin env conda environ notebook dropdown select instal note shut notebook loos packag environ default notic panda idea idea slightli adapt straight reason close massiv direct cheer tom team produc knowledg document keen plai nice access context alpha version plugin plai nice allow run process job knowledg articl publish",
        "Solution_readability":11.1,
        "Solution_reading_time":32.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":397.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0844490216,
        "Challenge_watch_issue_ratio":0.052523172
    },
    {
        "Challenge_adjusted_solved_time":992.0369444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1623682749000,
        "Challenge_created_time":1620111416000,
        "Challenge_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.4,
        "Challenge_reading_time":7.39,
        "Challenge_repo_contributor_count":49.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":1491.0,
        "Challenge_repo_star_count":115.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":992.0369444444,
        "Challenge_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0328638498,
        "Challenge_watch_issue_ratio":0.0087189805
    },
    {
        "Challenge_adjusted_solved_time":15.6105555556,
        "Challenge_answer_count":2,
        "Challenge_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1657782683000,
        "Challenge_created_time":1657726485000,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":17.1,
        "Challenge_reading_time":154.35,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2570.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":100,
        "Challenge_solved_time":15.6105555556,
        "Challenge_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":829,
        "Platform":"Github",
        "Solution_body":"Hi @danguitavinas,\r\n\r\nI'm guessing from the stack trace that you're running on windows with the local orchestrator? If that's the case, my guess is that this issue should be fixed by #735.\r\n\r\nIf you're interested in trying this, you could install ZenML from that branch using the command `pip install git+https:\/\/github.com\/zenml-io\/zenml.git@bugfix\/windows-source-utils` @schustmi Thank you so much, that worked! Im closing the issue!",
        "Solution_gpt_summary":"instal zenml branch pip instal git http github com zenml zenml git bugfix window sourc util close",
        "Solution_link_count":1.0,
        "Solution_original_content":"danguitavina guess stack trace run window local orchestr guess instal zenml branch pip instal git http github com zenml zenml git bugfix window sourc util schustmi close",
        "Solution_preprocessed_content":"guess stack trace run window local orchestr guess instal zenml branch close",
        "Solution_readability":6.9,
        "Solution_reading_time":5.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":62.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0482758621,
        "Challenge_watch_issue_ratio":0.0318965517
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":24,
        "Challenge_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids, but the existing studies don't typically change.\r\n\r\nWhat could cause the fs role number to change for a study? What else could cause this permissions denied error? \r\n\r\nThis is a pretty big problem for us, as we have had people actively using SWB and all their work is gone on Sagemaker stop, because the folder they saved to isn't syncing.\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1668634688000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067",
        "Challenge_link_count":0,
        "Challenge_open_time":170.3644444444,
        "Challenge_readability":15.2,
        "Challenge_reading_time":29.34,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"[Bug] SWB Sagemaker Study permission denied",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":276,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":0.4308333333,
        "Challenge_answer_count":1,
        "Challenge_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids.\r\n\r\nMy question is: What could cause the fs role number to change for a study?\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1668634670000,
        "Challenge_created_time":1668633119000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1066",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":16.9,
        "Challenge_reading_time":26.21,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.4308333333,
        "Challenge_title":"[Bug] SWB Sagemaker Study permission denied",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":231,
        "Platform":"Github",
        "Solution_body":"I'm elevating this to a bug.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":4.5,
        "Solution_reading_time":0.34,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":720.8744444444,
        "Challenge_answer_count":16,
        "Challenge_body":"**Describe the bug**\r\nWe encountered an interesting issue regarding the auto stop script. We had no code changes, but suddenly, Sagemaker instances started hanging around for days, with no use. Looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. When I look at the script, it has this line `print(f'Notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. However, the file on the repo, as well as the s3 bucket, does not contain this line. So, after some digging, I found that this line was introduced here, in this commit [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). What I don't understand is how it got into the Sagemaker notebook, and why it's not being overridden by the custom config start we have here [sagemaker-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/sagemaker-notebook-instance.cfn.yml#L264-L272) This script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**To Reproduce**\r\nLaunch a Sagemaker instance. You can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nThe AWS version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L96-L100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n```\r\nAnd on the `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L96-L101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n    print('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**Expected behavior**\r\nThe autostop script in the s3 bucket should be the one used for SWB Sagemaker instances.\r\n\r\n**Screenshots**\r\n<img width=\"1510\" alt=\"Screen Shot 2022-11-16 at 12 14 21 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Challenge_closed_time":1671215663000,
        "Challenge_created_time":1668620515000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":14.7,
        "Challenge_reading_time":36.84,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":720.8744444444,
        "Challenge_title":"[Bug] Sagemaker autostop script not pulling from s3 bucket",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":279,
        "Platform":"Github",
        "Solution_body":"Just want to verify that the autostop script in your bucket had not been updated at some point in the past unexpectedly. Is that correct? Yes-- none of the files on the s3 bucket were changed in several months. I also downloaded the autostop script from the bucket to verify manually that it matches the SWB repo version. I was not able to replicate this in v5.2.2:\r\n<img width=\"937\" alt=\"Screen Shot 2022-11-30 at 3 34 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903064-013c1899-2763-4c88-8cce-7b39128b0240.png\">\r\n\r\nIf you look at the CloudWatch log group \/aws\/sagemaker\/NotebookInstances\/BasicNotebookInstance-<id>\/LifecycleConfigOnStart do you see the following output (would be towards the end):\r\n<img width=\"960\" alt=\"Screen Shot 2022-11-30 at 3 36 08 PM\" src=\"https:\/\/user-images.githubusercontent.com\/43092418\/204903303-d180144c-62d9-4775-a233-83687012715b.png\">\r\nThis is triggered on start of the instance. The screenshot you posted was from your instance, correct? Do you see the lines, \r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n``` \r\nthis line comes from this repo [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L100-L101). It should be only the one line, like below\r\n```\r\nidle = False\r\n``` \r\nwhich comes from SWB here [awslabs\/service-workbench-on-aws](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L100)\r\n\r\nThe s3 file says it's downloaded, but for whatever reason it's not using that downloaded file, and instead using the file on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples. This introduces SWB to vulnerabilities when this code is changes and a bug is introduced in that repo. SWB should instead use the autostop script that it has saved in s3, because that is locked and changes that aren't intended wouldn't be introduced. The screenshot is from my instance, yes. SWB repo contains the lines:\r\n```\r\nprint('Notebook is not idle:', notebook['kernel']['execution_state'])\r\nidle = False\r\n```\r\n[here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/61200d06d1a607b9c0a209240813b261ade2c5e9\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L105). It is the lines:\r\n```\r\nidle = False\r\nprint('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\nthat I thought you said were presenting the problem (that are in the samples repo but not SWB). Is that correct?\r\n\r\nHowever, I see that my instance is not stopping even though the autostop script is the same as the SWB repo.\r\n\r\nWhere did you see the error on the cron job for the autostop? Also, to clarify, you see that the Cloudwatch logs copy from the s3 bucket to local and that the s3 bucket file is the correct file? Yet, you see the wrong file when you less the file on the instance? Oh, you're right I was looking at the wrong line. My apologies! \r\n\r\nYes- for us it's successfully copying the correct file from s3 (which I downloaded to verify), but the autostop script (`\/usr\/local\/bin\/autostop.py`) is the wrong copy. Got it. And where do you see the cron job failing? Because it was not overwriting the autostop script with the one from our s3 bucket, and instead defaulting to the script from `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples`,  when the repo owners introduced this commit: https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e, the cron job failed on lines like `print(f'Notebook idle state set as {idle} since kernel connections are ignored.')`, stating that the `print(f` part was invalid syntax.\r\n\r\nI guess the key issue is really why it didn't overwrite this default script with the s3 one, considering it successfully downloaded the one from s3. Secondly, it seems odd that this repo is somehow the default autostop script that the Sagemaker system uses. This introduces bugs if there's a failure in that script or a malicious commit. Yes, I see the problem in the other repo's commit. I am still trying to debug how the script is on your Sagemaker instance.\r\n\r\nGot ~two~ three more questions:\r\n1. Where _in you account_ did you see that error message from the cron job? CloudWatch logs? Sagemaker? etc.\r\n2. Are you working with AppStream-enabled SWB? Does Sagemaker have to go through AppStream to connect?\r\n3. What is the output from running this command in a terminal on the sagemaker instance: `\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 300 --ignore-connections`? Sure. :D Yeah I don't know how the script was there automatically. I didn't see any code in our repo that would cause it to pull from there.\r\n\r\nWe do not have appstream enabled, but we do send traffic through a proxy lambda as well as a firewall instance. However, all the environment files that get downloaded for the bootstrap process were successful, so I don't think it was a network issue.\r\n\r\nThe reason I checked the autostop script was because I saw no note in the `\/var\/log\/autostop.log` file that the script is sent to via cron job. So I ran the script by hand.\r\n\r\nFor instance, right now autostop is not working. There's no messaging that tells you it's not working. When you look at the cron logs it shows this, with no errors. The `\/var\/log\/autostop.log` script doesn't show any messages or errors.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# grep autostop \/var\/log\/cron | tail -n 1\r\nDec  1 18:14:01 ip-10-10-57-235 CROND[9860]: (root) CMD (\/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600 --ignore-connections >> \/var\/log\/autostop.log)\r\n```\r\n\r\nBut if you run the autostop script exactly as the cron job has it, like below, you get an error \/right now\/ related to a boto3 import issue.\r\n```\r\n[root@ip-10-10-57-235 ec2-user]# \/usr\/bin\/python \/usr\/local\/bin\/autostop.py --time 3600\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/autostop.py\", line 18, in <module>\r\n    import boto3\r\nImportError: No module named boto3\r\n```\r\n\r\nBoto3 changes were introduced in a commit here, [in the on-start script on aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/13b4023c9dca45fea58b2129fe5848619284653a#diff-54051e148aa00ee3fa158cc346d6c243418d14718a6760171ef562887977748f) Yup, so I also get that problem when I try to invoke the autostop script (and my autostop script matches the SWB one). I think that is the root cause of this problem. I will add a backlog item to figure out why boto3 is not being imported correctly so that sagemaker notebooks can use them for autostop. \r\n\r\nIt still does not explain why you got the amazon-sagemaker-notebook-instance-lifecycle-config-samples in the instance. Was that only present in one instance or all instances? Is it possible someone manually changed the files when trying to debug the autostop not working?\r\n\r\nThanks so much for working through this with me! Yeah, no problem-- thanks for your patience and attention! :D\r\n\r\nI don't believe that the files have been altered. I am currently the only person on my team actively responsible for doing admin\/infrastructure activities for these systems. I've tried this on new instances to rule out individual Sagemaker systems changes by users. We have two different version of SWB deployed-- maybe there's a difference between versions.  @srpiatt please upgrade your SWB installation to the latest release [v5.2.5](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v5.2.5). Sagemaker made a change that caused all new instances to be spun up with the AL2 operating system. New Sagemaker instances will no longer be able to mount studies or autostop without the fix in v5.2.5 Hi! I also want to note that you may need to stop and start any affected instances after upgrade and deploying SWB v5.2.5.\r\n\r\nIf this fixes your issue, please go ahead and close this issue. I am going to mark as closing-soon-if-no-response so we will close in about 7 days if we do not hear that this did not resolve the issue.\r\n\r\nThank you for the report! I pulled the changes committed for v5.2.5 into our forked repository, which is locked at 5.0.0 version. Auto stop works. Still not sure how the file got replaced with the one in that repo, but it's a non-issue at the moment. Thank you! :D",
        "Solution_gpt_summary":"syntax autostop present file repo bucket introduc commit unclear notebook overridden config start autostop bucket swb instanc upgrad swb instal latest releas instanc spun oper stop start affect instanc upgrad deploi swb",
        "Solution_link_count":8.0,
        "Solution_original_content":"verifi autostop bucket updat past unexpectedli file bucket month download autostop bucket verifi manual match swb repo version replic cloudwatch log group notebookinst basicnotebookinst lifecycleconfigonstart output end trigger start instanc screenshot instanc line print notebook idl notebook kernel execut state idl line come repo sampl notebook instanc lifecycl config sampl http github com sampl notebook instanc lifecycl config sampl blob master auto stop idl autostop line idl come swb awslab servic workbench http github com awslab servic workbench blob mainlin deploy config environ file offlin packag autostop file sai download reason download file file sampl notebook instanc lifecycl config sampl introduc swb vulner introduc repo swb autostop save lock aren intend introduc screenshot instanc swb repo line print notebook idl notebook kernel execut state idl http github com awslab servic workbench blob ddabcabadec deploy config environ file offlin packag autostop line idl print notebook idl state set kernel detect idl said present sampl repo swb instanc stop autostop swb repo cron job autostop clarifi cloudwatch log copi bucket local bucket file file file file instanc line apolog successfulli copi file download verifi autostop usr local bin autostop copi cron job overwrit autostop bucket default sampl notebook instanc lifecycl config sampl repo owner introduc commit http github com sampl notebook instanc lifecycl config sampl commit fdaceabcdcfcbefecdbc cron job line print notebook idl state set idl kernel connect ignor state print syntax guess kei overwrit default successfulli download odd repo default autostop introduc malici commit repo commit debug instanc account messag cron job cloudwatch log appstream enabl swb appstream connect output run termin instanc usr bin usr local bin autostop time ignor connect yeah automat repo pull appstream enabl send traffic proxi lambda firewal instanc environ file download bootstrap process network reason autostop note var log autostop log file sent cron job ran hand instanc autostop messag cron log var log autostop log messag root grep autostop var log cron tail dec crond root cmd usr bin usr local bin autostop time ignor connect var log autostop log run autostop exactli cron job relat boto import root usr bin usr local bin autostop time traceback file usr local bin autostop line import boto importerror modul boto boto introduc commit start sampl notebook instanc lifecycl config sampl http github com sampl notebook instanc lifecycl config sampl commit bcdcafeabfea diff eaaeefaccdcdaeff yup invok autostop autostop match swb root add backlog item figur boto import notebook autostop explain notebook instanc lifecycl config sampl instanc present instanc instanc manual file debug autostop yeah patienc attent believ file alter team activ respons admin infrastructur activ system tri instanc rule individu system version swb deploi mayb version srpiatt upgrad swb instal latest releas http github com awslab servic workbench releas tag instanc spun oper instanc longer mount studi autostop note stop start affect instanc upgrad deploi swb ahead close mark close soon respons close dai report pull commit fork repositori lock version auto stop file replac repo moment",
        "Solution_preprocessed_content":"verifi autostop bucket updat past unexpectedli file bucket month download autostop bucket verifi manual match swb repo version replic img width alt screen shot cloudwatch log group output img width alt screen shot trigger start instanc screenshot instanc line line come repo line come swb file sai download reason download file file introduc swb vulner introduc repo swb autostop save lock aren intend introduc screenshot instanc swb repo line line said present instanc stop autostop swb repo cron job autostop clarifi cloudwatch log copi bucket local bucket file file file file instanc line apolog successfulli copi file autostop copi cron job overwrit autostop bucket default repo owner introduc commit cron job line state syntax guess kei overwrit default successfulli download odd repo default autostop introduc malici commit repo commit debug instanc messag cron job cloudwatch log swb appstream connect output run termin instanc yeah automat repo pull appstream enabl send traffic proxi lambda firewal instanc environ file download bootstrap process network reason autostop note file sent cron job ran hand instanc autostop messag cron log messag run autostop exactli cron job relat boto import boto introduc commit yup invok autostop root add backlog item figur boto import notebook autostop explain instanc present instanc instanc manual file debug autostop yeah patienc attent believ file alter team activ respons activ system tri instanc rule individu system version swb mayb version upgrad swb instal latest releas instanc spun oper instanc longer mount studi autostop note stop start affect instanc upgrad deploi swb ahead close mark close dai report pull commit fork repositori lock version auto stop file replac repo moment",
        "Solution_readability":8.9,
        "Solution_reading_time":108.09,
        "Solution_score_count":5.0,
        "Solution_sentence_count":81.0,
        "Solution_word_count":1171.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":219.2780555556,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1659686697000,
        "Challenge_created_time":1658897296000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":19.7,
        "Challenge_reading_time":33.11,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":219.2780555556,
        "Challenge_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":223,
        "Platform":"Github",
        "Solution_body":"Further context - this only started happening approx. 32 hours ago. If I had to guess... maybe it should have never worked and something just happened to get 'fixed' in the IAM API yesterday? \ud83d\ude01  Hi @tdmalone is this still an issue? Hi @kcadette, it is, yes:\r\n\r\n```\r\nUser: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:xxxxxxxxxxxx:notebook-instance\/basicnotebookinstance-lqerepcrnmaw because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: 4f72193d-aa17-41b9-8ed0-ee381686cb5b; Proxy: null)}\r\n```\r\n\r\nIt should be a one-line fix, so I've submitted a PR: https:\/\/github.com\/awslabs\/service-workbench-on-aws\/pull\/1021",
        "Solution_gpt_summary":"line submit pull request",
        "Solution_link_count":1.0,
        "Solution_original_content":"context start approx hour ago guess mayb iam api yesterdai tdmalon kcadett arn st role dev syd timswb launchconstraint servicecatalog author perform addtag resourc arn southeast notebook instanc basicnotebookinst lqerepcrnmaw ident base polici allow addtag action servic statu accessdeniedexcept request eecbb proxi null line submit http github com awslab servic workbench pull",
        "Solution_preprocessed_content":"context start approx hour ago mayb iam api yesterdai submit",
        "Solution_readability":13.8,
        "Solution_reading_time":11.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":89.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":529.4461111111,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Challenge_closed_time":1633460282000,
        "Challenge_created_time":1631554276000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":6.9,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":529.4461111111,
        "Challenge_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":"I think there's a good chance this instance was autostopped, but that information was not propagated to DDB correctly.\r\n\r\nCan you log onto the hosting account for that Sagemaker instance and check if it's currently in the `Stopped` state. If yes, the latest code fixes that issue.\r\nhttps:\/\/github.com\/awslabs\/service-workbench-on-aws\/commit\/8cb199b8093f5e799d2d87c228930a4929ebebb7 Hi @nguyen102 yes, I can confirm that the Sagemaker instance is  in the Stopped sate. So then, the latest code that you mention should fix the issue. ",
        "Solution_gpt_summary":"notebook workspac unknown statu connect instanc autostop propag ddb log host account instanc stop state latest",
        "Solution_link_count":1.0,
        "Solution_original_content":"chanc instanc autostop propag ddb log host account instanc stop state latest http github com awslab servic workbench commit cbbfeddcaebebb nguyen instanc stop sate latest",
        "Solution_preprocessed_content":"chanc instanc autostop propag ddb log host account instanc state latest instanc stop sate latest",
        "Solution_readability":8.0,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":75.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":4421.2883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Challenge_closed_time":1643923114000,
        "Challenge_created_time":1628006476000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.5,
        "Challenge_reading_time":12.78,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":4421.2883333333,
        "Challenge_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":164,
        "Platform":"Github",
        "Solution_body":"Hi @tom-christie, we believe the issue mentioned is due to access token getting expired. Please feel free to use the latest version with the fix ([v3.3.1](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/releases\/tag\/v3.3.1)). We're seeing this issue on 4.1.1 as well. However, it appears to be persistent (i.e. it happens every time we connect to a SageMaker workspace). So far, we've only tested a single workspace config, but the error consistently shows up when we try to connect to different workspace instances using the same config. The workspace instances are new and running, at least as shown in the SWB UI. We haven't verified if the instances are available in the SageMaker console, however. Is it possible this is related to a popup blocker as reported in GALI-1224? It creates a similar error message.\r\nhttps:\/\/sim.amazon.com\/issues\/CHAMDOC-17 Yeah, I've seen the error happen because popups are disabled for the SWB domain. Once you enable popup for the SWB domain, it should allow you to connect to Sagemaker. Feel free to reopen this ticket if enabling popups didn't resolve your issue.",
        "Solution_gpt_summary":"latest version access token expir enabl popup swb domain relat popup blocker",
        "Solution_link_count":2.0,
        "Solution_original_content":"tom christi believ access token expir free latest version http github com awslab servic workbench releas tag see persist time connect workspac test singl workspac config consist connect workspac instanc config workspac instanc run shown swb haven verifi instanc consol relat popup blocker report gali creat messag http sim com chamdoc yeah popup disabl swb domain enabl popup swb domain allow connect free reopen ticket enabl popup",
        "Solution_preprocessed_content":"believ access token expir free latest version see persist test singl workspac config consist connect workspac instanc config workspac instanc run shown swb haven verifi instanc consol relat popup blocker report creat messag yeah popup disabl swb domain enabl popup swb domain allow connect free reopen ticket enabl popup",
        "Solution_readability":8.0,
        "Solution_reading_time":13.74,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":171.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":431.3538888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen connecting to Sagemaker workspaces, there is an intermittent issue where a blank browser launches instead of Sagemaker.  The issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nSTEP 1: Login as an Admin \r\nSTEP 2: Create a workspace (SageMaker)\r\nSTEP 3: Start the Workspace \r\nSTEP 5: Click \"Connect\"\r\nSTEP 6: A new blank web browser tab opens \r\nSTEP 7: Click \"Connect\" again, another blank web browser tab opens\r\n\r\nUser receives a \"Something Went Wrong\" general error in SWB at Step 6\r\n\r\nIn the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"Error from cloudfront\"\r\n   \r\n\r\n**Expected behavior**\r\nSagemaker workspace launch in browser\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed 3.0.0\r\n\r\n**Additional context**\r\nUser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nThe issue is experienced approximately once a week. Sometimes clearing cache solves the issue. Other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Challenge_closed_time":1624287519000,
        "Challenge_created_time":1622734645000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/509",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.2,
        "Challenge_reading_time":16.66,
        "Challenge_repo_contributor_count":37.0,
        "Challenge_repo_fork_count":101.0,
        "Challenge_repo_issue_count":1083.0,
        "Challenge_repo_star_count":153.0,
        "Challenge_repo_watch_count":24.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":431.3538888889,
        "Challenge_title":"[Bug] Blank Page on Sagemaker Workspace Connect",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":210,
        "Platform":"Github",
        "Solution_body":"The relevant code snippet for this issue is [here](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/7988c933d5f4d6c9878249a7521d64d891707824\/addons\/addon-base-raas-ui\/packages\/base-raas-ui\/src\/parts\/environments-sc\/parts\/ScEnvironmentHttpConnections.js#L60).\r\n\r\nI was unable to reproduce this issue (I'm using Chrome 90.0.4430.212) , but let's try to track this issue down.\r\n\r\nAre we unable to get the Sagemaker URL or is the browser failing to refer the user to the Sagemaker url?\r\n\r\nTo check if the browser is getting the Sagemaker URL correctly:\r\n1. Go to the Sagemaker workspace and click the connection button to open the \"HTTP Connections\" card \r\n2. Open the network tab in Chrome Inspect tool\r\n3. Clear all of the network activity\r\n4. Click on the `url` row on the sidebar\r\n5. In the new tab that opens up, select the sub tab for `response`.\r\n6. Check if a `url` is provided and try navigating to that `url` in the browser.\r\n\r\nIf the URL is valid and you can can navigate to the Sagemaker instance that way, then the issue is with the browser failing to redirect the user.\r\n\r\nhttps:\/\/user-images.githubusercontent.com\/3661906\/120824482-84a88d80-c526-11eb-883b-13d9b4cfa7f4.mov\r\nHere's a video of the process.\r\n\r\nPlease follow the instructions above to help us debug what is the cause of the issue. ",
        "Solution_gpt_summary":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"relev http github com awslab servic workbench blob cdfdcadd addon addon base raa packag base raa src environ scenvironmenthttpconnect reproduc chrome track url browser url browser url workspac click connect button open http connect card open network tab chrome inspect clear network activ click url row sidebar tab open select sub tab respons url navig url browser url navig instanc browser redirect http imag githubusercont com dbcfaf mov video process instruct debug",
        "Solution_preprocessed_content":"relev reproduc track url browser url browser url workspac click connect button open http connect card open network tab chrome inspect clear network activ click row sidebar tab open select sub tab navig browser url navig instanc browser redirect video process instruct debug",
        "Solution_readability":8.4,
        "Solution_reading_time":16.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":183.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0341643583,
        "Challenge_watch_issue_ratio":0.0221606648
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Calling `stop_training_job` from the SageMaker client against an existing by not \"InProgress\" job, causes the client to hang. This only seems to happen within the sm-executor though. \r\n\r\nHere's the output calling the method from the python interpreter within the pod:\r\n```\r\n# .\/python\r\nPython 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import boto3\r\n>>> client = boto3.client(\"sagemaker\")\r\n>>> client.stop_training_job(TrainingJobName=\"98cb7232-02b1-4a1b-a59e-55a8eca9e048\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the StopTrainingJob operation: The request was rejected because the training job is in status Stopped.\r\n>>>\r\n```\r\n\r\nHere is the DEBUG output from the sm-executor\r\n```\r\n2019-10-09 06:39:38,252 INFO: Attempting to stop training job 98cb7232-02b1-4a1b-a59e-55a8eca9e048\r\n2019-10-09 06:39:38,252 DEBUG: Event before-parameter-build.sagemaker.StopTrainingJob: calling handler <function generate_idempotent_uuid at 0x7f54d82671e0>\r\n2019-10-09 06:39:38,253 DEBUG: Event before-call.sagemaker.StopTrainingJob: calling handler <function inject_api_version_header_if_needed at 0x7f54d8268b70>\r\n2019-10-09 06:39:38,253 DEBUG: Making request for OperationModel(name=StopTrainingJob) with params: {'url_path': '\/', 'query_string': '', 'method': 'POST', 'headers': {'X-Amz-Target': 'SageMaker.StopTrainingJob', 'Content-Type': 'application\/x-amz-json-1.1', 'User-Agent': 'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221'}, 'body': b'{\"TrainingJobName\": \"98cb7232-02b1-4a1b-a59e-55a8eca9e048\"}', 'url': 'https:\/\/api.sagemaker.us-east-1.amazonaws.com\/', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x7f54bacde518>, 'has_streaming_input': False, 'auth_type': None}}\r\n2019-10-09 06:39:38,253 DEBUG: Event request-created.sagemaker.StopTrainingJob: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x7f54bacde4e0>>\r\n2019-10-09 06:39:38,254 DEBUG: Event choose-signer.sagemaker.StopTrainingJob: calling handler <function set_operation_specific_signer at 0x7f54d82670d0>\r\n2019-10-09 06:39:38,254 DEBUG: Calculating signature using v4 auth.\r\n2019-10-09 06:39:38,254 DEBUG: CanonicalRequest:\r\nPOST\r\n\/\r\n\r\ncontent-type:application\/x-amz-json-1.1\r\nhost:api.sagemaker.us-east-1.amazonaws.com\r\nx-amz-date:20191009T063938Z\r\nx-amz-security-token:FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF\r\nx-amz-target:SageMaker.StopTrainingJob\r\n\r\ncontent-type;host;x-amz-date;x-amz-security-token;x-amz-target\r\n84e242897f2f826cc224094427e7ba8bc4c2f559097741460b59e162e8114c40\r\n2019-10-09 06:39:38,254 DEBUG: StringToSign:\r\nAWS4-HMAC-SHA256\r\n20191009T063938Z\r\n20191009\/us-east-1\/sagemaker\/aws4_request\r\n4320231908e4cd91204a6044a6201b1a74c0a63a2f708f9c4c27df2d6a6344db\r\n2019-10-09 06:39:38,255 DEBUG: Signature:\r\nc074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae\r\n2019-10-09 06:39:38,255 DEBUG: Sending http request: <AWSPreparedRequest stream_output=False, method=POST, url=https:\/\/api.sagemaker.us-east-1.amazonaws.com\/, headers={'X-Amz-Target': b'SageMaker.StopTrainingJob', 'Content-Type': b'application\/x-amz-json-1.1', 'User-Agent': b'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221', 'X-Amz-Date': b'20191009T063938Z', 'X-Amz-Security-Token': b'FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF', 'Authorization': b'AWS4-HMAC-SHA256 Credential=ASIAYNI7SS57NFEDAZHQ\/20191009\/us-east-1\/sagemaker\/aws4_request, SignedHeaders=content-type;host;x-amz-date;x-amz-security-token;x-amz-target, Signature=c074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae', 'Content-Length': '59'}>\r\n2019-10-09 06:39:38,320 DEBUG: Response headers: {'x-amzn-RequestId': '03dd9de3-3672-4f4b-b575-a06d29e15e6b', 'Content-Type': 'application\/x-amz-json-1.1', 'Content-Length': '116', 'Date': 'Wed, 09 Oct 2019 06:39:37 GMT', 'Connection': 'close'}\r\n2019-10-09 06:39:38,320 DEBUG: Response body:\r\nb'{\"__type\":\"ValidationException\",\"message\":\"The request was rejected because the training job is in status Stopped.\"}'\r\n2019-10-09 06:39:38,320 DEBUG: Event needs-retry.sagemaker.StopTrainingJob: calling handler <botocore.retryhandler.RetryHandler object at 0x7f54bacde898>\r\n2019-10-09 06:39:38,321 DEBUG: No retry needed.\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1570606746000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/928",
        "Challenge_link_count":2,
        "Challenge_open_time":27400.3483333333,
        "Challenge_readability":21.2,
        "Challenge_reading_time":75.68,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":1054.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":null,
        "Challenge_title":"[SM-Executor] SageMaker.stop_training_job hangs",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":346,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0132827324,
        "Challenge_watch_issue_ratio":0.0075901328
    },
    {
        "Challenge_adjusted_solved_time":326.7363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Challenge_closed_time":1571326528000,
        "Challenge_created_time":1570150277000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.2,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":1054.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":326.7363888889,
        "Challenge_title":"Cannot run benchmark for sagemaker",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"3 tactics were used to address this issue (by @perdasilva): \r\nStop gap, watcher, error reporting in the status.\r\n@haohanchen-yagao - Please confirm and the close this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tactic address perdasilva stop gap watcher report statu haohanchen yagao close",
        "Solution_preprocessed_content":"tactic address stop gap watcher report statu close",
        "Solution_readability":7.2,
        "Solution_reading_time":2.12,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":26.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0132827324,
        "Challenge_watch_issue_ratio":0.0075901328
    },
    {
        "Challenge_adjusted_solved_time":457.1169444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1621931178000,
        "Challenge_created_time":1620285557000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.7,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":25.0,
        "Challenge_repo_issue_count":1008.0,
        "Challenge_repo_star_count":130.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":457.1169444444,
        "Challenge_title":"The data path inside sagemaker notebook does not work",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":85,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0099206349,
        "Challenge_watch_issue_ratio":0.0178571429
    },
    {
        "Challenge_adjusted_solved_time":18.7375,
        "Challenge_answer_count":0,
        "Challenge_body":"Invoke Endpoint response time out. \r\n\r\n### Reproduction Steps\r\n\r\n{\r\n  \"trainingJob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instanceType\": \"ml.c5.9xlarge\",\r\n    \"timeoutInSeconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### Error Log\r\nIn Inference Lambda CloudWatch:\r\n\r\nTask timed out after 120.10 seconds\r\n\r\n\r\nIn Sagemaker Training CloudWatch:\r\n\r\n2021-04-09   04:53:46,902 [INFO ] main org.pytorch.serve.ModelServer - Loading initial   models: model.mar\r\n--\r\n2021-04-09 04:53:49,837 [INFO ] main   org.pytorch.serve.archive.ModelArchive - eTag   8ff2b3de4bed4fb1bc7fe969652117ff\r\n2021-04-09 04:53:49,847 [INFO ] main   org.pytorch.serve.wlm.ModelManager - Model model loaded.\r\n2021-04-09 04:53:49,865 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Inference server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Metrics server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,931 [INFO ] main   org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\r\nModel server started.\r\n2021-04-09 04:53:49,957 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on   port: \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker   started.\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime:   3.6.13\r\n2021-04-09 04:53:49,963 [INFO ]   W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to:   \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,972 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection   accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   CPUUtilization.Percent:33.3\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskAvailable.Gigabytes:19.622234344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUsage.Gigabytes:4.731609344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUtilization.Percent:19.4\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryAvailable.Megabytes:30089.12109375\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUsed.Megabytes:902.6953125\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUtilization.Percent:4.1\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Setting the   default backend to \"pytorch\". You can change it in the   ~\/.dgl\/config.json file or export the DGLBACKEND environment variable.\u00a0 Valid options are: pytorch, mxnet,   tensorflow (all lowercase)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   ------------------ Loading model -------------------\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker   process died.\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most   recent call last):\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 176, in <module>\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 worker.run_server()\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 148, in run_server\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self.handle_connection(cl_socket)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 112, in handle_connection\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service, result, code =   self.load_model(msg)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 85, in load_model\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service = model_loader.load(model_name,   model_dir, handler, gpu, batch_size)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py\", line   117, in load\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   model_service.initialize(service.context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/home\/model-server\/tmp\/models\/8ff2b3de4bed4fb1bc7fe969652117ff\/handler_service.py\",   line 51, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 super().initialize(context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py\",   line 66, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self._service.validate_and_initialize(model_dir=model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\",   line 158, in validate_and_initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self._model = self._model_fn(model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/ml\/model\/code\/fd_sl_deployment_entry_point.py\", line 149, in   model_fn\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 rgcn_model.load_state_dict(stat_dict)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\",   line 1045, in load_state_dict\r\n2021-04-09   04:53:51,251 [INFO ] W-9000-model_1-stdout   org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self.__class__.__name__, \"     \\t\".join(error_msgs)))\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError:   Error(s) in loading state_dict for HeteroRGCN:\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.weight: copying a   param with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.bias: copying a   param with shape torch.Size([2]) from checkpoint, the shape in current model   is torch.Size([16]).\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** <!-- Output of `cdk version` -->\r\n  - **Framework Version:**\r\n  - **Node.js Version:** <!-- Version of Node.js (run the command `node -v`) -->\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\nCause of this bug:\r\n\r\nBackend worker process died.\r\nSagemaker Endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Challenge_closed_time":1618279737000,
        "Challenge_created_time":1618212282000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/57",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":17.5,
        "Challenge_reading_time":124.73,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":25.0,
        "Challenge_repo_issue_count":1008.0,
        "Challenge_repo_star_count":130.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":107,
        "Challenge_solved_time":18.7375,
        "Challenge_title":"sagemaker endpoint fail to deploy or time out server error(0) bug",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":650,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0099206349,
        "Challenge_watch_issue_ratio":0.0178571429
    },
    {
        "Challenge_adjusted_solved_time":142.7236111111,
        "Challenge_answer_count":7,
        "Challenge_body":"### System Info\n\n```Shell\npytorch: 1.10.2\r\npython:3.8\n```\n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] One of the scripts in the examples\/ folder of Accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSagemaker Multi-GPU distributed data training, while \"model.generate\" it always returns empty tensors.\n\n### Expected behavior\n\n```Shell\nI'm trying to run a distributed training in a Sagemaker training job, the inference is not working properly, I found it as a future work on huggingface documentation so I'm wondering If that's why it's not working yet on sagemaker Multi-GPU.\r\n\r\nThanks\n```\n",
        "Challenge_closed_time":1664255368000,
        "Challenge_created_time":1663741563000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/accelerate\/issues\/706",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.6,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":76.0,
        "Challenge_repo_fork_count":280.0,
        "Challenge_repo_issue_count":921.0,
        "Challenge_repo_star_count":3349.0,
        "Challenge_repo_watch_count":64.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":142.7236111111,
        "Challenge_title":"Have accelerate for  Distributed Training: Data Parallelism feature working on AWS Sagemaker yet?",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":"Hello @HebaGamalElDin, please provide minimal reproducible example for us to deep dive and help you.  Hello @pacman100, I'm fine tuning a transformer model from the hub of huggingface.. below is the training function that utilizes the accelerator on sagemaker training jobs.\r\n\r\n```\r\ndef train(context: Context, num_epochs):\r\n    model = context.model\r\n    model = accelerator.prepare(model)\r\n    optimizer = AdamW(model.parameters(), lr=1e-3)\r\n    \r\n    num_training_steps = num_epochs * len(context.train_dataloader)\r\n    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\r\n    optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(optimizer, context.train_dataloader, context.val_dataloader, lr_scheduler)\r\n    \r\n    \r\n    losses = []\r\n    min_cer = 1.0\r\n    min_train_loss = 1.0\r\n    for epoch in range(num_epochs):\r\n        model.train()\r\n        for j, batch in enumerate(train_dataloader):\r\n            inputs: torch.Tensor = batch[\"input\"]#.to(accelerator.device)\r\n            labels: torch.Tensor = batch[\"label_tensor\"]#.to(accelerator.device)\r\n\r\n            outputs = model(pixel_values=inputs, labels=labels)\r\n            #print(outputs)\r\n            loss = outputs.loss\r\n            accelerator.backward(loss)\r\n            #loss.backward()\r\n\r\n            optimizer.step()\r\n            lr_scheduler.step()\r\n            optimizer.zero_grad()\r\n            losses.append(loss)\r\n            accelerator.print(f\"Epoch {epoch}-------Batch---{j}-----Loss---{loss}\")\r\n            \r\n        model.eval()\r\n        for i, batch in enumerate(eval_dataloader):\r\n            inputs: torch.Tensor = batch[\"input\"]#.to(accelerator.device)\r\n            with torch.no_grad():\r\n                predictions = accelerator.unwrap_model(model).generate(inputs)\r\n\r\n                generated_ids = accelerator.gather(predictions).cpu().numpy()\r\n                print(f\"Generated IDs: {generated_ids}\")\r\n                labels = accelerator.gather(batch[\"label_tensor\"]).cpu().numpy()\r\n                \r\n                generated_text = context.processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n                labels_text = context.processor.batch_decode(labels, skip_special_tokens=True)\r\n                \r\n                predictions, labels = postprocess_text(generated_text, labels_text)\r\n                \r\n                cer_metric.add_batch(predictions=predictions, references=labels)\r\n                wer_metric.add_batch(predictions=predictions, references=labels)\r\n                print(f\"Predictions: {predictions}-----------Labels: {labels}\")\r\n        cer = cer_metric.compute()\r\n        wer = wer_metric.compute()\r\n\r\n        accelerator.print(f\"Average CER: {cer}------ Average WER: {wer}\")\r\n```\r\n\r\nthe python estimator is as follows:\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nimport sagemaker\r\nrole = sagemaker.get_execution_role()\r\npt_estimator = PyTorch(\r\n    base_job_name=\"transformer-ocr-training\",\r\n    source_dir=\"source\",\r\n    entry_point=\"Train.py\",\r\n    role=role,    \r\n    py_version=\"py38\",\r\n    \r\n    image_uri =\"763104351884.dkr.ecr.us-east-1.amazonaws.com\/huggingface-pytorch-training:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04\",\r\n\r\n    #framework_version=\"1.12.0\",\r\n\r\n    instance_count=1,\r\n    instance_type=\"ml.p3.16xlarge\"\r\n    #distribution={'smdistributed':{'dataparallel':{ 'enabled': True }}}\r\n)\r\n\r\npt_estimator.fit(\"s3:\/\/handwritten-ocr-training\")\r\n```\r\n\r\nExactly when I'm generate in for the evaluation set it always retrieves empty tensors. What am I missing here? However the number of processes is 8 GPUs so the accelerate has access to all of them however it's not generating in the validation all decoded strings are empty, appreciate your help! Hello @HebaGamalElDin, you are not using the \ud83e\udd17 Accelerate integration of AWS SageMaker correctly. To help you and others going forwards, I have spent time creating this repo https:\/\/github.com\/pacman100\/accelerate-aws-sagemaker which details on how to correctly use AWS SageMaker with \ud83e\udd17 Accelerate. it works correctly with generation `model.generate`. Please go through the README and files in the above repo and let us know if you still have issues.  Hello @pacman100 .. Thank you for the warm help.\r\nI have one question please, what I didn't get is how to configure accelerate inside the training job?\r\nmeaning where to run the command `accelerate config --config_file accelerate_config.yaml`? Have the [accelerate_config.yaml](https:\/\/github.com\/pacman100\/accelerate-aws-sagemaker\/blob\/af5caadcea0fa8186c11a784b6f86591c8fa5b3f\/src\/seq2seq\/accelerate_config.yaml) file should been replaced the python SDK estimator *PyTorch* in my case? Hello, you don't have to use any SageMaker estimator (PyTorch estimator in your case) as Accelerate internally uses Hugging Face SageMaker Estimator https:\/\/github.com\/huggingface\/accelerate\/blob\/main\/src\/accelerate\/commands\/launch.py#L776 along with all the necessary env variables to handle SageMaker DDP.\r\n\r\nJust create the accelerate config with command `accelerate config` on any virtual machine\/local machine\/sagemaker notebooks on which you have aws cli installed with aws credentials setup. After that when you run `accelerate launch` it will internally use HF estimator to create the training job on AWS SageMaker. I am running `accelerate config` and `accelerate launch` on a local machine with aws credentials setup.\r\n\r\n\r\n\r\n @pacman100 Okay I got that thank you.\r\nOne more question please, I'm encountering an issue when I'm testing, most of validation batches entirely are empty while some others are okay, this problem doesn't happen while training is on 1 GPU, What could be the problem here please?!\r\n**HINT: I'm logging the length of the text predictions coming by model.generate() for each batch, the majority is zero as shown in the below screenshot.**\r\n![image](https:\/\/user-images.githubusercontent.com\/36745656\/192077881-b5a598d0-1762-4335-9f2b-f07daa627318.png)\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":4.0,
        "Solution_original_content":"hebagamaleldin minim reproduc dive pacman tune transform model hub huggingfac train function util acceler train job train context context num epoch model context model model acceler prepar model optim adamw model paramet num train step num epoch len context train dataload schedul schedul linear optim optim num warmup step num train step num train step optim train dataload eval dataload schedul acceler prepar optim context train dataload context val dataload schedul loss cer train loss epoch rang num epoch model train batch enumer train dataload input torch tensor batch input acceler devic label torch tensor batch label tensor acceler devic output model pixel valu input label label print output loss output loss acceler backward loss loss backward optim step schedul step optim grad loss append loss acceler print epoch epoch batch loss loss model eval batch enumer eval dataload input torch tensor batch input acceler devic torch grad predict acceler unwrap model model gener input gener id acceler gather predict cpu numpi print gener id gener id label acceler gather batch label tensor cpu numpi gener text context processor batch decod gener id skip token label text context processor batch decod label skip token predict label postprocess text gener text label text cer metric add batch predict predict label wer metric add batch predict predict label print predict predict label label cer cer metric comput wer wer metric comput acceler print averag cer cer averag wer wer estim pytorch import pytorch import role execut role estim pytorch base job transform ocr train sourc dir sourc entri train role role version imag uri dkr ecr amazonaw com huggingfac pytorch train transform gpu ubuntu framework version instanc count instanc type xlarg distribut smdistribut dataparallel enabl estim fit handwritten ocr train exactli gener evalu set retriev tensor miss process gpu acceler access gener decod hebagamaleldin acceler integr forward spent time creat repo http github com pacman acceler acceler gener model gener readm file repo pacman warm configur acceler insid train job run acceler config config file acceler config yaml acceler config yaml http github com pacman acceler blob afcaadceafacabfcfabf src seqseq acceler config yaml file replac sdk estim pytorch estim pytorch estim acceler intern hug estim http github com huggingfac acceler blob src acceler launch env variabl ddp creat acceler config acceler config virtual local notebook cli instal credenti setup run acceler launch intern estim creat train job run acceler config acceler launch local credenti setup pacman okai test batch entir okai train gpu hint log length text predict come model gener batch shown screenshot imag http imag githubusercont com fdaa png",
        "Solution_preprocessed_content":"minim reproduc dive tune transform model hub train function util acceler train job estim exactli gener evalu set retriev tensor miss process gpu acceler access gener decod acceler integr forward spent time creat repo acceler gener readm file repo warm configur acceler insid train job run file replac sdk estim pytorch estim acceler intern hug estim env variabl ddp creat acceler config virtual notebook cli instal credenti setup run intern estim creat train job run local credenti setup okai test batch entir okai train gpu hint log length text predict come batch shown",
        "Solution_readability":16.5,
        "Solution_reading_time":70.66,
        "Solution_score_count":1.0,
        "Solution_sentence_count":54.0,
        "Solution_word_count":517.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0825190011,
        "Challenge_watch_issue_ratio":0.0694896851
    },
    {
        "Challenge_adjusted_solved_time":2.8202777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Challenge_closed_time":1666960895000,
        "Challenge_created_time":1666950742000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.6,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":2.8202777778,
        "Challenge_title":"Sagemaker example DAG to use aws session token",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":120,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":18.1363888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nXCom return value of `SageMakerTransformOperatorAsync`  and `SageMakerTrainingOperatorAsync` does not produce the expected output.\r\n\r\nIt seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a dag with traditional operators\r\n2. Run same dag with Async operators\r\n3. Compare outputs\r\n\r\n**Expected behavior**\r\nThe Xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n",
        "Challenge_closed_time":1666957435000,
        "Challenge_created_time":1666892144000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/736",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.76,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":18.1363888889,
        "Challenge_title":"XCom Output of Sagemaker Async Operators",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":84,
        "Platform":"Github",
        "Solution_body":"@bharanidharan14  I tested locally with the branch for the fix and seems to be fixed with your patch. Thank you!",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"bharanidharan test local branch patch",
        "Solution_preprocessed_content":"test local branch patch",
        "Solution_readability":7.6,
        "Solution_reading_time":1.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":40.4833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Challenge_closed_time":1666859588000,
        "Challenge_created_time":1666713848000,
        "Challenge_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.7,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":807.0,
        "Challenge_repo_star_count":97.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":40.4833333333,
        "Challenge_title":"Token error with Sagemaker Async Operators",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":127,
        "Platform":"Github",
        "Solution_body":"The issue is with the session token is not considered while the secrete and access key is given in the connection proper field, not in the extra config",
        "Solution_gpt_summary":"token session token extra config field access kei secret session token authent async oper",
        "Solution_link_count":0.0,
        "Solution_original_content":"session token secret access kei connect field extra config",
        "Solution_preprocessed_content":"session token secret access kei connect field extra config",
        "Solution_readability":13.0,
        "Solution_reading_time":1.82,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":28.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0223048327,
        "Challenge_watch_issue_ratio":0.0408921933
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Can you please confirm if Sagemaker Debugger works with HPO. I get errors when the code that works perfectly fine with SM script mode fails when extended to HPO.\r\n\r\n` FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/config\/debughookconfig.json'`",
        "Challenge_closed_time":null,
        "Challenge_created_time":1597167105000,
        "Challenge_link":"https:\/\/github.com\/awslabs\/sagemaker-debugger\/issues\/325",
        "Challenge_link_count":0,
        "Challenge_open_time":20022.4708333333,
        "Challenge_readability":8.5,
        "Challenge_reading_time":3.69,
        "Challenge_repo_contributor_count":36.0,
        "Challenge_repo_fork_count":80.0,
        "Challenge_repo_issue_count":628.0,
        "Challenge_repo_star_count":139.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker Debugger with HPO",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":41,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0573248408,
        "Challenge_watch_issue_ratio":0.0414012739
    },
    {
        "Challenge_adjusted_solved_time":10230.7861111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Challenge_closed_time":1646674184000,
        "Challenge_created_time":1609843354000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":11.8,
        "Challenge_reading_time":9.83,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":10230.7861111111,
        "Challenge_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":67,
        "Platform":"Github",
        "Solution_body":"Thanks for reaching out! We haven't taken the work to support jupyterlabs yet, though we do build our visualization widget for labs already. Seems like the Tab widget isn't being displayed properly in the screenshot provided of labs, but that could be because our Force widget isn't installed properly. \r\n\r\nI have cut a feature request for this: #55 Thanks a lot!\r\nAppreciate it \ud83d\udc4d \r\n Widgets now render properly in JupyterLab as of #271 .",
        "Solution_gpt_summary":"featur request cut widget render properli jupyterlab",
        "Solution_link_count":0.0,
        "Solution_original_content":"reach haven taken jupyterlab build visual widget lab tab widget isn displai properli screenshot lab forc widget isn instal properli cut featur request widget render properli jupyterlab",
        "Solution_preprocessed_content":"reach haven taken jupyterlab build visual widget lab tab widget isn displai properli screenshot lab forc widget isn instal properli cut featur request widget render properli jupyterlab",
        "Solution_readability":7.7,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":72.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":933.8588888889,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nHello,\r\n\r\nThis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nWe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nIt helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nUnexpectedStatusException: Error for Training job huggingface-pytorch-training-2022-06-29-04-04-58-606: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\r\nExitCode 1\r\nErrorMessage \":RuntimeError: Tensors must have same number of dimensions: got 4 and 3\r\n :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[41154,1],0]   Exit code:    1\"\r\nCommand \"mpirun --host algo-1:8 \r\n\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.\n\n### Who can help?\n\n[SageMakerAprilTraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/SageMakerAprilTraining.zip)\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning this attached file with the training python file\n\n### Expected behavior\n\nI have shared the notebook and the error raised in it for clarification",
        "Challenge_closed_time":1660575729000,
        "Challenge_created_time":1657213837000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18060",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":14.3,
        "Challenge_reading_time":36.15,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":933.8588888889,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training #16890",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":355,
        "Platform":"Github",
        "Solution_body":"@omid0001 @kanwari3, \r\n\r\nWould it be possible for you to reproduce this issue (`not enough values to unpack`) without using SageMaker, i.e. just with a Python script?\r\n```bash\r\n[1,0]: bsz, seq_len = input_ids_shape[:2]\r\n[1,0]:ValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\nIt would be a good idea to verify what data is received by the model first. Usually the batches in data (`input_ids`) should be already of the format `(batch_size, sequence_length)`, and if you see the above error, it is likely the data or its processing pipeline has some issues. Using `torch.unsqueeze` is not really a good idea, as it implies you have only `batch_size` being 1.\r\n\r\nMy suggestion:\r\n- Try to run your training without SageMaker (and without the using the fix `torch.unsqueeze`)\r\n- Check what is received by the model, and check in the data pipeline if it prepares the correct input format\r\n  - If you still get the issue and can't figure it out:\r\n    - I could try to help if you could provide the training script + data processing script + a tiny portion of your data    \r\n  - If the issue only occurs when you wrap the training in SageMaker, I don't have the competence to help in this case, sorry. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_gpt_summary":"verifi data receiv model data pipelin prepar input format run train torch unsqueez persist train data process small portion data assist relat algorithmerror tensor dimens",
        "Solution_link_count":1.0,
        "Solution_original_content":"omid kanwari reproduc valu unpack bash bsz seq len input id shape valueerror valu unpack idea verifi data receiv model batch data input id format batch size sequenc length data process pipelin torch unsqueez idea batch size run train torch unsqueez receiv model data pipelin prepar input format figur train data process tini portion data wrap train compet sorri automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor",
        "Solution_preprocessed_content":"reproduc idea verifi data receiv model batch data format data process pipelin idea run train receiv model data pipelin prepar input format figur train data process tini portion data wrap train compet sorri automat mark stale activ address comment thread note ignor",
        "Solution_readability":10.7,
        "Solution_reading_time":18.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":243.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":763.1580555556,
        "Challenge_answer_count":3,
        "Challenge_body":"### System Info\n\n```shell\nThis was verified today on a fresh SageMaker Studio instance running in us-west-2.\r\n\r\nIt's not a Transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on SageMaker Studio at some point.\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1) Open an SM Studio notebook\r\n\r\n2) Run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nThe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### Expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Challenge_closed_time":1654502136000,
        "Challenge_created_time":1651754767000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17096",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.3,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":763.1580555556,
        "Challenge_title":"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"Thanks for the issue @juliensimon, this should be fixed by https:\/\/github.com\/huggingface\/transformers\/pull\/17049. It will be in the next release which should drop early next week. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. Should be fixed now!",
        "Solution_gpt_summary":"instal sacremos break studio pull request github releas softwar releas earli week workaround instal sacremos",
        "Solution_link_count":2.0,
        "Solution_original_content":"juliensimon http github com huggingfac transform pull releas drop earli week automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor",
        "Solution_preprocessed_content":"releas drop earli week automat mark stale activ address comment thread note ignor",
        "Solution_readability":8.5,
        "Solution_reading_time":6.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":73.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":913.5883333333,
        "Challenge_answer_count":4,
        "Challenge_body":"### System Info\n\n```shell\nusing sagemaker \r\nmpi_options = {\r\n    \"enabled\" : True,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":True,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": True,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':HHousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = HuggingFace(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### Who can help?\n\n@ydshieh @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Create huggingface estimator\r\n2.     training_args = Seq2SeqTrainingArguments(\r\n        predict_with_generate=True,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=True,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nError I get:\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:ValueError: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[41156,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### Expected behavior\n\n```shell\nTraining on a sagemaker notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\nI can successfully train the LED model with my training data. Trying to get it to work with Huggingface estimator and sagemaker SMP I would assume the same outcome.\n```\n",
        "Challenge_closed_time":1653922922000,
        "Challenge_created_time":1650634004000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/16890",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.2,
        "Challenge_reading_time":50.05,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17219.0,
        "Challenge_repo_issue_count":20692.0,
        "Challenge_repo_star_count":76135.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":913.5883333333,
        "Challenge_title":"LED Model returns AlgorithmError when using SageMaker SMP training",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":296,
        "Platform":"Github",
        "Solution_body":"cc @philschmid  I would also suggest @kanwari3 to\r\n- try to use the same Python\/PyTorch\/transformers versions (and other libraries) on SageMaker that work locally (if possible)\r\n- if the above doesn't work, try to use on local machine the same versions as those used on SageMaker, and see if you still get errors\r\n\r\nSo we have a better idea about if this is indeed a SageMaker issue or libraries issue This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. cc @philschmid  , cc @ydshieh ,  cc @sgugger \r\nHi, \r\n\r\nThis is a follow up on this post with the same title. We are trying to fix the issue and are still getting the same error after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. We tried unsqueezing the input_ids and attention_masks but it didn\u2019t fix the error.\r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\n    return {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.",
        "Solution_gpt_summary":"tri match transform pytorch version unsqueez input id attent mask persist feedback assist",
        "Solution_link_count":1.0,
        "Solution_original_content":"philschmid kanwari pytorch transform version librari local local version idea librari automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor philschmid ydshieh sgugger titl match transform pytorch version accord respect valueerror valu unpack model led transform modul input id shape tri unsqueez input id attent mask didnt updat tri unsqueez input tensor model led unsqueez col return input id torch unsqueez input id pubm train pubm train map unsqueez col greatli feedback",
        "Solution_preprocessed_content":"version local local version idea librari automat mark stale activ address comment thread note ignor titl match transform pytorch version accord valueerror valu unpack transform modul shape tri unsqueez didnt updat tri unsqueez input tensor return greatli feedback",
        "Solution_readability":10.8,
        "Solution_reading_time":20.89,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":250.0,
        "Tool":"Amazon SageMaker",
        "Challenge_contributor_issue_ratio":0.0212159289,
        "Challenge_watch_issue_ratio":0.0415619563
    },
    {
        "Challenge_adjusted_solved_time":29.7938888889,
        "Challenge_answer_count":1,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Challenge_closed_time":1645604942000,
        "Challenge_created_time":1645497684000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.3,
        "Challenge_reading_time":24.77,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":650.0,
        "Challenge_repo_issue_count":1974.0,
        "Challenge_repo_star_count":882.0,
        "Challenge_repo_watch_count":2756.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":29.7938888889,
        "Challenge_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"This issue should be mitigated once the PR is merged: https:\/\/github.com\/Azure\/azureml-examples\/pull\/981",
        "Solution_gpt_summary":"pull request merg github repositori",
        "Solution_link_count":1.0,
        "Solution_original_content":"mitig merg http github com pull",
        "Solution_preprocessed_content":null,
        "Solution_readability":13.5,
        "Solution_reading_time":1.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0683890578,
        "Challenge_watch_issue_ratio":1.3961499493
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nAzureML tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3485981939\/jobs\/5832009213\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\nWe want to send back a signal to GitHub so if the tests fail, the badge is red and we are notified\r\n\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1668674781000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1852",
        "Challenge_link_count":1,
        "Challenge_open_time":159.2275,
        "Challenge_readability":8.1,
        "Challenge_reading_time":13.88,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] AzureML test process is not failing if there is an error in the tests",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":2.4247222222,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n                EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n                BATCH_SIZE_SYNTHETIC=1024,\r\n                BATCH_SIZE_CRITEO=1024,\r\n                RANDOM_SEED=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\nE           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\nE             comparison failed\r\nE             Obtained: 0.5131\r\nE             Expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668600473000,
        "Challenge_created_time":1668591744000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1848",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.18,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.4247222222,
        "Challenge_title":"[BUG] xdeepfm error in AzureML test",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":162,
        "Platform":"Github",
        "Solution_body":"```\r\n pytest tests\/integration\/examples\/test_notebooks_gpu.py::test_xdeepfm_integration --disable-warnings --durations 0\r\n```\r\nwith \r\n```\r\n@pytest.mark.gpu\r\n@pytest.mark.notebooks\r\n@pytest.mark.integration\r\n@pytest.mark.parametrize(\r\n    \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n    [\r\n        (\r\n            15,\r\n            10,\r\n            {\r\n                \"res_syn\": {\"auc\": 0.9716, \"logloss\": 0.699},\r\n                \"res_real\": {\"auc\": 0.749, \"logloss\": 0.4926},\r\n            },\r\n            42,\r\n        )\r\n    ],\r\n)\r\ndef test_xdeepfm_integration(\r\n    notebooks,\r\n    output_notebook,\r\n    kernel_name,\r\n    syn_epochs,\r\n    criteo_epochs,\r\n    expected_values,\r\n    seed,\r\n):\r\n    notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n    pm.execute_notebook(\r\n        notebook_path,\r\n        output_notebook,\r\n        kernel_name=kernel_name,\r\n        parameters=dict(\r\n            EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n            EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n            BATCH_SIZE_SYNTHETIC=1024,\r\n            BATCH_SIZE_CRITEO=1024,\r\n            RANDOM_SEED=seed,\r\n        ),\r\n    )\r\n    results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n        \"data\"\r\n    ]\r\n\r\n    for key, value in expected_values.items():\r\n        assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\n        assert results[key][\"logloss\"] == pytest.approx(\r\n            value[\"logloss\"], rel=TOL, abs=ABS_TOL\r\n        )\r\n```",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pytest test integr test notebook gpu test xdeepfm integr disabl warn durat pytest mark gpu pytest mark notebook pytest mark integr pytest mark parametr syn epoch criteo epoch valu seed re syn auc logloss re auc logloss test xdeepfm integr notebook output notebook kernel syn epoch criteo epoch valu seed notebook path notebook xdeepfm quickstart execut notebook notebook path output notebook kernel kernel paramet dict epoch synthet run syn epoch epoch criteo run criteo epoch batch size synthet batch size criteo random seed seed read notebook output notebook scrap datafram set index data kei valu valu item assert kei auc pytest approx valu auc rel tol ab ab tol assert kei logloss pytest approx valu logloss rel tol ab ab tol",
        "Solution_preprocessed_content":null,
        "Solution_readability":19.8,
        "Solution_reading_time":15.13,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":139.4886111111,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1668591607000,
        "Challenge_created_time":1668089448000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.5,
        "Challenge_reading_time":32.96,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":139.4886111111,
        "Challenge_title":"[BUG] Error in some of the AzureML tests",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"@pradnyeshjoshi any thoughts for this error?",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.1,
        "Solution_reading_time":0.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":6.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nRuntime of [tests\/integration\/examples\/test_notebooks_gpu.py::test_sasrec_quickstart_integration](https:\/\/github.com\/microsoft\/recommenders\/blob\/6987858116d21699f6d92661f03c1529383c7d88\/tests\/integration\/examples\/test_notebooks_gpu.py#L679) varies a lot on the following platforms:\r\n- As part of ADO pipeline, it takes ~562 sec to complete.\r\n- When run as an experiment on AzureML compute cluster triggered using a [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml), it takes ~7080 sec.\r\n\r\nWe need to investigate why this happens.\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nBoth the machines are of same type (NC6s_V2), and use the same CUDA and CuDNN versions:\r\n`cudatoolkit=11.2`\r\n`cudnn=8.1`\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nTrigger the [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml) manually and take a look at pytest logs in the dashboard to see the execution times.\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1652368299000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1716",
        "Challenge_link_count":3,
        "Challenge_open_time":4688.8058333333,
        "Challenge_readability":10.5,
        "Challenge_reading_time":22.27,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] SASRec integration test unusually long time on AzureML compute cluster",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":179,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":2127.9833333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Challenge_closed_time":1603980914000,
        "Challenge_created_time":1596320174000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.2,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2127.9833333333,
        "Challenge_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"Seems we need to fix `azure-mgmt-cosmosdb` version as well... \r\n```\r\nAttributeError: module 'azure.mgmt.cosmosdb' has no attribute 'CosmosDB'\r\n```\r\n",
        "Solution_gpt_summary":"addit version cli core version mgmt cosmosdb",
        "Solution_link_count":0.0,
        "Solution_original_content":"mgmt cosmosdb version attributeerror modul mgmt cosmosdb attribut cosmosdb",
        "Solution_preprocessed_content":null,
        "Solution_readability":8.9,
        "Solution_reading_time":1.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":142.3802777778,
        "Challenge_answer_count":3,
        "Challenge_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Challenge_closed_time":1548948415000,
        "Challenge_created_time":1548435846000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":18.3,
        "Challenge_reading_time":7.92,
        "Challenge_repo_contributor_count":92.0,
        "Challenge_repo_fork_count":2591.0,
        "Challenge_repo_issue_count":1867.0,
        "Challenge_repo_star_count":14671.0,
        "Challenge_repo_watch_count":266.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":142.3802777778,
        "Challenge_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"hey @rastala thanks for the pointer, we are working on updating that notebook to a newer version of databricks and spark. @jreynolds01 is looking at this based on this issue https:\/\/github.com\/Microsoft\/Recommenders\/issues\/427 yes, this should be fixed with my PR. fixed with #438 ",
        "Solution_gpt_summary":"updat notebook newer version spark remov extra index url sdk regular pypi preview version unsupport implement pull request github",
        "Solution_link_count":1.0,
        "Solution_original_content":"rastala pointer updat notebook newer version spark jreynold base http github com",
        "Solution_preprocessed_content":"pointer updat notebook newer version spark base",
        "Solution_readability":6.2,
        "Solution_reading_time":3.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":42.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0492769148,
        "Challenge_watch_issue_ratio":0.1424745581
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"I have trained a model locally using the R package locfit. I am now trying to run this in Azure Machine Learning.\r\n\r\nMost guides\/previous questions appear to be in relation to Azure Machine Learning (classic). Although I believe the process outlined in similar posts will be similar (e.g. here, here, I am still unable to get it to work.\r\n\r\nI have outlined the steps I have followed below:\r\n\r\nDownload locfit R package for windows Zip file from here\r\n\r\nPut this downloaded Zip file into a new Zip file entitled \"locfit_package\"\r\n\r\nI upload this \"locfit_package\" zip folder to AML as a dataset (Create Dataset > From Local Files > name: locfit_package dataset type: file > Upload the zip (\"locfit_package\") > Confirm upload is correct\r\n\r\nIn the R terminal I then execute the following code:\r\n\r\n```\r\ninstall.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, verbose = TRUE)\r\n\r\nlibrary(locfit_package, lib.loc=\".\", verbose=TRUE)\r\n\r\nlibrary(locfit)\r\n\r\n```\r\nThe following error message is then returned:\r\n\r\n```\r\nsystem (cmd0): \/usr\/lib\/R\/bin\/R CMD INSTALL\r\n\r\nWarning: invalid package \u2018src\/locfit_package.zip\u2019 Error: ERROR: no packages specified Warning message:\r\n\r\nIn install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, : installation of package \u2018src\/locfit_package.zip\u2019 had non-zero exit status Error in library(locfit_package, lib.loc = \".\", verbose = TRUE) : there is no package called \u2018locfit_package\u2019 Execution halted\r\n\r\n\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1631291354000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1590",
        "Challenge_link_count":0,
        "Challenge_open_time":10543.5127777778,
        "Challenge_readability":11.6,
        "Challenge_reading_time":18.3,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to open R locfit package in Azure Machine Learning",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":198,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello, \r\n\r\nWe are trying to mount an Azure Storage account in Azure ML. This works perfectly fine, until we start a child run. In the logs of the child run, we can see the following:\r\nSet Dataset input__c79bd306's target path to \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4\r\n\r\nBut when we try to access the mount, we get the following error: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4': No such file or directory\r\n\r\nThe code to start the child run can be found below.\r\nThank you for your help.\r\n\r\n`child_config = ScriptRunConfig(source_directory='.',\r\n                                       script='src\/main_child.py',\r\n                                       arguments=arguments,\r\n                                       environment=environment,\r\n                                       docker_runtime_config=DockerConfiguration(use_docker=True),\r\n                                       compute_target=compute_target)\r\nrun.submit_child(child_config)`\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1631278836000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_open_time":10546.99,
        "Challenge_readability":17.3,
        "Challenge_reading_time":13.75,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML mounting Storage Account",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Pandas dataframes with arrays as column values seem to be incorrectly persisted. An example:\r\n\r\n```python\r\ntest_df = pd.DataFrame({'x': [np.random.rand(1000) for _ in range(1000)]})\r\nds = Datastore.get_default(ws)\r\nDataset.Tabular.register_pandas_dataframe(test_df, ds, 'test_dataset')\r\n\r\ntest_df.head()\r\n###\r\n\tx\r\n0\t[0.5044850335733219, 0.6054305053424696, 0.669...\r\n1\t[0.41759815476145723, 0.266477750018155, 0.511...\r\n2\t[0.6777708610872593, 0.16925324567267985, 0.16...\r\n3\t[0.4268294269387616, 0.6540643485117185, 0.033...\r\n4\t[0.6560106490417036, 0.5804652379458484, 0.582...\r\n\r\nDataset.get_by_name(ws, 'test_dataset').to_pandas_dataframe().head()\r\n###\r\nx\r\n0\tERROR\r\n1\tERROR\r\n2\tERROR\r\n3\tERROR\r\n4\tERROR\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1630657501000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1587",
        "Challenge_link_count":0,
        "Challenge_open_time":10719.5830555556,
        "Challenge_readability":8.6,
        "Challenge_reading_time":10.17,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Pandas dataframes with array column values are not correctly persisted as AzureML datasets",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":622.6719444444,
        "Challenge_answer_count":6,
        "Challenge_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Challenge_closed_time":1632248052000,
        "Challenge_created_time":1630006433000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":622.6719444444,
        "Challenge_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":204,
        "Platform":"Github",
        "Solution_body":"[70_driver_log.txt](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/7062339\/70_driver_log.txt)\r\n\r\nError generated in new compute that uses the V1.33 SDK Any updates to this? I had created another AML Workspace and issue disappeared but for some other subscriptions it still doesnt work and errors with the same thing as in the logs. Everything works perfectly fine in V1.32 of the SDK so not sure if new update changed some sort of Identity SDK used in Azure? The driver log had error message \"Compute has no identity provisioned.\" Try updating the compute to enable managed identity, and grant managed identity access to the data storage. Ah ok I was under the impression only the compute clusters had MI and not the compute instance. I'll take a look at the docs and will also re-configure the datastore which might be issue. @rudizhou428 we had some new feature for Compute Instance, which can use your identity in the CI, but, you need to re-create the CI as it won't automatically update the existing one. @chunyli0328 Ah ok cool, I ended up creating a new Azure ML Workspace and moved all my files over and since you need to recreate the CI and clusters, I'm guessing thats why it started to work again. Closing this issue, thanks for the help everyone!",
        "Solution_gpt_summary":"updat comput enabl ident grant ident access data storag creat comput instanc ident configur datastor creat workspac creat cluster ident base access featur sdk",
        "Solution_link_count":1.0,
        "Solution_original_content":"driver log txt http github com machinelearningnotebook file driver log txt gener comput sdk updat creat aml workspac disappear subscript doesnt log perfectli sdk updat sort ident sdk driver log messag comput ident provis updat comput enabl ident grant ident access data storag impress comput cluster comput instanc doc configur datastor rudizh featur comput instanc ident creat automat updat chunyli cool end creat workspac move file recreat cluster guess that start close",
        "Solution_preprocessed_content":"gener comput sdk updat creat aml workspac disappear subscript doesnt log perfectli sdk updat sort ident sdk driver log messag comput ident updat comput enabl ident grant ident access data storag impress comput cluster comput instanc doc datastor featur comput instanc ident automat updat cool end creat workspac move file recreat cluster guess that start close",
        "Solution_readability":8.9,
        "Solution_reading_time":15.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":208.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":384.0669444444,
        "Challenge_answer_count":2,
        "Challenge_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Challenge_closed_time":1629244160000,
        "Challenge_created_time":1627861519000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":12.03,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":384.0669444444,
        "Challenge_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":119,
        "Platform":"Github",
        "Solution_body":"0.17.5 introduced a breaking change hence there is an upperbound Thank you @vizhur! @areed1192, I'm closing this issue. Please reopen if you still have questions.",
        "Solution_gpt_summary":"later version ruamel yaml instal core packag window bit version ruamel yaml introduc break upperbound",
        "Solution_link_count":0.0,
        "Solution_original_content":"introduc break upperbound vizhur are close reopen",
        "Solution_preprocessed_content":"introduc break upperbound close reopen",
        "Solution_readability":4.2,
        "Solution_reading_time":2.03,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi!\r\n\r\nWhen trying to download a registered model from the AMLS workspace, I'm getting the following traceback. The file shows up in the `target_dir` (and ADLS path) however the size is 0 bytes, so it is making the file, however no data is being transferred into it.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    432         try:\r\n--> 433             return exec_func()\r\n    434         except exceptions as request_exception:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in exec_func()\r\n    212                                                           max_connections=max_concurrency,\r\n--> 213                                                           validate_content=_validate_check_sum)\r\n    214             file_size = os.stat(path).st_size\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/baseblobservice.py in get_blob_to_path(self, container_name, blob_name, file_path, open_mode, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout)\r\n   1855 \r\n-> 1856         with open(file_path, open_mode) as stream:\r\n   1857             blob = self.get_blob_to_stream(\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAzureMLException                          Traceback (most recent call last)\r\n<command-3894832347418984> in <module>\r\n----> 1 existing_model.download(target_dir=\"\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\")\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in download(self, target_dir, exist_ok, exists_ok)\r\n    999 \r\n   1000         # download files using sas\r\n-> 1001         file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\r\n   1002         if len(file_paths) == 0:\r\n   1003             raise WebserviceException(\"Illegal state. Unpack={}, Paths in target_dir is \"\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in _download_model_files(self, sas_to_relative_download_path, target_dir, exist_ok)\r\n    940                                           \"{}\".format(target_path), logger=module_logger)\r\n    941             sas_to_relative_download_path[sas] = target_path\r\n--> 942             download_file(sas, target_path, stream=True)\r\n    943 \r\n    944         if self.unpack:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in download_file(source_uri, path, max_retries, stream, protocol, session, _validate_check_sum, max_concurrency)\r\n    219                                        'present in blob.'.format(file_size, content_length))\r\n    220 \r\n--> 221         return _retry(exec_func, max_retries=max_retries)\r\n    222 \r\n    223     # download using requests.Session\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    443             else:\r\n    444                 module_logger.error('Failed to download file with error: {}'.format(request_exception))\r\n--> 445                 raise AzureMLException('Download of file failed with error: {}'.format(request_exception))\r\n    446         finally:\r\n    447             clean_up_func()\r\n\r\nAzureMLException: AzureMLException:\r\n\tMessage: Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\"\r\n    }\r\n}\r\n\r\n```\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/7530947\/125011096-a8c32700-e01c-11eb-83b4-4305be4095df.png)\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1625795312000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1545",
        "Challenge_link_count":1,
        "Challenge_open_time":12070.1911111111,
        "Challenge_readability":20.3,
        "Challenge_reading_time":51.46,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureMLException with model.download",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":281,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":1430.2352777778,
        "Challenge_answer_count":1,
        "Challenge_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":1630367426000,
        "Challenge_created_time":1625218579000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":13.3,
        "Challenge_reading_time":21.68,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1430.2352777778,
        "Challenge_title":"Bug: Failure while loading azureml_run_type_providers",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":129,
        "Platform":"Github",
        "Solution_body":"Thanks for the report! azureml-sdk==1.13.0 does specify docker<5.0.0, while mlflow==1.18.0 requires 5.0.0. \r\n\r\nI'm going to close this issue as there is no action for azureml-sdk.",
        "Solution_gpt_summary":"specifi docker depend depend close action sdk",
        "Solution_link_count":0.0,
        "Solution_original_content":"report sdk specifi docker close action sdk",
        "Solution_preprocessed_content":"report specifi close action sdk",
        "Solution_readability":2.9,
        "Solution_reading_time":2.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":25.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":386.3980555556,
        "Challenge_answer_count":2,
        "Challenge_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1626388206000,
        "Challenge_created_time":1624997173000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":25.6,
        "Challenge_reading_time":13.49,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":386.3980555556,
        "Challenge_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"Thanks for submitting the issue. I am fixing this broken link now.  The links should be fixed on next SDK release on Aug 3rd",
        "Solution_gpt_summary":"respons broken link sdk releas august",
        "Solution_link_count":0.0,
        "Solution_original_content":"submit broken link link sdk releas aug",
        "Solution_preprocessed_content":"submit broken link link sdk releas aug",
        "Solution_readability":2.9,
        "Solution_reading_time":1.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":24.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":519.1605555556,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\n\r\nI have installed the azure ml using below environment yml, installation happened without any issues but when I import the azureml.core I am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**Exception**\r\nimport azureml.core\r\n`Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).`\r\n\r\nAzure ML SDK Version:  1.31.0\r\n\r\n\r\nPlease help.\r\nThanks",
        "Challenge_closed_time":1626388114000,
        "Challenge_created_time":1624519136000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1523",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":14.93,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":519.1605555556,
        "Challenge_title":"Warning while loading the azureml.core",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":101,
        "Platform":"Github",
        "Solution_body":"Can you try installing [azureml-core](https:\/\/pypi.org\/project\/azureml-core\/) instead? This is unfortunately a package conflict issue. I was able to create the conda environment by removing all the version pinning on `matplotlib`, `numpy`, ... all the way to `pytorch`, and changed `azureml-sdk` to `azureml-core`.  \r\n\r\n```yml\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib\r\n- numpy\r\n- seaborn\r\n- urllib3\r\n- scipy\r\n- scikit-learn\r\n- pandas\r\n- py-xgboost\r\n- jupyterlab\r\n- ipykernel\r\n- pytorch\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-core\r\n\r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]                               \r\n```\r\n\r\n```bash\r\n(base) \u279c  jiazho_playground \u2717 conda activate ati_reranking_automl_py36\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground git:(split-merge) \u2717 python\r\nPython 3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59)\r\n[GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import azureml.core\r\n>>>\r\n\r\n\r\n(ati_reranking_automl_py36) \u279c  jiazho_playground \u2717 pip freeze | grep azureml.core\r\nazureml-core==1.32.0\r\n```\r\n",
        "Solution_gpt_summary":"instal core sdk remov version pin packag matplotlib numpi sdk core conda environ yml file",
        "Solution_link_count":1.0,
        "Solution_original_content":"instal core http pypi org core unfortun packag conflict creat conda environ remov version pin matplotlib numpi pytorch sdk core yml ati rerank automl depend interpret version later pip conda matplotlib numpi seaborn urllib scipi scikit panda jupyterlab ipykernel pytorch pip base sdk core pytorch transform score dep infer schema numpi bash base jiazho playground conda activ ati rerank automl ati rerank automl jiazho playground git split merg anaconda default jun gcc linux type copyright credit licens import core ati rerank automl jiazho playground pip freez grep core core",
        "Solution_preprocessed_content":"instal unfortun packag conflict creat conda environ remov version pin",
        "Solution_readability":10.4,
        "Solution_reading_time":15.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":120.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":674.2494444444,
        "Challenge_answer_count":7,
        "Challenge_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Challenge_closed_time":1626719342000,
        "Challenge_created_time":1624292044000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":15.5,
        "Challenge_reading_time":36.56,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":674.2494444444,
        "Challenge_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":249,
        "Platform":"Github",
        "Solution_body":"@afogarty85 can you share the version of SDK you are using? ```\r\nimport azureml\r\nprint(azureml.core.__version__)\r\n1.31.0\r\n``` @afogarty85, I'm unable to reproduce the error you are seeing. Is the pipeline running despite the error\/warning? It is running\/working anyways and indeed -- on a different workspace, I too cannot reproduce it. I am not sure why it is a symptom of the one I am on. I am opening a bug for investigation and will update you when I have a response.  I am also running into this issue with code that was working previously. Had a weekly pipeline scheduled to run at the start of every Monday. It usually took around a couple of minutes  to finish but looking back at some logs it seems like after June 13  runs were taking 100+ hours and most timed out. I tried to manually run the pipeline and hit the exact same issue with Expecting StepRun object, not sure if there was some sort of update around the middle of June to the SDK?\r\n\r\n\r\n***EDIT Had to update the Azure ML SDK along with the azureml-automl-core, azureml-pipeline-core, and azureml-pipeline packages*** I'm sharing the investigation from engineering below. Since this is expected behavior, we will not be fixing it. Hope this helps. \r\n\r\nThis bug is activated if the user has a package version conflict in their local python environment, the PipelineRun.wait_for_completion() method may fail with an error 'Unexpected keyword argument timeout_seconds'. This is because the run rehydration fails and we receive a run object with the wrong type, which doesn't have this argument.",
        "Solution_gpt_summary":"version sdk updat sdk automl core pipelin core pipelin packag messag packag version conflict local environ pipelinerun wait complet keyword argument timeout packag version conflict local environ",
        "Solution_link_count":0.0,
        "Solution_original_content":"afogarti share version sdk import print core version afogarti reproduc see pipelin run warn run anywai workspac reproduc symptom open updat respons run previous weekli pipelin schedul run start mondai took coupl minut finish log june run hour time tri manual run pipelin hit exact steprun object sort updat middl june sdk edit updat sdk automl core pipelin core pipelin packag share engin hope activ packag version conflict local environ pipelinerun wait complet keyword argument timeout run rehydr receiv run object type argument",
        "Solution_preprocessed_content":"share version sdk reproduc see pipelin run anywai workspac reproduc symptom open updat respons run previous weekli pipelin schedul run start mondai took coupl minut finish log june run hour time tri manual run pipelin hit exact steprun object sort updat middl june sdk edit updat sdk pipelin packag share engin hope activ packag version conflict local environ keyword argument run rehydr receiv run object type argument",
        "Solution_readability":8.0,
        "Solution_reading_time":18.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":257.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":890.5602777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Challenge_closed_time":1626798489000,
        "Challenge_created_time":1623592472000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.9,
        "Challenge_reading_time":12.91,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":890.5602777778,
        "Challenge_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":130,
        "Platform":"Github",
        "Solution_body":"Extra docker steps is no longer supported. Please create an environment instead where you can inject files as you wish and use that environment for deployment. Here is a sample.\r\n\r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb\r\n",
        "Solution_gpt_summary":"creat environ extra docker step inject file sampl environ github repositori notebook",
        "Solution_link_count":1.0,
        "Solution_original_content":"extra docker step longer creat environ inject file wish environ deploy sampl http github com machinelearningnotebook blob master deploy deploi cloud model regist deploi ipynb",
        "Solution_preprocessed_content":"extra docker step longer creat environ inject file wish environ deploy sampl",
        "Solution_readability":16.7,
        "Solution_reading_time":4.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":31.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":2342.6461111111,
        "Challenge_answer_count":1,
        "Challenge_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Challenge_closed_time":1631108652000,
        "Challenge_created_time":1622675126000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":15.1,
        "Challenge_reading_time":43.59,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":2342.6461111111,
        "Challenge_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":292,
        "Platform":"Github",
        "Solution_body":"After a long while I've tried the exact same script in the same compute instance again. I don't experience the connection error anymore so I will close this ticket. I don't know what changed.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tri exact comput instanc connect anymor close ticket",
        "Solution_preprocessed_content":"tri exact comput instanc connect anymor close ticket",
        "Solution_readability":5.3,
        "Solution_reading_time":2.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":34.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":524.3688888889,
        "Challenge_answer_count":10,
        "Challenge_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1622654301000,
        "Challenge_created_time":1620766573000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Challenge_link_count":12,
        "Challenge_open_time":null,
        "Challenge_readability":29.1,
        "Challenge_reading_time":35.07,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":524.3688888889,
        "Challenge_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":112,
        "Platform":"Github",
        "Solution_body":"Hi,\r\n\r\nI have some updates:\r\n\r\n- I put the code below to set the environment.\r\n\r\nfrom azureml.core.environment import Environment\r\n\r\nenv = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\nmyenv = env.clone(\"automl_env\")\r\n\r\ntrain_steps = AutoMLPipelineBuilder.get_many_models_train_steps(experiment=experiment,\r\n                                                                automl_settings=automl_settings,\r\n                                                                train_data=dataset_input,\r\n                                                                compute_target=compute_target,\r\n                                                                partition_column_names=partition_column_names,\r\n                                                                node_count=1,\r\n                                                                process_count_per_node=2,\r\n                                                                run_invocation_timeout=3700,\r\n                                                                train_env=myenv)\r\n\r\n- The environment problem has been resolved, but now the process displays the message **ValueError: None is not in list**. I don't know what this means.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/118014360-82894f80-b329-11eb-8e6a-558d6606d7b1.png)\r\n\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 3.0.0 (\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages), Requirement.parse('pyarrow<2.0.0,>=0.17.0'), {'azureml-dataset-runtime'}).\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 212, in <module>\r\n    logs = run(data_file_path, args, automl_settings, current_step_run)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_many_models\/train_model.py\", line 100, in run\r\n    data = pd.read_csv(file_path, parse_dates=[timestamp_column])\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 676, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 448, in _read\r\n    parser = TextFileReader(fp_or_buf, **kwds)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 880, in __init__\r\n    self._make_engine(self.engine)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1114, in _make_engine\r\n    self._engine = CParserWrapper(self.f, **self.options)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 1949, in __init__\r\n    self._set_noconvert_columns()\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2015, in _set_noconvert_columns\r\n    _set(val)\r\n  File \"\/azureml-envs\/azureml_f3e17a31e8bb78187505ee1343fa990d\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py\", line 2005, in _set\r\n    x = names.index(x)\r\nValueError: None is not in list\r\n\r\nBest regards,\r\nCristina @crisansou is there any error surfaced in 70_driver_log? \r\nHi @shbijlan ,\r\n\r\nI deleted the workspace. I tried to reproduce the steps again but I couldn't even create the experiment, below is the error message. Can you tell which is the recommended version to use this solution?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119876912-c752e000-befe-11eb-9a18-c8f03afae48c.png)\r\n\r\nI don't know if it's related, but I realized that now compute instance is using version 1.29.\r\n\r\n!pip install --upgrade azureml-sdk[automl]\r\n!pip install azureml-contrib-automl-pipeline-steps\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/119877097-03864080-beff-11eb-8134-07d22a243284.png)\r\n\r\n\r\n\r\n\r\n> @crisansou is there any error surfaced in 70_driver_log?\r\n\r\n from azureml.core. import Environment\r\ntrain_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n\r\nCan you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n\r\nAlso this solution only supports 'forecasting' and needs a time_column_name passed in automl settings Hi @deeptim123 ,\r\n\r\nThanks for the instructions! After including the environment I was able to run the cell, but the pipeline ended with an error because I am using a regression model.\r\n\r\nIn the next release, in addition to fixing the bug, will it be possible to use regression?\r\n\r\n> from azureml.core. import Environment\r\n> train_env = Environment.get(workspace = ws, name = 'AzureML-AutoML')\r\n> \r\n> Can you please pass train_env like above for the workaround? There is a bug in our code that needs to be fixed. we will fix it in our next release.\r\n> \r\n> Also this solution only supports 'forecasting' and needs a time_column_name passed in automl settings\r\n\r\n There are currently no plans to support regression. @cartacioS  for visibility of this ask @deeptim123 ,\r\n\r\nThanks for the info. I think it's important to add this functionality for regression and classification as well.\r\n\r\n> There are currently no plans to support regression. @cartacioS for visibility of this ask\r\n\r\n @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at sabina.cartacio@microsoft.com and we can further discuss.\r\n\r\nThanks! Hi @cartacioS ,\r\n\r\nGot it, thanks for the info! The project is starting now, but if it is really necessary to use the multiple models solution for regression I'll send you an email.\r\n\r\n> @crisansou - this is currently not on our roadmap, and purposefully unprioritized as 90% of our customer base, especially customers investing in thousands+ models are leveraging only forecasting scenarios. Priorities change from one semester to the next, and it may be supported at a later date but is not in scope right now. If you are or have a direct customer who is blocked by the lack of many model support for regression and classification please contact me at [sabina.cartacio@microsoft.com](mailto:sabina.cartacio@microsoft.com) and we can further discuss.\r\n> \r\n> Thanks!\r\n\r\n Closing as this is being tracked offline as a feature request for MANY MODELS, by Sabina.",
        "Solution_gpt_summary":"set environ env environ workspac tutori myenv env clone automl env environ process displai messag valueerror list tri reproduc step creat workaround pass train env train env environ workspac automl releas forecast time column pass automl set plan regress",
        "Solution_link_count":3.0,
        "Solution_original_content":"updat set environ core environ import environ env environ workspac tutori myenv env clone automl env train step automlpipelinebuild model train step automl set automl set train data dataset input comput target comput target partit column partit column node count process count node run invoc timeout train env myenv environ process displai messag valueerror list imag http imag githubusercont com ddb png load run type load entrypoint automl train automl run auto run dto except pyarrow env feaebbeefad lib site packag pars pyarrow dataset runtim traceback file env feaebbeefad lib site packag train automl runtim model train model line log run data file path arg automl set step run file env feaebbeefad lib site packag train automl runtim model train model line run data read csv file path pars date timestamp column file env feaebbeefad lib site packag panda parser line parser return read filepath buffer kwd file env feaebbeefad lib site packag panda parser line read parser textfileread buf kwd file env feaebbeefad lib site packag panda parser line init engin engin file env feaebbeefad lib site packag panda parser line engin engin cparserwrapp option file env feaebbeefad lib site packag panda parser line init set noconvert column file env feaebbeefad lib site packag panda parser line set noconvert column set val file env feaebbeefad lib site packag panda parser line set index valueerror list cristina crisans surfac driver log shbijlan delet workspac tri reproduc step creat messag version imag http imag githubusercont com befe cfafaec png relat realiz comput instanc version pip instal upgrad sdk automl pip instal contrib automl pipelin step imag http imag githubusercont com beff png crisans surfac driver log core import environ train env environ workspac automl pass train env workaround releas forecast time column pass automl set deeptim instruct environ run cell pipelin end regress model releas addit regress core import environ train env environ workspac automl pass train env workaround releas forecast time column pass automl set plan regress cartacio visibl deeptim import add function regress classif plan regress cartacio visibl crisans roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact sabina cartacio com cartacio start multipl model regress send email crisans roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact sabina cartacio com mailto sabina cartacio com close track offlin featur request model sabina",
        "Solution_preprocessed_content":"updat set environ import environ env myenv environ process displai messag valueerror list load load entrypoint automl except traceback file line log arg file line run data file line return kwd file line parser kwd file line file line file line file line file line valueerror list cristina surfac delet workspac tri reproduc step creat messag version relat realiz comput instanc version pip instal sdk pip instal surfac core import environ pass workaround releas forecast pass automl set instruct environ run cell pipelin end regress model releas addit regress core import environ pass workaround releas forecast pass automl set plan regress visibl import add function regress classif plan regress visibl roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact start multipl model regress send email roadmap purposefulli unpriorit base invest thousand model leverag forecast prioriti semest later date scope direct block lack model regress classif contact close track offlin featur request model sabina",
        "Solution_readability":11.4,
        "Solution_reading_time":82.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":57.0,
        "Solution_word_count":664.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":2331.6441666667,
        "Challenge_answer_count":6,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/74793968\/110592377-36daee00-81a0-11eb-8fb0-de7e2ba93af1.png)\r\n\r\nThis issue wasn't present until a few days ago. Issue shows up when we submit an experiment to azure ml workspace in the image build logs. We are using mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 base image",
        "Challenge_closed_time":1623755236000,
        "Challenge_created_time":1615361317000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1387",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.8,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":2331.6441666667,
        "Challenge_title":"azure ml Python SDK 1.24.0 image build fails with the error failed to get layer was working fine before",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":51,
        "Platform":"Github",
        "Solution_body":"I would think that is some transient issue. \r\nSide note, mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 was deprecated in 2019, please use \r\nmcr.microsoft.com\/azureml\/intelmpi2018.3-ubuntu16.04 or better default cpu image from your client version that is pinned to a versioned tag Still facing the same issue. Kind of blocked is their some way i could fix this. Updated the image thanks for that is it local build? if yes, try to remove the image Nope it is remote compute build. It is in AML Compute cluster The issue got resolved we were earlier creating an image and storing it in Azure Container Registry. Now we don't pass it to RunConfiguration() object. We create it directly in the AML Build process and that has fixed the issue though now the image is not cached anymore so that is problematic. @MAQ-Ravijit-Ramana it would be great to get some details of your scenario, like the script you running",
        "Solution_gpt_summary":"creat imag directli build process creat store registri pass runconfigur object imag cach anymor problemat",
        "Solution_link_count":0.0,
        "Solution_original_content":"transient note mcr com base intelmpi ubuntu deprec mcr com intelmpi ubuntu default cpu imag client version pin version tag block updat imag local build remov imag nope remot comput build aml comput cluster earlier creat imag store registri pass runconfigur object creat directli aml build process imag cach anymor problemat maq ravijit ramana great run",
        "Solution_preprocessed_content":"transient note deprec default cpu imag client version pin version tag block updat imag local build remov imag nope remot comput build aml comput cluster earlier creat imag store registri pass runconfigur object creat directli aml build process imag cach anymor problemat great run",
        "Solution_readability":7.4,
        "Solution_reading_time":11.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":148.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"\r\nAs, title says: there are no instructions on where to submit bugs\/glitches related to the Python azureml-sdk. \r\nIs it through github somewhere? Is it through an Azure support ticket? \r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Challenge_closed_time":null,
        "Challenge_created_time":1615198296000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1378",
        "Challenge_link_count":2,
        "Challenge_open_time":15013.8066666667,
        "Challenge_readability":12.5,
        "Challenge_reading_time":12.4,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"No instructions on where to submit bugs\/glitches related to the Python azureml-sdk",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":86,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":91.9480555556,
        "Challenge_answer_count":2,
        "Challenge_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Challenge_closed_time":1614604742000,
        "Challenge_created_time":1614273729000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.6,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":91.9480555556,
        "Challenge_title":"Azure ML Run docker command to pull public image failed ",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":179,
        "Platform":"Github",
        "Solution_body":"please try this \r\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1247#issuecomment-738887772 Seems like it solved the issue. Thanks!",
        "Solution_gpt_summary":"link github clear express gratitud",
        "Solution_link_count":1.0,
        "Solution_original_content":"http github com machinelearningnotebook issuecom",
        "Solution_preprocessed_content":null,
        "Solution_readability":13.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":11.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":82.9133333333,
        "Challenge_answer_count":4,
        "Challenge_body":"**Upload data to a datastore**\r\n![AzureUpload](https:\/\/user-images.githubusercontent.com\/947785\/108407641-3e325b80-71e1-11eb-85df-58479ed8db52.png)\r\n\r\nNow that you have determined the available datastores, you can upload files from your local file system to a datastore so that it will be accessible to experiments running in the workspace, regardless of where the experiment script is actually being run.\r\n\r\n_default_ds.upload_files(files=['.\/data\/diabetes.csv', '.\/data\/diabetes2.csv'], # Upload the diabetes csv files in \/data\r\n                       target_path='diabetes-data\/', # Put it in a folder path in the datastore\r\n                       overwrite=True, # Replace existing files of the same name\r\n                       show_progress=True)_\r\n\r\nUploading an estimated of 2 files\r\nUploading .\/data\/diabetes.csv\r\nUploading .\/data\/diabetes2.csv\r\nUploaded 0 files\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 332, in handler\r\n    result = future.result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in <lambda>\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 463, in create_blob_from_path\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 582, in create_blob_from_stream\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 971, in _put_blob\r\n    return self._perform_request(request, _parse_base_properties)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 381, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 306, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 292, in _perform_request\r\n    HTTPError(response.status, response.message, response.headers, response.body))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\r\n    raise ex\r\nazure.common.AzureHttpError: This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\r\nRequestId:a9ffd72c-c01e-00d9-5220-064b2e000000\r\nTime:2021-02-18T18:02:54.3372191Z<\/Message><\/Error>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 994, in emit\r\n    msg = self.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 840, in format\r\n    return fmt.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 577, in format\r\n    record.message = record.getMessage()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/traitlets\/config\/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelapp.py\", line 612, in start\r\n    self.io_loop.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/platform\/asyncio.py\", line 199, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 814, in inner\r\n    self.ctx_run(self.run)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 775, in run\r\n    yielded = self.gen.send(value)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 362, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 265, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2867, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2895, in _run_cell\r\n    return runner(coro)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3072, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3263, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3343, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-0f28dc9194af>\", line 4, in <module>\r\n    show_progress=True)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in upload_files\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 321, in _start_upload_task\r\n    tq.add_task(async_task)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 55, in __exit__\r\n    self.flush(self.identity)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in flush\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in <genexpr>\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/async_task.py\", line 58, in wait\r\n    res = self._handler(self._future, self._logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 340, in handler\r\n    exception_handler(e, logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 304, in exception_handler\r\n    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\r\nMessage: 'Upload failed, please make sure target_path does not start with invalid characters.'\r\nArguments: (AzureHttpError('This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\\nRequestId:a9ffd72c-c01e-00d9-5220-064b2e000000\\nTime:2021-02-18T18:02:54.3372191Z<\/Message><\/Error>',),)\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 332, in handler\r\n    result = future.result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in <lambda>\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 463, in create_blob_from_path\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 582, in create_blob_from_stream\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 971, in _put_blob\r\n    return self._perform_request(request, _parse_base_properties)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 381, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 306, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 292, in _perform_request\r\n    HTTPError(response.status, response.message, response.headers, response.body))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\r\n    raise ex\r\nazure.common.AzureHttpError: This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\r\nRequestId:5488fc90-001e-0080-5d20-064ea8000000\r\nTime:2021-02-18T18:02:54.3372332Z<\/Message><\/Error>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 994, in emit\r\n    msg = self.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 840, in format\r\n    return fmt.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 577, in format\r\n    record.message = record.getMessage()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/traitlets\/config\/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelapp.py\", line 612, in start\r\n    self.io_loop.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/platform\/asyncio.py\", line 199, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 814, in inner\r\n    self.ctx_run(self.run)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 775, in run\r\n    yielded = self.gen.send(value)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 362, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 265, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2867, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2895, in _run_cell\r\n    return runner(coro)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3072, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3263, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3343, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-0f28dc9194af>\", line 4, in <module>\r\n    show_progress=True)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in upload_files\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 321, in _start_upload_task\r\n    tq.add_task(async_task)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 55, in __exit__\r\n    self.flush(self.identity)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in flush\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in <genexpr>\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/async_task.py\", line 58, in wait\r\n    res = self._handler(self._future, self._logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 340, in handler\r\n    exception_handler(e, logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 304, in exception_handler\r\n    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\r\nMessage: 'Upload failed, please make sure target_path does not start with invalid characters.'\r\nArguments: (AzureHttpError('This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\\nRequestId:5488fc90-001e-0080-5d20-064ea8000000\\nTime:2021-02-18T18:02:54.3372332Z<\/Message><\/Error>',),)\r\n$AZUREML_DATAREFERENCE_010e49b94ea645928f99f4a15d7b3a00\r\nfrom azurem",
        "Challenge_closed_time":1613973498000,
        "Challenge_created_time":1613675010000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1348",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.8,
        "Challenge_reading_time":280.06,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":199,
        "Challenge_solved_time":82.9133333333,
        "Challenge_title":"ERROR:  Learning: Build AI solutions with Azure Machine Learning - 06 - Work with Data - Upload data to a datastore",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":1293,
        "Platform":"Github",
        "Solution_body":"@jb80016 can you provide more context on this issue? which example are you using? is it from this repository or elsewhere?  LEARNING PATH\r\nBuild AI solutions with Azure Machine Learning\r\nWork with Data in Azure Machine Learning  link:  https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/work-with-data-in-aml\/ \r\nCloned this repository to workspace within Azure using public key:  git@github.com:MicrosoftLearning\/mslearn-dp100.git \r\n\r\nLet me know if any other info would be helpful.   I figured it out using:  \r\n\r\nfrom azureml.core import Workspace\r\nws = Workspace.from_config()\r\ndatastore = ws.get_default_datastore()\r\ndatastore.upload(src_dir='.\/data',\r\n                 target_path='diabetes-data',\r\n                 overwrite=True)\r\n\r\nfrom azureml.core import Dataset\r\n\r\n# Get the default datastore\r\ndefault_ds = ws.get_default_datastore()\r\n\r\n#Create a tabular dataset from the path on the datastore (this may take a short while)\r\ntab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data\/*.csv'))\r\n\r\n# Display the first 20 rows as a Pandas dataframe\r\ntab_data_set.take(20).to_pandas_dataframe() \r\n We are closing this issue, but if you have any follow-ups, please reopen it!  #please-close",
        "Solution_gpt_summary":"workspac upload data default datastor creat tabular dataset path datastor displai row panda datafram",
        "Solution_link_count":1.0,
        "Solution_original_content":"context repositori path build data link http doc com modul data aml clone repositori workspac public kei git github com microsoftlearn mslearn git figur core import workspac workspac config datastor default datastor datastor upload src dir data target path diabet data overwrit core import dataset default datastor default default datastor creat tabular dataset path datastor short tab data set dataset tabular delimit file path default diabet data csv displai row panda datafram tab data set panda datafram close up reopen close",
        "Solution_preprocessed_content":"context repositori path build data link clone repositori workspac public kei figur core import workspac datastor overwrit core import dataset default datastor creat tabular dataset path datastor displai row panda datafram close reopen",
        "Solution_readability":10.5,
        "Solution_reading_time":14.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":14.0,
        "Solution_word_count":130.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"\r\n[Enter feedback here]\r\n\r\nWe need details description of `azureml-defaults`. \r\n\r\nWe need this when deployment. In training, we usually use `azureml-core`. In deployment, `azureml-defaults` is necessary (only `azureml-core` is not enough to deploy). I heard `azureml-defaults` includes `azureml-core`. But it is not documented.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":null,
        "Challenge_created_time":1613523098000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1341",
        "Challenge_link_count":2,
        "Challenge_open_time":15479.1394444444,
        "Challenge_readability":13.1,
        "Challenge_reading_time":13.97,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"azureml-defaults not described ",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":92,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":2210.2472222222,
        "Challenge_answer_count":8,
        "Challenge_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Challenge_closed_time":1620257629000,
        "Challenge_created_time":1612300739000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":15.0,
        "Challenge_reading_time":19.6,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":2210.2472222222,
        "Challenge_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":134,
        "Platform":"Github",
        "Solution_body":"apologies, we understand the frustration and are working to fully support local execution through Azure Machine Learning with our v2 developer experience, which is approaching public preview While it is allowed to Run AzureML experiments in Local Target using the Python SDK, I am expecting the pipelines as well to be allowed to run on local target. If this is an exception then it should be clearly flagged out & documented by Microsoft at all relevant places. Below 2 pages should definitely contain this note\r\n1. \r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#azureml_core_Workspace_compute_targets\r\n(under compute_targets section)\r\n\r\n2.\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py\r\n(under target section)\r\n\r\nAlso please mention the target release date of v2 developer experience unfortunately the initial preview of v2 will not address this issue, I will allow the Pipelines team to give a more clear ETA for that. but initial preview is tentatively March 2021 Thank you for quick reply. I would be happy if this feature is included in the 2.0 release. Let me know if there is any way to rate this feature on higher priority.\r\n\r\nPS: Please change your screen name,  \"lostmygithubaccount\" is very confusing & unprofessional.  Hi @lostmygithubaccount and @meghalv .  I'm currently blocked by this issue.  I'm unable to allocate a remote Compute Target and I don't find an example on how to use my local computer.\r\n\r\nIs this feature already delivered?.  Do you have an example? Hi @lostmygithubaccount, \r\n\r\nwhat is the status of local execution of Pipelines in Azure Machine Learning? Why was this issue closed without any conclusive information or workaround? \r\n\r\nThis missing feature is blocking customers that want to use local IDE and debugging. The local pipeline is still in development. We don't have an ETA for the release date. Hi, I just wanted to contribute to the conversation and say that this feature would be much appreciated. Currently, it is difficult to bounce between local debugging and cloud deployment. This is because the lack of local pipeline support requires change in data-flow as well as various azureml-core variables that are accessible during pipeline runs. ",
        "Solution_gpt_summary":"team fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev miss featur local pipelin eta releas date run pipelin local target",
        "Solution_link_count":2.0,
        "Solution_original_content":"apolog frustrat fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev page definit note http doc com api core core workspac class core workspac comput target comput target section http doc com api core core runconfig runconfigur target section target releas date unfortun initi preview address allow pipelin team clear eta initi preview tent march quick repli happi featur releas rate featur higher prioriti screen lostmygithubaccount unprofession lostmygithubaccount meghalv block alloc remot comput target local featur deliv lostmygithubaccount statu local execut pipelin close conclus workaround miss featur block local id debug local pipelin eta releas date contribut convers featur bounc local debug cloud deploy lack local pipelin data flow core variabl access pipelin run",
        "Solution_preprocessed_content":"apolog frustrat fulli local execut public preview allow run local target sdk pipelin allow run local target except clearli flag document relev page definit note section target section target releas date unfortun initi preview address allow pipelin team clear eta initi preview tent march quick repli happi featur releas rate featur higher prioriti screen lostmygithubaccount unprofession block alloc remot comput target local featur deliv statu local execut pipelin close conclus workaround miss featur block local id debug local pipelin eta releas date contribut convers featur bounc local debug cloud deploy lack local pipelin core variabl access pipelin run",
        "Solution_readability":10.4,
        "Solution_reading_time":28.7,
        "Solution_score_count":4.0,
        "Solution_sentence_count":22.0,
        "Solution_word_count":333.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"Azure's TabularDataset implementation introduces an index, \\_\\_index\\_level_0\\_\\_ when creating or reading parquet files that were originally written by Pandas\/Python.  This occurs when an index is unnamed but has been modified at some point; if an index is named we get an extra column with the same name as the index.\r\n\r\nWhen making changes to datasets, this additional field causes Azure errors if not handled.  Depending on what's been done to the index of the original dataset, you may or may not get that additional field.\r\n\r\nI have an example notebook that can be run to reproduce the issue.  It's here: https:\/\/github.com\/vla6\/Azure_notes\/blob\/main\/tabulardataset_parquet_index_di_issue.ipynb\r\n\r\nThe notebook requires an Azure Machine Learning workspace and a storage account to run",
        "Challenge_closed_time":null,
        "Challenge_created_time":1611344395000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1299",
        "Challenge_link_count":1,
        "Challenge_open_time":16084.3347222222,
        "Challenge_readability":9.3,
        "Challenge_reading_time":10.5,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML TabularDataSet via parquet and pandas index error",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"## Describe the issue\r\n\r\nversion 1.20.0 of python package azureml-contrib-pipeline-steps throws (works fine on version 1.19 or 1.18)\r\n\r\n File \"C:\/Users\/v-songshanli\/projects\/ashexplore\/object_identification\/obj_segmentation_azure_2_steps.py\", line 88, in run\r\n    pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\core\\_experiment_method.py\", line 97, in wrapper\r\n    return init_func(self, *args, **kwargs)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\pipeline.py\", line 177, in __init__\r\n    self._graph = self._graph_builder.build(self._name, steps, finalize=False)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1481, in build\r\n    graph = self.construct(name, steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1503, in construct\r\n    self.process_collection(steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1539, in process_collection\r\n    builder.process_collection(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1830, in process_collection\r\n    self._base_builder.process_collection(item)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1533, in process_collection\r\n    return self.process_step(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1577, in process_step\r\n    node = step.create_node(self._graph, self._default_datastore, self._context)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\steps\\python_script_step.py\", line 243, in create_node\r\n    return super(PythonScriptStep, self).create_node(\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 140, in create_node\r\n    self._set_compute_params_to_node(node,\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 229, in _set_compute_params_to_node\r\n    self._module_param_provider.set_params_to_node(\r\nTypeError: _set_params_to_node_hook() got an unexpected keyword argument 'command'\r\n\r\n## Minimal example\r\n\r\n```python\r\nfrom azureml.core import Workspace\r\n\r\nws = Workspace.from_config()\r\n\r\n\r\nsplit_step = PythonScriptStep(\r\n        name=\"Train Test Split\",\r\n        script_name=\"obj_segment_step_data_process.py\",\r\n        arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\r\n                   \"--train-split\", train_split_data, \"--test-split\", test_split_data,\r\n                   \"--test-size\", 50],\r\n        compute_target=compute_target,\r\n        runconfig=aml_run_config,\r\n        source_directory=source_directory,\r\n        allow_reuse=False\r\n    )\r\n\r\npipeline_steps = [split_step ]\r\n\r\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n```\r\n\r\n## Additional context\r\nI am using aml sdk 1.20. no type errors with version 1.19\/1.18 of azureml-contrib-pipeline-steps.\r\n-\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1611092820000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1417",
        "Challenge_link_count":0,
        "Challenge_open_time":16154.2166666667,
        "Challenge_readability":23.1,
        "Challenge_reading_time":44.03,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":null,
        "Challenge_title":"python package azureml-contrib-pipeline-steps 1.20.0 not working ",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":179,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":7538.9758333333,
        "Challenge_answer_count":15,
        "Challenge_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Challenge_closed_time":1637097588000,
        "Challenge_created_time":1609957275000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":13.9,
        "Challenge_reading_time":14.88,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":3.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":7538.9758333333,
        "Challenge_title":"Error Installing Azureml. (Python 3.9 support)",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@klawrawkz :  What is your OS? What is the python and pip version? \r\n\r\nazureml-sdk only supports Python 3.5 to 3.8. So, if you're using an out-of-range version of Python (older or newer), then you'll need to use a different version. Thanks for the reply @harneetvirk. I'm pretty sure it's not a python version issue.\r\n```\r\npy --version\r\nPython 3.9.1\r\n```\r\nCould be a Win 10 version issue?\r\n![image](https:\/\/user-images.githubusercontent.com\/48074223\/103943498-2f478c00-5100-11eb-9bfd-43443a4cb582.png)\r\n\r\nI ran this command and got farther. \r\n```\r\npip install --upgrade --upgrade-strategy eager azureml-sdk\r\n```\r\nI am stuck at this point now.\r\n```\r\n...\r\nINFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of azure-mgmt-containerregistry to determine which version is compatible with other requirements. This could take a while.\r\nCollecting azure-mgmt-containerregistry>=2.0.0\r\n  Downloading azure_mgmt_containerregistry-2.7.0-py2.py3-none-any.whl (509 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 509 kB ...\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.6.0-py2.py3-none-any.whl (501 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 501 kB 1.6 MB\/s\r\nINFO: pip is looking at multiple versions of azure-mgmt-core to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading azure_mgmt_containerregistry-2.5.0-py2.py3-none-any.whl (494 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 494 kB 6.4 MB\/s\r\n  Downloading azure_mgmt_containerregistry-2.4.0-py2.py3-none-any.whl (482 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 482 kB 6.4 MB\/s\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https:\/\/pip.pypa.io\/surveys\/backtracking\r\n  Downloading azure_mgmt_containerregistry-2.3.0-py2.py3-none-any.whl (481 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481 kB 6.8 MB\/s\r\n```\r\n\r\nWhat's your advice on commands to provide \"stricter constraints to reduce runtime?\" The command (above) has been \"running\" for ~24 hours, so I'm guessing that it's dead in the H20.\r\n\r\nKlaw azureml-sdk only supports Python 3.5 to 3.8, but you are having python 3.9.1 installed in the environment.  Please change the python version between 3.5 to 3.8.\r\n\r\nAlso, the latest pip 20.3 has a new dependency resolver which is resulting in this long running dependency resolutions. If you switch to older version of pip (<20.3), you will notice the difference in the performance. Gotcha, thanks for the info. I'll make the change.\r\n\r\nKlaw If azureml-sdk does not support Python 3.9, then the metadata should be updated from:\r\n```\r\nRequires-Python: >=3.5,<4\r\n```\r\nto:\r\n```\r\nRequires-Python: >=3.5,<3.9\r\n```\r\nIs this also true for the hundreds of subpackages that azureml-sdk depends on? When is Python 3.9 support coming? when will azureml-core be compatible with python 3.9? I am currently using azureml-sdk under Python 3.9 by installing with pip's `--ignore-requires-python` option, and everything I am using seems to work fine. But there are probably some other parts that don't work... @johan12345 is this in production environment? you are using it like this? or in your local env? In my local development environment.  `azureml-core` now supports Python 3.9. unfortunately although `azureml-core` might install w\/o errors in 3.9, `azureml-sdk` still creates errors. Installed w\/o errors in 3.8.12   azureml-sdk is a meta package.  azureml-core is one of the upstream that supports python 3.9 but there are some other AutoML dependencies in azureml-sdk  which do not support python 3.9.\r\n I have just updated azureml-sdk to allow Python 3.9.\r\nThis should be included in the next Azure ML SDK release, 1.45.0. What about 3.10? 3.11 is coming out soon too. @adamjstewart Python 3.10 is already supported in the new SDK V2 preview: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-v2\r\nI expect that we will support 3.10 in SDK V1 as well but I don't have a date for that.",
        "Solution_gpt_summary":"sdk rang version older newer version switch older version pip improv perform version updat metadata sdk core sdk updat allow sdk releas sdk preview",
        "Solution_link_count":4.0,
        "Solution_original_content":"klawrawkz pip version sdk rang version older newer version repli harneetvirk pretti version version win version imag http imag githubusercont com bfd acb png ran farther pip instal upgrad upgrad strategi eager sdk stuck pip multipl version core determin version compat pip multipl version mgmt containerregistri determin version compat collect mgmt containerregistri download mgmt containerregistri whl longer depend stricter constraint reduc runtim abort run press ctrl improv pip perform http pip pypa survei backtrack download mgmt containerregistri whl pip multipl version mgmt core determin version compat download mgmt containerregistri whl download mgmt containerregistri whl longer depend stricter constraint reduc runtim abort run press ctrl improv pip perform http pip pypa survei backtrack download mgmt containerregistri whl advic stricter constraint reduc runtim run hour guess dead klaw sdk instal environ version latest pip depend run depend resolut switch older version pip hundr subpackag sdk depend come core compat sdk instal pip ignor option probabl johan environ local env local environ core unfortun core instal sdk creat instal sdk meta packag core upstream automl depend sdk updat sdk allow sdk releas come soon adamjstewart sdk preview http doc com concept sdk date",
        "Solution_preprocessed_content":"pip version sdk version version repli pretti version win version ran farther stuck advic stricter constraint reduc runtim run hour guess dead klaw sdk instal environ version latest pip depend run depend resolut switch older version pip notic perform gotcha klaw sdk metadata updat hundr subpackag sdk depend come core compat sdk instal pip option probabl environ local env local environ unfortun instal creat instal sdk meta packag core upstream automl depend sdk updat sdk allow sdk releas come soon sdk preview sdk date",
        "Solution_readability":5.2,
        "Solution_reading_time":55.84,
        "Solution_score_count":16.0,
        "Solution_sentence_count":77.0,
        "Solution_word_count":608.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":94.4830555556,
        "Challenge_answer_count":5,
        "Challenge_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Challenge_closed_time":1598387113000,
        "Challenge_created_time":1598046974000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.0,
        "Challenge_reading_time":18.26,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":94.4830555556,
        "Challenge_title":"error: azureml-train-automl-runtime is required however it is included",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":115,
        "Platform":"Github",
        "Solution_body":"can you share the full stacktrace? and is the error happening when you submit the pipeline script? or is it happening in the logs of the `PythonScriptStep`? ```\r\n\"error\": {\r\n        \"code\": \"UserError\",\r\n        \"message\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"detailsUri\": \"https:\/\/aka.ms\/azureml-known-errors\",\r\n        \"details\": [],\r\n        \"debugInfo\": {\r\n            \"type\": \"UserScriptException\",\r\n            \"message\": \"UserScriptException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException OptionalDependencyMissingException:\\n\\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\n\\tInnerException: None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"ValidationError\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ScenarioNotSuported\\\",\\n                \\\"inner_error\\\": {\\n                    \\\"code\\\": \\\"OptionalDependencyMissing\\\"\\n                }\\n            }\\n        },\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\\\"\\n    }\\n}\",\r\n            \"stackTrace\": \"  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 197, in execute_with_context\\n    raise UserScriptException(baseEx).with_traceback(exceptionInfo[2])\\n  File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/pjx-d-cu1-mlw-models\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/mounts\/workspaceblobstore\/azureml\/d881de2a-9dba-4e74-805e-d3c6eaab2076\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"run_models.py\\\", line 286, in \\n    main()\\n  File \\\"run_models.py\\\", line 197, in main\\n    run = experiment.submit(config=automl_config, tags=tags)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\\\", line 211, in submit\\n    run = submit_func(config, self.workspace, self.name, **kwargs)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 97, in _automl_static_submit\\n    show_output)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 255, in _start_execution\\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\\\", line 121, in _default_execution\\n    return automl_estimator.fit(**fit_params)\\n  File \\\"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\\\", line 349, in fit\\n    \\\"azureml-train-automl-runtime must be installed in the current environment to run local in \\\"\\n\"\r\n        },\r\n        \"messageFormat\": \"azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\",\r\n        \"messageParameters\": {}\r\n    },\r\n    \"time\": \"0001-01-01T00:00:00.000Z\"\r\n}\r\n``` Here is my stack trace from the 70_driver_log.txt:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_models.py\", line 286, in <module>\r\n    main()\r\n  File \"run_models.py\", line 197, in main\r\n    run = experiment.submit(config=automl_config, tags=tags)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/core\/experiment.py\", line 211, in submit\r\n    run = submit_func(config, self.workspace, self.name, **kwargs)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 97, in _automl_static_submit\r\n    show_output)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 255, in _start_execution\r\n    automl_run = _default_execution(experiment, settings_obj, fit_params, True, show_output, parent_run_id)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 121, in _default_execution\r\n    return automl_estimator.fit(**fit_params)\r\n  File \"\/azureml-envs\/azureml_e96633b6ad93e7baf9c7240cba821e53\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 349, in fit\r\n    \"azureml-train-automl-runtime must be installed in the current environment to run local in \"\r\nUserScriptException: UserScriptException:\r\n\tMessage: azureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n``` @swatig007 this is an error, @BillmanH is experiencing when submitting an AutoML run from within a `PythonScriptStep` rather than using an `AutoMLStep`. This approach worked for over a year, but is now throwing an error about `azureml-train-automl-runtime` not being installed. upgraded to 1.12.0, which solved this problem and opened other issues. ",
        "Solution_gpt_summary":"upgrad version automlstep pythonscriptstep submit automl run",
        "Solution_link_count":1.0,
        "Solution_original_content":"share stacktrac submit pipelin log pythonscriptstep usererror messag train automl runtim instal environ run local process run instal depend runconfigur detailsuri http aka debuginfo type userscriptexcept messag userscriptexcept tmessag train automl runtim instal environ run local process run instal depend runconfigur tinnerexcept optionaldependencymissingexcept tmessag train automl runtim instal environ run local process run instal depend runconfigur tinnerexcept terrorrespons usererror validationerror scenarionotsuport optionaldependencymiss messag train automl runtim instal environ run local process run instal depend runconfigur terrorrespons usererror messag train automl runtim instal environ run local process run instal depend runconfigur stacktrac file mnt batch task share root job pjx mlw model ddea dba dceaab mount workspaceblobstor ddea dba dceaab setup context injector line execut context rais userscriptexcept baseex traceback exceptioninfo file mnt batch task share root job pjx mlw model ddea dba dceaab mount workspaceblobstor ddea dba dceaab setup context injector line execut context runpi run path sy argv global run file env ebadebafccba lib runpi line run path pkg pkg fname file env ebadebafccba lib runpi line run modul mod mod spec pkg file env ebadebafccba lib runpi line run exec run global file run model line file run model line run submit config automl config tag tag file env ebadebafccba lib site packag core line submit run submit func config workspac kwarg file env ebadebafccba lib site packag train automl automlconfig line automl static submit output file env ebadebafccba lib site packag train automl automlconfig line start execut automl run default execut set obj fit param output parent run file env ebadebafccba lib site packag train automl automlconfig line default execut return automl estim fit fit param file env ebadebafccba lib site packag train automl azureautomlcli line fit train automl runtim instal environ run local messageformat train automl runtim instal environ run local process run instal depend runconfigur messageparamet time stack trace driver log txt traceback file run model line file run model line run submit config automl config tag tag file env ebadebafccba lib site packag core line submit run submit func config workspac kwarg file env ebadebafccba lib site packag train automl automlconfig line automl static submit output file env ebadebafccba lib site packag train automl automlconfig line start execut automl run default execut set obj fit param output parent run file env ebadebafccba lib site packag train automl automlconfig line default execut return automl estim fit fit param file env ebadebafccba lib site packag train automl azureautomlcli line fit train automl runtim instal environ run local userscriptexcept userscriptexcept messag train automl runtim instal environ run local process run instal depend runconfigur swatig billmanh experienc submit automl run pythonscriptstep automlstep year throw train automl runtim instal upgrad open",
        "Solution_preprocessed_content":"share stacktrac submit pipelin log stack trace experienc submit automl run year throw instal upgrad open",
        "Solution_readability":16.5,
        "Solution_reading_time":83.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":50.0,
        "Solution_word_count":486.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":3154.2575,
        "Challenge_answer_count":4,
        "Challenge_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Challenge_closed_time":1599067481000,
        "Challenge_created_time":1587712154000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.6,
        "Challenge_reading_time":6.1,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":3154.2575,
        "Challenge_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":55,
        "Platform":"Github",
        "Solution_body":"@swaticolab Could you please check if all versions are available when you specify the version with [get_by_name()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.abstract_dataset.abstractdataset?view=azure-ml-py#get-by-name-workspace--name--version--latest--)\r\n\r\nAlso, a note in azureml.core.dataset.dataset [documentation ](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) mentions that [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.dataset.dataset?view=azure-ml-py#to-pandas-dataframe--) is deprecated and replaced by azureml.data.tabulardataset [to_pandas_dataframe()](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py#to-pandas-dataframe-on-error--null---out-of-range-datetime--null--). Could you please check with this implementation to check if all versions are shown? @RohitMungi-MSFT Yes I did try using the get_by_name() approach. But it was still not working. @MayMSFT  dataset is just a pointer to data in your storage. here is an article that explains how dataset versioning works:\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-version-track-datasets",
        "Solution_gpt_summary":"version specifi version data tabulardataset panda datafram deprec core dataset dataset panda datafram version shown review articl dataset version version histori maintain pull data sql dataset",
        "Solution_link_count":5.0,
        "Solution_original_content":"swaticolab version specifi version http doc com api core data abstract dataset abstractdataset workspac version latest note core dataset dataset document http doc com api core core dataset dataset panda datafram panda datafram http doc com api core core dataset dataset panda datafram deprec replac data tabulardataset panda datafram http doc com api core data tabulardataset panda datafram null rang datetim null implement version shown rohitmungi msft maymsft dataset pointer data storag articl explain dataset version http doc com version track dataset",
        "Solution_preprocessed_content":"version specifi version note deprec replac implement version shown dataset pointer data storag articl explain dataset version",
        "Solution_readability":18.4,
        "Solution_reading_time":17.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":85.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":123.0280555556,
        "Challenge_answer_count":1,
        "Challenge_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Challenge_closed_time":1583847372000,
        "Challenge_created_time":1583404471000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.6,
        "Challenge_reading_time":28.94,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":123.0280555556,
        "Challenge_title":"Internal server error when deploying from Azure ML to AKS",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":201,
        "Platform":"Github",
        "Solution_body":"@robinvdheijden \r\n\r\nThanks for reaching out to us. This is forum for Machine Learning Notebook only. Please open a new forum thread in [MSDN forum](https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/home?forum=AzureMachineLearningService)as it could be better place to get help on your scenario. These forum community members could provide their expert guidance on your scenario based on their experience. Thanks.\r\n\r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"robinvdheijden reach forum notebook open forum thread msdn forum http social msdn com forum home forum azuremachinelearningservic forum commun member expert guidanc base proce close thread matter yutongti msft gladli",
        "Solution_preprocessed_content":"reach forum notebook open forum thread forum commun member expert guidanc base proce close thread matter gladli",
        "Solution_readability":8.9,
        "Solution_reading_time":7.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":80.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":0.5533333333,
        "Challenge_answer_count":2,
        "Challenge_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Challenge_closed_time":1581440044000,
        "Challenge_created_time":1581438052000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.47,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.5533333333,
        "Challenge_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"@andrewkinsella, version `1.0.17` is from [almost a year ago](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2019-02-25). During that time, the `Datasets` class has evolved significantly (for the better). Can you try upgrading the SDK to the newest version and trying again? @MayMSFT  Thank you very much @swanderz \r\nI will try your recommendation.",
        "Solution_gpt_summary":"upgrad sdk newest version dataset class evolv significantli version",
        "Solution_link_count":1.0,
        "Solution_original_content":"andrewkinsella version year ago http doc com releas note time dataset class evolv significantli upgrad sdk newest version maymsft swanderz",
        "Solution_preprocessed_content":"version time class evolv significantli upgrad sdk newest version",
        "Solution_readability":8.0,
        "Solution_reading_time":5.1,
        "Solution_score_count":3.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":45.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":795.3458333333,
        "Challenge_answer_count":8,
        "Challenge_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Challenge_closed_time":1583863460000,
        "Challenge_created_time":1581000215000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":16.2,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":795.3458333333,
        "Challenge_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":98,
        "Platform":"Github",
        "Solution_body":"Hello,\r\n\r\nWe're so sorry you've encountered this issue. I've gone ahead and filed a work item to investigate and fix the issue around whitelisting XGBoost. We will reach out again here once a fix is in.\r\n\r\nThank you,\r\nSabina It could be possible that XGBoostClassifier was blacklisted by the system. We can double check if you can share your runId. In the meanwhile, we will improve the error msg for this scenario. Thanks! @waltz2u Can you please run the following line of code in your jupyter notebook and let me know what it says? \r\n\r\n`import xgboost`\r\n\r\nThanks,\r\nSabina Hi @waltz2u, I was able to reproduce and overcome this issue by double checking that import xgboost was installed correctly by trying `import xgboost`.\r\n\r\n\r\n`pip install \"py-xgboost<=0.80\"` fixed it on my end. Can you please try that and let us know if it solved the issue?  Hi @cartacioS and @jialiu103, sorry for the late reply. Yes it works now for me. Thank you very much.\r\n\r\nCD\r\n Will now proceed to close this thread. Thanks. @cartacioS I'm facing the same error, except that I'm kicking off the AutoML run from my local machine, using a remote compute as my aml compute target. Using this issue above, and this [one](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/313), it seems that I would still need to add xgboost to my env (locally) although technically I won't be using that package in my AutoML exercise? @jadhosn If you do not require XGBoost for your training, you can simply ignore this warning. But if you want XGBoost to be a potential recommended model, then yes you will need to add XGBoost to your local environment regardless of local\/remote compute.",
        "Solution_gpt_summary":"item file whitelist doubl xgboostclassifi blacklist share runid import instal import pip instal train warn ignor potenti model local environ regardless local remot comput bias summari",
        "Solution_link_count":1.0,
        "Solution_original_content":"sorri gone ahead file item whitelist reach sabina xgboostclassifi blacklist doubl share runid improv msg waltzu run line jupyt notebook sai import sabina waltzu reproduc overcom doubl import instal import pip instal end cartacio jialiu sorri late repli proce close thread cartacio kick automl run local remot comput aml comput target http github com machinelearningnotebook add env local technic packag automl exercis jadhosn train simpli ignor warn potenti model add local environ regardless local remot comput",
        "Solution_preprocessed_content":"sorri gone ahead file item whitelist reach sabina xgboostclassifi blacklist doubl share runid improv msg run line jupyt notebook sai sabina reproduc overcom doubl import instal end sorri late repli proce close thread kick automl run local remot comput aml comput target add env technic packag automl exercis train simpli ignor warn potenti model add local environ regardless comput",
        "Solution_readability":6.3,
        "Solution_reading_time":19.93,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":275.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":1045.5080555556,
        "Challenge_answer_count":14,
        "Challenge_body":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_closed_time":1582730951000,
        "Challenge_created_time":1578967122000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/735",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.3,
        "Challenge_reading_time":1.92,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":1045.5080555556,
        "Challenge_title":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":13,
        "Platform":"Github",
        "Solution_body":"@alla15747 Hi, thanks for reaching out to us. Could you please share your environment file so that we can know the details of this issue? @alla15747  please make sure that azureml-train-automl-runtime is installed in your environment if you using sdk>=1.0.76 or azureml-train-automl if using older version I'm running the code on the compute target and not my local machine. SDK 1.0.72\r\nHow to install packages in Azure Devops environment like azureml-train-automl? (base) C:\\Users\\aabdel137>pip freeze\r\nabsl-py==0.8.1\r\nadal==1.2.2\r\nalabaster==0.7.12\r\nanaconda-client==1.7.2\r\nanaconda-navigator==1.9.7\r\nanaconda-project==0.8.3\r\nansiwrap==0.8.4\r\napplicationinsights==0.11.9\r\nasn1crypto==0.24.0\r\nastor==0.8.0\r\nastroid==2.3.1\r\nastropy==3.2.1\r\natomicwrites==1.3.0\r\nattrs==19.3.0\r\nazure-common==1.1.23\r\nazure-graphrbac==0.61.1\r\nazure-mgmt-authorization==0.60.0\r\nazure-mgmt-containerregistry==2.8.0\r\nazure-mgmt-keyvault==2.0.0\r\nazure-mgmt-resource==5.1.0\r\nazure-mgmt-storage==6.0.0\r\nazureml-contrib-interpret==1.0.72\r\nazureml-contrib-notebook==1.0.72\r\nazureml-core==1.0.72\r\nazureml-dataprep==1.1.29\r\nazureml-dataprep-native==13.1.0\r\nazureml-explain-model==1.0.72\r\nazureml-interpret==1.0.72.1\r\nazureml-pipeline==1.0.72\r\nazureml-pipeline-core==1.0.72\r\nazureml-pipeline-steps==1.0.72\r\nazureml-sdk==1.0.72\r\nazureml-telemetry==1.0.72\r\nazureml-train==1.0.72\r\nazureml-train-core==1.0.72\r\nazureml-train-restclients-hyperdrive==1.0.72\r\nazureml-widgets==1.0.72\r\nBabel==2.7.0\r\nbackcall==0.1.0\r\nbackports.functools-lru-cache==1.5\r\nbackports.os==0.1.1\r\nbackports.shutil-get-terminal-size==1.0.0\r\nbackports.tempfile==1.0\r\nbackports.weakref==1.0.post1\r\nbeautifulsoup4==4.7.1\r\nbitarray==0.9.3\r\nbkcharts==0.2\r\nbleach==3.1.0\r\nbokeh==1.2.0\r\nboto==2.49.0\r\nBottleneck==1.2.1\r\ncertifi==2019.6.16\r\ncffi==1.12.3\r\nchardet==3.0.4\r\nClick==7.0\r\ncloudpickle==1.2.1\r\nclyent==1.2.2\r\ncolorama==0.4.1\r\ncomtypes==1.1.7\r\nconda==4.7.10\r\nconda-build==3.18.8\r\nconda-package-handling==1.3.11\r\nconda-verify==3.4.2\r\ncontextlib2==0.5.5\r\ncoverage==4.5.4\r\ncryptography==2.7\r\ncycler==0.10.0\r\nCython==0.29.12\r\ncytoolz==0.10.0\r\ndask==2.1.0\r\ndecorator==4.4.0\r\ndefusedxml==0.6.0\r\ndistributed==2.1.0\r\ndistro==1.4.0\r\ndocker==4.1.0\r\ndocutils==0.14\r\ndotnetcore2==2.1.10\r\nentrypoints==0.3\r\net-xmlfile==1.0.1\r\nfastcache==1.1.0\r\nfilelock==3.0.12\r\nflake8==3.7.9\r\nflake8-formatter-junit-xml==0.0.6\r\nFlask==1.1.1\r\nfusepy==3.0.1\r\nfuture==0.17.1\r\ngast==0.3.2\r\ngevent==1.4.0\r\nglob2==0.7\r\ngoogle-pasta==0.1.7\r\ngreenlet==0.4.15\r\ngrpcio==1.24.3\r\nh5py==2.9.0\r\nheapdict==1.0.0\r\nhtml5lib==1.0.1\r\nidna==2.8\r\nimageio==2.5.0\r\nimagesize==1.1.0\r\nimportlib-metadata==0.23\r\ninterpret-community==0.1.0.3.3\r\ninterpret-core==0.1.18\r\nipykernel==5.1.1\r\nipython==7.6.1\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.0\r\nisodate==0.6.0\r\nisort==4.3.21\r\nitsdangerous==1.1.0\r\njdcal==1.4.1\r\njedi==0.13.3\r\njeepney==0.4.1\r\nJinja2==2.10.1\r\njmespath==0.9.4\r\njoblib==0.13.2\r\njson5==0.8.4\r\njsonpickle==1.2\r\njsonschema==3.0.1\r\njunit-xml==1.8\r\njupyter==1.0.0\r\njupyter-client==5.3.1\r\njupyter-console==6.0.0\r\njupyter-core==4.5.0\r\njupyterlab==1.0.2\r\njupyterlab-server==1.0.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nkeyring==18.0.0\r\nkiwisolver==1.1.0\r\nkmodes==0.10.1\r\nlazy-object-proxy==1.4.2\r\nlibarchive-c==2.8\r\nllvmlite==0.29.0\r\nlocket==0.2.0\r\nlxml==4.3.4\r\nMarkdown==3.1.1\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.1.0\r\nmccabe==0.6.1\r\nmenuinst==1.4.16\r\nmistune==0.8.4\r\nmkl-fft==1.0.12\r\nmkl-random==1.0.2\r\nmkl-service==2.0.2\r\nmock==3.0.5\r\nmore-itertools==7.2.0\r\nmpmath==1.1.0\r\nmsgpack==0.6.1\r\nmsrest==0.6.10\r\nmsrestazure==0.6.2\r\nmultipledispatch==0.6.0\r\nnavigator-updater==0.2.1\r\nnbconvert==5.5.0\r\nnbformat==4.4.0\r\nndg-httpsclient==0.5.1\r\nnetworkx==2.3\r\nnltk==3.4.4\r\nnose==1.3.7\r\nnotebook==6.0.0\r\nnumba==0.44.1\r\nnumexpr==2.6.9\r\nnumpy==1.16.4\r\nnumpydoc==0.9.1\r\noauthlib==3.1.0\r\nolefile==0.46\r\nopenpyxl==2.6.2\r\npackaging==19.2\r\npandas==0.24.2\r\npandocfilters==1.4.2\r\npapermill==1.2.1\r\nparso==0.5.0\r\npartd==1.0.0\r\npath.py==12.0.1\r\npathlib2==2.3.4\r\npathspec==0.6.0\r\npatsy==0.5.1\r\npep8==1.7.1\r\npickleshare==0.7.5\r\nPillow==6.1.0\r\npkginfo==1.5.0.1\r\npluggy==0.13.0\r\nply==3.11\r\nprometheus-client==0.7.1\r\nprompt-toolkit==2.0.9\r\nprotobuf==3.10.0\r\npsutil==5.6.3\r\npy==1.8.0\r\npy4j==0.10.7\r\npyasn1==0.4.7\r\npycodestyle==2.5.0\r\npycosat==0.6.3\r\npycparser==2.19\r\npycrypto==2.6.1\r\npycurl==7.43.0.3\r\npyflakes==2.1.1\r\nPygments==2.4.2\r\nPyJWT==1.7.1\r\npylint==2.4.2\r\npyodbc==4.0.26\r\npyOpenSSL==19.0.0\r\npyparsing==2.4.2\r\npypiwin32==223\r\npyreadline==2.1\r\npyrsistent==0.14.11\r\nPySocks==1.7.0\r\npyspark==2.4.4\r\npytest==5.2.2\r\npytest-arraydiff==0.3\r\npytest-astropy==0.5.0\r\npytest-cov==2.7.1\r\npytest-doctestplus==0.3.0\r\npytest-openfiles==0.3.2\r\npytest-remotedata==0.3.1\r\npython-dateutil==2.8.0\r\npython-dotenv==0.10.3\r\npytz==2019.1\r\nPyWavelets==1.0.3\r\npywin32==223\r\npywinpty==0.5.5\r\nPyYAML==5.1.1\r\npyzmq==18.0.0\r\nQtAwesome==0.5.7\r\nqtconsole==4.5.1\r\nQtPy==1.8.0\r\nrequests==2.22.0\r\nrequests-oauthlib==1.2.0\r\nrope==0.14.0\r\nruamel-yaml==0.15.46\r\nruamel.yaml==0.15.89\r\nscikit-image==0.15.0\r\nscikit-learn==0.21.2\r\nscipy==1.2.1\r\nseaborn==0.9.0\r\nSecretStorage==3.1.1\r\nSend2Trash==1.5.0\r\nshap==0.29.3\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.12.0\r\nsklearn==0.0\r\nsnowballstemmer==1.9.0\r\nsortedcollections==1.1.2\r\nsortedcontainers==2.1.0\r\nsoupsieve==1.8\r\nSphinx==2.1.2\r\nsphinxcontrib-applehelp==1.0.1\r\nsphinxcontrib-devhelp==1.0.1\r\nsphinxcontrib-htmlhelp==1.0.2\r\nsphinxcontrib-jsmath==1.0.1\r\nsphinxcontrib-qthelp==1.0.2\r\nsphinxcontrib-serializinghtml==1.1.3\r\nsphinxcontrib-websupport==1.1.2\r\nspyder==3.3.6\r\nspyder-kernels==0.5.1\r\nSQLAlchemy==1.3.5\r\nstatsmodels==0.10.0\r\nsympy==1.4\r\ntables==3.5.2\r\ntblib==1.4.0\r\ntenacity==5.1.5\r\ntensorboard==1.14.0\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntensorflow-gpu==1.14.0\r\ntermcolor==1.1.0\r\nterminado==0.8.2\r\ntestpath==0.4.2\r\ntextwrap3==0.9.2\r\ntf-estimator-nightly==1.14.0.dev2019031401\r\ntoolz==0.10.0\r\ntornado==6.0.3\r\ntqdm==4.37.0\r\ntraitlets==4.3.2\r\ntyped-ast==1.4.0\r\nunicodecsv==0.14.1\r\nunittest-xml-reporting==2.5.2\r\nurllib3==1.24.2\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nwebsocket-client==0.56.0\r\nWerkzeug==0.15.4\r\nwidgetsnbextension==3.5.0\r\nwin-inet-pton==1.1.0\r\nwin-unicode-console==0.5\r\nwincertstore==0.2\r\nwrapt==1.11.2\r\nxlrd==1.2.0\r\nXlsxWriter==1.1.8\r\nxlwings==0.15.8\r\nxlwt==1.3.0\r\nzict==1.0.0\r\nzipp==0.6.0\r\n AutoML became a part of default distribution (azureml-sdk) since 1.0.83\r\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/azure-machine-learning-release-notes#2020-01-06\r\n\r\nif your client, that I believe pins the version of azureml sdk packages for the remote environment is 1.0.83, you will have automl on remote. \r\n\r\nIf you want to stay with 1.0.72 you can either reference automl extras azureml-sdk[automl] or explicitly reference azureml-train-automl (prefered).\r\n\r\nI would recommend to do both, upgrade client to the latest version and explicitly reference packages you need for your particular scenario not relying on metapackages like azureml-sdk\r\n\r\nOur reference doc will help you to get a set of the packages needed for your scenario\r\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py\r\n\r\nBy design every AzureML Python SDK package will bring necessary internal dependencies (of course except some corner cases :) )\r\n Thanks Vizhur, do you mind showing an example on how to reference azureml-train-automl in Azure Devops or Portal? \r\nThank you for the links! Not sure about your particular scenario, would you mind to share your ADO scenario so I can think of how to update it? MY scenario is implementing MLOPs example with automl step. Let me try to pull some relevant folks into the thread For AutoML, all the remote dependencies will get taken care of and will match whatever local dependencies are installed, e.g. if you have azureml-train-automl==1.0.72 installed, that version will be installed remotely for the training job.\r\nWe provide 2 clients to submitting these remote jobs currently, a thin client for submitting some types of remote jobs which is included as part of azureml-sdk, and a fuller client which enables more experiences such as Pipeline runs as part of azureml-train-automl. Since it looks like you are trying to use Pipelines, you will need to install the full azureml-train-automl client.\r\n\r\nFurthermore, the namespace for AutoMLStep changed recently, if you are using <1.0.76 the namespace would be \"from azureml.train.automl import AutoMLStep\", for >=1.0.76, you'll want to use \"from azureml.train.automl.runtime import AutoMLStep\" instead. I'm have sdk 1.0.72 installed. And I'm using from azureml.train.automl import AutoMLStep. Is there anyway to check the sdk version on the compute target machine? From your pip freeze, it doesn't look like you have the AutoML SDK installed. For the pipelines experience, you will need to have the SDK installed locally, not just on the target compute. Could you run \"pip install azureml-train-automl\"? @alla15747 \r\nWe will now proceed to close this thread. If there are further questions regarding this matter, please respond here and @YutongTie-MSFT and we will gladly continue the discussion. @SKrupa - Are you running your own code or a particular notebook sample from this repo?\r\n\r\n",
        "Solution_gpt_summary":"train automl runtim instal environ sdk train automl older version upgrad client latest version explicitli packag reli metapackag sdk automl remot depend taken care match local depend instal instal train automl client pipelin instal automl sdk local target comput pipelin sdk version comput target run pip instal train automl automl sdk instal",
        "Solution_link_count":2.0,
        "Solution_original_content":"alla reach share environ file alla train automl runtim instal environ sdk train automl older version run comput target local sdk instal packag devop environ train automl base aabdel pip freez absl adal alabast anaconda client anaconda navig anaconda ansiwrap applicationinsight asncrypto astor astroid astropi atomicwrit attr common graphrbac mgmt author mgmt containerregistri mgmt keyvault mgmt resourc mgmt storag contrib interpret contrib notebook core dataprep dataprep nativ explain model interpret pipelin pipelin core pipelin step sdk telemetri train train core train restclient hyperdr widget babel backcal backport functool lru cach backport backport shutil termin size backport tempfil backport weakref beautifulsoup bitarrai bkchart bleach bokeh boto bottleneck certifi cffi chardet click cloudpickl clyent colorama comtyp conda conda build conda packag conda verifi contextlib coverag cryptographi cycler cython cytoolz dask decor defusedxml distribut distro docker docutil dotnetcor entrypoint xmlfile fastcach filelock flake flake formatt junit xml flask fusepi futur gast gevent glob pasta greenlet grpcio hpy heapdict htmllib idna imageio images importlib metadata interpret commun interpret core ipykernel ipython ipython genutil ipywidget isod isort itsdanger jdcal jedi jeepnei jinja jmespath joblib json jsonpickl jsonschema junit xml jupyt jupyt client jupyt consol jupyt core jupyterlab jupyterlab server kera kera preprocess keyr kiwisolv kmode lazi object proxi libarch llvmlite locket lxml markdown markupsaf matplotlib mccabe menuinst mistun mkl fft mkl random mkl servic mock itertool mpmath msgpack msrest msrestazur multipledispatch navig updat nbconvert nbformat ndg httpsclient networkx nltk nose notebook numba numexpr numpi numpydoc oauthlib olefil openpyxl packag panda pandocfilt papermil parso partd path pathlib pathspec patsi pep pickleshar pillow pkginfo pluggi ply prometheu client prompt toolkit protobuf psutil pyj pyasn pycodestyl pycosat pycpars pycrypto pycurl pyflak pygment pyjwt pylint pyodbc pyopenssl pypars pypiwin pyreadlin pyrsist pysock pyspark pytest pytest arraydiff pytest astropi pytest cov pytest doctestplu pytest openfil pytest remotedata dateutil dotenv pytz pywavelet pywin pywinpti pyyaml pyzmq qtawesom qtconsol qtpy request request oauthlib rope ruamel yaml ruamel yaml scikit imag scikit scipi seaborn secretstorag sendtrash shap simplegener singledispatch sklearn snowballstemm sortedcollect sortedcontain soupsiev sphinx sphinxcontrib applehelp sphinxcontrib devhelp sphinxcontrib htmlhelp sphinxcontrib jsmath sphinxcontrib qthelp sphinxcontrib serializinghtml sphinxcontrib websupport spyder spyder kernel sqlalchemi statsmodel sympi tabl tblib tenac tensorboard tensorflow tensorflow estim tensorflow gpu termcolor terminado testpath textwrap estim nightli dev toolz tornado tqdm traitlet type ast unicodecsv unittest xml report urllib wcwidth webencod websocket client werkzeug widgetsnbextens win inet pton win unicod consol wincertstor wrapt xlrd xlsxwriter xlwing xlwt zict zipp automl default distribut sdk http doc com releas note client believ pin version sdk packag remot environ automl remot stai automl extra sdk automl explicitli train automl prefer upgrad client latest version explicitli packag reli metapackag sdk doc set packag http doc com api overview design sdk packag intern depend cours corner vizhur train automl devop portal link share ado updat implement mlop automl step pull relev folk thread automl remot depend taken care match local depend instal train automl instal version instal remot train job client submit remot job client submit type remot job sdk fuller client enabl pipelin run train automl pipelin instal train automl client furthermor namespac automlstep train automl runtim import automlstep sdk instal train automl import automlstep sdk version comput target pip freez automl sdk instal pipelin sdk instal local target comput run pip instal train automl alla proce close thread matter yutongti msft gladli skrupa run notebook sampl repo",
        "Solution_preprocessed_content":"reach share environ file instal environ older version run comput target local sdk instal packag devop environ freez automl default distribut client believ pin version sdk packag remot environ automl remot stai automl extra sdk explicitli upgrad client latest version explicitli packag reli metapackag sdk doc set packag design sdk packag intern depend vizhur devop portal link share ado updat implement mlop automl step pull relev folk thread automl remot depend taken care match local depend instal instal version instal remot train job client submit remot job client submit type remot job sdk fuller client enabl pipelin run pipelin instal client furthermor namespac automlstep import automlstep sdk instal import automlstep sdk version comput target pip freez automl sdk instal pipelin sdk instal local target comput run pip instal proce close thread matter gladli run notebook sampl repo",
        "Solution_readability":16.1,
        "Solution_reading_time":117.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":38.0,
        "Solution_word_count":786.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":75.5002777778,
        "Challenge_answer_count":1,
        "Challenge_body":"I've installed and updated the sdk yet when attempting to import the module for the ExplainationDashboard i keep getting the error that the interpret module does not exist.\r\ni am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb \r\nthe failing line in the sample notebook is:\r\n**from azureml.contrib.interpret.visualize import ExplanationDashboard**\r\n**\"ModuleNotFoundError: No module named 'azureml.contrib.interpret' \"**\r\nthis is the update command i ran:\r\npip install --upgrade azureml-sdk[explain,automl,contrib] \r\n(the install ran fine - no errors)\r\njim",
        "Challenge_closed_time":1573060395000,
        "Challenge_created_time":1572788594000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/639",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.6,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":75.5002777778,
        "Challenge_title":"azureml.contrib.interpret - ModuleNotFoundError - build 1.0.72",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":79,
        "Platform":"Github",
        "Solution_body":"azureml-contrib-interpret is not a part of azureml-sdk contrib extras. Please install it separately",
        "Solution_gpt_summary":"instal contrib interpret modul separ sdk explain automl contrib packag",
        "Solution_link_count":0.0,
        "Solution_original_content":"contrib interpret sdk contrib extra instal separ",
        "Solution_preprocessed_content":"sdk contrib extra instal separ",
        "Solution_readability":9.4,
        "Solution_reading_time":1.28,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":4111.2283333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Challenge_closed_time":1581034133000,
        "Challenge_created_time":1566233711000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":8.08,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":4111.2283333333,
        "Challenge_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"Hi Nema. Unfortunately I don't have access to your workspace. Would you provide more details on the failed experiment such as experiment\/pipeline id so that I can take a look at the logs of it? Hi Sonny, here is the run id: `eb6f111d-1251-40d2-b745-e3c4fbb31fcf` Thank you for the runid. I found automl setup has been timed out after some time. I will work with automl team for more details.  It seems to have been a one-off random occurrence. Considering solved. Somehow I lost track on this. You can let me know if you have any further issues.  ",
        "Solution_gpt_summary":"team member request run log team member automl setup time automl team random occurr consist dataset approxim featur",
        "Solution_link_count":0.0,
        "Solution_original_content":"nema unfortun access workspac pipelin log sonni run ebfd ecfbbfcf runid automl setup time time automl team random occurr lost track",
        "Solution_preprocessed_content":"nema unfortun access workspac log sonni run runid automl setup time time automl team random occurr lost track",
        "Solution_readability":4.4,
        "Solution_reading_time":6.6,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":96.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":0.4416666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Challenge_closed_time":1565217619000,
        "Challenge_created_time":1565216029000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":7.24,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":0.4416666667,
        "Challenge_title":"No name 'opendatasets' in module 'azureml' Error",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":71,
        "Platform":"Github",
        "Solution_body":"Find the solution, maybe because `opendatasets` is a preview module, so it is not included in azureml sdk yet. You can download through pip `pip install azureml-opendatasets` in your env. > pip install azureml-opendatasets\r\n\r\nThanks, was looking for the solution, this worked !! However, I had another error \" [WinError 5] Access is denied:\" This was solved by adding --user at the end of your command.",
        "Solution_gpt_summary":"opendataset modul version sdk download pip pip instal opendataset environ messag winerror access end",
        "Solution_link_count":0.0,
        "Solution_original_content":"mayb opendataset preview modul sdk download pip pip instal opendataset env pip instal opendataset winerror access end",
        "Solution_preprocessed_content":"mayb preview modul sdk download pip env pip instal opendataset access end",
        "Solution_readability":7.1,
        "Solution_reading_time":4.91,
        "Solution_score_count":10.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":63.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":9752.3283333333,
        "Challenge_answer_count":2,
        "Challenge_body":"> from azureml.core.model import Model\r\nmodel = Model.register(model_path = MODEL_FILENAME,\r\n                       model_name = \"MyONNXmodel\",\r\n                       tags = {\"onnx\":\"V0\"},\r\n                       description = \"test\",\r\n                       workspace = ws)\r\nthis is the python code I am using to register the model.",
        "Challenge_closed_time":1587148100000,
        "Challenge_created_time":1552039718000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/243",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":16.4,
        "Challenge_reading_time":6.17,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2296.0,
        "Challenge_repo_issue_count":1858.0,
        "Challenge_repo_star_count":3528.0,
        "Challenge_repo_watch_count":2030.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":9752.3283333333,
        "Challenge_title":"Unable to register the model using Jupyter Notebook with error message: \"HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET https:\/\/cert-westeurope.experiments.azureml.net\/rp\/workspaces'\"",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":50,
        "Platform":"Github",
        "Solution_body":"Thanks for your report. Are you still experiencing this issue? Have you posted on the forum https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/home?forum=AzureMachineLearningService? Thank you for reaching out to us.  We see our answer this issue was delayed. Our apologies. We did not receive a response to our response, so will close this issue for now. Should you need further assistance, please submit a post on this forum. We are happy to help.\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"report experienc forum http social msdn com forum home forum azuremachinelearningservic reach delai apolog receiv respons respons close assist submit forum happi",
        "Solution_preprocessed_content":"report experienc forum reach delai apolog receiv respons respons close assist submit forum happi",
        "Solution_readability":6.6,
        "Solution_reading_time":5.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":67.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0296017223,
        "Challenge_watch_issue_ratio":1.0925726588
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.22.0\r\nOS: linux\r\nOS Release: 5.15.0-1022-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.68.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1976096\r\ns extension.js:2:1972783\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669213419000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1817",
        "Challenge_link_count":0,
        "Challenge_open_time":9.6058333333,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.06,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to login in Azure ML extension in VSCode",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":88,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1668197837000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1797",
        "Challenge_link_count":0,
        "Challenge_open_time":291.7119444444,
        "Challenge_readability":9.8,
        "Challenge_reading_time":8.78,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"connecting to VScode to AzureML",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":84,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: linux\r\nOS Release: 5.15.0-1017-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1665696667000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1754",
        "Challenge_link_count":0,
        "Challenge_open_time":986.4813888889,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.33,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Mutliple consecutive sign-in requests from Azure ML plugin VS Code",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1664990324000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1745",
        "Challenge_link_count":0,
        "Challenge_open_time":1182.6877777778,
        "Challenge_readability":10.0,
        "Challenge_reading_time":9.17,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Problem signing into Azure ML using VSCode with latest version",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":89,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.18.0\r\nOS: darwin\r\nOS Release: 21.6.0\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1664468718000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1737",
        "Challenge_link_count":0,
        "Challenge_open_time":1327.5783333333,
        "Challenge_readability":8.7,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Working with Azure ML Studio on VSCode",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":86,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Azure sign in\r\n2. Sign in using Azure portal. You get the sign in successful, you may close the window message, but Azure asks to sign in again.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.17.2022090809\r\nOS: win32\r\nOS Release: 10.0.19042\r\nProduct: Visual Studio Code - Insiders\r\nProduct Version: 1.72.0-insider\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1662929331000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1714",
        "Challenge_link_count":0,
        "Challenge_open_time":1755.1858333333,
        "Challenge_readability":7.8,
        "Challenge_reading_time":11.43,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"I keep on getting this error continuously for Azure Machine Learning extension",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":119,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":572.4141666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Challenge_closed_time":1663956142000,
        "Challenge_created_time":1661895451000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":3.19,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":572.4141666667,
        "Challenge_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":40,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":1032.455,
        "Challenge_answer_count":2,
        "Challenge_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Challenge_closed_time":1665527881000,
        "Challenge_created_time":1661811043000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.0,
        "Challenge_reading_time":4.74,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":1032.455,
        "Challenge_title":"Can't use Azure ML features when remotely connected to a compute",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"Seems to work when remotely connected via VS Code desktop, but it definitely doesn't work when connected via vscode.dev. The azure ml remote extension is disabled in this case. Seems like we should be able to support this. Yes, this is only on web platforms like vscode.dev or codespaces.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"remot connect desktop definit connect vscode dev remot extens disabl web platform vscode dev codespac",
        "Solution_preprocessed_content":"remot connect desktop definit connect remot extens disabl web platform codespac",
        "Solution_readability":4.8,
        "Solution_reading_time":3.53,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"## Expected Behavior\r\nIf the user is logged out, the AML extension should not prompt to login until the user specifically tries to run an AzureML command. Prompting when VS Code loads is disruptive and unnecessary, and no other extensions for AWS or Azure do this.\r\n\r\n## Actual Behavior\r\nIf you are signed out of the Azure ML extension and reload VS Code, you are prompted to login when it loads (Issue #1). If you click cancel, you are prompted again (#2). \r\n\r\n## Steps to Reproduce the Problem\r\n  1. Install the Azure ML Extension\r\n  2. Login\r\n  3. Logout\r\n  4. Reload VS Code\r\n  5. Click \"Cancel\" when prompted to login\r\n\r\n\r\n## Specifications\r\nAzure ML Extension Version 0.16.0\r\n \r\nVersion: 1.70.0-insider (Universal)\r\nCommit: da76f93349a72022ca4670c1b84860304616aaa2\r\nDate: 2022-08-03T05:55:27.651Z (1 day ago)\r\nElectron: 18.3.5\r\nChromium: 100.0.4896.160\r\nNode.js: 16.13.2\r\nV8: 10.0.139.17-electron.0\r\nOS: Darwin x64 21.6.0\r\n\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1659626460000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1627",
        "Challenge_link_count":0,
        "Challenge_open_time":2672.65,
        "Challenge_readability":5.9,
        "Challenge_reading_time":11.77,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML Prompts twice to login when VS Code (Insiders) loads",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":6.2444444444,
        "Challenge_answer_count":1,
        "Challenge_body":"## Expected Behavior\r\nIt seems new version on AzureML extension to VS Code doesn't have this option in settings. I needed to downgrade to 0.6x.\r\n\r\n## Actual Behavior\r\nCurrent version 0.10.0 doesn't have the option. Cannot locally debug or documentation doesn't provide info about that.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.10.0\r\n  - Platform: VS Code, Windows\r\n",
        "Challenge_closed_time":1654701310000,
        "Challenge_created_time":1654678830000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1589",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.42,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":6.2444444444,
        "Challenge_title":"Run and debug experiments locally - azureML.CLI Compatibility Mode for CLI v1 - cannot find",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":63,
        "Platform":"Github",
        "Solution_body":"@michalmar We have completely deprecated the v1 CLI Compatibility mode settings from v0.8.0 onwards and v2 mode will be the way going forward :).",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"michalmar complet deprec cli compat mode set onward mode forward",
        "Solution_preprocessed_content":"complet deprec cli compat mode set onward mode forward",
        "Solution_readability":9.0,
        "Solution_reading_time":1.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":23.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":430.5058333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the customer is shown an error message but also has the option to report an issue which is misleading, we should remove the report issue button for this scenario.",
        "Challenge_closed_time":1655835283000,
        "Challenge_created_time":1654285462000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1588",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":430.5058333333,
        "Challenge_title":"Improve the error message when trying to execute a YAML that is not Azure ML realted",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":45,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":659.0783333333,
        "Challenge_answer_count":4,
        "Challenge_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Challenge_closed_time":1652117002000,
        "Challenge_created_time":1649744320000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":13.57,
        "Challenge_repo_contributor_count":19.0,
        "Challenge_repo_fork_count":94.0,
        "Challenge_repo_issue_count":1834.0,
        "Challenge_repo_star_count":281.0,
        "Challenge_repo_watch_count":36.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":659.0783333333,
        "Challenge_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"@evakkuri thanks for filing this issue. Is this happening every time you open a remote connection? Are you connecting to Compute Instance through the ML Studio? Currently this happens every time I connect. I'm not connecting via ML Studio, instead through VS Code with the Azure Machine Learning extension. @sevillal Can you please follow up here :) ? @evakkuri we have published version v0.10.0 of the extension. Could you please upgrade and retry to check if you issue is still reproducible? Please reopen this issue if that's the case.",
        "Solution_gpt_summary":"upgrad version extens retri reproduc persist reopen",
        "Solution_link_count":0.0,
        "Solution_original_content":"evakkuri file time open remot connect connect comput instanc studio time connect connect studio extens sevil evakkuri publish version extens upgrad retri reproduc reopen",
        "Solution_preprocessed_content":"file time open remot connect connect comput instanc studio time connect connect studio extens publish version extens upgrad retri reproduc reopen",
        "Solution_readability":4.3,
        "Solution_reading_time":6.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":10.0,
        "Solution_word_count":87.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0103598691,
        "Challenge_watch_issue_ratio":0.0196292257
    },
    {
        "Challenge_adjusted_solved_time":243.16,
        "Challenge_answer_count":3,
        "Challenge_body":"Error when adding extetnion azureml\r\naz k8s-extension create --name azureml-extension --extension-type Microsoft.AzureML.Kubernetes --config enableTraining= cluster-type conneced--cluster-name <your-AKS-cluster-name> --resource-group <your-RG-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Creating\"}\r\ncli.azure.cli.core.sdk.policies: Request URL: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: Request method: 'GET'\r\ncli.azure.cli.core.sdk.policies: Request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'CommandName': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'ParameterSetName': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'User-Agent': 'AZURECLI\/2.36.0 (MSI) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 Python\/3.10.4 (Windows-10-10.0.19044-SP0)'\r\ncli.azure.cli.core.sdk.policies:     'Authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: Request body:\r\ncli.azure.cli.core.sdk.policies: This request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmanagement.azure.com%2F&data=05%7C01%7Cjohan.andolf%40microsoft.com%7C37b5d083c3d7447f133208da3e347a4a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637890692414003835%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=1kWOqV7FwAgqmYol4W7wfZRbf%2BCTKz9XucDBe%2FKgGKA%3D&reserved=0) \"GET \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01 HTTP\/1.1\" 200 None\r\ncli.azure.cli.core.sdk.policies: Response status: 200\r\ncli.azure.cli.core.sdk.policies: Response headers:\r\ncli.azure.cli.core.sdk.policies:     'Cache-Control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Transfer-Encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'Content-Type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'Content-Encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'Expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'Vary': 'Accept-Encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-Preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'X-Content-Type-Options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'SWEDENCENTRAL:20220525T095135Z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'Date': 'Wed, 25 May 2022 09:51:34 GMT'\r\ncli.azure.cli.core.sdk.policies: Response content:\r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Failed\",\"error\":{\"code\":\"ExtensionCreationFailed\",\"message\":\" error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: Traceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.OperationFailed: Operation failed or canceled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.HttpResponseError: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x0387C190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: Command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: Save telemetry record of length 3581 in cache\r\ntelemetry.check: Returns Positive.\r\ntelemetry.main: Begin creating telemetry upload process.\r\ntelemetry.process: Creating upload process: \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\python.exe C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\Lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc C:\\Users\\ropa04\\.azure\"\r\ntelemetry.process: Return from creating process\r\ntelemetry.main: Finish creating telemetry upload process.",
        "Challenge_closed_time":1654438640000,
        "Challenge_created_time":1653563264000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/1213",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":19.4,
        "Challenge_reading_time":114.38,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":66,
        "Challenge_solved_time":243.16,
        "Challenge_title":"Can not add AzureML extention on openshift cluster ",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":533,
        "Platform":"Github",
        "Solution_body":"Hey friend! Thanks for opening this issue. We appreciate your contribution and welcome you to our community! We are glad to have you here and to have your input on the Azure Arc Jumpstart.<p><\/p> Hello Johan, thanks for submitting feedback. Have you checked the [prerequisites specific to ARO](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-kubernetes-anywhere?tabs=studio#prerequisites) prior to attempting the extension installation?  Hello @rataxe , have you checked the pre-reqs above. If you already had and still facing a problem, we recommend you open a support case as this is not strictly related to the Jumpstart project but to the product itself. ",
        "Solution_gpt_summary":"prerequisit aro extens instal prerequisit persist open relat jumpstart",
        "Solution_link_count":1.0,
        "Solution_original_content":"friend open contribut welcom commun glad input arc jumpstart johan submit feedback prerequisit aro http doc com attach kubernet tab studio prerequisit prior extens instal ratax pre req open strictli relat jumpstart",
        "Solution_preprocessed_content":"friend open contribut welcom commun glad input arc johan submit feedback prior extens instal open strictli relat jumpstart",
        "Solution_readability":9.0,
        "Solution_reading_time":8.61,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":93.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0396927017,
        "Challenge_watch_issue_ratio":0.0166453265
    },
    {
        "Challenge_adjusted_solved_time":41.2755555556,
        "Challenge_answer_count":3,
        "Challenge_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Challenge_closed_time":1631711922000,
        "Challenge_created_time":1631563330000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":10.4,
        "Challenge_reading_time":56.26,
        "Challenge_repo_contributor_count":62.0,
        "Challenge_repo_fork_count":369.0,
        "Challenge_repo_issue_count":1562.0,
        "Challenge_repo_star_count":527.0,
        "Challenge_repo_watch_count":26.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":41.2755555556,
        "Challenge_title":"error when installing AZURE ML training model piece",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":477,
        "Platform":"Github",
        "Solution_body":"Hi @arturoqu77 - thanks for reaching out. We tried to repro this issue but couldn't.\r\n\r\nThis [line of code](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/AzureMLLogonScript.ps1#L266) leverages grep to parse the file name. `grep` should have been installed as part of the [bootstrap](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73). If  `grep` wasn't installed, this implies something must have interrupted the install before it got there.\r\n\r\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working. \r\n\r\nAre you seeing Postman installed - this happens [after `grep`](https:\/\/github.com\/microsoft\/azure_arc\/blob\/a322f4915a72f860779e4d92d7d111848883a344\/azure_arc_ml_jumpstart\/aks\/arm_template\/artifacts\/Bootstrap.ps1#L73)? If not, this is probably what happened.\r\n\r\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above. Hello,\n\nThank you for your reply. I may have logged on before the bootstrap completed, I re-started the deployment to a new RG and seems to be working now.\n\nThanks for the help.\n\nRegards\n\n***@***.***\nArturo Quiroga\nSr. Cloud Solutions Architect (CSA)\nAzure Applications & Infrastructure\n***@***.******@***.***>\n[MSFT_logo_Gray DE sized SIG1.png]\n\n\nFrom: Raki ***@***.***>\nDate: Tuesday, September 14, 2021 at 6:22 PM\nTo: microsoft\/azure_arc ***@***.***>\nCc: Arturo Quiroga ***@***.***>, Mention ***@***.***>\nSubject: Re: [microsoft\/azure_arc] error when installing AZURE ML training model piece (#758)\n\nHi @arturoqu77<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Farturoqu77&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666347896%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=r1kAuKxYlYhONjoSTk83SERggUvNcbP1Hr4vmNh29io%3D&reserved=0> - thanks for reaching out. We tried to repro this issue but couldn't.\n\nThis line of code<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FAzureMLLogonScript.ps1%23L266&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=oAjL%2BfBBF4QXfnwN9gcM9UqEB4OA0ZZrzMuKilatz5A%3D&reserved=0> leverages grep to parse the file name. grep should have been installed as part of the bootstrap<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666357889%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=V8dzJxj3W5a6IL8T%2BvB0mijBm5Ng4G46bb%2Fcdo2uvz4%3D&reserved=0>. If grep wasn't installed, this implies something must have interrupted the install before it got there.\n\nDid you by any chance RDP into the VM before the Deployment was fully finished? That would cause the chocolatey install flow to break - which would also explain why the Training above isn't working.\n\nAre you seeing Postman installed - this happens after grep<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fblob%2Fa322f4915a72f860779e4d92d7d111848883a344%2Fazure_arc_ml_jumpstart%2Faks%2Farm_template%2Fartifacts%2FBootstrap.ps1%23L73&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=XvpeFo2T7Kjr4qrIZYKO7eM0khlOddES9O3DGaw1yZ4%3D&reserved=0>? If not, this is probably what happened.\n\nCould you try the deployment in a new RG, but this time ensuring you RDP in once ARM returns success (and the Bootstrap script is successful in running - you can see this in the ARM deployment status from the RG)? If you can't repro this issue once more, we can eliminate the above.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fazure_arc%2Fissues%2F758%23issuecomment-919554382&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666367883%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ZBGNkrDGFcqvrdWHXy5iEGluQiq2Ph%2BZnfosqC3qTTU%3D&reserved=0>, or unsubscribe<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAHV4QUFA72NR7CEJ3UPS5NLUB7DLDANCNFSM5D6SSBHA&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ctEevpiqzC%2FQnTc6ho2hfr2PVGA%2FqwGJzj1pPUCEylY%3D&reserved=0>.\nTriage notifications on the go with GitHub Mobile for iOS<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666377877%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=cMCZqYPB6q8c9n%2BgPTk9f3MCQr%2BlV4GsOW9iPFSZtgE%3D&reserved=0> or Android<https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Carturoqu%40microsoft.com%7C4df75e22c063478509d008d977ce2b14%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637672549666387876%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6B5T09q%2Bx2Q2rWftui6b32lD1VLrCRMPiLSrTUS7xnI%3D&reserved=0>.\n Great!",
        "Solution_gpt_summary":"log bootstrap complet chocolatei instal flow break deploy rdp arm return bootstrap run",
        "Solution_link_count":11.0,
        "Solution_original_content":"arturoqu reach tri repro line http github com arc blob afafeddda arc jumpstart ak arm templat artifact logonscript leverag grep pars file grep instal bootstrap http github com arc blob afafeddda arc jumpstart ak arm templat artifact bootstrap grep wasn instal interrupt instal chanc rdp deploy fulli finish chocolatei instal flow break explain train isn see postman instal grep http github com arc blob afafeddda arc jumpstart ak arm templat artifact bootstrap probabl deploy time rdp arm return bootstrap run arm deploy statu repro elimin repli log bootstrap complet start deploy arturo quiroga cloud architect csa infrastructur msft logo grai size sig png raki date tuesdai septemb arc arturo quiroga subject arc instal train model piec arturoqu reach tri repro line leverag grep pars file grep instal bootstrap grep wasn instal interrupt instal chanc rdp deploy fulli finish chocolatei instal flow break explain train isn see postman instal grep probabl deploy time rdp arm return bootstrap run arm deploy statu repro elimin receiv repli email directli github unsubscrib triag notif github mobil io android great",
        "Solution_preprocessed_content":"reach tri repro leverag grep pars file instal wasn instal interrupt instal chanc rdp deploy fulli finish chocolatei instal flow break explain train isn see postman instal probabl deploy time rdp arm return repro elimin repli log bootstrap complet deploy arturo quiroga cloud architect infrastructur size raki date tuesdai septemb arturo quiroga subject instal train model piec reach tri repro line leverag grep pars file grep instal grep wasn instal interrupt instal chanc rdp deploy fulli finish chocolatei instal flow break explain train isn see postman instal probabl deploy time rdp arm return repro elimin receiv repli email directli triag notif github mobil great",
        "Solution_readability":21.6,
        "Solution_reading_time":96.07,
        "Solution_score_count":1.0,
        "Solution_sentence_count":39.0,
        "Solution_word_count":416.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0396927017,
        "Challenge_watch_issue_ratio":0.0166453265
    },
    {
        "Challenge_adjusted_solved_time":1510.63,
        "Challenge_answer_count":1,
        "Challenge_body":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False\r\n\r\nThis behaviour is a bit confusing and I had to debug the code to understand what was happening. I would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what I want that is train locally.\r\n\r\nRepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.DeepSMILECrck \r\n\r\nAlso the histopathology.DeepSMILECrck is not trainable because it does not have a default encoder type. Should we flag base classes as not trainable and throw an error?",
        "Challenge_closed_time":1657547192000,
        "Challenge_created_time":1652108924000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/hi-ml\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":22.0,
        "Challenge_repo_issue_count":693.0,
        "Challenge_repo_star_count":111.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":1510.63,
        "Challenge_title":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":120,
        "Platform":"Github",
        "Solution_body":"Resolved in #420",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":0.9,
        "Solution_reading_time":0.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0245310245,
        "Challenge_watch_issue_ratio":0.0101010101
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n\r\nThe introduction section of the 11_exploring_hyperparameters_on_azureml notebook under object detection includes two broken links:\r\n` [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb)`\r\n`[03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb)`\r\n\r\nThe master branch of this repo (which I am working from...please tell me that was intended...) does not contain these notebooks. \r\n\r\n### In which platform does it happen?\r\nAll.\r\n\r\n### How do we replicate the issue?\r\nClick the links\r\n\r\n### Expected behavior (i.e. solution)\r\nNotebooks are present or links are removed\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573072566000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/410",
        "Challenge_link_count":0,
        "Challenge_open_time":26715.3983333333,
        "Challenge_readability":11.4,
        "Challenge_reading_time":9.05,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Some links to notebooks in introduction are broken in 11_exploring_hyperparameters_on_azureml notebook",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":79,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n\r\nUsers need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. Some guidance within the notebook on how to obtain these values and fill in the strings would be helpful.\r\n\r\nIt would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to `get_or_create_workspace()` later on.\r\n\r\n### Expected behavior with the suggested feature\r\n\r\nUsers who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. Novice users receive some guidance on how to obtain their Azure subscription id without having to reference other notebooks.\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573072237000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/409",
        "Challenge_link_count":0,
        "Challenge_open_time":26715.4897222222,
        "Challenge_readability":11.8,
        "Challenge_reading_time":11.22,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"[FEATURE_REQUEST] Provide guidance on how to obtain a subscription id in 11_exploring_hyperparameters_on_azureml notebook",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":149,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\n\r\nThe 11_exploring_hyperparameters_on_azureml notebook contains the following link in markdown:\r\n`[20_azure_workspace_setup.ipynb](..\/..\/classification\/notebooks\/20_azure_workspace_setup.ipynb)`\r\n\r\nThe link does not resolve properly -- it appears the relative location of the notebook has changed.\r\n\r\n### In which platform does it happen?\r\nAll\r\n\r\n### How do we replicate the issue?\r\nClick the link\r\n\r\n### Expected behavior (i.e. solution)\r\nLink works\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573071661000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/408",
        "Challenge_link_count":0,
        "Challenge_open_time":26715.6497222222,
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.49,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Link to 20_azure_workspace_setup.ipynb in 11_exploring_hyperparameters_on_azureml notebook is broken",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\nThe current version of AzureML is a little dated \r\nhttps:\/\/github.com\/microsoft\/ComputerVision\/blob\/3e0631e0dc7d5ddbfc6283b1e89b3ce51f0bd449\/environment.yml#L41\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573068707000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/404",
        "Challenge_link_count":1,
        "Challenge_open_time":26716.4702777778,
        "Challenge_readability":17.6,
        "Challenge_reading_time":3.25,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[FEATURE_REQUEST] AzureML may need to be updated 1.0.30->1.0.72?",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":19,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### Description\r\nIn https:\/\/github.com\/microsoft\/ComputerVision\/blob\/master\/scenarios\/detection\/11_exploring_hyperparameters_on_azureml.ipynb\r\nyou copy the whole directory in order to make use of the utils_cv\r\nThis is a bit cumbersome and unecesarily copies things around. You can create a pip wheel package of your utils_cv and add it as a dependency see here https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#add-private-pip-wheel-workspace--file-path--exist-ok-false-\r\n\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1573065169000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/396",
        "Challenge_link_count":2,
        "Challenge_open_time":26717.4530555556,
        "Challenge_readability":16.8,
        "Challenge_reading_time":7.8,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"[FEATURE_REQUEST] Install utils_cv as a pip wheel in AzureML",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":54,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nThis is the error, it looks it is related to the deployment of ACI and AKS resources. \r\n\r\n\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:35:17.380577', 'duration': 1113.334717, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [26]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-26-21aec20dbbb2> in <module>\r\nE                 1 # Deploy the web service\r\nE           ----> 2 service.wait_for_deployment(show_output=True)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<01:03,  1.01cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:44,  1.40cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:31,  1.92cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:04<01:13,  1.24s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<01:00,  1.07s\/cell]\r\nExecuting:  15%|\u2588\u258c        | 10\/65 [00:05<00:43,  1.27cell\/s]\r\nExecuting:  18%|\u2588\u258a        | 12\/65 [00:05<00:30,  1.72cell\/s]\r\nExecuting:  20%|\u2588\u2588        | 13\/65 [00:06<00:22,  2.26cell\/s]\r\nExecuting:  23%|\u2588\u2588\u258e       | 15\/65 [00:07<00:23,  2.15cell\/s]\r\nExecuting:  26%|\u2588\u2588\u258c       | 17\/65 [00:07<00:16,  2.87cell\/s]\r\nExecuting:  28%|\u2588\u2588\u258a       | 18\/65 [00:13<01:34,  2.00s\/cell]\r\nExecuting:  31%|\u2588\u2588\u2588       | 20\/65 [00:13<01:04,  1.43s\/cell]\r\nExecuting:  32%|\u2588\u2588\u2588\u258f      | 21\/65 [00:15<01:06,  1.51s\/cell]\r\nExecuting:  35%|\u2588\u2588\u2588\u258c      | 23\/65 [00:15<00:45,  1.08s\/cell]\r\nExecuting:  37%|\u2588\u2588\u2588\u258b      | 24\/65 [00:16<00:45,  1.11s\/cell]\r\nExecuting:  38%|\u2588\u2588\u2588\u258a      | 25\/65 [00:18<00:54,  1.37s\/cell]\r\nExecuting:  42%|\u2588\u2588\u2588\u2588\u258f     | 27\/65 [00:18<00:37,  1.01cell\/s]\r\nExecuting:  43%|\u2588\u2588\u2588\u2588\u258e     | 28\/65 [00:20<00:50,  1.37s\/cell]\r\nExecuting:  45%|\u2588\u2588\u2588\u2588\u258d     | 29\/65 [00:21<00:38,  1.07s\/cell]\r\nExecuting:  48%|\u2588\u2588\u2588\u2588\u258a     | 31\/65 [00:22<00:33,  1.01cell\/s]\r\nExecuting:  51%|\u2588\u2588\u2588\u2588\u2588     | 33\/65 [00:22<00:22,  1.39cell\/s]\r\nExecuting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 34\/65 [00:23<00:19,  1.61cell\/s]\r\nExecuting:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 35\/65 [00:23<00:14,  2.11cell\/s]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 37\/65 [00:23<00:10,  2.76cell\/s]\r\nExecuting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 38\/65 [00:23<00:07,  3.41cell\/s]\r\nExecuting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 40\/65 [00:24<00:05,  4.18cell\/s]\r\nExecuting:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 42\/65 [00:24<00:04,  5.02cell\/s]\r\nExecuting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 43\/65 [00:32<00:59,  2.70s\/cell]\r\nExecuting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 44\/65 [11:52<1:12:00, 205.75s\/cell]\r\nExecuting:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 45\/65 [11:52<48:01, 144.08s\/cell]  \r\nExecuting:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 46\/65 [11:53<32:00, 101.08s\/cell]\r\nExecuting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 47\/65 [11:53<21:14, 70.80s\/cell] \r\nExecuting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 48\/65 [11:53<14:03, 49.63s\/cell]\r\nExecuting:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 49\/65 [11:53<09:16, 34.79s\/cell]\r\nExecuting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 50\/65 [11:53<06:05, 24.40s\/cell]\r\nExecuting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 51\/65 [11:56<04:07, 17.70s\/cell]\r\nExecuting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 52\/65 [11:56<02:41, 12.44s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:32<25:30, 127.58s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:33<04:12, 21.01s\/cell] \r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:58:40.389449', 'duration': 1402.445046, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [12]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-12-ea5338712650> in <module>\r\nE                 8         deployment_target = aks_target\r\nE                 9     )\r\nE           ---> 10     aks_service.wait_for_deployment(show_output = True)\r\nE                11     print(f\"The web service is {aks_service.state}\")\r\nE                12 else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n```\r\n\r\nFYI @PatrickBue @jiata any idea of what could be happening?\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1569349581000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/332",
        "Challenge_link_count":12,
        "Challenge_open_time":27749.5608333333,
        "Challenge_readability":10.6,
        "Challenge_reading_time":263.67,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":217,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Error in o16n with AzureML  notebooks",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":2075,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":263.7483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:40.699401', 'duration': 5.033488, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [2]\":\r\nE           ---------------------------------------------------------------------------\r\nE           SSLError                                  Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1317                 h.request(req.get_method(), req.selector, req.data, headers,\r\nE           -> 1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\r\nE              1238         \"\"\"Send a complete request to the server.\"\"\"\r\nE           -> 1239         self._send_request(method, url, body, headers, encode_chunked)\r\nE              1240 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\nE              1284             body = _encode(body, 'body')\r\nE           -> 1285         self.endheaders(body, encode_chunked=encode_chunked)\r\nE              1286 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\r\nE              1233             raise CannotSendHeader()\r\nE           -> 1234         self._send_output(message_body, encode_chunked=encode_chunked)\r\nE              1235 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_output(self, message_body, encode_chunked)\r\nE              1025         del self._buffer[:]\r\nE           -> 1026         self.send(msg)\r\nE              1027 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in send(self, data)\r\nE               963             if self.auto_open:\r\nE           --> 964                 self.connect()\r\nE               965             else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in connect(self)\r\nE              1399             self.sock = self._context.wrap_socket(self.sock,\r\nE           -> 1400                                                   server_hostname=server_hostname)\r\nE              1401             if not self._context.check_hostname and self._check_hostname:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\nE               406                          server_hostname=server_hostname,\r\nE           --> 407                          _context=self, _session=session)\r\nE               408 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in __init__(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\r\nE               816                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\nE           --> 817                     self.do_handshake()\r\nE               818 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self, block)\r\nE              1076                 self.settimeout(None)\r\nE           -> 1077             self._sslobj.do_handshake()\r\nE              1078         finally:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self)\r\nE               688         \"\"\"Start the SSL\/TLS handshake.\"\"\"\r\nE           --> 689         self._sslobj.do_handshake()\r\nE               690         if self.context.check_hostname:\r\nE           \r\nE           SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           URLError                                  Traceback (most recent call last)\r\nE           <ipython-input-2-2e2a8adec5e2> in <module>\r\nE           ----> 1 learn = model_to_learner(models.resnet18(pretrained=True), IMAGENET_IM_SIZE)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in resnet18(pretrained, progress, **kwargs)\r\nE               229     \"\"\"\r\nE               230     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\nE           --> 231                    **kwargs)\r\nE               232 \r\nE               233 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)\r\nE               215     if pretrained:\r\nE               216         state_dict = load_state_dict_from_url(model_urls[arch],\r\nE           --> 217                                               progress=progress)\r\nE               218         model.load_state_dict(state_dict)\r\nE               219     return model\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)\r\nE               460         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\r\nE               461         hash_prefix = HASH_REGEX.search(filename).group(1)\r\nE           --> 462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\r\nE               463     return torch.load(cached_file, map_location=map_location)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in _download_url_to_file(url, dst, hash_prefix, progress)\r\nE               370 def _download_url_to_file(url, dst, hash_prefix, progress):\r\nE               371     file_size = None\r\nE           --> 372     u = urlopen(url)\r\nE               373     meta = u.info()\r\nE               374     if hasattr(meta, 'getheaders'):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)\r\nE               221     else:\r\nE               222         opener = _opener\r\nE           --> 223     return opener.open(url, data, timeout)\r\nE               224 \r\nE               225 def install_opener(opener):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in open(self, fullurl, data, timeout)\r\nE               524             req = meth(req)\r\nE               525 \r\nE           --> 526         response = self._open(req, data)\r\nE               527 \r\nE               528         # post-process response\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _open(self, req, data)\r\nE               542         protocol = req.type\r\nE               543         result = self._call_chain(self.handle_open, protocol, protocol +\r\nE           --> 544                                   '_open', req)\r\nE               545         if result:\r\nE               546             return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _call_chain(self, chain, kind, meth_name, *args)\r\nE               502         for handler in handlers:\r\nE               503             func = getattr(handler, meth_name)\r\nE           --> 504             result = func(*args)\r\nE               505             if result is not None:\r\nE               506                 return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in https_open(self, req)\r\nE              1359         def https_open(self, req):\r\nE              1360             return self.do_open(http.client.HTTPSConnection, req,\r\nE           -> 1361                 context=self._context, check_hostname=self._check_hostname)\r\nE              1362 \r\nE              1363         https_request = AbstractHTTPHandler.do_request_\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           -> 1320                 raise URLError(err)\r\nE              1321             r = h.getresponse()\r\nE              1322         except:\r\nE           \r\nE           URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)>\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<00:56,  1.14cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:39,  1.58cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:27,  2.16cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:03<01:00,  1.03s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:04<00:47,  1.19cell\/s]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<00:35,  1.59cell\/s]\r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:46.959285', 'duration': 5.817276, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-af5043783823> in <module>\r\nE           ----> 1 docker_image = ws.images[\"image-classif-resnet18-f48\"]\r\nE           \r\nE           KeyError: 'image-classif-resnet18-f48'\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/36 [00:00<?, ?cell\/s]\r\nExecuting:   3%|\u258e         | 1\/36 [00:00<00:30,  1.16cell\/s]\r\nExecuting:  11%|\u2588         | 4\/36 [00:02<00:24,  1.32cell\/s]\r\nExecuting:  19%|\u2588\u2589        | 7\/36 [00:02<00:15,  1.84cell\/s]\r\nExecuting:  25%|\u2588\u2588\u258c       | 9\/36 [00:02<00:10,  2.52cell\/s]\r\nExecuting:  31%|\u2588\u2588\u2588       | 11\/36 [00:03<00:10,  2.47cell\/s]\r\nExecuting:  33%|\u2588\u2588\u2588\u258e      | 12\/36 [00:04<00:16,  1.50cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:12,  1.81cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:09,  2.41cell\/s]\r\n_____________________________ test_23_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_23_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\"23_aci_aks_web_service_testing\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:106: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:53.061402', 'duration': 6.023939, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-883397ed965d> in <module>\r\nE                 1 # Retrieve the web services\r\nE           ----> 2 aci_service = ws.webservices['im-classif-websvc']\r\nE                 3 aks_service = ws.webservices['aks-cpu-image-classif-web-svc']\r\nE           \r\nE           KeyError: 'im-classif-websvc'\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Challenge_closed_time":1569234937000,
        "Challenge_created_time":1568285443000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/320",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.1,
        "Challenge_reading_time":224.21,
        "Challenge_repo_contributor_count":40.0,
        "Challenge_repo_fork_count":1102.0,
        "Challenge_repo_issue_count":681.0,
        "Challenge_repo_star_count":8768.0,
        "Challenge_repo_watch_count":287.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":155,
        "Challenge_solved_time":263.7483333333,
        "Challenge_title":"[BUG] pipeline azureml-notebook-test-linux-cpu failing",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":1547,
        "Platform":"Github",
        "Solution_body":"fixed with new pipeline and test machines",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.5,
        "Solution_reading_time":0.51,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":7.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0587371512,
        "Challenge_watch_issue_ratio":0.4214390602
    },
    {
        "Challenge_adjusted_solved_time":343.0208333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Run 2236 in experiment \"master\" in RadiomicsNN: \r\n- Only metrics for 3 out of the 4 GPUs are visible\r\n- The MemAllocated and MemReserved metrics are all zero and hence meaningless.",
        "Challenge_closed_time":1613669349000,
        "Challenge_created_time":1612434474000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/InnerEye-DeepLearning\/issues\/389",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.9,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":27.0,
        "Challenge_repo_fork_count":129.0,
        "Challenge_repo_issue_count":676.0,
        "Challenge_repo_star_count":471.0,
        "Challenge_repo_watch_count":27.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":343.0208333333,
        "Challenge_title":"Memory utilization metrics are not correctly visible in AzureML",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":37,
        "Platform":"Github",
        "Solution_body":"Root cause: We are hitting the 50 metrics limit, https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-limits-quotas-capacity.\r\nRemoving the meaningless metrics should reduce impact.",
        "Solution_gpt_summary":"root hit metric limit remov metric reduc impact potenti miss metric visibl",
        "Solution_link_count":1.0,
        "Solution_original_content":"root hit metric limit http doc com resourc limit quota capac remov metric reduc impact",
        "Solution_preprocessed_content":"root hit metric limit remov metric reduc impact",
        "Solution_readability":17.2,
        "Solution_reading_time":2.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":17.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0399408284,
        "Challenge_watch_issue_ratio":0.0399408284
    },
    {
        "Challenge_adjusted_solved_time":1985.0091666667,
        "Challenge_answer_count":2,
        "Challenge_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Challenge_closed_time":1597072927000,
        "Challenge_created_time":1589926894000,
        "Challenge_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":12.25,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":614.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":1985.0091666667,
        "Challenge_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":"try again pls, I cannot reproduce it with latest azure ml Not able to reproduce now.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pl reproduc latest reproduc",
        "Solution_preprocessed_content":"pl reproduc latest reproduc",
        "Solution_readability":7.2,
        "Solution_reading_time":1.01,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0211726384,
        "Challenge_watch_issue_ratio":0.013029316
    },
    {
        "Challenge_adjusted_solved_time":8.1033333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Challenge_closed_time":1630110931000,
        "Challenge_created_time":1630081759000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.2,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":270.0,
        "Challenge_repo_star_count":13.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8.1033333333,
        "Challenge_title":"Show lightgbm logs in the logs in AzureML",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":27,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0407407407,
        "Challenge_watch_issue_ratio":0.0222222222
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### Steps to reproduce\r\n\r\n1. Create a fresh RAPIDS conda environment <br\/> `conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.8 cudatoolkit=11.5`\r\n2. `conda activate rapids-22.06`\r\n3. `conda list | grep pyarrow` shows 7.0.0 installed\r\n4. Launch python\/ipython and `import cudf` should work\r\n5. `pip install azureml-sdk`\r\n6. Launch python\/ipython and `import cudf` fails\r\n7. `conda list | grep pyarrow` shows 3.0.0 installed\r\n\r\n#### Error:\r\n```\r\n$ python -m cudf\r\nTraceback (most recent call last):\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 144, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/__init__.py\", line 13, in <module>\r\n    from cudf import api, core, datasets, testing\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/datasets.py\", line 7, in <module>\r\n    from cudf._lib.transform import bools_to_mask\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/_lib\/__init__.py\", line 4, in <module>\r\n    from . import (\r\n  File \"cudf\/_lib\/avro.pyx\", line 1, in init cudf._lib.avro\r\n  File \"cudf\/_lib\/column.pyx\", line 1, in init cudf._lib.column\r\n  File \"cudf\/_lib\/scalar.pyx\", line 37, in init cudf._lib.scalar\r\n  File \"cudf\/_lib\/interop.pyx\", line 1, in init cudf._lib.interop\r\nAttributeError: module 'pyarrow.lib' has no attribute 'MonthDayNanoIntervalArray'\r\n```",
        "Challenge_closed_time":null,
        "Challenge_created_time":1655998483000,
        "Challenge_link":"https:\/\/github.com\/rapidsai\/cloud-ml-examples\/issues\/165",
        "Challenge_link_count":0,
        "Challenge_open_time":3680.4213888889,
        "Challenge_readability":11.5,
        "Challenge_reading_time":25.18,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":68.0,
        "Challenge_repo_issue_count":201.0,
        "Challenge_repo_star_count":121.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"azureml-sdk downgrades pyarrow to 3.0.0 which breaks cudf",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":173,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.1094527363,
        "Challenge_watch_issue_ratio":0.0447761194
    },
    {
        "Challenge_adjusted_solved_time":12402.8375,
        "Challenge_answer_count":7,
        "Challenge_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Challenge_closed_time":1662517763000,
        "Challenge_created_time":1617867548000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":12.4,
        "Challenge_reading_time":208.42,
        "Challenge_repo_contributor_count":171.0,
        "Challenge_repo_fork_count":1727.0,
        "Challenge_repo_issue_count":5102.0,
        "Challenge_repo_star_count":12323.0,
        "Challenge_repo_watch_count":282.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":130,
        "Challenge_solved_time":12402.8375,
        "Challenge_title":"Training extremely slow with Azure Machine Learning",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":1467,
        "Platform":"Github",
        "Solution_body":"@yangmingwanli Each run only start one trial job, and then start new run? @SparkSnail After adding gpuNum: 1 and useActiveGpu: true, yes each run only start one trial job, and then start new run.\r\nWithout making these changes, it will finish all trials in one run, just very slowly. I reproduced this issue, and this seems to be a bug, will fix it ASAP. @SparkSnail , does it look like going to be a hard to fix bug? Is there any workaround before fix is released? Thanks!  Have you tried setting gpuNum:0, and resubmit the job? Just tried that, after setting gpuNum:0, training is still extremely slow, didn't start new run for new trial, but failed after two trials due to \"Converting circular structure to JSON\" error. @SparkSnail is it a bug that needs to be fixed? \r\n\r\n> \"Converting circular structure to JSON\" error.\r\n   \r\nthis error had been fixed in NNI v2.3.\r\n\r\n",
        "Solution_gpt_summary":"gpunum useactivegpu start trial job run speed train process slow train soon set gpunum convert circular structur json nni",
        "Solution_link_count":0.0,
        "Solution_original_content":"yangmingwanli run start trial job start run sparksnail gpunum useactivegpu run start trial job start run finish trial run slowli reproduc asap sparksnail workaround releas tri set gpunum resubmit job tri set gpunum train extrem slow start run trial trial convert circular structur json sparksnail convert circular structur json nni",
        "Solution_preprocessed_content":"run start trial job start run gpunum useactivegpu run start trial job start run finish trial run slowli reproduc asap workaround releas tri set gpunum resubmit job tri set gpunum train extrem slow start run trial trial convert circular structur json convert circular structur json nni",
        "Solution_readability":5.1,
        "Solution_reading_time":10.34,
        "Solution_score_count":1.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":150.0,
        "Tool":"Azure Machine Learning",
        "Challenge_contributor_issue_ratio":0.0335162681,
        "Challenge_watch_issue_ratio":0.0552724422
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"If a parent model was trained with the Alignment Enhanced architecture and the dictionary on, preprocessing for the child model will look for the dict.*.txt files (dict.src.txt, dict.trg.txt, dict.vref.txt) from the parent model.  Those files are not currently being copied into the \/tmp directory on the AQUA server when the experiment is launched through ClearML, so preprocessing fails on the child model.\r\n\r\nSample [experiment ](https:\/\/app.pro.clear.ml\/projects\/2243ca6c76d642699db1f28951bbb78a\/experiments\/fc444552b21243149fd3c90a9a4c6c8d\/execution?columns=selected&columns=type&columns=name&columns=tags&columns=status&columns=project.name&columns=users&columns=started&columns=last_update&columns=last_iteration&columns=parent.name&order=-last_update&filter=)with this failure.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1644785881000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/125",
        "Challenge_link_count":1,
        "Challenge_open_time":6795.0330555556,
        "Challenge_readability":14.5,
        "Challenge_reading_time":11.85,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Child models need to copy the dict.*.txt files from the parent model when launching an experiment on ClearML",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":84,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":4565.0625,
        "Challenge_answer_count":3,
        "Challenge_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Challenge_closed_time":1657980432000,
        "Challenge_created_time":1641546207000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.7,
        "Challenge_reading_time":2.56,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":4565.0625,
        "Challenge_title":"Execute translate script without creating ClearML task",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"I think that this error might be preventing me from using the Translate script locally.\r\n\r\nWhen I try I get the following error:\r\nInstalling the current project: silnlp (1.0.0)\r\n(silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate BT-English\/cba-en\/cba-en_cp01 --src-project cba --trg-iso en --books EXO --output-usfm BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT --checkpoint best\r\n2022-06-28 21:02:08,808 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-06-28 21:02:09,149 - silnlp.common.utils - INFO - Git commit: f46a63c3b3\r\nRetrying (Retry(total=239, connect=239, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4556fa0>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\nRetrying (Retry(total=238, connect=238, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569190>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n^CRetrying (Retry(total=237, connect=237, read=240, redirect=240, status=240)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f97e4569340>: Failed to establish a new connection: [Errno -5] No address associated with hostname')': \/auth.login\r\n\r\nprint(args):\r\nNamespace(books=['EXO'], checkpoint='best', clearml_queue=None, eager_execution=False, end_seq=None, experiment='BT-English\/cba-en\/cba-en_cp01', memory_growth=False, output_usfm='BT-English\/cba-en\/cba-en_cp01\/02EXOcbaNT', src=None, src_prefix=None, src_project='cbaNT', start_seq=None, trg=None, trg_iso='en', trg_prefix=None)\r\n Tested this for translating and it worked fine.   (silnlp-gt_VMn9E-py3.8) david@pop-os:~\/silnlp$ python -m silnlp.nmt.translate --checkpoint last --src-project tl-TCB --src \/home\/david\/disk2\/gutenberg\/Paratext\/projects\/TCB\/091SAtlASD15.SFM --trg-iso blx --output-usfm \/home\/david\/disk2\/gutenberg\/BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\/results\/091SAAMIU_last.sfm BT-Tagalog\/to_blx\/tl_blx_uni_dup_share_preserve\r\n2022-07-16 15:03:40,452 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/disk2\/gutenberg as per environment variable SIL_NLP_DATA_PATH.\r\n2022-07-16 15:03:40,828 - silnlp.common.utils - INFO - Git commit: 8cd5b9c649\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 231, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 225, in main\r\n    translator.translate_text_file(args.src, args.trg_iso, args.trg)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 151, in translate_text_file\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{os.path.basename(src_file_path)}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 79, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 9, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml_connection.py\", line 24, in __post_init__\r\n    self.name = self.name.replace(\"\\\\\", \"\/\")\r\nAttributeError: 'NoneType' object has no attribute 'replace'\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"prevent translat local instal silnlp silnlp vmne david pop silnlp silnlp nmt translat english cba cba src cba trg iso book exo output usfm english cba cba exocb checkpoint silnlp common environ workspac home david disk gutenberg environ variabl sil nlp data path silnlp common util git commit facb retri retri connect read redirect statu connect broken newconnectionerror establish connect errno address associ hostnam auth login retri retri connect read redirect statu connect broken newconnectionerror establish connect errno address associ hostnam auth login cretri retri connect read redirect statu connect broken newconnectionerror establish connect errno address associ hostnam auth login print arg namespac book exo checkpoint queue eager execut end seq english cba cba memori growth output usfm english cba cba exocb src src prefix src cbant start seq trg trg iso trg prefix test translat silnlp vmne david pop silnlp silnlp nmt translat checkpoint src tcb src home david disk gutenberg paratext tcb satlasd sfm trg iso blx output usfm home david disk gutenberg tagalog blx blx uni dup share preserv saamiu sfm tagalog blx blx uni dup share preserv silnlp common environ workspac home david disk gutenberg environ variabl sil nlp data path silnlp common util git commit cdbc traceback file usr lib runpi line run modul return run global file usr lib runpi line run exec run global file home david silnlp silnlp nmt translat line file home david silnlp silnlp nmt translat line translat translat text file arg src arg trg iso arg trg file home david silnlp silnlp nmt translat line translat text file init translat task suffix checkpoint path basenam src file path file home david silnlp silnlp nmt translat line init translat task sil file line init file home david silnlp silnlp common connect line init replac attributeerror nonetyp object attribut replac",
        "Solution_preprocessed_content":"prevent translat local instal silnlp cba exo workspac environ variabl git commit retri connect broken object establish connect address associ hostnam retri connect broken object establish connect address associ hostnam cretri connect broken object establish connect address associ hostnam print namespac test translat blx workspac environ variabl git commit traceback file line return file line exec file line file line file line file line sil attributeerror nonetyp object attribut replac",
        "Solution_readability":15.1,
        "Solution_reading_time":46.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":282.0,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":4.1744444444,
        "Challenge_answer_count":2,
        "Challenge_body":"I tried to translate with the following command line and trace.\r\nThe command is meant to run locally, but there is an error about ClearML credentials. The ClearML argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project GELA3_2021_11_22 --book OT --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/Gutenberg_new as per environment variable SIL_NLP_DATA_PATH.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - INFO - Git commit: 12aca87cab\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 8, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml.py\", line 27, in __post_init__\r\n    self.task = Task.init(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 164, in __init__\r\n    super(Task, self).__init__(**kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(Task, self).__init__(id=task_id, session=session, log=log)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 131, in __init__\r\n    super(IdObjectBase, self).__init__(session, log, **kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    InterfaceBase._default_session = Session(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise LoginError(\r\nclearml.backend_api.session.session.LoginError: Failed getting token (error 401 from https:\/\/api.pro.clear.ml): Unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Challenge_closed_time":1637601038000,
        "Challenge_created_time":1637586010000,
        "Challenge_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/109",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.5,
        "Challenge_reading_time":56.44,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":147.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":4.1744444444,
        "Challenge_title":"Translate is trying to use ClearML even though it was not requested. Preventing translation on local machine.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":275,
        "Platform":"Github",
        "Solution_body":"@davidbaines, Did that fix it? Yes! Thanks so much.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-1.0,
        "Solution_reading_time":0.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":9.0,
        "Tool":"ClearML",
        "Challenge_contributor_issue_ratio":0.0408163265,
        "Challenge_watch_issue_ratio":0.0544217687
    },
    {
        "Challenge_adjusted_solved_time":1943.8975,
        "Challenge_answer_count":8,
        "Challenge_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Challenge_closed_time":1662130932000,
        "Challenge_created_time":1655132901000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":13.4,
        "Challenge_reading_time":41.68,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17217.0,
        "Challenge_repo_issue_count":20687.0,
        "Challenge_repo_star_count":76119.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":1943.8975,
        "Challenge_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":298,
        "Platform":"Github",
        "Solution_body":"cc @sgugger  As the error message indicates, you need to have cometml installed to use it `report_to=\"comet_ml\"`\r\n```\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\r\nIt also tells you exactly which command to run to fix this: `pip install comet-ml`. Hey,\r\nThe issue here is that error appears despite cometml being installed (with pip).\r\n\r\nEDIT: Edited the issue title to make it more clear.\r\n\r\nOn Mon, Jul 4, 2022, 14:33 Sylvain Gugger ***@***.***> wrote:\r\n\r\n> As the error message indicates, you need to have cometml installed to use\r\n> it report_to=\"comet_ml\"\r\n>\r\n> RuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n>\r\n> It also tells you exactly which command to run to fix this: pip install\r\n> comet-ml.\r\n>\r\n> \u2014\r\n> Reply to this email directly, view it on GitHub\r\n> <https:\/\/github.com\/huggingface\/transformers\/issues\/17691#issuecomment-1173767326>,\r\n> or unsubscribe\r\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AF7MPQSGKFHH4UZWW3JTEWLVSLKYRANCNFSM5YURU4KQ>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n Did you properly initialize it with your API key then? This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored. @sgugger How to do it? In [this](https:\/\/huggingface.co\/docs\/transformers\/main_classes\/callback) doc, there's no mentioning about API key in comet callback. I tried set up COMET_API_KEY, COMET_MODE, COMET_PROJECT_NAME inside function that runs on spawn, but no luck so far. Also downgraded comet-ml till 3.1.17.\r\n\r\n`os.environ[\"COMET_API_KEY\"] = \"<api-key>\"`\r\n`os.environ[\"COMET_MODE\"] = \"ONLINE\"`\r\n`os.environ[\"COMET_PROJECT_NAME\"] = \"<project-name>\"` Maybe open an issue with them? We did not write this integration with comet-ml and we don't maintain it. It was written by the Comet team :-) This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_gpt_summary":"messag instal report run pip instal instal persist instal set api kei mode environ variabl persist open team",
        "Solution_link_count":5.0,
        "Solution_original_content":"sgugger messag instal report runtimeerror callback instal run pip instal exactli run pip instal instal pip edit edit titl clear mon jul sylvain gugger wrote messag instal report runtimeerror callback instal run pip instal exactli run pip instal repli email directli github unsubscrib receiv author thread messag properli initi api kei automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor sgugger http huggingfac doc transform class callback doc api kei callback tri set api kei mode insid function run spawn luck downgrad till environ api kei environ mode onlin environ mayb open write integr maintain written team automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor",
        "Solution_preprocessed_content":"messag instal exactli run instal edit edit titl clear mon jul sylvain gugger wrote messag instal runtimeerror callback instal run exactli run pip instal repli email directli github unsubscrib receiv author properli initi api kei automat mark stale activ address comment thread note ignor doc api kei callback tri set insid function run spawn luck downgrad till mayb open write integr maintain written team automat mark stale activ address comment thread note ignor",
        "Solution_readability":8.7,
        "Solution_reading_time":30.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":312.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0212210567,
        "Challenge_watch_issue_ratio":0.0415720017
    },
    {
        "Challenge_adjusted_solved_time":75.8813888889,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Challenge_closed_time":1650063297000,
        "Challenge_created_time":1649790124000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":20.9,
        "Challenge_reading_time":56.91,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":75.8813888889,
        "Challenge_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":286,
        "Platform":"Github",
        "Solution_body":"Facing the same issue but with W and B. see https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12529 and https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/12714 This was fixed and included in the 1.6.1 release. Could you try upgrading lightning? \r\n`pip install --upgrade pytorch-lightning` \ud83e\udd1f  @awaelchli not the same bug, but my colab link to reproduce the bug is now throwing another error.",
        "Solution_gpt_summary":"keyword argument agg kei func pytorch lightn cli upgrad latest version pytorch lightn pip instal upgrad pytorch lightn",
        "Solution_link_count":2.0,
        "Solution_original_content":"http github com pytorchlightn pytorch lightn http github com pytorchlightn pytorch lightn releas upgrad lightn pip instal upgrad pytorch lightn awaelchli colab link reproduc throw",
        "Solution_preprocessed_content":"releas upgrad lightn colab link reproduc throw",
        "Solution_readability":9.5,
        "Solution_reading_time":5.29,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":49.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":7593.3588888889,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRichProgressBar doesn't display progress bar when using Comet logger.\r\nI verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### To Reproduce\r\n```python\r\nimport comet_ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning.loggers import CometLogger\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nlogger = CometLogger(api_key=os.environ.get(\"COMET_API_TOKEN\"))\r\n\r\ntrainer = Trainer(logger=logger, max_epochs=100, callbacks=[RichProgressBar()])\r\n# trainer = Trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### Environment\r\n- PyTorch Lightning Version 1.5.5\r\n- PyTorch Version 1.10.0\r\n- Python version 3.8\r\n- OS Ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @SeanNaren",
        "Challenge_closed_time":1666742778000,
        "Challenge_created_time":1639406686000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/11043",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.3,
        "Challenge_reading_time":36.09,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":6.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":7593.3588888889,
        "Challenge_title":"RichProgressBar doesn't display progress bar when using Comet logger.",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":251,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Is there any update for this bug? Any update on this issue?\r\nI'm experiencing the same problem when using `comet` logger and `RichProgressBar` @ItamarKanter @JackLin-Authme I just tried this and can see the rich progress bar working fine. Is it possible that I am using a newer version of either rich or comet that now fixed the problem? Do you still have documentation of what version(s) you were using?\r\n\r\nI'm closing the issue now, but if you find any more issues related to this we can continue the investigation.  I still experience the issue. Adding more information on this, the progress bar DO show, however only after it has been completed. Moreover, any `rich.print` calls show no color, including the progress bar itself. The only solution I found is to stop using the Comet logger.\r\n\r\npackage versions:\r\npytorch-lightning     1.9.0\r\ncomet-ml                 3.32.0\r\nrich                          13.3.1\r\n\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team automat mark stale hasn activ close dai activ contribut pytorch lightn team updat updat experienc logger richprogressbar itamarkant jacklin authm tri rich progress bar newer version rich document version close relat progress bar complet rich print call color progress bar stop logger packag version pytorch lightn rich",
        "Solution_preprocessed_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team automat mark stale hasn activ close dai activ contribut pytorch lightn team updat updat experienc richprogressbar call color progress bar stop logger packag version rich",
        "Solution_readability":6.3,
        "Solution_reading_time":15.87,
        "Solution_score_count":0.0,
        "Solution_sentence_count":20.0,
        "Solution_word_count":215.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":2330.3280555556,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Challenge_closed_time":1642181493000,
        "Challenge_created_time":1633792312000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":16.4,
        "Challenge_reading_time":156.3,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":236,
        "Challenge_solved_time":2330.3280555556,
        "Challenge_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":786,
        "Platform":"Github",
        "Solution_body":"Thanks @rusty-electron for opening the issue.\r\n\r\nIs there any more information before the line \"Dumping Computation:\"?  No error output, just the logs from wandb logger and the progressbars created by `tqdm`. Dear @rusty-electron,\r\n\r\nWe are working with the Wandb Team on a large fix. Hopefully it will work for this use-case too.\r\n\r\nWe will keep you updated.\r\n\r\nBest,\r\nT.C @tchaton Thanks for the info. I shall be looking out for the fix. @tchaton Is there an issue to track the Wandb updates? @borisdayma Any idea ?\r\n It's actually a few different PR's ongoing.\r\nI think we should have something next week that will handle these scenarios. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n @borisdayma Did it end up being an issue on the wandb side? I didn't follow the development lately. If it's still work in progress, could you point us to a PR or issue? Thx in advance <3  We're actually still in the process of updating the way multiprocess is supported.\r\nThere's been good progress, just a few edge cases to handle. This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_gpt_summary":"pytorch lightn team team larg hopefulli ongo pr updat multiprocess progress edg bias respons",
        "Solution_link_count":0.0,
        "Solution_original_content":"rusti electron open line dump comput output log logger progressbar creat tqdm dear rusti electron team larg hopefulli updat tchaton tchaton track updat borisdayma idea ongo week automat mark stale hasn activ close dai activ contribut pytorch lightn team borisdayma end late progress thx advanc process updat multiprocess progress edg automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"open line dump comput output log logger progressbar creat dear team larg hopefulli updat track updat idea ongo week automat mark stale hasn activ close dai activ contribut pytorch lightn team end late progress thx advanc process updat multiprocess progress edg automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":6.1,
        "Solution_reading_time":16.94,
        "Solution_score_count":1.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":238.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":3846.9952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Challenge_closed_time":1636988013000,
        "Challenge_created_time":1623138830000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":21.46,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3846.9952777778,
        "Challenge_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":172,
        "Platform":"Github",
        "Solution_body":"@tchaton Is this a lightning issue? Closing this issue as there is no progress nor manifestation from the Comet Team.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tchaton lightn close progress manifest team",
        "Solution_preprocessed_content":"lightn close progress manifest team",
        "Solution_readability":6.0,
        "Solution_reading_time":1.44,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":20.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":2840.7811111111,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Challenge_closed_time":1631600832000,
        "Challenge_created_time":1621374020000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Challenge_link_count":8,
        "Challenge_open_time":null,
        "Challenge_readability":10.9,
        "Challenge_reading_time":37.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":2840.7811111111,
        "Challenge_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":325,
        "Platform":"Github",
        "Solution_body":"Hey @bw4sz,\r\n\r\nThanks for reporting this bug. While we investigate the source of bug, I think you could use this workaround in the meanwhile.\r\n\r\n`COMET_EXPERIMENT_KEY='something' python ...` and use it in your code ?\r\n\r\n```\r\n        comet_logger = CometLogger(\r\n            api_key=os.environ.get('COMET_API_KEY'),\r\n            workspace=os.environ.get('COMET_WORKSPACE'),  # Optional\r\n            save_dir='.',  # Optional\r\n            project_name='default_project',  # Optional\r\n            rest_api_key=os.environ.get('COMET_REST_API_KEY'),  # Optional\r\n            experiment_key=os.environ.get('COMET_EXPERIMENT_KEY'),  # Optional\r\n            experiment_name='default'  # Optional\r\n        )\r\n```\r\n\r\nBest,\r\nT.C Hi, I have a similar bug using wandb using a similar setup (slurm, ddp) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I've been investigating a bit with Wandb, and i only have the bug when using SLURM. When using ddp on a local machine, i don't have duplicated runs I have the same issue with MLFlow using SLURM. I also find this with comet_ml on SLURM. Tough to make a reproducible thing\nhere. maintainers, what can we do to move this forward?\n\nOn Thu, Aug 5, 2021 at 7:35 AM Andre Costa ***@***.***> wrote:\n\n> I have the same issue with MLFlow using SLURM.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/7599#issuecomment-893510320>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AAJHBLC5WEF6ZMD5IYI4F4LT3KOSFANCNFSM45DLJZPA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https:\/\/apps.apple.com\/app\/apple-store\/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https:\/\/play.google.com\/store\/apps\/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n\n\n-- \nBen Weinstein, Ph.D.\nPostdoctoral Fellow\nUniversity of Florida\nhttp:\/\/benweinstein.weebly.com\/\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_gpt_summary":"workaround kei concret multipl produc pytorch logger upgrad pytorch lightn report slurm ddp multi gpu reproduc mark stale",
        "Solution_link_count":5.0,
        "Solution_original_content":"bwsz report sourc workaround kei logger logger api kei environ api kei workspac environ workspac option save dir option default option rest api kei environ rest api kei option kei environ kei option default option setup slurm ddp automat mark stale hasn activ close dai activ contribut pytorch lightn team bit slurm ddp local duplic run slurm slurm tough reproduc maintain forward thu aug andr costa wrote slurm receiv repli email directli github unsubscrib triag notif github mobil io android ben weinstein postdoctor fellow univers florida http benweinstein weebli com automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"report sourc workaround setup automat mark stale hasn activ close dai activ contribut pytorch lightn team bit slurm ddp local duplic run slurm slurm tough reproduc maintain forward thu aug andr costa wrote slurm receiv repli email directli github unsubscrib triag notif github mobil io android ben weinstein postdoctor fellow univers florida automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":10.5,
        "Solution_reading_time":28.47,
        "Solution_score_count":1.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":260.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":3324.4138888889,
        "Challenge_answer_count":10,
        "Challenge_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Challenge_closed_time":1630398077000,
        "Challenge_created_time":1618430187000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":37.84,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":3324.4138888889,
        "Challenge_title":"CometLogger can modify logged metrics in-place ",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":439,
        "Platform":"Github",
        "Solution_body":"PR on this is more than welcome! Great observation. Btw I believe we don't expect users to directly call `self.logger.log_metrics`, but we should still fix it :) \n\n\n> val.cpu().detach() vs val.item()\n\nDoes Comet accept scalar tensors? If it can do the tensor->Python conversion (why wouldn't it), I would go with `val.cpu().detach()` as in the other loggers. @neighthan still interested to send a fix for this?  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n Hi @awaelchli! I am new to open source contribution and since this is a good first issue, I would like to try my hand at it! Dear @sohamtiwari3120,\r\n\r\nYes, feel free to take on this one and open a PR.\r\n\r\nBest,\r\nT.C Hi @tchaton,\r\n\r\nCan you please review my PR. There are a few checks that failed and I am unable to determine the exact cause for the same.\r\n\r\nSincerely,\r\nSoham Hey @ sohamtiwari3120,\r\n\r\nApproved. Mind adding a test to prevent regression ?\r\n\r\nBest,\r\nT.C Hi @tchaton \r\n\r\nI would love to try! However, it would be my first time writing tests. Therefore could you please help me with the following:\r\n- can you explain how will the test to prevent regression look like,\r\n- also could you provide any references useful for beginners in writing tests.\r\n\r\nSincerely,\r\nSoham Dear @sohamtiwari3120,\r\n\r\nCheck out this document: https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/.github\/CONTRIBUTING.md\r\n\r\nIn this case, the test should ensure the values aren't modified the logged metrics owned by the trainer.\r\n\r\nBest,\r\nT.C",
        "Solution_gpt_summary":"submit logger metric modifi logger log metric metric call val cpu detach val item updat logger accept metric dict str union torch tensor convert dict str contributor taken submit approv test valu aren modifi log metric trainer",
        "Solution_link_count":1.0,
        "Solution_original_content":"welcom great observ btw believ directli logger log metric val cpu detach val item accept scalar tensor tensor convers val cpu detach logger neighthan send automat mark stale hasn activ close dai activ contribut pytorch lightn team automat mark stale hasn activ close dai activ contribut pytorch lightn team awaelchli open sourc contribut hand dear sohamtiwari free open tchaton review determin exact sincer soham sohamtiwari approv test prevent regress tchaton love time write test explain test prevent regress beginn write test sincer soham dear sohamtiwari document http github com pytorchlightn pytorch lightn blob master github contribut test valu aren modifi log metric trainer",
        "Solution_preprocessed_content":"welcom great observ btw believ directli accept scalar tensor convers logger send automat mark stale hasn activ close dai activ contribut pytorch lightn team automat mark stale hasn activ close dai activ contribut pytorch lightn team open sourc contribut hand dear free open review determin exact sincer soham sohamtiwari approv test prevent regress love time write test explain test prevent regress beginn write test sincer soham dear document test valu aren modifi log metric trainer",
        "Solution_readability":6.7,
        "Solution_reading_time":22.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":25.0,
        "Solution_word_count":296.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":195.695,
        "Challenge_answer_count":0,
        "Challenge_body":"After https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/2553  there is a changed logger behavior. It starts using `COMET_EXPERIMENT_KEY`. But it doesn't respect it if it is set already.\r\nSo the bug is in the following.\r\nI already set this variable \r\nThen logger overwrites my value here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L189\r\nThen it deletes this variable at all here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L215\r\nThis way it ignores my variable and deletes it at all later\r\nMoreover in version function it also ignores my set variable\r\nI will create a pull request to fix it ",
        "Challenge_closed_time":1603809056000,
        "Challenge_created_time":1603104554000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4229",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.2,
        "Challenge_reading_time":9.93,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":195.695,
        "Challenge_title":"Comet logger overrides COMET_EXPERIMENT_KEY env variable",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":86,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":0.8413888889,
        "Challenge_answer_count":1,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCometmllogger with api key and  without save dir results in error.\r\nThis happens due to this if https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nThis can be fixed by setting _save_dir to None. I will supply PR in a moment\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\n    model = LightningModel({})\r\n    comet_logger = CometLogger(\r\n        api_key=KEY,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = Trainer(logger=comet_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\nTraceback (most recent call last):\r\ntrainer.fit(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/comet.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1599658056000,
        "Challenge_created_time":1599655027000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3417",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.9,
        "Challenge_reading_time":31.44,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":32,
        "Challenge_solved_time":0.8413888889,
        "Challenge_title":"CometLogger failing without save_dir",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":206,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue!",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.7,
        "Solution_reading_time":0.68,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":719.2222222222,
        "Challenge_answer_count":1,
        "Challenge_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Challenge_closed_time":1591023634000,
        "Challenge_created_time":1588434434000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.6,
        "Challenge_reading_time":23.8,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":6.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":719.2222222222,
        "Challenge_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":171,
        "Platform":"Github",
        "Solution_body":"@ceyzaguirre4 pls ^^",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":8.8,
        "Solution_reading_time":0.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":2.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":755.505,
        "Challenge_answer_count":11,
        "Challenge_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Challenge_closed_time":1591023635000,
        "Challenge_created_time":1588303817000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":16.81,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":755.505,
        "Challenge_title":"Comet logger cannot be pickled after creating an experiment",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":"@ceyzaguirre4 pls ^^ I don't know if it can help or if it is the right place, but a similar error occurswhen running in ddp mode with the WandB logger.\r\n\r\nWandB uses a lambda function at some point.\r\n\r\nDoes the logger have to pickled ? Couldn't it log only on rank 0 at epoch_end ?\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"..\/train.py\", line 140, in <module>\r\n    main(args.gpus, args.nodes, args.fast_dev_run, args.mixed_precision, project_config, hparams)\r\n  File \"..\/train.py\", line 117, in main\r\n    trainer.fit(model)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/context.py\", line 283, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/clear\/fbartocc\/miniconda3\/envs\/Depth_env\/lib\/python3.8\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n```\r\n\r\nalso related: \r\n#1704 I had the same error as @jeremyjordan  `can't pickle _thread.lock objects`. This happened when I added the  `logger` and additional `callbacks` in `from_argparse_args`, as explained here https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/hyperparameters.html\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams, logger=logger, callbacks=[PrinterCallback(), ])\r\n```\r\nI could make the problem go away by directly overwriting the members of `Trainer`\r\n\r\n```\r\ntrainer = pl.Trainer.from_argparse_args(hparams)\r\ntrainer.logger = logger\r\ntrainer.callbacks.append(PrinterCallback())\r\n``` Same issue as @F-Barto using a wandb logger across 2 nodes with `ddp`. same issue when using wandb logger with ddp same here.. @joseluisvaz your workaround doesn't solve the callback issue.. when I try to add a callback like this it is simply being ignored :\/ but adding it the Trainer init call normally works.. so I'm pretty sure the error is thrown by the logger (I'm using TB) not the callbacks. Same issue, using wandb logger with 8 gpus in an AWS p2.8xlarge machine  With CometLogger, I get this error only when the experiment name is declared. If it is not declared, I get no issue. I still have this error with 1.5.10 on macOS\r\n\r\n```\r\nError executing job with overrides: ['train.pl_trainer.fast_dev_run=False', 'train.pl_trainer.gpus=0', 'train.pl_trainer.precision=32', 'logging.wandb_arg.mode=offline']\r\nTraceback (most recent call last):\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 78, in main\r\n    train(conf)\r\n  File \"\/Users\/ric\/Documents\/PhD\/Projects\/ed-experiments\/src\/train.py\", line 70, in train\r\n    trainer.fit(pl_module, datamodule=pl_data_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 740, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/plugins\/training_type\/training_type_plugin.py\", line 202, in start_training\r\n    self._results = trainer.run_stage()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1289, in run_stage\r\n    return self._run_train()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1311, in _run_train\r\n    self._run_sanity_check(self.lightning_module)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1375, in _run_sanity_check\r\n    self._evaluation_loop.run()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 145, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 110, in advance\r\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/base.py\", line 140, in run\r\n    self.on_run_start(*args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/epoch\/evaluation_epoch_loop.py\", line 86, in on_run_start\r\n    self._dataloader_iter = _update_dataloader_iter(data_fetcher, self.batch_progress.current.ready)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/loops\/utilities.py\", line 121, in _update_dataloader_iter\r\n    dataloader_iter = enumerate(data_fetcher, batch_idx)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 198, in __iter__\r\n    self._apply_patch()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 133, in _apply_patch\r\n    apply_to_collections(self.loaders, self.loader_iters, (Iterator, DataLoader), _apply_patch_fn)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/fetching.py\", line 181, in loader_iters\r\n    loader_iters = self.dataloader_iter.loader_iters\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 537, in loader_iters\r\n    self._loader_iters = self.create_loader_iters(self.loaders)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 577, in create_loader_iters\r\n    return apply_to_collection(loaders, Iterable, iter, wrong_dtype=(Sequence, Mapping))\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 104, in apply_to_collection\r\n    v = apply_to_collection(\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/utilities\/apply_func.py\", line 96, in apply_to_collection\r\n    return function(data, *args, **kwargs)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/pytorch_lightning\/trainer\/supporters.py\", line 177, in __iter__\r\n    self._loader_iter = iter(self.loader)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 359, in __iter__\r\n    return self._get_iterator()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 305, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 918, in __init__\r\n    w.start()\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/Users\/ric\/mambaforge\/envs\/ed\/lib\/python3.9\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'TorchHistory.add_log_hooks_to_pytorch_module.<locals>.<lambda>'\r\n``` I still see this bug as well with WandB logger. Currently having this issue with wandbLogger.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"ceyzaguirr pl occurswhen run ddp mode logger lambda function logger pickl log rank epoch end traceback file train line arg gpu arg node arg fast dev run arg mix precis config hparam file train line trainer fit model file home clear fbartocc miniconda env depth env lib site packag pytorch lightn trainer trainer line fit spawn ddp train nproc num process arg model file home clear fbartocc miniconda env depth env lib site packag torch multiprocess spawn line spawn return start process arg nproc daemon start spawn file home clear fbartocc miniconda env depth env lib site packag torch multiprocess spawn line start process process start file home clear fbartocc miniconda env depth env lib multiprocess process line start popen popen file home clear fbartocc miniconda env depth env lib multiprocess context line popen return popen process obj file home clear fbartocc miniconda env depth env lib multiprocess popen spawn posix line init init process obj file home clear fbartocc miniconda env depth env lib multiprocess popen fork line init launch process obj file home clear fbartocc miniconda env depth env lib multiprocess popen spawn posix line launch reduct dump process obj file home clear fbartocc miniconda env depth env lib multiprocess reduct line dump forkingpickl file protocol dump obj attributeerror pickl local object torchhistori add log hook pytorch modul relat jeremyjordan pickl thread lock object logger addit callback argpars arg explain http pytorch lightn readthedoc latest hyperparamet html trainer trainer argpars arg hparam logger logger callback printercallback awai directli overwrit member trainer trainer trainer argpars arg hparam trainer logger logger trainer callback append printercallback barto logger node ddp logger ddp joseluisvaz workaround callback add callback simpli ignor trainer init normal pretti thrown logger callback logger gpu xlarg logger declar declar maco execut job overrid train trainer fast dev run train trainer gpu train trainer precis log arg mode offlin traceback file ric document phd src train line train conf file ric document phd src train line train trainer fit modul datamodul data modul file ric mambaforg env lib site packag pytorch lightn trainer trainer line fit interrupt file ric mambaforg env lib site packag pytorch lightn trainer trainer line interrupt return trainer arg kwarg file ric mambaforg env lib site packag pytorch lightn trainer trainer line fit impl run model ckpt path ckpt path file ric mambaforg env lib site packag pytorch lightn trainer trainer line run dispatch file ric mambaforg env lib site packag pytorch lightn trainer trainer line dispatch train type plugin start train file ric mambaforg env lib site packag pytorch lightn plugin train type train type plugin line start train trainer run stage file ric mambaforg env lib site packag pytorch lightn trainer trainer line run stage return run train file ric mambaforg env lib site packag pytorch lightn trainer trainer line run train run saniti lightn modul file ric mambaforg env lib site packag pytorch lightn trainer trainer line run saniti evalu loop run file ric mambaforg env lib site packag pytorch lightn loop base line run advanc arg kwarg file ric mambaforg env lib site packag pytorch lightn loop dataload evalu loop line advanc output epoch loop run dataload dataload idx batch num dataload file ric mambaforg env lib site packag pytorch lightn loop base line run run start arg kwarg file ric mambaforg env lib site packag pytorch lightn loop epoch evalu epoch loop line run start dataload iter updat dataload iter data fetcher batch progress readi file ric mambaforg env lib site packag pytorch lightn loop util line updat dataload iter dataload iter enumer data fetcher batch idx file ric mambaforg env lib site packag pytorch lightn util fetch line iter appli patch file ric mambaforg env lib site packag pytorch lightn util fetch line appli patch appli collect loader loader iter iter dataload appli patch file ric mambaforg env lib site packag pytorch lightn util fetch line loader iter loader iter dataload iter loader iter file ric mambaforg env lib site packag pytorch lightn trainer line loader iter loader iter creat loader iter loader file ric mambaforg env lib site packag pytorch lightn trainer line creat loader iter return appli collect loader iter iter dtype sequenc map file ric mambaforg env lib site packag pytorch lightn util appli func line appli collect appli collect file ric mambaforg env lib site packag pytorch lightn util appli func line appli collect return function data arg kwarg file ric mambaforg env lib site packag pytorch lightn trainer line iter loader iter iter loader file ric mambaforg env lib site packag torch util data dataload line iter return iter file ric mambaforg env lib site packag torch util data dataload line iter return multiprocessingdataloaderit file ric mambaforg env lib site packag torch util data dataload line init start file ric mambaforg env lib multiprocess process line start popen popen file ric mambaforg env lib multiprocess context line popen return default context context process popen process obj file ric mambaforg env lib multiprocess context line popen return popen process obj file ric mambaforg env lib multiprocess popen spawn posix line init init process obj file ric mambaforg env lib multiprocess popen fork line init launch process obj file ric mambaforg env lib multiprocess popen spawn posix line launch reduct dump process obj file ric mambaforg env lib multiprocess reduct line dump forkingpickl file protocol dump obj attributeerror pickl local object torchhistori add log hook pytorch modul logger logger",
        "Solution_preprocessed_content":"pl occurswhen run ddp mode logger lambda function logger pickl log rank relat addit explain awai directli overwrit member logger node logger ddp workaround callback add callback simpli ignor trainer init normal pretti thrown logger callback logger gpu logger declar declar maco logger logger",
        "Solution_readability":19.8,
        "Solution_reading_time":127.3,
        "Solution_score_count":23.0,
        "Solution_sentence_count":105.0,
        "Solution_word_count":638.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":73.1380555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Challenge_closed_time":1586910754000,
        "Challenge_created_time":1586647457000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.1,
        "Challenge_reading_time":10.33,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":73.1380555556,
        "Challenge_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":95,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! @PyTorchLightning\/core-contributors or @alsrgv mind submitting a PR? good catch! Happy to, but I could use some pointers into what may be broken.  Does logging use aggregation with flush in the end, and that flush is somehow not called for the test pass?  @alexeykarnachev, any ideas? Shall be fixed in #1459 Sorry, guys, totally missed the messages.\r\n@Borda , is anything required from my end? I think it is fine, just if you have an idea why the Github Actions fails\/hangs...\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/1459\/checks?check_run_id=584135478",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"contribut great pytorchlightn core contributor alsrgv submit catch happi pointer broken log aggreg flush end flush call test pass alexeykarnachev idea sorri gui miss messag borda end idea github action hang http github com pytorchlightn pytorch lightn pull run",
        "Solution_preprocessed_content":"contribut great submit catch happi pointer broken log aggreg flush end flush call test pass idea sorri gui miss messag end idea github action",
        "Solution_readability":5.9,
        "Solution_reading_time":7.84,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":88.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":703.9719444444,
        "Challenge_answer_count":10,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Challenge_closed_time":1582760093000,
        "Challenge_created_time":1580225794000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":26.63,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":703.9719444444,
        "Challenge_title":"Test metrics not logging to Comet after training",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":277,
        "Platform":"Github",
        "Solution_body":"Did you find a solution?\r\nMind submitting a PR?\r\n@fdelrio89  I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.\r\n\r\nI solved it by getting the experiment key and creating another logger and trainer with it.\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model)\r\n\r\n    experiment_key = comet_logger.experiment.get_key()\r\n    comet_logger = CometLogger(experiment_key=experiment_key)\r\n    trainer = Trainer(logger=comet_logger)\r\n\r\n    trainer.test(model)\r\n```\r\n\r\nFor this to work, I had to modify the `CometLogger` class to accept the `experiment_key` and create a `CometExistingExperiment` from the Comet SDK when this param is present.\r\n\r\n```\r\nclass CometLogger(LightningLoggerBase):\r\n     ...\r\n\r\n    @property\r\n    def experiment(self):\r\n        ...\r\n\r\n        if self.mode == \"online\":\r\n            if self.experiment_key is None:\r\n                self._experiment = CometExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    **self._kwargs\r\n                )\r\n            else:\r\n                self._experiment = CometExistingExperiment(\r\n                    api_key=self.api_key,\r\n                    workspace=self.workspace,\r\n                    project_name=self.project_name,\r\n                    previous_experiment=self.experiment_key,\r\n                    **self._kwargs\r\n                )\r\n        else:\r\n            ...\r\n\r\n        return self._experiment\r\n```\r\n\r\nI can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it @williamFalcon. @williamFalcon Any progress on this Issue? I am facing the same problem.\r\n @fdelrio89 Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the `experiment_key` directly in the logger object itself, instead of having to re-instantiate the logger.  @xssChauhan good idea, I just submitted a PR (https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/892) considering this. Thanks!\r\n I assume that it was fixed by #892\r\n if you have some other problems feel free to reopen or create a new... :robot:  Actually I'm still facing the problem. @dvirginz are you using the latest master? may you provide a minimal example? > @dvirginz are you using the latest master? may you provide a minimal example?\r\n\r\nYou are right, sorry. \r\nAfter building from source it works.  I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.",
        "Solution_gpt_summary":"creat trainer object logger existingexperi object sdk workaround modifi logger class accept kei creat existingexperi sdk paramet present refactor store kei directli logger object instanti logger implement pull request report logger",
        "Solution_link_count":1.0,
        "Solution_original_content":"submit fdelrio hacki eleg haven time kei creat logger trainer logger logger trainer trainer logger logger model model trainer fit model kei logger kei logger logger kei kei trainer trainer logger logger trainer test model modifi logger class accept kei creat existingexperi sdk param present class logger lightningloggerbas properti mode onlin kei api kei api kei workspac workspac kwarg existingexperi api kei api kei workspac workspac previou kei kwarg return happili accept gui achiev haven time williamfalcon williamfalcon progress fdelrio logger object lifetim trainer mayb refactor store kei directli logger object instanti logger xsschauhan idea submit http github com pytorchlightn pytorch lightn pull free reopen creat robot dvirginz latest master minim dvirginz latest master minim sorri build sourc probabl open logger haven time delv",
        "Solution_preprocessed_content":"submit hacki eleg haven time kei creat logger trainer modifi class accept creat sdk param present happili accept gui achiev haven time progress logger object lifetim trainer mayb refactor store directli logger object logger idea submit free reopen creat robot latest master minim latest master minim sorri build sourc probabl open logger haven time delv",
        "Solution_readability":9.6,
        "Solution_reading_time":29.86,
        "Solution_score_count":4.0,
        "Solution_sentence_count":31.0,
        "Solution_word_count":312.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":15.6752777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Challenge_closed_time":1576023863000,
        "Challenge_created_time":1575967432000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":4.88,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":15.6752777778,
        "Challenge_title":"Comet PAPI Depreciated",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":44,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":121.7916666667,
        "Challenge_answer_count":0,
        "Challenge_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Challenge_closed_time":1573531232000,
        "Challenge_created_time":1573092782000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":22.1,
        "Challenge_reading_time":25.49,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2674.0,
        "Challenge_repo_issue_count":13570.0,
        "Challenge_repo_star_count":20939.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":121.7916666667,
        "Challenge_title":"CometLogger does not implement name() and version() class methods",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":123,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0330876934,
        "Challenge_watch_issue_ratio":0.0167280766
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"Hi! I've been trying the comet.ml integration and I must say this has been a great addition to the framework. \ud83d\ude4c\r\n\r\nI wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that I expected, especially when trying the learning_rate_warmup_epochs option, which I set to 6 as suggested. The learning rate that is plot on comet is the one set in learning_rate, and it's constant for the first epochs.\r\n\r\nCould this be related to this error?\r\n\r\n`COMET ERROR: Failed to extract parameters from Optimizer.init()\r\n`\r\n\r\n**To Reproduce**\r\n1. Setup comet\r\n2. Set  learning_rate_warmup_epochs option to 6\r\n\r\n**Expected behavior**\r\nI expected to see the lr increase in the first 6 epochs, reach the lr set in learning_rate, and eventually decrease, as I set also reduce_learning_rate_on_plateau .\r\n\r\n**Actual behavior**\r\nThe lr is equal to the set learning_rate in the first epochs, and eventually decreases due to reduce_learning_rate_on_plateau .\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1591889615000,
        "Challenge_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/733",
        "Challenge_link_count":0,
        "Challenge_open_time":21488.4402777778,
        "Challenge_readability":7.9,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":123.0,
        "Challenge_repo_fork_count":1021.0,
        "Challenge_repo_issue_count":2798.0,
        "Challenge_repo_star_count":8658.0,
        "Challenge_repo_watch_count":181.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"The learning rate plot in Comet is not the expected one",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":163,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0439599714,
        "Challenge_watch_issue_ratio":0.0646890636
    },
    {
        "Challenge_adjusted_solved_time":347.7586111111,
        "Challenge_answer_count":8,
        "Challenge_body":"**Describe the bug**\r\n\r\nWhen activating the Comet contrib, most of Ludwig log message disappears.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nLaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --comet`\r\n\r\nYou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ V  V \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - Experiment\r\n\r\nExperiment name: experiment\r\nModel name: run\r\nOutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe log messages should be displayed when the Comet contrib is activated.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Fedora\r\n - Version 28\r\n- Python version: 3.6.8\r\n- Ludwig version: 0.1.2\r\n\r\n**Additional context**\r\n\r\nI think the issue is that ludwig is using the root-level logger configured through `logging.basicConfig`. The comet contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/comet.py#L56.\r\n\r\nThose calls happen before any `basicConfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#L461.\r\n\r\nThe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicConfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/Lib\/logging\/__init__.py#L2065. The direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a StreamHandler pointing to `\/dev\/stderr`.\r\n\r\nThe unfortunate side-effect is that calling `basicConfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\nI would recommend moving from using the root logger and configure the logger through `basicConfig` to using a `ludwig` logger and configure it manually, it's not that more complex. I can help if wanted.\r\n\r\nOne last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. That includes requests and is polluting the output. Using a separate logger would also solve this issue.\r\n",
        "Challenge_closed_time":1559077203000,
        "Challenge_created_time":1557825272000,
        "Challenge_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/340",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":11.0,
        "Challenge_reading_time":29.59,
        "Challenge_repo_contributor_count":123.0,
        "Challenge_repo_fork_count":1021.0,
        "Challenge_repo_issue_count":2798.0,
        "Challenge_repo_star_count":8658.0,
        "Challenge_repo_watch_count":181.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":347.7586111111,
        "Challenge_title":"Logging issue when activating Comet contrib",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":315,
        "Platform":"Github",
        "Solution_body":"@Lothiraldan thanks for reporting this. It is indeed a bug to fix. Also notifying @dsblank as he is the main contributor of the comet integration.\r\nWould gladly accept your help offer on this. I'd like to avoid having a logger object that is passed around the whole codebase, but apart from that I'm open to suggestions.\r\n @w4nderlust Yes, and @Lothiraldan and I both are on the Comet team, so we have already been discussing this. @Lothiraldan has much expertise in loggers, so I look forward to his suggestions as well. Hi @w4nderlust, thank you for your prompt answer. I forgot to said that I'm working with @dsblank in the Comet team.\r\n\r\nHaving a non-global logger is ideal but not always feasible.\r\n\r\nThe approach I'm using in my projects is the following, use a logger per module: `LOGGER = logging.getLogger(__name__)`. As the __name__ often contains your project name, you get will get loggers like `dulwich`, `dulwich.experiment`, `dulwich.contribs.comet`. I think I have taken this idea from Django https:\/\/docs.djangoproject.com\/en\/2.1\/topics\/logging\/#using-logging.\r\n\r\nThis way you can configure the top-level logger for your project and every other loggers will just propagate the log messages to it and uses the configured handlers. This unlock having different log level on a module basis or even different handlers if needed.\r\n\r\nI would highly recommend having a central function where the top-level logger is configured, something like https:\/\/github.com\/Lothiraldan\/balto\/blob\/master\/balto\/_logging.py#L6, I found it that it really helps for maintaining a coherent logging configuration.\r\n\r\nApart from that, the Ludwig project seems to be only using a StreamHandler right now so there is no much expertise I can give you on the handlers subjects.\r\n\r\nDon't hesitate if you have some questions. Thanks for the detailed explanation @Lothiraldan . @msaisumanth Is on top of it. It looks pretty straightforward: have a single global logger setup function, add a logger in every module, use that logger instead of logging. I expect this to be solved pretty quickly.\r\nThank you again! @Lothiraldan please take a look at https:\/\/github.com\/uber\/ludwig\/pull\/352\r\nI was able to verify that the output is getting printed as expected. @w4nderlust if this makes sense, I'll modify the other modules as well.  @Lothiraldan it would be great if you could take a look at the PR, it should solve the issue, but wanted to doublecheck with you before merging it. We merged the PR as we believe it works fine, @Lothiraldan if you could take a look at it to confirm it's fine for you too, that owuld be great. I made some comments, sorry for the delay, I was busy with some other stuff.",
        "Solution_gpt_summary":"move root logger ludwig logger configur manual avoid log logger modul configur central function ludwig implement singl global logger setup function logger modul logger log pull request creat merg review team",
        "Solution_link_count":3.0,
        "Solution_original_content":"lothiraldan report notifi dsblank contributor integr gladli accept avoid logger object pass codebas apart open wnderlust lothiraldan team lothiraldan expertis logger forward wnderlust prompt forgot said dsblank team global logger ideal feasibl logger modul logger log getlogg logger dulwich dulwich dulwich contrib taken idea django http doc djangoproject com log log configur level logger logger propag log messag configur handler unlock log level modul basi handler highli central function level logger configur http github com lothiraldan balto blob master balto log maintain coher log configur apart ludwig streamhandl expertis handler subject hesit explan lothiraldan msaisumanth pretti straightforward singl global logger setup function add logger modul logger log pretti quickli lothiraldan http github com uber ludwig pull verifi output print wnderlust sens modifi modul lothiraldan great doublecheck merg merg believ lothiraldan owuld great comment sorri delai busi stuff",
        "Solution_preprocessed_content":"report notifi contributor integr gladli accept avoid logger object pass codebas apart open team expertis logger forward prompt forgot said team logger ideal feasibl logger modul logger taken idea django configur logger logger propag log messag configur handler unlock log level modul basi handler highli central function logger configur maintain coher log configur apart ludwig streamhandl expertis handler subject hesit explan pretti straightforward singl global logger setup function add logger modul logger log pretti quickli verifi output print sens modifi modul great doublecheck merg merg believ owuld great comment sorri delai busi stuff",
        "Solution_readability":7.6,
        "Solution_reading_time":32.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":30.0,
        "Solution_word_count":426.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0439599714,
        "Challenge_watch_issue_ratio":0.0646890636
    },
    {
        "Challenge_adjusted_solved_time":8082.0786111111,
        "Challenge_answer_count":3,
        "Challenge_body":"It fails at the \"apply patch\" stage",
        "Challenge_closed_time":1624956881000,
        "Challenge_created_time":1595861398000,
        "Challenge_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":4.3,
        "Challenge_reading_time":0.94,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":12.0,
        "Challenge_repo_issue_count":219.0,
        "Challenge_repo_star_count":42.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":8082.0786111111,
        "Challenge_title":"Comet \"Reproduce\" feature doesn't work",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":11,
        "Platform":"Github",
        "Solution_body":"is this still an issue @51N84D ? Yeah, this still doesn't work. I don't think anyone has tried to resolve it yet Ok ; should we in your opinion?",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.1,
        "Solution_reading_time":1.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.0410958904,
        "Challenge_watch_issue_ratio":0.0182648402
    },
    {
        "Challenge_adjusted_solved_time":1.09,
        "Challenge_answer_count":1,
        "Challenge_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600309841000,
        "Challenge_created_time":1600305917000,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":1.09,
        "Challenge_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":96,
        "Platform":"Github",
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_gpt_summary":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"label bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link app homepag http github com marketplac label bot dashboard http mlbot net data khirotaka enchant http github com hamelsmu mlapp bot",
        "Solution_preprocessed_content":"bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link bot",
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.018957346,
        "Challenge_watch_issue_ratio":0.0142180095
    },
    {
        "Challenge_adjusted_solved_time":0.4752777778,
        "Challenge_answer_count":1,
        "Challenge_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Challenge_closed_time":1600153381000,
        "Challenge_created_time":1600151670000,
        "Challenge_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":211.0,
        "Challenge_repo_star_count":7.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":0.4752777778,
        "Challenge_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":109,
        "Platform":"Github",
        "Solution_body":"Issue-Label Bot is automatically applying the label `bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! \n\n Links: [app homepage](https:\/\/github.com\/marketplace\/issue-label-bot), [dashboard](https:\/\/mlbot.net\/data\/khirotaka\/enchanter) and [code](https:\/\/github.com\/hamelsmu\/MLapp) for this bot.",
        "Solution_gpt_summary":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"label bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link app homepag http github com marketplac label bot dashboard http mlbot net data khirotaka enchant http github com hamelsmu mlapp bot",
        "Solution_preprocessed_content":"bot automat appli label confid mark comment thumbsup thumbsdown bot feedback link bot",
        "Solution_readability":13.3,
        "Solution_reading_time":4.88,
        "Solution_score_count":1.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":38.0,
        "Tool":"Comet",
        "Challenge_contributor_issue_ratio":0.018957346,
        "Challenge_watch_issue_ratio":0.0142180095
    },
    {
        "Challenge_adjusted_solved_time":426.2658333333,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Challenge_closed_time":1626683431000,
        "Challenge_created_time":1625148874000,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.8,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":426.2658333333,
        "Challenge_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":118,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0217391304,
        "Challenge_watch_issue_ratio":0.0093167702
    },
    {
        "Challenge_adjusted_solved_time":2.1180555556,
        "Challenge_answer_count":0,
        "Challenge_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Challenge_closed_time":1620393602000,
        "Challenge_created_time":1620385977000,
        "Challenge_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":11.4,
        "Challenge_reading_time":21.45,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":7.0,
        "Challenge_repo_issue_count":644.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":2.1180555556,
        "Challenge_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":166,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0217391304,
        "Challenge_watch_issue_ratio":0.0093167702
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"- [ ] fix docstring\r\n- [ ] test with a Node that has `dvc.params` and `dvc.outs`\r\n\r\nhttps:\/\/github.com\/zincware\/ZnTrack\/blob\/cd2c4f05ad5abf2b23da80fe56558cef6c73e636\/zntrack\/zn\/nodes.py#L11-L28",
        "Challenge_closed_time":null,
        "Challenge_created_time":1658850596000,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/348",
        "Challenge_link_count":1,
        "Challenge_open_time":2888.1677777778,
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.01,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"znNodes not working with `dvc.<...>`",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":17,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0065359477,
        "Challenge_watch_issue_ratio":0.0043572985
    },
    {
        "Challenge_adjusted_solved_time":2.0441666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Challenge_closed_time":1643235530000,
        "Challenge_created_time":1643228171000,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":2.8,
        "Challenge_reading_time":1.95,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2.0441666667,
        "Challenge_title":"zn.Method does not add params to `dvc.yaml`",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":24,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0065359477,
        "Challenge_watch_issue_ratio":0.0043572985
    },
    {
        "Challenge_adjusted_solved_time":473.2511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"One can either define a DVC option with default values in the init, which could be considered a constant, or change a DVC option that has no default values in the call method.\r\n\r\nIf a pre-intialized DVC option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Challenge_closed_time":1634716886000,
        "Challenge_created_time":1633013182000,
        "Challenge_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/76",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":459.0,
        "Challenge_repo_star_count":32.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":473.2511111111,
        "Challenge_title":"raise Error if pre-initialized DVC option is being changed",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0065359477,
        "Challenge_watch_issue_ratio":0.0043572985
    },
    {
        "Challenge_adjusted_solved_time":64.6766666667,
        "Challenge_answer_count":0,
        "Challenge_body":"In every run you can see:\r\n```\r\n               2020-07-03 23:24:19,549 DEBUG: Trying to spawn '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\r\n\/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               2020-07-03 23:24:19,550 DEBUG: Spawned '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef\r\niop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               Unknown mode daemon\r\n```\r\nwe clearly need to take more care on dvc-side, but a good enough workaround is to set CI or DVC_TEST env var to make dvc skip launching the updater.",
        "Challenge_closed_time":1594041670000,
        "Challenge_created_time":1593808834000,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/149",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.9,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":400.0,
        "Challenge_repo_star_count":19.0,
        "Challenge_repo_watch_count":17.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":64.6766666667,
        "Challenge_title":"dvc tries to launch updater using asv script",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":66,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.025,
        "Challenge_watch_issue_ratio":0.0425
    },
    {
        "Challenge_adjusted_solved_time":1034.8597222222,
        "Challenge_answer_count":11,
        "Challenge_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Challenge_closed_time":1642605521000,
        "Challenge_created_time":1638880026000,
        "Challenge_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":5.9,
        "Challenge_reading_time":13.98,
        "Challenge_repo_contributor_count":17.0,
        "Challenge_repo_fork_count":11.0,
        "Challenge_repo_issue_count":154.0,
        "Challenge_repo_star_count":15.0,
        "Challenge_repo_watch_count":14.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":1034.8597222222,
        "Challenge_title":"Various issues in `example-dvc-experiments`",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":176,
        "Platform":"Github",
        "Solution_body":"This seems high priority. We can remove `bug` and change to `p1` after 2. is addressed at least, I think. > 1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n\r\nThis was a bit intentional to let the users install DVC themselves, and a bit to prevent version conflicts. There are some conditions (like installing DVC to system and venv both with different dependencies) that cause weird behavior. \r\n\r\nWe can go on to this route though, it's a single line of change. Is it better to add `dvc` to the `requirements.txt` @shcheklein?  If this was intentional and we don't want to include `dvc` in `requirements.txt`, then we should add an instruction that the user should install `dvc`. Currently, such an instruction is missing. It is unlikely that many people will reach the experiments page of the tutorial without first having installed `dvc`. But in case they try to work a new venv, it can be a `lil confusing. I remembered why I left `-v` in `tar`, it was taking some time after `extract` to start running and the experiment looks like it's frozen. I've now updated the project not to use `-v` in `tar`, and also updated `model.h5` in the remote. (We had a bug in DVC that was preventing to upload experiments.) Could you now check whether the project works as intended? @tapadipti \r\n\r\nI'll create separate PRs in the docs for content updates. Thank you.  Thanks @iesahin \r\n\r\n`dvc pull` gave this error:\r\n```                                                                                                                    \r\nERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\nIs your cache up to date?\r\n<https:\/\/error.dvc.org\/missing-files>\r\n```\r\nSo looks like `metrics` worked but not `model.h5`. And this time, the full file path is displayed.\r\n\r\nRemoving `-v` worked. The files are not listed anymore.\r\n\r\n ```\r\n> ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n\/Users\/tapadiptisitaula\/Documents\/test\/example-dvc-experiments\/models\/model.h5\r\n```\r\n\r\nInteresting. I double checked yesterday that the script pushing the artifacts has completed successfully. Now, I've checked again and it says:\r\n\r\n```\r\ndvc push\r\nEverything is up to date.\r\n```\r\n\r\nCould you check the MD5 line in `dvc.lock`, corresponding to this line: https:\/\/github.com\/iterative\/example-dvc-experiments\/blob\/main\/dvc.lock#L36\r\n\r\nWhat's the MD5 hash value there, in your installation?\r\n Also, I've checked after cloning the repository: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/476310\/145449841-16ae8f43-7ce3-4459-a0d3-225d67214ab0.png)\r\n\r\n@tapadipti  The current staging version in https:\/\/github.com\/iterative\/example-dvc-staging resolves all of these issues. I think we can push it to `example-dvc-experiments`.  @iesahin sounds good. The most recent https:\/\/github.com\/iterative\/example-dvc-experiments resolves all these issues. The codification changes are in #97. Closing this. ",
        "Solution_gpt_summary":"add txt remov tar xvzf data imag tar directori data line lock file date version repositori unclear document section",
        "Solution_link_count":5.0,
        "Solution_original_content":"high prioriti remov address instal pip instal txt virtual env instal separ txt bit intent instal bit prevent version conflict condit instal venv depend rout singl line add txt shcheklein intent txt add instruct instal instruct miss unlik peopl reach page tutori instal venv lil rememb left tar time extract start run frozen updat tar updat model remot prevent upload intend tapadipti creat separ pr doc updat iesahin pull gave pull data cloud checkout target tapadiptisitaula document test model model cach date metric model time file path displai remov file list anymor pull data cloud checkout target tapadiptisitaula document test model model doubl yesterdai push artifact complet successfulli sai push date line lock line http github com iter blob lock hash valu instal clone repositori imag http imag githubusercont com aef dab png tapadipti stage version http github com iter stage push iesahin http github com iter codif close",
        "Solution_preprocessed_content":"high prioriti remov address instal virtual env instal separ intent instruct miss unlik peopl reach page tutori instal lock codif close",
        "Solution_readability":7.3,
        "Solution_reading_time":37.61,
        "Solution_score_count":2.0,
        "Solution_sentence_count":41.0,
        "Solution_word_count":426.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1103896104,
        "Challenge_watch_issue_ratio":0.0909090909
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"When running pull command on DagsHub remote. I receive dvc pull failure, so I have to manually pull dvc again. \n\nThis issue permanent issue on windows. \n\n```bash\nfds clone <remote> \n\n```\n\nIt is not urgent issue, but in annoyance category. ",
        "Challenge_closed_time":null,
        "Challenge_created_time":1647838452000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/121",
        "Challenge_link_count":0,
        "Challenge_open_time":5947.0966666667,
        "Challenge_readability":5.6,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"fds fails to pull dvc on windows",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":45,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":575.7216666667,
        "Challenge_answer_count":0,
        "Challenge_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1630576282000,
        "Challenge_created_time":1628503684000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.8,
        "Challenge_reading_time":15.28,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":575.7216666667,
        "Challenge_title":"fsd clone for non-DVC repos throws an error",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":232,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":120.5566666667,
        "Challenge_answer_count":1,
        "Challenge_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Challenge_closed_time":1622551859000,
        "Challenge_created_time":1622117855000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.3,
        "Challenge_reading_time":4.48,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":120.5566666667,
        "Challenge_title":"Only display the DVC add prompt if there is anything to add",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":40,
        "Platform":"Github",
        "Solution_body":"Fixed in #46 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":3.7161111111,
        "Challenge_answer_count":0,
        "Challenge_body":"`Should we install dvc[https:\/\/dvc.org\/] (`pip install dvc <3`) for you right now?`\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#L27",
        "Challenge_closed_time":1621785253000,
        "Challenge_created_time":1621771875000,
        "Challenge_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/13",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":3.13,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":139.0,
        "Challenge_repo_star_count":357.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":3.7161111111,
        "Challenge_title":"Markdown in dvc install prompt isn't rendered as markdown",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":21,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.071942446,
        "Challenge_watch_issue_ratio":0.0647482014
    },
    {
        "Challenge_adjusted_solved_time":147.5366666667,
        "Challenge_answer_count":0,
        "Challenge_body":"The problem described in this issue es very similar to #77 .\r\n\r\nCurrently, the \"delete\" action just removes the base image file. This is not correct for some reasons:\r\n\r\n- The base images are under control by DVC. The right way to remove a file that has been previously added to DVC is using its remove command, which removes the file pointer.\r\n- The deletion of the base image is not needed because it is not actually in the repository: it is pushed to the DVC remote storage during the base image generation and does not persist after this finishes. In case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow.\r\n\r\nTo summarize: the right way to do the deletion would be using DVC _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.\r\n",
        "Challenge_closed_time":1643645434000,
        "Challenge_created_time":1643114302000,
        "Challenge_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/79",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.7,
        "Challenge_reading_time":11.96,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":112.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":147.5366666667,
        "Challenge_title":"Use DVC remove instead of just removing the base image file",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":180,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0357142857,
        "Challenge_watch_issue_ratio":0.0267857143
    },
    {
        "Challenge_adjusted_solved_time":94.8744444444,
        "Challenge_answer_count":1,
        "Challenge_body":"load_dataset function from hugging face can't access the dvc tracked data directory \r\n--> OSError: [Errno 30] Read-only file system: '\/data'",
        "Challenge_closed_time":1642070875000,
        "Challenge_created_time":1641729327000,
        "Challenge_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/11",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":95.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":94.8744444444,
        "Challenge_title":"data loading bug with dvc",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":23,
        "Platform":"Github",
        "Solution_body":"What command are you using? Note `\/data` is not same as `.\/data`",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.1,
        "Solution_reading_time":0.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0210526316,
        "Challenge_watch_issue_ratio":0.0105263158
    },
    {
        "Challenge_adjusted_solved_time":0.4325,
        "Challenge_answer_count":0,
        "Challenge_body":"",
        "Challenge_closed_time":1638706309000,
        "Challenge_created_time":1638704752000,
        "Challenge_link":"https:\/\/github.com\/se4ai2122-cs-uniba\/CT-COVID\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.2,
        "Challenge_reading_time":0.66,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":66.0,
        "Challenge_repo_star_count":1.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":0.4325,
        "Challenge_title":"Missing params field for evaluate stage in dvc.yaml",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":8,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0606060606,
        "Challenge_watch_issue_ratio":0.0151515152
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"CALL DVC-CC INIT just takes the first three letters of the repo name???\r\n\r\nHere you can enter the folder where you want to store the DVC files on the DVC Storage Server.\r\n\tThe remote DVC folder that you want use (default: ~\/*****\/***\/TES): \r\nThe username with that you can access the DVC storage server \"dt1.f4.htw-berlin.de\".\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1584006633000,
        "Challenge_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/28",
        "Challenge_link_count":0,
        "Challenge_open_time":23678.1575,
        "Challenge_readability":6.0,
        "Challenge_reading_time":4.69,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":30.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"\"dvc-cc init\" just take three letters for the dvc folder name?",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":64,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1333333333,
        "Challenge_watch_issue_ratio":0.0666666667
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n> Entsprechend dem Tutorial habe ich mit sshfs den data Ordner\r\n> gemountet, um externe Daten verwenden zu k\u00f6nnen, was soweit auch\r\n> funktioniert.\r\n> Wenn ich dann aber nach erfolgreich abgeschlossenem Job die Ergebnisse\r\n> ansehen will (git pull, git checkout rcc_00XX_ergebnis_branch, dvc\r\n> pull), bekomme ich eine Fehlermeldung:\r\n> \r\n> rmdir: data: Das Ger\u00e4t oder die Ressource ist belegt\r\n> \r\n> Wenn ich vorher mit fusermount -u data den Dataordner wieder unmounte,\r\n> funktioniert alles wie erwartet. Ist das das zu erwartende Verhalten?\r\n> Muss ich also \"data\" unmounten, um die Ergebnisse ansehen zu k\u00f6nnen?\r\n> Und dann erneut mounten, um einen neuen Job zu starten?\r\n\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66",
        "Challenge_closed_time":null,
        "Challenge_created_time":1583006053000,
        "Challenge_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/27",
        "Challenge_link_count":0,
        "Challenge_open_time":23956.0963888889,
        "Challenge_readability":6.2,
        "Challenge_reading_time":9.95,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":30.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"\"dvc pull\" does not work in the result branch if a sshfs connection is mounted",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1333333333,
        "Challenge_watch_issue_ratio":0.0666666667
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nIf the dvc\/config file is created with whitespaces dvc-cc cann't read the config file.\r\n\r\n**To Reproduce**\r\nCreate a dvc\/config file like this:\r\n`[core]\r\n    remote = dvc_connection\r\n['remote \"dvc_connection\"']\r\n    url = ...............\r\n    ask_password = true\r\n\r\n**Additional context**\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1583005907000,
        "Challenge_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/26",
        "Challenge_link_count":0,
        "Challenge_open_time":23956.1369444444,
        "Challenge_readability":6.7,
        "Challenge_reading_time":4.92,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":1.0,
        "Challenge_repo_issue_count":30.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"dvc servername and url not found by calling \"dvc-cc run\"",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.1333333333,
        "Challenge_watch_issue_ratio":0.0666666667
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":11,
        "Challenge_body":"UPDATE: Summary in https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20#issuecomment-1164570090\r\n\r\nI cloned https:\/\/github.com\/iterative\/dvc-checkpoints-mnist. I setup the IDE workspace so the extension is active.\r\n\r\nI haven't run any experiments:\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174509065-ac8f2c97-0d7f-4b1f-b6c4-e36603406c50.png)\r\n\r\nI check out the [`make_checkpoint`](https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/tree\/make_checkpoint) branch. The DVC view and Plots Dashboard never load.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174508899-c1e5788a-2ead-446d-bab6-0239cbc27519.png)\r\n\r\nThe Experiments Table says \"No Experiments to Display.\"\r\n\r\nOther components do load.\r\n\r\nDVC virtual env is loaded via MS Python extension.\r\n\r\n```console\r\n$ dvc version\r\nDVC version: 2.11.0 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.13 on macOS-12.4-arm64-arm-64bit\r\nSupports:\r\n        webhdfs (fsspec = 2022.5.0),\r\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\r\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)\r\nCache types: <https:\/\/error.dvc.org\/no-dvc-cache>\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk3s1s1\r\nRepo: dvc, git\r\n```\r\n\r\n---\r\n\r\n~~p.s. the same happens in the included `demo\/` project if I set up the extension with `\"dvc.dvcPath\": \"demo\/.env\/bin\/dvc\"` in .vscode\/settings.json (no MS Python extension).~~",
        "Challenge_closed_time":null,
        "Challenge_created_time":1655687938000,
        "Challenge_link":"https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20",
        "Challenge_link_count":6,
        "Challenge_open_time":3766.6838888889,
        "Challenge_readability":10.4,
        "Challenge_reading_time":18.61,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":20.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC View and Plots don't load in `vscode-dvc`",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":131,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.25,
        "Challenge_watch_issue_ratio":0.9
    },
    {
        "Challenge_adjusted_solved_time":9316.4808333333,
        "Challenge_answer_count":4,
        "Challenge_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Challenge_closed_time":1668173479000,
        "Challenge_created_time":1634634148000,
        "Challenge_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.57,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":2.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":9316.4808333333,
        "Challenge_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":46,
        "Platform":"Github",
        "Solution_body":"So:\r\n1. You will need to have aws credentials set up with `aws configure`, having installed awscli (which is in the virtualenv)\r\n2. I'm having some issues getting the mantisnlp-public bucket to be accessible to dvc with a non mantis aws profile. I don't know if this is related but did you try `--acl public-read`? I had some problems with public buckets in grants tagger and for me it was resolved by adding this flag. example https:\/\/github.com\/wellcometrust\/grants_tagger\/blob\/970abbc63b448c4d14d7b70fa13ca29760a897ce\/Makefile#L94 I've done this at the bucket level, not at the individual object level, because they are added by dvc. It _should_ be working... btw this issue is probably badly named because:\r\n1. You only need to set `AWS_PROFILE` if you have more than one set of aws credentials\r\n2. You can also set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in the folder to the same effect, and users can decide how best to do this.",
        "Solution_gpt_summary":"bucket public read credenti set configur instal awscli acl public read flag set profil set credenti set access kei secret access kei folder",
        "Solution_link_count":1.0,
        "Solution_original_content":"credenti set configur instal awscli virtualenv mantisnlp public bucket access manti profil relat acl public read public bucket grant tagger flag http github com wellrust grant tagger blob abbcbcddbfacaac makefil bucket level individu object level btw probabl badli set profil set credenti set access kei secret access kei folder decid",
        "Solution_preprocessed_content":"credenti set instal awscli bucket access manti profil relat public bucket grant tagger flag bucket level individu object level btw probabl badli set set credenti set folder decid",
        "Solution_readability":7.4,
        "Solution_reading_time":11.62,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":149.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0714285714,
        "Challenge_watch_issue_ratio":0.0714285714
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"If the DVC remote name is left blank, the post-gen hook shouldn't try to set one. Currently this raises a (non-fatal) error.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1623947751000,
        "Challenge_link":"https:\/\/github.com\/adamtupper\/cookiecutter-lvsn-workflow\/issues\/9",
        "Challenge_link_count":0,
        "Challenge_open_time":12583.4025,
        "Challenge_readability":6.4,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":0.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":14.0,
        "Challenge_repo_star_count":0.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Post-gen hook shouldn't configure a DVC remote if no name is provided",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":33,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.0,
        "Challenge_watch_issue_ratio":0.1428571429
    },
    {
        "Challenge_adjusted_solved_time":0.9458333333,
        "Challenge_answer_count":2,
        "Challenge_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Challenge_closed_time":1633697794000,
        "Challenge_created_time":1633694389000,
        "Challenge_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.7,
        "Challenge_reading_time":20.74,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":3.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":0.9458333333,
        "Challenge_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":191,
        "Platform":"Github",
        "Solution_body":"Thanks for the catch @mike0sv ! No trouble (literally, no trouble at all since it was @mnrozhkov :))",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"catch mikesv troubl liter troubl mnrozhkov",
        "Solution_preprocessed_content":null,
        "Solution_readability":4.1,
        "Solution_reading_time":1.22,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":16.0,
        "Tool":"DVC",
        "Challenge_contributor_issue_ratio":0.6666666667,
        "Challenge_watch_issue_ratio":6.0
    },
    {
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_created_time":1656532075000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_created_time":1652380533000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":185,
        "Platform":"Github",
        "Solution_body":"Closed by #313 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1326.7408333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi @Galileo-Galilei\r\n\r\n## Description\r\nthe KedroPipelineModel has a `initial_catalog` property which causes some problems. This `initial_catalog` can contain some Kedro Datasets but it's not necessary to log them when you train your model. because of this property I can't load my model anymore. I have to train it again.\r\n\r\nI explain : when I trained my model I used a kedro home-made plugin to load a specific dataset (which has no impact for my model). After that, I updated this plugin independently of my ML project. Today, I want to load my model but I can't because the load function uses the old Kedro Catalog with my old plugin version which is not in my environnement anymore. \r\n\r\n## Context\r\nIt would be great if we can update the kedro-catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models.\r\n\r\n## Possible Implementation\r\nLog in Mlflow what is only necessary.\r\n\r\nI hope my issue is clear.\r\n\r\nthank you",
        "Challenge_closed_time":1644791409000,
        "Challenge_created_time":1640015142000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/273",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.8,
        "Challenge_reading_time":12.57,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":1326.7408333333,
        "Challenge_title":"KedroPipelineModel requires unnecessary pipeline input dependencies to be executed",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":168,
        "Platform":"Github",
        "Solution_body":"Hi, I can reproduce the issue, thank you very much for the feedback. To clarify, what happens here is the following: \r\n\r\n- the input of your inference pipeline is persisted in Kedro because you load it from the disk (e.g., pandas.ExcelDataSet)\r\n- after you log it in mlflow, it will be converted to a ``MemoryDataSet``, and you directly pass a pandas Dataframe when you want to reuse it. Mlflow complains that you need to have ``openpyxl`` installed, while you never use it in your pipeline, and you don't need it to predict.\r\n\r\nThis extra dependency is not useful as you mention. I will remove it in a patch release soon.\r\n\r\n For anyone having the same issue, notice that you can now export a pipeline as a mlflow model with the [``kedro mlflow modelify``](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/05_pipeline_serving\/03_cli_modelify.html) command.",
        "Solution_gpt_summary":"unnecessari pipelin input depend pipelinemodel initi catalog properti patch releas soon export pipelin model modelifi updat catalog retrain model",
        "Solution_link_count":1.0,
        "Solution_original_content":"reproduc feedback clarifi input infer pipelin persist load disk panda exceldataset log convert memorydataset directli pass panda datafram reus complain openpyxl instal pipelin predict extra depend remov patch releas soon notic export pipelin model modelifi http readthedoc stabl sourc pipelin serv cli modelifi html",
        "Solution_preprocessed_content":"reproduc feedback clarifi input infer pipelin persist load disk log convert directli pass panda datafram reus complain instal pipelin predict extra depend remov patch releas soon notic export pipelin model",
        "Solution_readability":9.5,
        "Solution_reading_time":10.52,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":132.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_created_time":1619193727000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":273,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_gpt_summary":"setup panda iri starter add yml manual conf local folder instal version remov global group on bias summari",
        "Solution_link_count":3.0,
        "Solution_original_content":"wil weekend version brand releas yesterdai past version updat broken auto discoveri mechan releas note cli discoveri enal overrid plugin updat panda iri starter match version compliant setup panda iri starter add od test plugin glad plugin viz cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin viz viz global properli hook properli load add yml manual conf local folder folder conf short term ven conveni allow kei document http readthedoc latest sourc experiment track configur html yml file irectli copi past http github com galileo galilei blob master templat yml test pass test relat cli plugin local starter quickli read messag detect consist independ panda iri starter viz workaround directli mondai morn nice plugin hook load function depend yml present init copi file templat conf local instal version consol pip uninstal pip instal git http github com galileo galilei git cli bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur group global init level unknown reason account group version report core team affect plugin viz docker airflow global quickest hacki remov global group on branch cli repo plugin hook load function depend yml present init copi file templat conf local exactli init render templat copi past replac jinja tag dynam valu folder conf folder default local specifi environ init env hook logic yml file pass paramet wrapper paramet track uri port host defin yml file quick test patch charm dmb deploi patch pypi pip instal close free reopen version",
        "Solution_preprocessed_content":"wil weekend version brand past version updat broken mechan updat starter match version compliant setup starter add od test plugin glad plugin cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin global properli hook properli load add manual folder short term ven conveni allow kei irectli test pass test relat cli plugin local quickli read messag detect consist independ starter workaround directli mondai morn nice plugin hook load function depend present copi file templat instal version bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur account group version report core team affect plugin global quickest remov global group on branch repo plugin hook load function depend yml present init copi file templat exactli render templat folder folder hook logic yml file pass paramet wrapper paramet defin file quick test patch charm deploi patch pypi close free reopen version",
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":7.0,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_created_time":1617627646000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_created_time":1610404594000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_gpt_summary":"wait migrat mileston remov boilerpl plugin consist futur",
        "Solution_link_count":0.0,
        "Solution_original_content":"wait migrat mileston bradnd util remov boilerpl plugin consist futur",
        "Solution_preprocessed_content":"wait migrat mileston bradnd util remov boilerpl plugin consist futur",
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_created_time":1605983313000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":137,
        "Platform":"Github",
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_gpt_summary":"modifi line packag remov faulti line initi catalog directli copi dataset catalog loader involv store model modul attribut properti attribut load modul fly remov deepcopi pipelinemodel intend separ process infer time note dataset deepcopi radic frequent",
        "Solution_link_count":0.0,
        "Solution_original_content":"remov faulti line directli initi catalog model loadabl option longer deepcopi initi catalog copi dataset catalog loader kera model clone model kera model dataset pipelinemodel intent separ process infer time remov deepcopi conflict function catalog come abstractmodeldataset model modul attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi underli valu dataset load safe radic on",
        "Solution_preprocessed_content":"remov faulti line directli model loadabl option longer deepcopi copi dataset catalog loader intent separ process remov deepcopi come abstractmodeldataset attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi safe radic on",
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_created_time":1605982845000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_gpt_summary":"manual specifi run statu run replac line retriev run statu import note run termin manual avoid interfer",
        "Solution_link_count":0.0,
        "Solution_original_content":"catch catch manual end run receiv process longer end run manual tag run control pipelin appli specifii statu termin run manual interact cli ti avoid interfer",
        "Solution_preprocessed_content":"catch catch manual end run receiv process longer end run manual tag run control pipelin appli termin run manual interact avoid interfer",
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":0.6055555556,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Challenge_closed_time":1605720463000,
        "Challenge_created_time":1605718283000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.5,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":0.6055555556,
        "Challenge_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":218,
        "Platform":"Github",
        "Solution_body":"The issue is not confirmed and was due to adding a Pipeline and a PipelineML object.\r\nI close it.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"pipelin pipelineml object close",
        "Solution_preprocessed_content":"pipelin pipelineml object close",
        "Solution_readability":2.3,
        "Solution_reading_time":1.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":19.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":0.935,
        "Challenge_answer_count":1,
        "Challenge_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Challenge_closed_time":1601893558000,
        "Challenge_created_time":1601890192000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":22.2,
        "Challenge_reading_time":48.48,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":0.935,
        "Challenge_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":212,
        "Platform":"Github",
        "Solution_body":"I see its fixed now so I'm closing this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.5,
        "Solution_reading_time":0.54,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_created_time":1593379921000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Since kedro 0.17.7(?) there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled.\r\n\r\nUnless the function to create kfp artifacts is disabled in `kubeflow.yaml` config:\r\n```yaml\r\nstore_kedro_outputs_as_kfp_artifacts: False\r\n```\r\nIt causes issues like:\r\n```\r\nValueError: Only letters, numbers, spaces, \"_\", and \"-\" are allowed in name. Must begin with a letter. Got name: data_science.active_modelling_pipeline.X_train\r\n```\r\nwhen trying to run or update the pipeline.\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1658927398000,
        "Challenge_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/160",
        "Challenge_link_count":0,
        "Challenge_open_time":2866.8338888889,
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.99,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":207.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Add support for kedro namespaces in data catalog",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.0531400966,
        "Challenge_watch_issue_ratio":0.0628019324
    },
    {
        "Challenge_adjusted_solved_time":1225.01,
        "Challenge_answer_count":1,
        "Challenge_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1668830449000,
        "Challenge_created_time":1664420413000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.5,
        "Challenge_reading_time":36.78,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1225.01,
        "Challenge_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":222,
        "Platform":"Github",
        "Solution_body":"I think you are missing the data from the catalog.\r\n\r\n```yml\r\nexample_iris_data:\r\n  type: pandas.CSVDataSet\r\n  filepath: data\/01_raw\/iris.csv\r\nexample_train_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_x.pkl\r\nexample_train_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_train_y.pkl\r\nexample_test_x:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_x.pkl\r\nexample_test_y:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/05_model_input\/example_test_y.pkl\r\nexample_model:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/06_models\/example_model.pkl\r\nexample_predictions:\r\n  type: pickle.PickleDataSet\r\n  filepath: data\/07_model_output\/example_predictions.pkl\r\n```\r\n\r\nSee https:\/\/kedro.readthedocs.io\/en\/stable\/deployment\/airflow_astronomer.html?highlight=astro-airflow-iris\r\n\r\nCan you provide the steps to reproduce the issue? What versions of `kedro`, `kedro-airflow` are you using and what commands did you run?\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"miss data catalog yml iri data type panda csvdataset filepath data raw iri csv train type pickl pickledataset filepath data model input train pkl train type pickl pickledataset filepath data model input train pkl test type pickl pickledataset filepath data model input test pkl test type pickl pickledataset filepath data model input test pkl model type pickl pickledataset filepath data model model pkl predict type pickl pickledataset filepath data model output predict pkl http readthedoc stabl deploy airflow astronom html highlight astro airflow iri step reproduc version airflow run",
        "Solution_preprocessed_content":"miss data catalog step reproduc version airflow run",
        "Solution_readability":17.8,
        "Solution_reading_time":12.75,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":71.0,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":12,
        "Challenge_body":"Raised by @jweiss-ocurate:\r\n\r\n## Description\r\nI am trying to run a simple spaceflights example with Astrocloud. I wasn't sure if anyone has been able to get it to work. \r\n\r\nHere is the DockerFile:\r\nFROM quay.io\/astronomer\/astro-runtime:4.1.0\r\n\r\nRUN pip install --user new_kedro_project-0.1-py3-none-any.whl --ignore-requires-python\r\n\r\n## Context\r\nI am trying to use kedro-airflow with astrocloud.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Follow directions here https:\/\/kedro.readthedocs.io\/en\/latest\/10_deployment\/11_airflow_astronomer.html\r\n2. Replace the DockerFile with the above mentioned image.\r\n\r\n## Expected Result\r\nComplete Kedro Run on local Airflow image.\r\n\r\n## Actual Result\r\nFailure in local Airflow image.\r\n[2022-02-26, 16:43:26 UTC] {store.py:32} INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\r\n[2022-02-26, 16:43:26 UTC] {session.py:78} WARNING - Unable to git describe \/usr\/local\/airflow\r\n[2022-02-26, 16:43:29 UTC] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment you experienced the bug in:\r\n\r\n* Kedro-Airflow plugin version used (get it by running `pip show kedro-airflow`): 0.4.1\r\n* Airflow version (`airflow --version`):\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.17.7\r\n* Python version used (`python -V`): > 2.0.0\r\n* Operating system and version: Ubuntu Linux 20.04",
        "Challenge_closed_time":null,
        "Challenge_created_time":1648473272000,
        "Challenge_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/13",
        "Challenge_link_count":1,
        "Challenge_open_time":5770.7577777778,
        "Challenge_readability":8.9,
        "Challenge_reading_time":18.22,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":13.0,
        "Challenge_repo_issue_count":97.0,
        "Challenge_repo_star_count":37.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":null,
        "Challenge_title":"Kedro-Airflow not working with Astrocloud",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":174,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":0.1443298969,
        "Challenge_watch_issue_ratio":0.0206185567
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"### Description\r\nRunning the kedro-pipeline \"dp\" via kedro-cli with \"kedro run --pipeline dp\" results in the following error:\r\n```cmd\r\nkedro.io.core.DataSetError: Failed while loading data from data set TextDataSet(filepath=C:\/EEAA\/Repos\/QuaselDoku\/data\/01_raw\/Doku_v1\/Bedienung\/EasyInsert.html, protocol=file).\r\n'charmap' codec can't decode byte 0x81 in position 5899: character maps to <undefined>\r\n```\r\n\r\n### Steps to reproduce\r\ncatalog.yml:\r\n```yml\r\necu_test_doku:\r\n  type: PartitionedDataSet\r\n  path: data\/01_raw\/Doku_v1\r\n  dataset: text.TextDataSet\r\n  filename_suffix: html\r\n```\r\n\r\npython-function to parse documentation-data:\r\n```python\r\ndef filter_doku(partitioned_input: Dict[str, Callable[[], Any]], params: Dict) -> Dict[str, Callable[[], Any]]:\r\n    \"\"\"\r\n    flatten input where html files can occur on multiple levels, as well as filter out files that match certain string.\r\n    Return new Dictionary with filenames and load functions from which a PartioniedDataset can be created and persisted.\r\n\r\n    Args:\r\n        partitioned_input: A dictionary with partition ids (file path) as keys and load functions as values.\r\n\r\n    Returns:\r\n        Dictionary with the partitions to create.\r\n    \"\"\"\r\n\r\n    result = {}\r\n\r\n    print(\"filtering out relevant html files from doku ...\")\r\n    for partition_key, partition_load_func in tqdm(sorted(partitioned_input.items())):\r\n        \r\n        exclude = False\r\n        for string in params['exclude_docs']:\r\n            \r\n            if string in partition_key:\r\n                \r\n                exclude = True\r\n                break\r\n\r\n        if exclude:\r\n            continue\r\n\r\n        filename = partition_key.replace('\/', ' ')\r\n        filename += 'html'\r\n\r\n        # append new filename with load function to results dictionary\r\n        result[filename] = partition_load_func\r\n\r\n    return result\r\n```\r\n\r\nkedro  0.18.0\r\n\r\nThanks! :)\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1654682698000,
        "Challenge_link":"https:\/\/github.com\/quaseldoku\/QuaselDoku\/issues\/1",
        "Challenge_link_count":0,
        "Challenge_open_time":4045.9172222222,
        "Challenge_readability":12.5,
        "Challenge_reading_time":21.73,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":1.0,
        "Challenge_repo_star_count":1.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Running kedro-pipeline \"dp\" results in \"character maps to <undefined>\"-error",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":188,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Kedro",
        "Challenge_contributor_issue_ratio":4.0,
        "Challenge_watch_issue_ratio":1.0
    },
    {
        "Challenge_adjusted_solved_time":2083.0769444444,
        "Challenge_answer_count":1,
        "Challenge_body":"### Summary\r\n\r\nProfiling with mlflow and without an mlflow writer fails silently. \r\n\r\n### Steps to Reproduce it\r\n\r\nuse mlflow with get_or_create_session and no files are written.\r\n\r\n### Example\r\n\r\nThere are examples of how to configure mlflow writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_mlflow.yaml\r\n\r\nwhylogs should mention the missing mlflow writer in a warning. Maybe we can automatically add the mlflow writer (with a warning), so that it works and draws attention to where the behavior can be modified.\r\n\r\n## What is the current *bug* behavior?\r\n\r\nlogging with mlflow and default configuration appears to fail silently.\r\n\r\n### What is the expected *correct* behavior?\r\n\r\nmlflow integration should write to mlflow by default and warn if missing or inconsistent config is set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Challenge_closed_time":1655127386000,
        "Challenge_created_time":1647628309000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/480",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":2083.0769444444,
        "Challenge_title":"using mlflow without an mlflow writer configured appears to fail silently",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":122,
        "Platform":"Github",
        "Solution_body":"This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"stale remov stale label close tomorrow",
        "Solution_preprocessed_content":"stale remov stale label close tomorrow",
        "Solution_readability":3.5,
        "Solution_reading_time":0.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":13.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":2491.9302777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Challenge_closed_time":1655127391000,
        "Challenge_created_time":1646156442000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":5.91,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":2491.9302777778,
        "Challenge_title":"Support writing out dataset profiles as json format with mlflow",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"How can i retrieve the profile while inside the start_run()? This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"retriev profil insid start run stale remov stale label close tomorrow",
        "Solution_preprocessed_content":"retriev profil insid stale remov stale label close tomorrow",
        "Solution_readability":2.8,
        "Solution_reading_time":1.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":23.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":3662.6730555556,
        "Challenge_answer_count":5,
        "Challenge_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Challenge_closed_time":1655127397000,
        "Challenge_created_time":1641941774000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.3,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":3662.6730555556,
        "Challenge_title":"MLflow example: close session error",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"The example closes the default session, and then later the mlflow.whylogs wrapper is using that closed session to create loggers. Need to update that example's initial session creation to something like:\r\n```\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session()\r\nsummary = session.profile_dataframe(train, \"training-data\").flat_summary()['summary']\r\n\r\nsummary\r\n``` Still need to update the example to work in Binder better:\r\n* install dependencies\r\n* coinfigure mlflow writer, currently the default session will just write to local disk Part of the reason is that it picks up the default YAML file with default list of writers - and they don't contain mlflow (for obvious reason): https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs.yaml\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/26821974\/149250188-154d5b19-348e-44ea-b64a-0c4724b3c0cd.png)\r\n\r\nHere's my fix in the notebook\r\n\r\nNow this poses interesting quesiton:\r\n* Should mlflow writer be allowed if you don't run mlflow? My instinct is to say yes, but you get a big warning. Or exception?\r\n* If you specify a config without mlflow writer, should we implicitly add mlflow writer? Maybe yes.\r\n\r\nHowever so far I'm not a fan of implicit behaviors because it's freaking hard for us to reason about (see this issue - took a bit of debugging to find out that it's config related). My vote is to throw exception with an option to disable that exception if user chooses the path of ignorance. Drop in the code of the two cells:\r\n\r\n```\r\nconfig = \"\"\"\r\nproject: example-project\r\npipeline: example-pipeline\r\nverbose: false\r\nwriters:\r\n# Save to mlflow\r\n- formats:\r\n    - protobuf\r\n  output_path: mlflow\r\n  type: mlflow\r\n\"\"\"\r\ncfg_file = \"mlflow_config.yaml\"\r\n\r\n!echo \"{config}\" > {cfg_file}\r\n\r\nfrom whylogs import get_or_create_session\r\n\r\nsession = get_or_create_session(cfg_file)\r\n\r\nassert whylogs.__version__ >= \"0.1.13\" # we need 0.1.13 or later for MLflow integration\r\nwhylogs.enable_mlflow(session)\r\n``` This issue is stale. Remove stale label or it will be closed tomorrow.",
        "Solution_gpt_summary":"initi session creation updat creat session whylog session depend instal writer configur binder writer allow implicitli specifi config throw except option disabl choos cell",
        "Solution_link_count":2.0,
        "Solution_original_content":"close default session later whylog wrapper close session creat logger updat initi session creation whylog import creat session session creat session summari session profil datafram train train data flat summari summari summari updat binder instal depend coinfigur writer default session write local disk reason pick default yaml file default list writer obviou reason http github com whylab whylog blob mainlin whylog yaml imag http imag githubusercont com cbccd png notebook pose quesiton writer allow run instinct big warn except specifi config writer implicitli add writer mayb fan implicit freak reason took bit debug config relat vote throw except option disabl except choos path ignor drop cell config pipelin pipelin verbos writer save format protobuf output path type cfg file config yaml echo config cfg file whylog import creat session session creat session cfg file assert whylog version later integr whylog enabl session stale remov stale label close tomorrow",
        "Solution_preprocessed_content":"close default session later whylog wrapper close session creat logger updat initi session creation updat binder instal depend coinfigur writer default session write local disk reason pick default yaml file default list writer notebook pose quesiton writer allow run instinct big warn except specifi config writer implicitli add writer mayb fan implicit freak reason vote throw except option disabl except choos path ignor drop cell stale remov stale label close tomorrow",
        "Solution_readability":11.3,
        "Solution_reading_time":25.46,
        "Solution_score_count":1.0,
        "Solution_sentence_count":15.0,
        "Solution_word_count":261.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":23.5547222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Challenge_closed_time":1603222865000,
        "Challenge_created_time":1603138068000,
        "Challenge_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.9,
        "Challenge_reading_time":14.1,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":86.0,
        "Challenge_repo_issue_count":1012.0,
        "Challenge_repo_star_count":1924.0,
        "Challenge_repo_watch_count":28.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":23.5547222222,
        "Challenge_title":"MLFlow NameError",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":108,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0128458498,
        "Challenge_watch_issue_ratio":0.0276679842
    },
    {
        "Challenge_adjusted_solved_time":3836.5522222222,
        "Challenge_answer_count":0,
        "Challenge_body":"Currently the `.drone.yaml` is referencing the k8s secret `mlflow-server-secret` which doesn't exist by default.\r\n\r\nWe have noticed that `{{ .ProjectID }}-mlflow-secret` secret is created when a `kdlProject` resource is created.\r\n\r\nTo solve this issue the name of the `mlflow-server-secret` must be changed into `{{ .ProjectID }}-mlflow-secret`",
        "Challenge_closed_time":1664791991000,
        "Challenge_created_time":1650980403000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/810",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.3,
        "Challenge_reading_time":4.82,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":3836.5522222222,
        "Challenge_title":"Project template mlflow secret bad name",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":49,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":122.8697222222,
        "Challenge_answer_count":0,
        "Challenge_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Challenge_closed_time":1635871931000,
        "Challenge_created_time":1635429600000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.5,
        "Challenge_reading_time":7.01,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":122.8697222222,
        "Challenge_title":"Project operator mlflow image tag is set to \"latest\"",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":243.3152777778,
        "Challenge_answer_count":0,
        "Challenge_body":"Only allow access to project members for the given MLflow.",
        "Challenge_closed_time":1620648494000,
        "Challenge_created_time":1619772559000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/404",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.2,
        "Challenge_reading_time":1.2,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":243.3152777778,
        "Challenge_title":"Users can access to any MLflow project",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":16,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":1124.7847222222,
        "Challenge_answer_count":0,
        "Challenge_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Challenge_closed_time":1623230615000,
        "Challenge_created_time":1619181390000,
        "Challenge_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":16.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":909.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":1124.7847222222,
        "Challenge_title":"All MLflow experiments are visible for any user",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":80,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0176017602,
        "Challenge_watch_issue_ratio":0.0077007701
    },
    {
        "Challenge_adjusted_solved_time":71.3516666667,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\nI try to do multi-label classification with \"doc_classification_multilabel.py\". It worked at first. However when it came to `\"Train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n......\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n```\r\n\r\n  I have checked that the Internet connection was ok. So I was confused why this error occured ?\r\n  \r\n\r\n**Error message**\r\nError that was thrown (if available)\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n**System:**\r\n - OS: \r\n - GPU\/CPU:\r\n - FARM version:\r\n",
        "Challenge_closed_time":1580393757000,
        "Challenge_created_time":1580136891000,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/FARM\/issues\/217",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.0,
        "Challenge_reading_time":21.8,
        "Challenge_repo_contributor_count":36.0,
        "Challenge_repo_fork_count":231.0,
        "Challenge_repo_issue_count":844.0,
        "Challenge_repo_star_count":1598.0,
        "Challenge_repo_watch_count":56.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":71.3516666667,
        "Challenge_title":"MLFlowLogger: \"Connection aborted.\" - RemoteDisconnected Error",
        "Challenge_topic":"Metric Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":177,
        "Platform":"Github",
        "Solution_body":"Hey @JiangYanting, \r\n\r\nAre you using our public mlflow server for logging (i.e. `ml_logger = MLFlowLogger(tracking_uri=\"https:\/\/public-mlflow.deepset.ai\/\")\r\n` in doc_classification_multilabel.py)? \r\n\r\nI would assume that your connection to that server was not available when the model tried to log the train_loss at step 17251. \r\n\r\nI see two solutions:\r\n- short term: you can log locally by setting `ml_logger = MLFlowLogger(tracking_uri=\"\")`\r\n- mid term: implementing a fix in FARM, so that we raise only a warning, if the logging doesn't succeed, but let the training continue. Let me know if you are interested in adding a PR for this. Otherwise, we can take care. It would be basically a try \/ catch block here: https:\/\/github.com\/deepset-ai\/FARM\/blob\/master\/farm\/utils.py#L126 @tholor By setting `ml_logger = MLFlowLogger(tracking_uri=\"\")` , it works. Thank you very much ! ^_^",
        "Solution_gpt_summary":"short term log local set logger logger track uri mid term implement farm rais warn log train catch block",
        "Solution_link_count":2.0,
        "Solution_original_content":"jiangyant public server log logger logger track uri http public deepset doc classif multilabel connect server model tri log train loss step short term log local set logger logger track uri mid term implement farm rais warn log train care catch block http github com deepset farm blob master farm util tholor set logger logger track uri",
        "Solution_preprocessed_content":"public server log mid term implement farm rais warn log train care catch block set",
        "Solution_readability":8.4,
        "Solution_reading_time":10.9,
        "Solution_score_count":1.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":117.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0426540284,
        "Challenge_watch_issue_ratio":0.0663507109
    },
    {
        "Challenge_adjusted_solved_time":5.2394444444,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Challenge_closed_time":1642206718000,
        "Challenge_created_time":1642187856000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":45.4,
        "Challenge_reading_time":14.23,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":5.2394444444,
        "Challenge_title":"Can not create model in MLflowCatalog",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":41,
        "Platform":"Github",
        "Solution_body":"Duplicated to #496 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":9.2,
        "Solution_reading_time":0.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":4.5558333333,
        "Challenge_answer_count":0,
        "Challenge_body":"Problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ML_predict(ssd, image) FROM coco limit 1;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Undefined function: '<anonymous>'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Register models into mlflow\r\n2. Start Spark thrift server\r\n3. Use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `SELECT ML_PREDICT(ssd, image) FROM coco`",
        "Challenge_closed_time":1642203169000,
        "Challenge_created_time":1642186768000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/492",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":39.2,
        "Challenge_reading_time":31.45,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":4.5558333333,
        "Challenge_title":"MlflowCatalog anonymous function is not registered ",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":0.5105555556,
        "Challenge_answer_count":0,
        "Challenge_body":"```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import mlflow_logger as mlflow\r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/mlflow_logger.py:19: in <module>\r\n    import mlflow\r\nE   ModuleNotFoundError: No module named 'mlflow'\r\n```",
        "Challenge_closed_time":1617994925000,
        "Challenge_created_time":1617993087000,
        "Challenge_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/207",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.0,
        "Challenge_reading_time":4.61,
        "Challenge_repo_contributor_count":10.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":715.0,
        "Challenge_repo_star_count":127.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":0.5105555556,
        "Challenge_title":"Leaking mlflow dependency",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":30,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.013986014,
        "Challenge_watch_issue_ratio":0.0097902098
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Keep it in mind before mindlessly updating\r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/4118",
        "Challenge_closed_time":null,
        "Challenge_created_time":1613838919000,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/270",
        "Challenge_link_count":1,
        "Challenge_open_time":15391.4113888889,
        "Challenge_readability":9.8,
        "Challenge_reading_time":1.82,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Pytorch Lightning 1.2.0 requires new MLflow version",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":14,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0099337748,
        "Challenge_watch_issue_ratio":0.0033112583
    },
    {
        "Challenge_adjusted_solved_time":267.6955555556,
        "Challenge_answer_count":2,
        "Challenge_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Challenge_closed_time":1613342987000,
        "Challenge_created_time":1612379283000,
        "Challenge_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":604.0,
        "Challenge_repo_star_count":35.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":267.6955555556,
        "Challenge_title":"Warning when training mlflow-pytorch 2.0.0",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":"Thats new I did not encountered this while I tested it.  Seems to be gone with my latest changes.\r\nPlease verify @Imipenem and close if not observed.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"that test gone latest verifi imipenem close observ",
        "Solution_preprocessed_content":"that test gone latest verifi close observ",
        "Solution_readability":3.3,
        "Solution_reading_time":1.78,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":27.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0099337748,
        "Challenge_watch_issue_ratio":0.0033112583
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"## Expected Behavior\r\nDeploy Jobs with --assets-only option\r\n## Current Behavior\r\nMLFlow API Request 409 Conflict \r\n## Steps to Reproduce (for bugs)\r\n[dbx][2022-11-03 12:30:40.370] \ud83d\udd0e Deployment file is not provided, searching in the conf directory\r\n[dbx][2022-11-03 12:30:40.375] \ud83d\udca1 Auto-discovery found deployment file conf\/deployment.json\r\n[dbx][2022-11-03 12:30:40.376] \ud83c\udd97 Deployment file conf\/deployment.json exists and will be used for deployment\r\n[dbx][2022-11-03 12:30:40.377] Starting new deployment for environment dev\r\n[dbx][2022-11-03 12:30:40.378] Using profile provided from the project file\r\n[dbx][2022-11-03 12:30:40.378] Found auth config from provider EnvironmentVariableConfigProvider, verifying it\r\n[dbx][2022-11-03 12:30:40.379] Found auth config from provider EnvironmentVariableConfigProvider, verification successful\r\n[dbx][2022-11-03 12:30:44.897] \r\n                Since v0.7.0 environment configurations should be nested under environments section.\r\n\r\n                Please nest environment configurations under this section to avoid potential issues while using \"build\"\r\n                configuration directive.\r\n            \r\n[dbx][2022-11-03 12:30:44.899] No build logic defined in the deployment file. Default pip-based build logic will be used.\r\n[dbx][2022-11-03 12:30:44.903] Usage of jobs keyword in deployment file is deprecated. Please use workflows instead (simply rename this section to workflows).\r\n[dbx][2022-11-03 12:30:44.906] Workflows ['add-on-chanel-AT', 'add-on-chanel-PL', 'add-on-PL'] were selected for further operations\r\n[dbx][2022-11-03 12:30:44.907] Following the provided build logic\r\n[dbx][2022-11-03 12:30:44.908] \ud83d\udc0d Building a Python-based project\r\n[dbx][2022-11-03 12:30:46.262] \u2705 Python-based project build finished\r\n[dbx][2022-11-03 12:30:46.264] Locating package file\r\n[dbx][2022-11-03 12:30:46.265] Package file located in: dist\/ds_recommenders-1.2.9-py3-none-any.whl\r\n[dbx][2022-11-03 12:30:47.221] Starting the traversal process\r\n[dbx][2022-11-03 12:30:47.222] Processing libraries for workflow add-on-chanel-AT\r\n[dbx][2022-11-03 12:30:47.223] \u2705 Processing libraries for workflow add-on-chanel-AT - done\r\n[dbx][2022-11-03 12:30:47.224] Processing libraries for workflow add-on-chanel-PL\r\n[dbx][2022-11-03 12:30:47.225] \u2705 Processing libraries for workflow add-on-chanel-PL - done\r\n[dbx][2022-11-03 12:30:47.225] Processing libraries for workflow add-on-PL\r\n[dbx][2022-11-03 12:30:47.226] \u2705 Processing libraries for workflow add-on-PL - done\r\n[dbx][2022-11-03 12:30:47.227] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.412] \u2705 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.414] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/comma \u2502\r\n\u2502 nds\/deploy.py:157 in deploy                                                  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, ele \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 157 \u2502   \u2502   \u2502   adjuster.traverse(deployable_workflows)                    \u2502\r\n\u2502   158 \u2502   \u2502   \u2502   if not _assets_only:                                       \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, dep \u2502\r\n\u2502   [16](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:17)0 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:185 in traverse                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   182 \u2502   def traverse(self, workflows: Union[WorkflowList, List[str]]):     \u2502\r\n\u2502   183 \u2502   \u2502   dbx_echo(\"Starting the traversal process\")                     \u2502\r\n\u2502   184 \u2502   \u2502   self.property_adjuster.library_traverse(workflows, self.additi \u2502\r\n\u2502 \u2771 185 \u2502   \u2502   self.property_adjuster.file_traverse(workflows, self.file_adju \u2502\r\n\u2502   186 \u2502   \u2502   self.property_adjuster.property_traverse(workflows)            \u2502\r\n\u2502   187 \u2502   \u2502   self.property_adjuster.cluster_policy_traverse(workflows)      \u2502\r\n\u2502   188 \u2502   \u2502   dbx_echo(\"Traversal process finished, all provided references  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:168 in file_traverse                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   165 \u2502   \u2502   for element, parent, index in self.traverse(workflows):        \u2502\r\n\u2502   166 \u2502   \u2502   \u2502   if isinstance(element, str):                               \u2502\r\n\u2502   167 \u2502   \u2502   \u2502   \u2502   if element.startswith(\"file:\/\/\") or element.startswith \u2502\r\n\u2502 \u2771 168 \u2502   \u2502   \u2502   \u2502   \u2502   file_adjuster.adjust_file_ref(element, parent, ind \u2502\r\n\u2502   169                                                                        \u2502\r\n\u2502   [17](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:18)0                                                                        \u2502\r\n\u2502   171 class Adjuster:                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/mixins\/file_reference.py:12 in adjust_file_ref                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    9 \u2502   \u2502   self._uploader = file_uploader                                  \u2502\r\n\u2502   10 \u2502                                                                       \u2502\r\n\u2502   11 \u2502   def adjust_file_ref(self, element: str, parent: Any, index: Any):   \u2502\r\n\u2502 \u2771 12 \u2502   \u2502   _uploaded = self._uploader.upload_and_provide_path(element)     \u2502\r\n\u2502   13 \u2502   \u2502   self.set_element_at_parent(_uploaded, parent, index)            \u2502\r\n\u2502   14                                                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/utils \u2502\r\n\u2502 \/file_uploader.py:59 in upload_and_provide_path                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   56 \u2502   \u2502   \u2502   self._verify_fuse_support()                                 \u2502\r\n\u2502   57 \u2502   \u2502                                                                   \u2502\r\n\u2502   58 \u2502   \u2502   dbx_echo(f\":arrow_up: Uploading local file {local_file_path}\")  \u2502\r\n\u2502 \u2771 59 \u2502   \u2502   self._upload_file(local_file_path)                              \u2502\r\n\u2502   60 \u2502   \u2502   dbx_echo(f\":white_check_mark: Uploading local file {local_file_ \u2502\r\n\u2502   61 \u2502   \u2502   return self._postprocess_path(local_file_path, as_fuse)         \u2502\r\n\u2502   62                                                                         \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[19](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:20)9 in http_request_safe                                   \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   196 \u2502   Wrapper around ``http_request`` that also verifies that the reques \u2502\r\n\u2502   197 \u2502   \"\"\"                                                                \u2502\r\n\u2502   198 \u2502   response = http_request(host_creds=host_creds, endpoint=endpoint,  \u2502\r\n\u2502 \u2771 199 \u2502   return verify_rest_response(response, endpoint)                    \u2502\r\n\u2502   [20](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:21)0                                                                        \u2502\r\n\u2502   201                                                                        \u2502\r\n\u2502   202 def verify_rest_response(response, endpoint):                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[21](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:22)2 in verify_rest_response                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   \u2502   endpoint,                                              \u2502\r\n\u2502   210 \u2502   \u2502   \u2502   \u2502   response.status_code,                                  \u2502\r\n\u2502   211 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502 \u2771 212 \u2502   \u2502   \u2502   raise MlflowException(                                     \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   \u2502   \"%s. Response body: '%s'\" % (base_msg, response.text), \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   \u2502   error_code=get_error_code(response.status_code),       \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nMlflowException: API request to endpoint \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f9[22](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:23)5de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n failed with error code 409 != 200. Response body: '<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text\/html;charset=ISO-8859-1\"\/>\r\n<title>Error 409 <\/title>\r\n<\/head>\r\n<body>\r\n<h2>HTTP ERROR: 409<\/h2>\r\n<p>Problem accessing \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f92[25](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:26)de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n. Reason:\r\n<pre>    File already exists, cannot overwrite: \r\n&apos;\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c908874\r\n[28](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:29)b97e6[37](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:38)1f[92](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:93)25de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.p\r\ny&apos;<\/pre><\/p>\r\n<hr \/>\r\n<\/body>\r\n<\/html>\r\n'\r\nError: Process completed with exit code 1.\r\n\r\n## Context\r\nUpdated few jobs today using the latest dbx version, and at the jobless deployment cicd step I get the error above.\r\nMLFlow is only used to define a specific experiment path. No path related updates or changes here!\r\n## Your Environment\r\n\r\n* dbx version used: 0.8.x\r\n* Databricks Runtime version:  10.4 LTS (standard or ML)\r\n* Python version: 3.8.11",
        "Challenge_closed_time":null,
        "Challenge_created_time":1667484803000,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/548",
        "Challenge_link_count":8,
        "Challenge_open_time":489.7769444444,
        "Challenge_readability":16.8,
        "Challenge_reading_time":109.95,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":79.0,
        "Challenge_repo_issue_count":582.0,
        "Challenge_repo_star_count":246.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":77,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Error 409 when deploying --assets-only",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":570,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0481099656,
        "Challenge_watch_issue_ratio":0.0274914089
    },
    {
        "Challenge_adjusted_solved_time":316.8641666667,
        "Challenge_answer_count":8,
        "Challenge_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Challenge_closed_time":1661539227000,
        "Challenge_created_time":1660398516000,
        "Challenge_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":8.05,
        "Challenge_repo_contributor_count":28.0,
        "Challenge_repo_fork_count":79.0,
        "Challenge_repo_issue_count":582.0,
        "Challenge_repo_star_count":246.0,
        "Challenge_repo_watch_count":16.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":316.8641666667,
        "Challenge_title":"dbx deploy fails due to mlflow experiment not found",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":73,
        "Platform":"Github",
        "Solution_body":"hi @zermelozf , \r\ncould you please provide full stack trace?  Sure, here it is:\r\n\r\n```\r\ndbx deploy --environment=default\r\n[dbx][2022-08-13 22:46:37.005] Starting new deployment for environment default\r\n[dbx][2022-08-13 22:46:37.006] Using profile provided from the project file\r\n[dbx][2022-08-13 22:46:37.006] Found auth config from provider ProfileEnvConfigProvider, verifying it\r\n[dbx][2022-08-13 22:46:37.007] Found auth config from provider ProfileEnvConfigProvider, verification successful\r\n[dbx][2022-08-13 22:46:37.007] Profile DEFAULT will be used for deployment\r\nTraceback (most recent call last):\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/bin\/dbx\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/click\/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/commands\/deploy.py\", line 143, in deploy\r\n    api_client = prepare_environment(environment)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/utils\/common.py\", line 38, in prepare_environment\r\n    MlflowStorageConfigurationManager.prepare(info)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 42, in prepare\r\n    cls._setup_experiment(properties)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/dbx\/api\/storage\/mlflow_based.py\", line 53, in _setup_experiment\r\n    experiment: Optional[Experiment] = mlflow.get_experiment_by_name(properties.workspace_dir)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 1042, in get_experiment_by_name\r\n    return MlflowClient().get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 566, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 226, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 365, in get_experiment_by_name\r\n    raise e\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 351, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 57, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 274, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/databricks\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 200, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.\r\n``` hi @zermelozf , \r\nit seems to me that you're using an old version of `dbx`. Please upgrade to the latest 0.7.0 (or at least to 0.6.12).  hi @renardeinside I had the same issue mentioned here. I upgraded to dbx 0.7.0 and now the error looks like this:\r\nRestException: INVALID_PARAMETER_VALUE: Experiment with id '2624352622693299' does not exist.\r\nIt only happens if you deploy a job for the first time. Deploying changes to an existing job works fine. hi @frida-ah , \r\nwhat's the MLflow version you're using? I'm asking because I'm not running into this issue in any of the tests  could you please also verify that you have correct [databricks profile configured as in Step 3 point 4 of the public doc](https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#step-3-install-the-code-samples-dependencies)?\r\n\r\nif it's still the case, please provide the deploy command with `dbx deploy --debug` option (please feel free to omit the host url)? \r\nReally curious where is this coming from.\r\n Hi @renardeinside I don't have mlflow in my requirements.txt. I can also confirm that I have the correct databricks profile configured in the deployment.json file as such:\r\n\r\n{\r\n  \"environments\": {\r\n    \"default\": {\r\n      \"profile\": \"DEFAULT\",\r\n      \"workspace_dir\": \"\/Shared\/dbx\/projects\/<project_name>\/<...>\",\r\n      \"artifact_location\": \"dbfs:\/Shared\/dbx\/projects\/<project_name>\/<...>\"\r\n    }\r\n  }\r\n}\r\n\r\ndbx deploy --environment default --deployment-file=conf\/deployment.json --jobs=<job_name>\r\n\r\nI have fixed the issue using a workaround - sorry I didn't have more time to invest in this. I created an artifact manually through the UI in the location where the artifact should be. Then I deleted it. And then the artifact was created again through the IDE and GitHub Actions. \r\n\r\nI think the issue is with Databricks having a bug when creating an artifact for the first time.  hi @frida-ah , \r\nstill pretty strange behaviour, but thanks a lot anyways. We're going to change the mlflow client logic accordingly to fix this issue.",
        "Solution_gpt_summary":"upgrad latest version dbx creat artifact manual locat artifact delet artifact creat id github action verifi profil configur step public doc creat artifact time",
        "Solution_link_count":1.0,
        "Solution_original_content":"zermelozf stack trace dbx deploi environ default dbx start deploy environ default dbx profil file dbx auth config profileenvconfigprovid verifi dbx auth config profileenvconfigprovid verif dbx profil default deploy traceback file opt homebrew anaconda env bin dbx line sy exit cli file opt homebrew anaconda env lib site packag click core line return arg kwarg file opt homebrew anaconda env lib site packag click core line invok ctx file opt homebrew anaconda env lib site packag click core line invok return process sub ctx invok sub ctx file opt homebrew anaconda env lib site packag click core line invok return ctx invok callback ctx param file opt homebrew anaconda env lib site packag click core line invok return callback arg kwarg file opt homebrew anaconda env lib site packag dbx deploi line deploi api client prepar environ environ file opt homebrew anaconda env lib site packag dbx util common line prepar environ storageconfigurationmanag prepar file opt homebrew anaconda env lib site packag dbx api storag base line prepar cl setup properti file opt homebrew anaconda env lib site packag dbx api storag base line setup option properti workspac dir file opt homebrew anaconda env lib site packag track fluent line return client file opt homebrew anaconda env lib site packag track client line return track client file opt homebrew anaconda env lib site packag track track servic client line return store file opt homebrew anaconda env lib site packag store track rest store line rais file opt homebrew anaconda env lib site packag store track rest store line respons proto endpoint getexperimentbynam req bodi file opt homebrew anaconda env lib site packag store track rest store line endpoint return endpoint host cred endpoint json bodi respons proto file opt homebrew anaconda env lib site packag util rest util line endpoint respons verifi rest respons respons endpoint file opt homebrew anaconda env lib site packag util rest util line verifi rest respons rais restexcept json load respons text except restexcept paramet valu zermelozf version dbx upgrad latest renardeinsid upgrad dbx restexcept paramet valu deploi job time deploi job frida version run test verifi profil configur step public doc http doc com dev id html step instal sampl depend deploi dbx deploi debug option free omit host url come renardeinsid txt profil configur deploy json file environ default profil default workspac dir share dbx artifact locat dbf share dbx dbx deploi environ default deploy file conf deploy json job workaround sorri time invest creat artifact manual locat artifact delet artifact creat id github action creat artifact time frida pretti anywai client logic accordingli",
        "Solution_preprocessed_content":"stack trace version upgrad latest upgrad dbx restexcept deploi job time deploi job version run test verifi deploi option come profil configur file environ dbx deploi default workaround sorri time invest creat artifact manual locat artifact delet artifact creat id github action creat artifact time pretti anywai client logic accordingli",
        "Solution_readability":14.8,
        "Solution_reading_time":77.33,
        "Solution_score_count":0.0,
        "Solution_sentence_count":61.0,
        "Solution_word_count":510.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0481099656,
        "Challenge_watch_issue_ratio":0.0274914089
    },
    {
        "Challenge_adjusted_solved_time":165.0966666667,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Challenge_closed_time":1654018977000,
        "Challenge_created_time":1653424629000,
        "Challenge_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":14.9,
        "Challenge_reading_time":23.83,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":43.0,
        "Challenge_repo_issue_count":536.0,
        "Challenge_repo_star_count":131.0,
        "Challenge_repo_watch_count":10.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":165.0966666667,
        "Challenge_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":157,
        "Platform":"Github",
        "Solution_body":"Error in LaunchPad notebooks. There was a change in the triton upstream where you previously didn't need to specify the model suffix as the path. Now you do. ",
        "Solution_gpt_summary":"delet pod start captur print env output ipykernel launchpad helm deploy tritoncli sqlite corrupt note triton upstream model suffix path specifi",
        "Solution_link_count":0.0,
        "Solution_original_content":"launchpad notebook triton upstream previous specifi model suffix path",
        "Solution_preprocessed_content":"launchpad notebook triton upstream previous specifi model suffix path",
        "Solution_readability":5.7,
        "Solution_reading_time":1.91,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":28.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0335820896,
        "Challenge_watch_issue_ratio":0.0186567164
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"When running hyperparameter tuning, MLflow expects an mlruns folder - which we don't create. If we stick with the standard we can ommit having to run `mlflow ui` with the backend store argument.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1621607539000,
        "Challenge_link":"https:\/\/github.com\/equinor\/flownet\/issues\/408",
        "Challenge_link_count":0,
        "Challenge_open_time":13233.4613888889,
        "Challenge_readability":7.9,
        "Challenge_reading_time":2.79,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":28.0,
        "Challenge_repo_issue_count":455.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow expecting mlruns folder",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":35,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0197802198,
        "Challenge_watch_issue_ratio":0.0065934066
    },
    {
        "Challenge_adjusted_solved_time":1.2725,
        "Challenge_answer_count":0,
        "Challenge_body":"If an ERT subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. This will then lead to a \"successful\" run in mlflow, whereas it should be registered as a failed run.",
        "Challenge_closed_time":1606475795000,
        "Challenge_created_time":1606471214000,
        "Challenge_link":"https:\/\/github.com\/equinor\/flownet\/issues\/269",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.8,
        "Challenge_reading_time":3.58,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":28.0,
        "Challenge_repo_issue_count":455.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.2725,
        "Challenge_title":"Failed ERT runs are not registered correctly in mlflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0197802198,
        "Challenge_watch_issue_ratio":0.0065934066
    },
    {
        "Challenge_adjusted_solved_time":590.8691666667,
        "Challenge_answer_count":4,
        "Challenge_body":"## Description\r\n\r\nThis happens when i tried to configure my own metric functions. \r\n\r\n## Context\r\n\r\nI am trying to create a custom metric indicator, to be logged after each experimentation. When i run `kedro mlflow ui`, this is what I'm getting on the UI.\r\n![image](https:\/\/user-images.githubusercontent.com\/54475793\/184276876-57872dd2-3fb3-41c9-b3a3-edd6a4396aca.png)\r\n\r\n\r\n## Steps to Reproduce\r\n\r\nThis is my nodes.py\r\n```\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n```\r\n\r\n\r\n## Expected Result\r\n\r\nHow do i get the metric to be displayed when i use the Mlflow ui? Are there specific keywords that mlflow is tracking to be logged as metric?\r\n\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): **0.10.0**\r\n* Python version used (`python -V`):  **3.9.0** \r\n* Operating system and version: Windows 10\r\n",
        "Challenge_closed_time":1662408460000,
        "Challenge_created_time":1660281331000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/346",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":14.31,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":590.8691666667,
        "Challenge_title":"MlflowMetricsDataSet logs invalid metric which breaks mlflow UI",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":142,
        "Platform":"Github",
        "Solution_body":"Hi @xjlwi, sorry to see that you are facing issues with the plugins. There are two problems here:\r\n- kedro-mlflow logs an incorrect metric. We will solve the problem together. \r\n- mlflow does not complain when the incorrect metric is logged, but it breaks the database and hence the UI => we should open an issue in mlflow repo once we know what is going on. \r\n\r\nWould you mind give me some extra informations: \r\n- ``mlflow`` version\r\n- the catalog entry for ``avg_pnl`` (I guess it is a ``kedro_mlflow.io.metrics.MlflowMetricsDataSet``?) **If yes, check the documentation: [it should return something like ``{'trader1': {'step': 0, 'value': df.pnl.mean()}}``](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/04_experimentation_tracking\/05_version_metrics.html#how-to-return-metrics-from-a-node)**\r\n- the type of ``avg_metric`` and ``total_metric``: are they ``float`` instead of string?\r\n- can you check if ``df.pnl.mean()`` and ``df.pnl.sum()`` returns a float and not a single-row ``pandas.Series``?\r\n\r\nIf I can reproduce the bug, I will be able to give you a workaround. \r\n > Hi @xjlwi, sorry to see that you are facing issues with the plugins. There are two problems here:\r\n> \r\n> * kedro-mlflow logs an incorrect metric. We will solve the problem together.\r\n> * mlflow does not complain when the incorrect metric is logged, but it breaks the database and hence the UI => we should open an issue in mlflow repo once we know what is going on.\r\n> \r\n> Would you mind give me some extra informations:\r\n> \r\n> * `mlflow` version: <b> 1.26.1 <\/b>\r\n> * the catalog entry for `avg_pnl` (I guess it is a `kedro_mlflow.io.metrics.MlflowMetricsDataSet`?) **If yes, check the documentation: [it should return something like `{'trader1': {'step': 0, 'value': df.pnl.mean()}}`](https:\/\/kedro-mlflow.readthedocs.io\/en\/stable\/source\/04_experimentation_tracking\/05_version_metrics.html#how-to-return-metrics-from-a-node)** : \r\n\r\nYes it's a `kedro_mlflow.io.metrics.MlflowMetricsDataSet`. \r\n\r\ntype: kedro_mlflow.io.artifacts.MlflowArtifactDataSet \r\ndata_set:\r\n    type: pandas.CSVDataSet \r\n    filepath: \"${ml_model_output}PnL_summary_metrics_${current_date}_${model}.csv\" \r\n    save_args:\r\n      index: True\r\n\r\nMust the keywords for the output be specifically 'step'? This is my current node to return the output.\r\n`\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n`\r\n\r\n> * the type of `avg_metric` and `total_metric`: are they `float` instead of string? Definitely float, because in my local mlruns folder, I am able to see them from the mlruns>metrics folder.\r\n\r\n1660874133345 [{'ml_model_13_logit_pnl_total': 0.0}, {'ml_model_13_logit_pnl_avg': nan}] ml_model_13_logit\r\n1660874133347 [{'ml_model_14_rf_pnl_total': 0.0}, {'ml_model_14_rf_pnl_avg': nan}] ml_model_14_rf\r\n1660874133349 [{'ml_model_15_naive_clf_pnl_total': 0.0}, {'ml_model_15_naive_clf_pnl_avg': nan}] ml_model_15_naive_clf\r\n1660874133352 [{'ml_model_16_svc_pnl_total': 0.0}, {'ml_model_16_svc_pnl_avg': nan}] ml_model_16_svc\r\n1660874133354 [{'ml_model_17_decisison_tree_pnl_total': 0.0}, {'ml_model_17_decisison_tree_pnl_avg': nan}] ml_model_17_decisison_tree\r\n1660874133356 [{'ml_model_18_grad_boost_pnl_total': 0.0}, {'ml_model_18_grad_boost_pnl_avg': nan}] ml_model_18_grad_boost\r\n\r\n> * can you check if `df.pnl.mean()` and `df.pnl.sum()` returns a float and not a single-row `pandas.Series`?\r\n\r\n> If I can reproduce the bug, I will be able to give you a workaround.\r\n\r\n > Must the keywords for the output be specifically 'step'? This is my current node to return the output.\r\n\r\nYes exactly. That's for consistency between loading and saving metrics.\r\n\r\nReplace each entry ``df.pnl.mean()`` by  a dict``{'step': 0, 'value': df.pnl.mean()}``and you will be fine. This adds an extra nested dict level and is not ideal. I let the issue opened to improve the API in the future. Hi, I close the issue but feel free to reopen if needed. ",
        "Solution_gpt_summary":"metric function displai run log metric complain metric log break databas diagnos version catalog entri avg pnl type avg metric metric pnl pnl sum return singl row panda seri keyword output step replac entri pnl dict step valu pnl",
        "Solution_link_count":2.0,
        "Solution_original_content":"xjlwi sorri plugin log metric complain metric log break databas open repo extra version catalog entri avg pnl guess metric metricsdataset document return trader step valu pnl http readthedoc stabl sourc experiment track version metric html return metric node type avg metric metric pnl pnl sum return singl row panda seri reproduc workaround xjlwi sorri plugin log metric complain metric log break databas open repo extra version catalog entri avg pnl guess metric metricsdataset document return trader step valu pnl http readthedoc stabl sourc experiment track version metric html return metric node metric metricsdataset type artifact artifactdataset data set type panda csvdataset filepath model output pnl summari metric date model csv save arg index keyword output step node return output pnl metric datafram avg pnl avg pnl avg metric trader pnl avg pnl metric trader pnl sum trader pnl sum return avg pnl type avg metric metric definit local folder metric folder model logit pnl model logit pnl avg nan model logit model pnl model pnl avg nan model model naiv clf pnl model naiv clf pnl avg nan model naiv clf model svc pnl model svc pnl avg nan model svc model decisison tree pnl model decisison tree pnl avg nan model decisison tree model grad boost pnl model grad boost pnl avg nan model grad boost pnl pnl sum return singl row panda seri reproduc workaround keyword output step node return output exactli consist load save metric replac entri pnl dict step valu pnl add extra nest dict level ideal open improv api futur close free reopen",
        "Solution_preprocessed_content":"sorri plugin log metric complain metric log break databas open repo extra document type return reproduc workaround sorri plugin log metric complain metric log break databas open repo extra dict add extra nest dict level ideal open improv api futur close free reopen",
        "Solution_readability":9.5,
        "Solution_reading_time":50.7,
        "Solution_score_count":0.0,
        "Solution_sentence_count":47.0,
        "Solution_word_count":461.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":168.6647222222,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Challenge_closed_time":1657139268000,
        "Challenge_created_time":1656532075000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":5.77,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":168.6647222222,
        "Challenge_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":52,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":72.1441666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Challenge_closed_time":1652640252000,
        "Challenge_created_time":1652380533000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":10.4,
        "Challenge_reading_time":21.98,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":72.1441666667,
        "Challenge_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":185,
        "Platform":"Github",
        "Solution_body":"Closed by #313 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-2.7,
        "Solution_reading_time":0.18,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":75.83,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nIf I specify an experiment in `mlflow.yml`, and the set up the mlflow configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in mlflow \"Default\" (0) experiment. This works when running \"kedro run\" through the CLI.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# mlflow.yml\r\nexperiment:\r\n  name: my_awesome_experiment\r\n  create: True  # if the specified `name` does not exists, should it be created?\r\n```\r\n\r\n```python\r\n# test.py\r\n\r\nfrom kedro.framework.session import KedroSession\r\nfrom kedro.framework.startup import bootstrap_project\r\nfrom kedro_mlflow.config import get_mlflow_config\r\n\r\nbootstrap_project(r\"path\/to\/project\")\r\nwith KedroSession.create(project_path=r\"path\/to\/project\"):\r\n    config=get_mlflow_config()\r\n    config.setup()\r\n    \r\n    mlflow.log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"Default\".\r\n\r\n```\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe faulty line is: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L100\r\n\r\n[We should use mlflow ``mlflow.set_experiment`` method](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.set_experiment), but it does not restore deleted experiment. This wil replace part of the logic here: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L124-L132",
        "Challenge_closed_time":1636318265000,
        "Challenge_created_time":1636045277000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/256",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":12.6,
        "Challenge_reading_time":20.71,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":75.83,
        "Challenge_title":"Setting the mlflow experiment does not work in interactive mode",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":148,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":220.4830555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Challenge_closed_time":1619987466000,
        "Challenge_created_time":1619193727000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":27.15,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":220.4830555556,
        "Challenge_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":273,
        "Platform":"Github",
        "Solution_body":"Hi, \r\n\r\nI wil try to check it out this weekend, but the `kedro==0.17.3` version is brand new (it was released yesterday), and given my experience with past kedro versions update 2 things might have happened on kedro's side: \r\n- They have broken the auto-discovery mechanism (I've seen in the release note that they change the CLI command discovery to enale overriding project commands by plugins)\r\n- They have not updated their `pandas-iris` starter yet which does not match the new version and is only compliant with `kedro==0.17.2`. \r\n\r\nWhile I am investigating, would you please confirm that :\r\n- `kedro-mlflow` works fine with kedro==0.17.2 with your setup\r\n- `kedro-mlflow` works fine if you don't use the `pandas-iris` starter: try `kedro new` with `kedro==0.17.3` and then add one ode to test the plugin\r\n- I'd be glad to see if another plugin (e.g. `kedro-viz`) is facing the same problem that kedro-mlflow. Would you mind checking?\r\n\r\nOf course there is the possibility that the problem comes from `kedro-mlflow` itself, but I hardly believe it. I'll tell you within 2 days. I am sorry, I am quite busy for now and I will not debug this before next week. Once again, it is very likely kedro's plugin discovery mechanism has been broken in the new release, I strongly suggest you go back to `kedro==0.17.2`.\r\n\r\nNext actions: \r\n- [X] reproduce the bug -> Done, thanks for the very good reproducible example\r\n- [X] Check if it happens with other plugins (say kedro-viz) -> `kedro viz` global command is properly discovered\r\n- [X] Check if hooks are properly loaded -> everything works fine if I add a `mlflow.yml` manually in the `conf\/local` folder (or any folder in `conf\/` actually). -> **This is a short term solution for you**,e ven if it is not very convenient. You can find allowed keys [in the documentation](https:\/\/kedro-mlflow.readthedocs.io\/en\/latest\/source\/04_experimentation_tracking\/01_configuration.html#the-mlflow-yml-file) or irectly [copy paste it from the code](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/master\/kedro_mlflow\/template\/project\/mlflow.yml)\r\n- [X] Check if the tests pass with kedro==0.17.3 -> *Some tests are failing, but not the one related to the CLI commands which seems discovered. I need to investigate further*.\r\n- [x] Check if other plugins with *local* commands are discovered\r\n- [x] Check if it also happens it an empty project (i.e. *not* a starter)\r\n First of all, thank you for looking so quickly into it!\r\n\r\nFrom how I read your second message you already know that, but to answer your questions:\r\n- detecting `kedro mlflow` works fine with `kedro==0.17.2`\r\n- the problem is consistent with kedro==0.17.3 independent if I use the pandas-iris starter or not\r\n- `kedro viz` is found also with `kedro=0.17.3`\r\n\r\nAgain, thank you for providing workarounds directly on Monday morning, I can nicely work with those! A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a `mlflow.yml` to be present, and all that `kedro mlflow init` does is copy this file from the template into `conf\/local`, is this correct? TL;DR: \r\n\r\nInstall this version for now, it should make the command available again:\r\n\r\n```console\r\npip uninstall kedro-mlflow\r\npip install git+https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow.git@bug\/no-cli\r\n```\r\n**Beware:** it is very important to uninstall your existing version of kedro-mlflow before reinstalling because the patch has the same version number that the current release.\r\n\r\nIf you confirm this works for you, I will deploy the patch to PyPI before kedro provides a patch on their side.\r\n_____________________________\r\n\r\nHi, some follow-up about this bug:\r\n\r\n- I've figured out *what* is going on but not *why* it happens. The `mlflow` group of command exists both at global (`new`) and project (`init`, `ui`) levels and for an unknown reason, `kedro` takes into account only one group of command in its `0.17.3` version. This is a bug I will report to the core team. However, it does not affect their other plugins (kedro-viz, kedro-docker, kedro-airflow) because none of them has both global and project commands.\r\n- The quickest (hacky) fix is to remove the global group of command to the make the project ones available. I've done this in the branch `bug\/no-cli` of the repo.\r\n\r\nTo answer your question: \r\n\r\n> A question for my understanding of the plugin: As long as the hooks are loaded, the mlflow functionality depends only on a mlflow.yml to be present, and all that kedro mlflow init does is copy this file from the template into conf\/local, is this correct?\r\n\r\nExactly: the `init` command renders the template (i.e. copy paste it + replace the jinja tags with dynamic values like the name of your project) to a folder in your `conf\/` folder (by default `local`, but you can specify an environment like this: `kedro mlflow init --env=<your-env-folder>`). The hooks contain all the code logic  and this mlflow.yml file is just here to pass parameters to them. \r\n\r\nThe other project command is `kedro mlflow ui` which is just a wrapper of \"mlflow ui\" with the parameters (mlflow_tracking_uri, port, host) defined in your `mlflow.yml` file.\r\n thanks, form a quick test I would say: the patch works like a charm! Hi @dmb23, I've just deployed the patch to PyPI. You can use `pip install kedro_mlflow==0.7.2`` and it should be ok for now. I close the issue, but feel free to reopen if you still encounter any issue in this new version.",
        "Solution_gpt_summary":"setup panda iri starter add yml manual conf local folder instal version remov global group on bias summari",
        "Solution_link_count":3.0,
        "Solution_original_content":"wil weekend version brand releas yesterdai past version updat broken auto discoveri mechan releas note cli discoveri enal overrid plugin updat panda iri starter match version compliant setup panda iri starter add od test plugin glad plugin viz cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin viz viz global properli hook properli load add yml manual conf local folder folder conf short term ven conveni allow kei document http readthedoc latest sourc experiment track configur html yml file irectli copi past http github com galileo galilei blob master templat yml test pass test relat cli plugin local starter quickli read messag detect consist independ panda iri starter viz workaround directli mondai morn nice plugin hook load function depend yml present init copi file templat conf local instal version consol pip uninstal pip instal git http github com galileo galilei git cli bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur group global init level unknown reason account group version report core team affect plugin viz docker airflow global quickest hacki remov global group on branch cli repo plugin hook load function depend yml present init copi file templat conf local exactli init render templat copi past replac jinja tag dynam valu folder conf folder default local specifi environ init env hook logic yml file pass paramet wrapper paramet track uri port host defin yml file quick test patch charm dmb deploi patch pypi pip instal close free reopen version",
        "Solution_preprocessed_content":"wil weekend version brand past version updat broken mechan updat starter match version compliant setup starter add od test plugin glad plugin cours come hardli believ dai sorri busi debug week plugin discoveri mechan broken releas strongli action reproduc reproduc plugin global properli hook properli load add manual folder short term ven conveni allow kei irectli test pass test relat cli plugin local quickli read messag detect consist independ starter workaround directli mondai morn nice plugin hook load function depend present copi file templat instal version bewar import uninstal version reinstal patch version releas deploi patch pypi patch figur account group version report core team affect plugin global quickest remov global group on branch repo plugin hook load function depend yml present init copi file templat exactli render templat folder folder hook logic yml file pass paramet wrapper paramet defin file quick test patch charm deploi patch pypi close free reopen version",
        "Solution_readability":7.8,
        "Solution_reading_time":66.71,
        "Solution_score_count":7.0,
        "Solution_sentence_count":48.0,
        "Solution_word_count":849.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":105.32,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Challenge_closed_time":1618006798000,
        "Challenge_created_time":1617627646000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.56,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":105.32,
        "Challenge_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":121,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1475.5611111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Challenge_closed_time":1615716614000,
        "Challenge_created_time":1610404594000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.8,
        "Challenge_reading_time":14.22,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":1475.5611111111,
        "Challenge_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"This will wait the migration to `kedro>=0.17.0` (cf. #144) in milestone 0.6.0 because kedro has bradnd new utilities to handle this part. This will remove boilerplate code from the plugin and ensure consistency with future kedro changes.",
        "Solution_gpt_summary":"wait migrat mileston remov boilerpl plugin consist futur",
        "Solution_link_count":0.0,
        "Solution_original_content":"wait migrat mileston bradnd util remov boilerpl plugin consist futur",
        "Solution_preprocessed_content":"wait migrat mileston bradnd util remov boilerpl plugin consist futur",
        "Solution_readability":5.7,
        "Solution_reading_time":2.95,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":171.2597222222,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Challenge_closed_time":1606599848000,
        "Challenge_created_time":1605983313000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.1,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":171.2597222222,
        "Challenge_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":137,
        "Platform":"Github",
        "Solution_body":"Does removing the faulty line and using directly the initial_catalog make the model loadable again ? if Yes, we have two options :\r\n\r\n* We no longer deepcopy the initial_catalog\r\n* We copy each DataSet of the catalog with his own loader (for example, we use tf.keras.models.clone_model for keras model DataSet ...)\r\n\r\nKnowing that the `KedroPipelineModel` is intented to be used in a separated process (at inference-time), we can just remove the deepcopy part (there won't be a conflict with another function using the same catalog)\r\n After some investigation, the issues comes from the MLflowAbstractModelDataSet, and particularly the `self._mlflow_model_module` attribute which is a module and not deepcopiable by nature. I suggest to store it as a string, and have a property attribute to load the module on the fly.\r\n\r\nNote that this is a problem which occurs only when the DataSet is not deepcopiable (and not the underlying value the DataSet can load(), so we can quite safely assume that it should not occur often). If it does, we should consider a more radical solution among the ones you suggest.",
        "Solution_gpt_summary":"modifi line packag remov faulti line initi catalog directli copi dataset catalog loader involv store model modul attribut properti attribut load modul fly remov deepcopi pipelinemodel intend separ process infer time note dataset deepcopi radic frequent",
        "Solution_link_count":0.0,
        "Solution_original_content":"remov faulti line directli initi catalog model loadabl option longer deepcopi initi catalog copi dataset catalog loader kera model clone model kera model dataset pipelinemodel intent separ process infer time remov deepcopi conflict function catalog come abstractmodeldataset model modul attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi underli valu dataset load safe radic on",
        "Solution_preprocessed_content":"remov faulti line directli model loadabl option longer deepcopi copi dataset catalog loader intent separ process remov deepcopi come abstractmodeldataset attribut modul deepcopi natur store properti attribut load modul fly note dataset deepcopi safe radic on",
        "Solution_readability":11.8,
        "Solution_reading_time":13.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":175.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":147.8475,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Challenge_closed_time":1606515096000,
        "Challenge_created_time":1605982845000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.8,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":147.8475,
        "Challenge_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"Good catch ! \r\nSince we catch the Error and manually end the run, mlflow do not receive the \"error code 1\" of the current process. If we no longer end run manually, mlflow will tag the run as FAILED. But since we want to control the pipeline error, we can apply your suggestion (specifiying the status as failed) Yes, but we need to terminate the run manually when it failed and one use it interactively (in CLI, tis makes no difference because it gets the error code as you say) to avoid further interference.",
        "Solution_gpt_summary":"manual specifi run statu run replac line retriev run statu import note run termin manual avoid interfer",
        "Solution_link_count":0.0,
        "Solution_original_content":"catch catch manual end run receiv process longer end run manual tag run control pipelin appli specifii statu termin run manual interact cli ti avoid interfer",
        "Solution_preprocessed_content":"catch catch manual end run receiv process longer end run manual tag run control pipelin appli termin run manual interact avoid interfer",
        "Solution_readability":13.0,
        "Solution_reading_time":6.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":93.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":291.4263888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\n`TypeError: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `MlflowArtifactDataSet` is used with `MlflowModelSaverDataSet`.\r\n\r\n## Context\r\n\r\nLogging locally and to MLflow in one step.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: kedro_mlflow.io.models.MlflowModelSaverDataSet\r\n        flavor: mlflow.sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## Expected Result\r\n\r\nThe model should be saved locally and in MLflow run at the same time.\r\n\r\n## Actual Result\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\nTypeError: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise DataSetError(message) from exc\r\nkedro.io.core.DataSetError: Failed while saving data to data set MlflowMlflowModelSaverDataSet(filepath=\/Users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=mlflow.sklearn, load_args={}, save_args={}, version=Version(load=None, save='2020-11-06T12.28.57.593Z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro-mlflow 0.4.0\r\n* Python 3.7.7\r\n* MacOS Catalina\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.",
        "Challenge_closed_time":1605715301000,
        "Challenge_created_time":1604666166000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":16.6,
        "Challenge_reading_time":65.61,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":291.4263888889,
        "Challenge_title":"TypeError: unsupported operand type(s) for \/: 'str' and 'str' when using MlflowArtifactDataSet with MlflowModelSaverDataSet",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":337,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":49.3241666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\nWhen `MlflowMetricsDataset` has no \"prefix\" specified, the name in the catalog is used instead. However, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a mlflow run interactively: \r\n```python\r\nmlflow.start_run()\r\nmlflow.end_run()\r\n```\r\nAnd browse the ui to retrieve the run_id\r\n\r\n2. Declare a `MlflowMetricsDataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_mlflow.io.MlflowMetricsDataSet\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. Launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## Expected Result\r\n\r\nA metric should be loggedin run \"1346579\".\r\n\r\n## Actual Result\r\n\r\nThe metric is logged is a new run.\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes",
        "Challenge_closed_time":1603665805000,
        "Challenge_created_time":1603488238000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/102",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.1,
        "Challenge_reading_time":11.05,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":49.3241666667,
        "Challenge_title":"MlflowMetricsDataSet ignores run_id when prefix is not specified",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":126,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":179.7819444444,
        "Challenge_answer_count":2,
        "Challenge_body":"## Description\r\n\r\nSince 0.16.5, kedro project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/439) instead of a `.kedro.yml` at the root of the projects. This breaks the `kedro mlflow init` command which is only compatible with `.kedro.yml` configuration file.\r\n\r\n## Context\r\nWe should remove the `_get_project_globals` util function in kedromlflow and use `kedro.framework.context import get_static_project_data` as suggested in #86. **Beware: this will break retrocompatibilty and work only with kedro>=0.16.5**\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch `kedro mlflow init` with no `.kedro.yml` config file in your project but a valid `pyproject.toml`.\r\n\r\n## Expected Result\r\nThe mlflow.yml file should be created\r\n\r\n## Actual Result\r\nAn error is raised.",
        "Challenge_closed_time":1603658563000,
        "Challenge_created_time":1603011348000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.3,
        "Challenge_reading_time":10.84,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":179.7819444444,
        "Challenge_title":"Make mlflow init work when configuration is in pyproject.toml",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":110,
        "Platform":"Github",
        "Solution_body":"Well spoted ! We can already solve this, by bundling kedro's `get_static_project_data` inside kedro_mlflow as long as we keep kedro < 0.16.5 retrocompatibility. We can switch then to `kedro.framework.context import get_static_project_data` when we drop this compatibility in the futur. I can add it to [Migrate 0.16.5 pull request](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/pull\/94) Ok let's do this!",
        "Solution_gpt_summary":"involv bundl static data insid maintain retrocompat retrocompat drop futur import switch framework context import static data migrat pull request",
        "Solution_link_count":1.0,
        "Solution_original_content":"spote bundl static data insid retrocompat switch framework context import static data drop compat futur add migrat pull request http github com galileo galilei pull",
        "Solution_preprocessed_content":"spote bundl insid retrocompat switch drop compat futur add",
        "Solution_readability":8.9,
        "Solution_reading_time":5.21,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":50.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":222.8511111111,
        "Challenge_answer_count":0,
        "Challenge_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Challenge_closed_time":1602278580000,
        "Challenge_created_time":1601476316000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.6,
        "Challenge_reading_time":14.69,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":222.8511111111,
        "Challenge_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":106,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":426.9047222222,
        "Challenge_answer_count":0,
        "Challenge_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Challenge_closed_time":1602948810000,
        "Challenge_created_time":1601411953000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":426.9047222222,
        "Challenge_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":64,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":2106.4869444444,
        "Challenge_answer_count":0,
        "Challenge_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Challenge_closed_time":1602948810000,
        "Challenge_created_time":1595365457000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":2.66,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":2106.4869444444,
        "Challenge_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":31,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":2038.3938888889,
        "Challenge_answer_count":0,
        "Challenge_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Challenge_closed_time":1600718139000,
        "Challenge_created_time":1593379921000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":385.0,
        "Challenge_repo_star_count":132.0,
        "Challenge_repo_watch_count":7.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":2038.3938888889,
        "Challenge_title":"Warning message appears when calling ``kedro mlflow init``",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0233766234,
        "Challenge_watch_issue_ratio":0.0181818182
    },
    {
        "Challenge_adjusted_solved_time":1881.8983333333,
        "Challenge_answer_count":0,
        "Challenge_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Challenge_closed_time":1598336871000,
        "Challenge_created_time":1591562037000,
        "Challenge_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.6,
        "Challenge_reading_time":13.06,
        "Challenge_repo_contributor_count":9.0,
        "Challenge_repo_fork_count":18.0,
        "Challenge_repo_issue_count":374.0,
        "Challenge_repo_star_count":126.0,
        "Challenge_repo_watch_count":8.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":1881.8983333333,
        "Challenge_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":126,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0240641711,
        "Challenge_watch_issue_ratio":0.0213903743
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"*Currently*\n\n* since mlflow 1.28, running mlflow projects form remote sources causes\n  *mlflow: not found* issue on starting the project\n\n*Reproduce*\n\n* run TestMLFlowProjects.test_mlflow_gitproject_remote_https\n\n*Expected*\n\n* running remote-sourced mlflow project is supported as previously",
        "Challenge_closed_time":null,
        "Challenge_created_time":1663358103000,
        "Challenge_link":"https:\/\/github.com\/omegaml\/omegaml\/issues\/258",
        "Challenge_link_count":0,
        "Challenge_open_time":1636.0825,
        "Challenge_readability":9.4,
        "Challenge_reading_time":4.45,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":10.0,
        "Challenge_repo_issue_count":313.0,
        "Challenge_repo_star_count":79.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"running mlflow>1.28 projects causes mlflow not found error",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0159744409,
        "Challenge_watch_issue_ratio":0.0127795527
    },
    {
        "Challenge_adjusted_solved_time":511.6538888889,
        "Challenge_answer_count":1,
        "Challenge_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Challenge_closed_time":1610230183000,
        "Challenge_created_time":1608388229000,
        "Challenge_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":23.7,
        "Challenge_reading_time":251.52,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":210.0,
        "Challenge_repo_star_count":8.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":290,
        "Challenge_solved_time":511.6538888889,
        "Challenge_title":"Error when starting new experiment in mlflow",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":1131,
        "Platform":"Github",
        "Solution_body":"Already fixed in #79 by introducing delay between mlflow server start and starting experiments in mlflow",
        "Solution_gpt_summary":"pipelin github action test pass start introduc delai server start start",
        "Solution_link_count":0.0,
        "Solution_original_content":"introduc delai server start start",
        "Solution_preprocessed_content":"introduc delai server start start",
        "Solution_readability":10.7,
        "Solution_reading_time":1.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":16.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.019047619,
        "Challenge_watch_issue_ratio":0.0285714286
    },
    {
        "Challenge_adjusted_solved_time":28.415,
        "Challenge_answer_count":0,
        "Challenge_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Challenge_closed_time":1580353817000,
        "Challenge_created_time":1580251523000,
        "Challenge_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":4.0,
        "Challenge_repo_contributor_count":7.0,
        "Challenge_repo_fork_count":30.0,
        "Challenge_repo_issue_count":92.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.415,
        "Challenge_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":44,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0760869565,
        "Challenge_watch_issue_ratio":0.1304347826
    },
    {
        "Challenge_adjusted_solved_time":338.3591666667,
        "Challenge_answer_count":0,
        "Challenge_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Challenge_closed_time":1647350309000,
        "Challenge_created_time":1646132216000,
        "Challenge_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.1,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":79.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":338.3591666667,
        "Challenge_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":47,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1772151899,
        "Challenge_watch_issue_ratio":0.0759493671
    },
    {
        "Challenge_adjusted_solved_time":44.3683333333,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Challenge_closed_time":1660908206000,
        "Challenge_created_time":1660748480000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.0,
        "Challenge_reading_time":27.79,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":44.3683333333,
        "Challenge_title":"[mlflow] Migration Job should run before upgrade",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":277,
        "Platform":"Github",
        "Solution_body":"Hi @faceless7171 \r\n\r\nThank you very much for reporting the issue. Yes, your suggestion can work. Let me create a PR and test it. Well, we can't use the pre-hook option because we need a configuration file for the DB connection. And I don't want to make secrets visible in the container.\r\n\r\n```console\r\n\u2502 Events:                                                                                                                                                          \u2502\r\n\u2502   Type     Reason       Age               From               Message                                                                                             \u2502\r\n\u2502   ----     ------       ----              ----               -------                                                                                             \u2502\r\n\u2502   Normal   Scheduled    22s               default-scheduler  Successfully assigned default\/mlflow-bzb8s to minikube                                              \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"migrations-config\" : configmap \"mlflow-migrations\" not found   \u2502\r\n\u2502   Warning  FailedMount  7s (x6 over 22s)  kubelet            MountVolume.SetUp failed for volume \"dbchecker\" : configmap \"mlflow-dbchecker\" not found            \u2502\r\n\u2502                                                                                                                                                                  \u2502\r\n```\r\n\r\nSo, we have another option. Maybe we can use the init container pattern for this purpose. Let me try. @all-contributors please add @faceless7171  for bug @burakince \n\nI've put up [a pull request](https:\/\/github.com\/community-charts\/helm-charts\/pull\/37) to add @faceless7171! :tada: Hi @faceless7171 \r\n\r\nCould you please try again with mlflow chart minimum 0.7.0 version? @burakince tested on 0.7.1 version. Everything is working now. Thanks for the fix.",
        "Solution_gpt_summary":"migrat job run pod upgrad commun agre pre hook option configur file connect decid init pattern commun member creat pull request add minimum version chart",
        "Solution_link_count":1.0,
        "Solution_original_content":"faceless report creat test pre hook option configur file connect secret visibl consol event type reason ag messag normal schedul default schedul successfulli assign default bzb minikub warn failedmount kubelet mountvolum setup volum migrat config configmap migrat warn failedmount kubelet mountvolum setup volum dbchecker configmap dbchecker option mayb init pattern purpos contributor add faceless burakinc pull request http github com commun chart helm chart pull add faceless tada faceless chart minimum version burakinc test version",
        "Solution_preprocessed_content":"report creat test option configur file connect secret visibl option mayb init pattern purpos add add tada chart minimum version test version",
        "Solution_readability":7.2,
        "Solution_reading_time":15.1,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":161.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":53.7558333333,
        "Challenge_answer_count":5,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Challenge_closed_time":1660345866000,
        "Challenge_created_time":1660152345000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":8.7,
        "Challenge_reading_time":30.0,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":53.7558333333,
        "Challenge_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":261,
        "Platform":"Github",
        "Solution_body":"Hi @mohittalele ,\r\n\r\nThank you very much for reporting the error. Could you please share your mlflow pod and training pod logs with me?\r\n\r\nBest,\r\nBurak Hi @mohittalele \r\n\r\nI think you have a misconfiguration. I added a [full example to here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/bitnami-postgresql-and-bitnami-minio-sklearn-training-example). Simply, your `MLFLOW_S3_ENDPOINT_URL` configuration is wrong. URL must be `http:\/\/minio.airflow.svc.cluster.local:9000`. Could you please fix your configuration and try again?\r\n\r\nBest,\r\nBurak @burakince  Here is the log of the training container. Somehow the trining container does not \"know\" of the s3 endpoint and it is using the local path. \r\n\r\n```\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - The git executable must be specified in one of the following ways:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be included in your $PATH\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - be set via $GIT_PYTHON_GIT_EXECUTABLE\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - explicitly set via git.refresh()\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - All git commands will error until this is rectified.\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - This initial warning can be silenced or aggravated in the future by setting the\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - $GIT_PYTHON_REFRESH environment variable. Use one of the following values:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - quiet|q|silence|s|none|n|0: for no warning or exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - warn|w|warning|1: for a printed warning\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     - error|e|raise|r|2: for a raised exception\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Example:\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -     export GIT_PYTHON_REFRESH=quiet\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - \r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO - Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   RMSE: 0.793164022927685\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   MAE: 0.6271946374319586\r\n[2022-08-11, 13:53:26 CEST] {pod_manager.py:226} INFO -   R2: 0.10862644997792636\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_artifact_uri ::  .\/mlruns\/0\/3b376331bcaa4894a0723fe4b690658f\/artifacts\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_registry_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:27 CEST] {pod_manager.py:226} INFO - get_tracking_uri ::  http:\/\/mlflow.airflow.svc.cluster.local:5000\/\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Registered model 'ElasticnetWineModel' already exists. Creating a new version of this model...\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - 2022\/08\/11 11:53:29 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ElasticnetWineModel, version 11\r\n[2022-08-11, 13:53:29 CEST] {pod_manager.py:226} INFO - Created version '11' of model 'ElasticnetWineModel'.\r\n[2022-08-11, 13:53:30 CEST] {kubernetes_pod.py:453} INFO - Deleting pod: mlflow-example-1-7e89c5e6540645e3822fbf34410c6b99\r\n[2022-08-11, 13:53:30 CEST] {taskinstance.py:1420} INFO - Marking task as SUCCESS. dag_id=mlflow, task_id=mlflow_example_1, execution_date=20220811T115225, start_date=20220811T115226, end_date=20220811T115330\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:156} INFO - Task exited with return code 0\r\n[2022-08-11, 13:53:30 CEST] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n```\r\n\r\n\r\n\r\nmlflow logs are quite, There is nothing logged there. I will try out the example. Thanks for the example. \r\n\r\nedit : with 9000 port number specifie, there is no improvement @burakince The setup now works. Actually there was problem with VPN setting since I was deploying mlflow behind mlflow. We can close the issue :) Hi @mohittalele,\r\n\r\nI'm glad to hear the problem was resolved. If you need anything else, please don't hesitate to open a new issue.\r\n\r\nBest,\r\nBurak",
        "Solution_gpt_summary":"burak endpoint url configur github repositori url http minio airflow svc cluster local vpn set deploi vpn set setup git execut",
        "Solution_link_count":4.0,
        "Solution_original_content":"mohittalel report share pod train pod log burak mohittalel misconfigur http github com commun chart tree bitnami postgresql bitnami minio sklearn train simpli endpoint url configur url http minio airflow svc cluster local configur burak burakinc log train trine endpoint local path cest pod warn util git util import git git execut probabl path git sha initi git execut cest pod git execut specifi cest pod path cest pod set git git execut cest pod explicitli set git refresh cest pod cest pod git rectifi cest pod cest pod initi warn silenc aggrav futur set cest pod git refresh environ variabl valu cest pod quiet silenc warn except cest pod warn warn print warn cest pod rais rais except cest pod cest pod cest pod export git refresh quiet cest pod cest pod elasticnet model alpha ratio cest pod rmse cest pod mae cest pod cest pod artifact uri bbcaaafebf artifact cest pod registri uri http airflow svc cluster local cest pod track uri http airflow svc cluster local cest pod regist model elasticnetwinemodel creat version model cest pod track model registri client wait model version finish creation model elasticnetwinemodel version cest pod creat version model elasticnetwinemodel cest kubernet pod delet pod eceefbfcb cest taskinst mark task dag task execut date start date end date cest local task job task exit return cest local task job downstream task schedul schedul log log edit port specifi improv burakinc setup vpn set deploi close mohittalel glad hesit open burak",
        "Solution_preprocessed_content":"report share pod train pod log burak misconfigur simpli configur url configur burak log train trine endpoint local path log log edit port specifi improv setup vpn set deploi close glad hesit open burak",
        "Solution_readability":7.5,
        "Solution_reading_time":60.05,
        "Solution_score_count":0.0,
        "Solution_sentence_count":66.0,
        "Solution_word_count":507.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":2.7277777778,
        "Challenge_answer_count":4,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1658854769000,
        "Challenge_created_time":1658844949000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.3,
        "Challenge_reading_time":25.56,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":2.7277777778,
        "Challenge_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":286,
        "Platform":"Github",
        "Solution_body":"Hi @mikwieczorek \r\n\r\nThanks to inform us. Yes, probably it's my mistake. I changed it to the name some time ago. Let's write some tests and fix the problem. Well, it looks like your link refers to the port (service port) rather than targetPort (pod's port. This is currently what we use.). But we can even make it optional (port or targetPort selection) and use a name rather than a port number. It looks like it works with the latest version of Prometheus but I think we need to support all versions together.\r\n\r\nAnd [this is the full schema of endpoints field](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/api.md#monitoring.coreos.com\/v1.Endpoint).\r\n\r\nI will do some additional manual tests and send the PR. Hi @mikwieczorek \r\n\r\nChart version 0.3.0 should solve your problem. If it still accrues, feel free to reopen this issue. You can use the following command to update your deployment without the need for additional changes.\r\n\r\n```\r\nhelm repo update\r\nhelm upgrade --install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\r\n```\r\n\r\nBest,\r\nBurak Thank you @burakince for your prompt fix. It works correctly after the update. \r\nNext time, I will make an MR instead of just reporting the issue",
        "Solution_gpt_summary":"manual targetport port http deploi servicemonitor manifest propos targetport port http templat servicemonitor yaml port port chart version updat deploy addit helm repo updat helm upgrad instal namespac track server commun chart set servicemonitor enabl bias summari",
        "Solution_link_count":1.0,
        "Solution_original_content":"mikwieczorek probabl time ago write test link port servic port targetport pod port option port targetport select port latest version prometheu version schema endpoint field http github com prometheu oper prometheu oper blob document api monitor coreo com endpoint addit manual test send mikwieczorek chart version accru free reopen updat deploy addit helm repo updat helm upgrad instal namespac track server commun chart set servicemonitor enabl burak burakinc prompt updat time report",
        "Solution_preprocessed_content":"probabl time ago write test link port targetport option port latest version prometheu version addit manual test send chart version accru free reopen updat deploy addit burak prompt updat time report",
        "Solution_readability":7.3,
        "Solution_reading_time":15.73,
        "Solution_score_count":0.0,
        "Solution_sentence_count":18.0,
        "Solution_word_count":187.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":18.5819444444,
        "Challenge_answer_count":6,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nThe new staticPrefix argument being under extraArgs breaks the chart for users that need to use the extraArgs\n\n### What's your helm version?\n\nversion.BuildInfo{Version:\"v3.8.1\", GitCommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", GitTreeState:\"clean\", GoVersion:\"go1.17.8\"}\n\n### What's your kubectl version?\n\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin\/arm64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/arm64\"}\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.7\n\n### What happened?\n\nThe newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesnt exist.\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f mlflow\/values.yaml mlflow .\/mlflow\/\n\n### Anything else we need to know?\n\nI am just creating a pull request to address this in a bit different way and havent tested it yet. Just wanted to create a request to highlight a solution.\r\n\r\nYou could also handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make this work smoother for a final user, but this solution should work as well.",
        "Challenge_closed_time":1657616964000,
        "Challenge_created_time":1657550069000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/18",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.1,
        "Challenge_reading_time":23.2,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":18.5819444444,
        "Challenge_title":"[mlflow] Extra args broken",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":213,
        "Platform":"Github",
        "Solution_body":"Hi @subramaniam20jan \r\n\r\nCould you please share your values.yaml file with me? Do you have any additional change in it? Hi @subramaniam20jan \r\n\r\nI really didn't understand the problem. Static prefix is valid argument for mlflow server. You can find more information [here](https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-static-prefix).\r\n\r\nAlso, it tested with argument and without argument in [the unit tests](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/charts\/mlflow\/unittests\/deployment_test.yaml#L65). Also, it tested without argument in the integration test which we run it with [kind here](https:\/\/github.com\/community-charts\/helm-charts\/runs\/7283774204?check_suite_focus=true#step:12:175).\r\n\r\nI'm really not able to recreate the issue. Could you please share the error message? Well, if you use `mlflow ui` command, you must change it to `mlflow server` command for production usage. You can find same explanation from here: https:\/\/mlflow.org\/docs\/latest\/cli.html#mlflow-ui Actually it was my bad. This wasnt really an issue but I appreciate the addition of the extra parameters to the readiness and liveness probe :) btw, @burakince great job on the chart and image! I was something I have made many times in individual assignments and missed having in open source somewhere. Came across your project when I was about to create one myself. Saves me a lot of work :) Thanks @subramaniam20jan :) I just added an example usage to [here](https:\/\/github.com\/community-charts\/examples\/tree\/main\/mlflow-examples\/liveness-probe-and-readiness-probe-example).\r\n\r\nBest",
        "Solution_gpt_summary":"staticprefix separ argument extraenv start server smoother end static prefix argument server link unit test integr test argument later misunderstand",
        "Solution_link_count":5.0,
        "Solution_original_content":"subramaniamjan share valu yaml file addit subramaniamjan static prefix argument server http org doc latest cli html cmdoption server static prefix test argument argument unit test http github com commun chart helm chart blob chart unittest deploy test yaml test argument integr test run http github com commun chart helm chart run suit focu step recreat share messag server usag explan http org doc latest cli html wasnt addit extra paramet readi live probe btw burakinc great job chart imag time individu assign miss open sourc came creat save subramaniamjan usag http github com commun chart tree live probe readi probe",
        "Solution_preprocessed_content":"share file addit static prefix argument server test argument argument test argument integr test run recreat share messag usag explan wasnt addit extra paramet readi live probe btw great job chart imag time individu assign miss open sourc came creat save usag",
        "Solution_readability":11.2,
        "Solution_reading_time":20.36,
        "Solution_score_count":0.0,
        "Solution_sentence_count":17.0,
        "Solution_word_count":191.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":1.4025,
        "Challenge_answer_count":0,
        "Challenge_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Challenge_closed_time":1656583953000,
        "Challenge_created_time":1656578904000,
        "Challenge_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.8,
        "Challenge_reading_time":18.48,
        "Challenge_repo_contributor_count":5.0,
        "Challenge_repo_fork_count":8.0,
        "Challenge_repo_issue_count":39.0,
        "Challenge_repo_star_count":9.0,
        "Challenge_repo_watch_count":1.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":1.4025,
        "Challenge_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":147,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.1282051282,
        "Challenge_watch_issue_ratio":0.0256410256
    },
    {
        "Challenge_adjusted_solved_time":142.5483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## Instructions \r\nPage 133 of chapter 7 requires the reader to navigate to the following directory and enter the commands below:\r\n\r\n`cd Chapter07\/psystock-data-features-main`\r\n `mlflow run . --experiement-name=psystock_data_pipelines`\r\n\r\n## Problem\r\n\r\nThe following error message appears when running line of code specified above:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"feature_set_generation.py\", line 30, in <module>\r\n    raise Exception('x should not exceed 5. The value of x was: {}'.format(x))\r\nNameError: name 'x' is not defined\r\n```\r\n\r\n## Solution\r\nResolve this by deleting line 30 below in `feature_set_generation.py`\r\n\r\n`30         raise Exception('x should not exceed 5. The value of x was: {}'.format(x))`\r\nThe stray `raise` statement is referencing an undefined variable `x`.\r\n\r\nRemoving this line of code removed the reference to this point and lead to the successful deployment of the experiment. I would consider adding such assertions in the `check_verify_data.py` file instead.\r\n\r\n",
        "Challenge_closed_time":1637063606000,
        "Challenge_created_time":1636550432000,
        "Challenge_link":"https:\/\/github.com\/PacktPublishing\/Machine-Learning-Engineering-with-MLflow\/issues\/7",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.8,
        "Challenge_reading_time":13.22,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":59.0,
        "Challenge_repo_issue_count":18.0,
        "Challenge_repo_star_count":82.0,
        "Challenge_repo_watch_count":9.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":142.5483333333,
        "Challenge_title":"Chapter 7 `mlflow run . --experiement-name=psystock_data_pipelines` fails - BUGFIX",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":138,
        "Platform":"Github",
        "Solution_body":"Thanks, invaluable contributions. We will add this to the Errata!!!",
        "Solution_gpt_summary":"run run psystock data pipelin directori chapter psystock data featur delet line featur set gener strai rais statement referenc undefin variabl",
        "Solution_link_count":0.0,
        "Solution_original_content":"invalu contribut add errata",
        "Solution_preprocessed_content":"invalu contribut add errata",
        "Solution_readability":6.4,
        "Solution_reading_time":0.85,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.3333333333,
        "Challenge_watch_issue_ratio":0.5
    },
    {
        "Challenge_adjusted_solved_time":112.1258333333,
        "Challenge_answer_count":1,
        "Challenge_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Challenge_closed_time":1601714116000,
        "Challenge_created_time":1601310463000,
        "Challenge_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":9.3,
        "Challenge_reading_time":2.38,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":11.0,
        "Challenge_repo_star_count":22.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":112.1258333333,
        "Challenge_title":"Improve mlflow logging for population",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":"0332ede5",
        "Solution_gpt_summary":"improv log popul creat separ graph individu perform sub run",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-3.5,
        "Solution_reading_time":0.12,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":1.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0909090909,
        "Challenge_watch_issue_ratio":0.1818181818
    },
    {
        "Challenge_adjusted_solved_time":1301.8425,
        "Challenge_answer_count":6,
        "Challenge_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Challenge_closed_time":1662562977000,
        "Challenge_created_time":1657876344000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":16.2,
        "Challenge_reading_time":101.65,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":1301.8425,
        "Challenge_title":"MLflow fails to log to a tracking server",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":588,
        "Platform":"Github",
        "Solution_body":"cc @sgugger  I'm not the one who wrote or supports the ML Flow callback :-) @noise-field wrote the integration two years ago, do you have an idea of why it doesn't seem to work anymore @noise-field? @juliensimon, I had an error message similar (I think). I found that the issue was related to values with empty string values  (https:\/\/github.com\/mlflow\/mlflow\/issues\/6253), and it looks like there is a patch in the upcoming MLFLOW version 1.28 (not yet released)\r\n\r\nIn my case, I had to set `mp_parameters` to `None` instead of leaving it as an empty string (the default value), and I see your error message has `{'key': 'mp_parameters', 'value': ''}`.\r\n\r\nWhile later MLflow version fix will address this issue, I think setting the `mp_parameters` to `None` instead of an empty string is cleaner. However, I'm not sure about the extent of this change.\r\n\r\n OK, I'll give it a try and I'll let you know. This issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the [contributing guidelines](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/CONTRIBUTING.md) are likely to be ignored.",
        "Solution_gpt_summary":"set paramet leav default valu messag relat valu valu patch version releas address clear extent",
        "Solution_link_count":2.0,
        "Solution_original_content":"sgugger wrote flow callback nois field wrote integr year ago idea anymor nois field juliensimon messag relat valu valu http github com patch version releas set paramet leav default valu messag kei paramet valu later version address set paramet cleaner extent automat mark stale activ address comment thread note contribut guidelin http github com huggingfac transform blob contribut ignor",
        "Solution_preprocessed_content":"wrote flow callback wrote integr year ago idea anymor messag relat valu valu patch version set leav messag later version address set cleaner extent automat mark stale activ address comment thread note ignor",
        "Solution_readability":8.5,
        "Solution_reading_time":15.15,
        "Solution_score_count":0.0,
        "Solution_sentence_count":12.0,
        "Solution_word_count":195.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0212652587,
        "Challenge_watch_issue_ratio":0.0417554737
    },
    {
        "Challenge_adjusted_solved_time":43.4361111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n```shell\r\n- mlflow==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- Platform: Linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- Huggingface_hub version: 0.2.1\r\n- PyTorch version (GPU?): 1.10.2 (False)\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\nShould be fixed by #17067\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n1. Follow Training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. Change the training arguments to use `TrainingArguments(output_dir=\"test_trainer\", report_to=['mlflow'], run_name=\"run0\")`\r\n3. On `trainer.train()` the MLFlow UI should report a run with a Run Name of `run0` which is not currently the case.\r\n\r\nCause of the Issue:\r\n```\r\n>> import mlflow\r\n>> print(mlflow.active_run is None, mlflow.active_run() is None)\r\nFalse True\r\n```\r\n\r\nIn `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is None:` need to be replaced by `if self._ml_flow.active_run() is None:`\r\n\r\n### Expected behavior\r\n\r\nPR #14894 introduce support for run_name in the MLflowCallback. Though, this does not work as expected since the active run is checked using a method reference that always returns true. Bug introduced by #16131.\r\n",
        "Challenge_closed_time":1651758596000,
        "Challenge_created_time":1651602226000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17066",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":6.6,
        "Challenge_reading_time":18.14,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17176.0,
        "Challenge_repo_issue_count":20644.0,
        "Challenge_repo_star_count":75873.0,
        "Challenge_repo_watch_count":862.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":43.4361111111,
        "Challenge_title":"Incorrect check for MLFlow active run in MLflowCallback",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":177,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0212652587,
        "Challenge_watch_issue_ratio":0.0417554737
    },
    {
        "Challenge_adjusted_solved_time":1209.0019444444,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1640642583000,
        "Challenge_created_time":1636290176000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":12.9,
        "Challenge_reading_time":45.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":36,
        "Challenge_solved_time":1209.0019444444,
        "Challenge_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":338,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":1106.8952777778,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Challenge_closed_time":1635553585000,
        "Challenge_created_time":1631568762000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":12.7,
        "Challenge_reading_time":26.53,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":1106.8952777778,
        "Challenge_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":226,
        "Platform":"Github",
        "Solution_body":"`log_metrics` is part of the implementation of `LightningLoggerBase` yet using the experiment property returns the MlFlowClient which can be used to access methods specific to mlflow. So simply removing the experiment property from your calls should solve your problem.\r\n\r\nThe log_metric option of the mlflow client requires different args, see [here](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/mlflow.py#L226) This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_gpt_summary":"inconsist log metric logger remov properti call log metric option client argument pytorch lightn team mark stale close dai activ",
        "Solution_link_count":1.0,
        "Solution_original_content":"log metric implement lightningloggerbas properti return client access simpli remov properti call log metric option client arg http github com pytorchlightn pytorch lightn blob master pytorch lightn logger automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"implement properti return client access simpli remov properti call option client arg automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":12.5,
        "Solution_reading_time":8.64,
        "Solution_score_count":1.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":87.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":14.3083333333,
        "Challenge_answer_count":3,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers.mlflow.html?highlight=logger#mlflow-logger) mentions there is an argument called run_name for the mlflow logger, where the run_name of a given experiment can be provided. Although,run_name is an unknown argument to the mlflow logger\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## Please reproduce using the BoringModel\r\nColab link:  https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n### To Reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nimport mlflow\r\n\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### Environment\r\nColab - https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n",
        "Challenge_closed_time":1626409391000,
        "Challenge_created_time":1626357881000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/8431",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":18.4,
        "Challenge_reading_time":11.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":14.3083333333,
        "Challenge_title":"mlflow run_name unexpected argument error",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":69,
        "Platform":"Github",
        "Solution_body":"That's because you are looking at the docs under \"latest\" which is the development version. On the master branch, there is a run_name argument but 1.3.x does not have that. You are probably using Lightning 1.3.x. \r\nIf you want, you can install Lightning rc0 to test the features before the 1.4 release.   Here are the docs for the stable version (1.3.x) https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html Okay got it. Thanks for clarifying ",
        "Solution_gpt_summary":"document version stabl version stabl version run argument logger instal lightn test featur releas stabl version document link",
        "Solution_link_count":1.0,
        "Solution_original_content":"doc latest version master branch run argument probabl lightn instal lightn test featur releas doc stabl version http pytorch lightn readthedoc stabl api pytorch lightn logger html okai clarifi",
        "Solution_preprocessed_content":"doc latest version master branch argument probabl lightn instal lightn test featur releas doc stabl version okai clarifi",
        "Solution_readability":6.6,
        "Solution_reading_time":6.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":68.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":210.9636111111,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Challenge_closed_time":1617875123000,
        "Challenge_created_time":1617115654000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.5,
        "Challenge_reading_time":15.41,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":210.9636111111,
        "Challenge_title":"mlflow run context is not logged when using MLFlowLogger",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":936.1641666667,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1619815518000,
        "Challenge_created_time":1616445327000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.0,
        "Challenge_reading_time":22.86,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":936.1641666667,
        "Challenge_title":"External MLFlow logging failures cause training job to fail",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":224,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_preprocessed_content":"automat mark stale hasn activ close dai activ contribut pytorch lightn team",
        "Solution_readability":8.0,
        "Solution_reading_time":2.67,
        "Solution_score_count":-1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":36.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nCurrently the `MLFlowLogger` creates a new run when resuming from an hpc checkpoint, e.g., after preemption by slurm and requeuing. Runs are an MLFlow concept that groups things in their UI, so when resuming after requeue, it should really be reusing the run ID. I think this can be patched into the hpc checkpoint using the logger which I believe exposes the run ID. This can also be seen on the `v_num` on the progress bar which changes after preemption (in general that `v_num` probably shouldnt be changing in this case). I'm happy to attempt to PR this if the owners agree that it's a bug.\r\n\r\n### To Reproduce\r\n\r\nUse `MLFlowLogger` on a slurm cluster and watch the mlflow UI when preemption happens, there will be a new run created.\r\n\r\n### Expected behavior\r\n\r\nRuns are grouped neatly on the MLFlow UI\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.20.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0\r\n        - tqdm:              4.57.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.1\r\n        - version:           #1 SMP Thu Jan 21 16:15:07 EST 2021\r\n\n\ncc @awaelchli @ananthsub @ninginthecloud @rohitgr7 @tchaton @akihironitta",
        "Challenge_closed_time":null,
        "Challenge_created_time":1614282696000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6205",
        "Challenge_link_count":0,
        "Challenge_open_time":15268.14,
        "Challenge_readability":7.2,
        "Challenge_reading_time":15.34,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Logger Makes a New Run When Resuming from hpc Checkpoint",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":195,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":162.0608333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1613510526000,
        "Challenge_created_time":1612927107000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":9.6,
        "Challenge_reading_time":14.59,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":4.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":162.0608333333,
        "Challenge_title":"MlflowLogger fail when logging long parameters",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":144,
        "Platform":"Github",
        "Solution_body":"Dear @ducthienbui97,\n\nThanks for opening a PR.\n\nBest,\nT.C",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.3,
        "Solution_reading_time":0.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":307.9769444444,
        "Challenge_answer_count":7,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nUsing log_gpu_memory with MLFLow logger causes an error. It appears the name of the metric is not supported by MLFLow.\r\n\r\n    MlflowException: Invalid metric name: 'gpu_id: 0\/memory.used (MB)'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/).\r\n\r\n### To Reproduce\r\nI reproduced the bug with the BoringModel, in the link bellow:\r\nhttps:\/\/colab.research.google.com\/drive\/1P8uhSfjvYhKPMyRZH-QmfbOUOfnePy6G?usp=sharing\r\n\r\n### Expected behavior\r\nlog_gpu_memory should log gpu memory correctly when using an MLFlow logger.\r\n\r\n### Environment\r\nColab environment:\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cu101\r\n\t- pytorch-lightning: 1.0.3\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.9\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n",
        "Challenge_closed_time":1605009026000,
        "Challenge_created_time":1603900309000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4411",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.7,
        "Challenge_reading_time":13.02,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":307.9769444444,
        "Challenge_title":"Using log_gpu_memory with MLFLow logger causes an exception.",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":125,
        "Platform":"Github",
        "Solution_body":"This could be fixed removing the parenthesis from the string (as in the linked pr), but requires discussion if you guys want to change this for all loggers. would [MB] work? or should MLFlowLogger sanitize keys automatically by removing the unsupported characters? Seems like MLFlow only wants: \"[...] Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/)\".\r\n\r\nMaybe make the GPU memory key different only if MLFlow is being used? Sanitizing mlflow metrics with a warning might also be an option.  I see sanitization directly in the MLFlowLogger as a better long term solution. We already do sanitization for other loggers (for hyperparameters). \r\n I changed the PR to remove parenthesis in the mlflow logger, however i don't know if it makes sense to give a warning, since the metric name for log_gpu_memory is not defined by the user. Should the removing of parenthesis be silent? Maybe do a PR on mlflow github too to support these or do sanitization process :joy: ",
        "Solution_gpt_summary":"remov parenthesi gpu memori kei sanit metric warn implement sanit directli logger github clear consensu implement",
        "Solution_link_count":0.0,
        "Solution_original_content":"remov parenthesi link gui logger logger sanit kei automat remov unsupport charact alphanumer underscor dash period space slash mayb gpu memori kei sanit metric warn option sanit directli logger term sanit logger hyperparamet remov parenthesi logger sens warn metric log gpu memori defin remov parenthesi silent mayb github sanit process joi",
        "Solution_preprocessed_content":"remov parenthesi gui logger logger sanit kei automat remov unsupport charact alphanumer underscor dash period space slash mayb gpu memori kei sanit metric warn option sanit directli logger term sanit logger remov parenthesi logger sens warn metric defin remov parenthesi silent mayb github sanit process joi",
        "Solution_readability":8.2,
        "Solution_reading_time":12.43,
        "Solution_score_count":0.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":163.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":6.9419444444,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Challenge_closed_time":1602141183000,
        "Challenge_created_time":1602116192000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":18.0,
        "Challenge_reading_time":59.98,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":6.9419444444,
        "Challenge_title":"mlflow logger complains about missing run_id",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":304,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Here, mlflow logger is actually an MlflowClient object. so you ll need to use the function calls specified in this doc - https:\/\/www.mlflow.org\/docs\/latest\/_modules\/mlflow\/tracking\/client.html . These functions needs run_id as first argument which can be accessed as self.logger.run_id @nazim1021 thx for clarification! @qianyu-berkeley feel free to reopen if needed... Thanks! Same problem here, working with @nazim1021 suggestion. What about adding it to the doc? > What about adding it to the doc?\r\n\r\ngood idea, mind send a PR? :]",
        "Solution_gpt_summary":"function call specifi client object run argument access logger run add document encourag send pull request",
        "Solution_link_count":1.0,
        "Solution_original_content":"contribut great logger client object function call specifi doc http org doc latest modul track client html function run argument access logger run nazim thx clarif qianyu berkelei free reopen nazim doc doc idea send",
        "Solution_preprocessed_content":"contribut great logger client object function call specifi doc function argument access thx clarif free reopen doc doc idea send",
        "Solution_readability":5.0,
        "Solution_reading_time":7.33,
        "Solution_score_count":4.0,
        "Solution_sentence_count":11.0,
        "Solution_word_count":82.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":1597.1266666667,
        "Challenge_answer_count":15,
        "Challenge_body":"## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nTrying to log model into mlflow using `mlflow.pytorch.log_model` in train end. Getting the above error only in multi gpu scenario. \r\n\r\n#### Code\r\n\r\n\r\nmnist script file - \r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom argparse import ArgumentParser\r\n#from mlflow.pytorch.pytorch_autolog import __MLflowPLCallback\r\nfrom pytorch_lightning.logging import MLFlowLogger\r\nfrom sklearn.metrics import accuracy_score\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        \"\"\"\r\n        Initializes the network\r\n        \"\"\"\r\n        super(LightningMNISTClassifier, self).__init__()\r\n\r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\r\n            \"--batch-size\",\r\n            type=int,\r\n            default=64,\r\n            metavar=\"N\",\r\n            help=\"input batch size for training (default: 64)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--num-workers\",\r\n            type=int,\r\n            default=0,\r\n            metavar=\"N\",\r\n            help=\"number of workers (default: 0)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--lr\",\r\n            type=float,\r\n            default=1e-3,\r\n            metavar=\"LR\",\r\n            help=\"learning rate (default: 1e-3)\",\r\n        )\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        Forward Function\r\n        \"\"\"\r\n        batch_size, channels, width, height = x.size()\r\n\r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n\r\n        # layer 1 (b, 1*28*28) -> (b, 128)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 2 (b, 128) -> (b, 256)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 3 (b, 256) -> (b, 10)\r\n        x = self.layer_3(x)\r\n\r\n        # probability distribution over labels\r\n        x = torch.log_softmax(x, dim=1)\r\n\r\n        return x\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        \"\"\"\r\n        Loss Fn to compute loss\r\n        \"\"\"\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        \"\"\"\r\n        training the data as batches and returns training loss on each batch\r\n        \"\"\"\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        \"\"\"\r\n        Performs validation of data in batches\r\n        \"\"\"\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average validation accuracy\r\n        \"\"\"\r\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs}\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Performs test and computes test accuracy\r\n        \"\"\"\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        a, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy_score(y_hat.cpu(), y.cpu())\r\n        return {\"test_acc\": torch.tensor(test_acc)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\r\n        return {\"avg_test_acc\": avg_test_acc}\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Preprocess the input data.\r\n        \"\"\"\r\n        return {}\r\n\r\n    def train_dataloader(self):\r\n        \"\"\"\r\n        Loading training data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_train,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        \"\"\"\r\n        Loading validation data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\r\n\r\n        return DataLoader(\r\n            mnist_val,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        \"\"\"\r\n        Loading test data as batches\r\n        \"\"\"\r\n        mnist_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_test,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Creates and returns Optimizer\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        self.scheduler = {\r\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n                self.optimizer,\r\n                mode=\"min\",\r\n                factor=0.2,\r\n                patience=2,\r\n                min_lr=1e-6,\r\n                verbose=True,\r\n            )\r\n        }\r\n        return [self.optimizer], [self.scheduler]\r\n\r\n    def optimizer_step(\r\n        self,\r\n        epoch,\r\n        batch_idx,\r\n        optimizer,\r\n        optimizer_idx,\r\n        second_order_closure=None,\r\n        on_tpu=False,\r\n        using_lbfgs=False,\r\n        using_native_amp=False,\r\n    ):\r\n        self.optimizer.step()\r\n        self.optimizer.zero_grad()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    from pytorch_autolog import autolog\r\n    autolog()\r\n    model = LightningMNISTClassifier()\r\n    mlflow_logger = MLFlowLogger(\r\n        experiment_name=\"Default\", tracking_uri=\"http:\/\/localhost:5000\/\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        logger=mlflow_logger,\r\n        gpus=2,\r\n        distributed_backend=\"ddp\",\r\n        max_epochs=1\r\n    )\r\n    trainer.fit(model)\r\n    trainer.test()\r\n\r\n```\r\n\r\nSample code from autolog - Callback class. \r\n\r\n```\r\n    class __MLflowPLCallback(pl.Callback):\r\n\r\n        def __init__(self):\r\n            super().__init__()\r\n\r\n        def on_train_end(self, trainer, pl_module):\r\n            \"\"\"\r\n            Logs the model checkpoint into mlflow - models folder on the training end\r\n            \"\"\"\r\n\r\n            mlflow.set_tracking_uri(trainer.logger._tracking_uri )\r\n            mlflow.set_experiment(trainer.logger._experiment_name)\r\n            mlflow.start_run(trainer.logger.run_id)\r\n            mlflow.pytorch.log_model(trainer.model, \"models\")\r\n            mlflow.end_run()\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\nStack Trace\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                          \r\n  File \"mnist.py\", line 231, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 218, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 209, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 992, in fit\r\n    results = self.spawn_ddp_children(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 462, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, q=None, model=model, is_master=True)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 560, in ddp_train\r\n    results = self.run_pretrain_routine(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1213, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 392, in train\r\n    self.run_training_teardown()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 872, in run_training_teardown\r\n    self.on_train_end()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 72, in on_train_end\r\n    callback.on_train_end(self, self.get_model())\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 120, in on_train_end\r\n    mlflow.pytorch.log_model(trainer.model, \"models\")\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 179, in log_model\r\n    signature=signature, input_example=input_example, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/models\/model.py\", line 154, in log\r\n    **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 300, in save_model\r\n    torch.save(pytorch_model, model_path, pickle_module=pickle_module, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 370, in save\r\n    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 443, in _legacy_save\r\n    pickler.dump(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 491, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 659, in save_reduce\r\n    self._batch_setitems(dictitems)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\n\r\n```\r\n\r\n\r\n\r\n#### What have you tried?\r\nTried out the possibilities mentioned in the similar thread - https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/2186\r\n\r\nTried  wrapping the code inside, `trainer.is_global_zero`  . And also tried `trainer.global_rank == 0`. Also tried decorating the method as `@rank_zero_only`. But no luck. Getting the same error. \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu\r\n - Packaging - torch, pytorch-lightning, torchvision, mlflow",
        "Challenge_closed_time":1605489870000,
        "Challenge_created_time":1599740214000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3444",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":14.2,
        "Challenge_reading_time":152.56,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":140,
        "Challenge_solved_time":1597.1266666667,
        "Challenge_title":"TypeError: can't pickle _thread.lock objects - Error while logging model into mlflow in multi gpu scenario",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":935,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nThanks for the reply. Sure i will try and update here. > What happens if you don't use the scheduler? Please try commenting out the scheduler definition and return only the optimizer.\r\n\r\nI removed the scheduler part and re-ran the script. Still experiencing the same error.  @lucadiliello @williamFalcon Any suggestions here ? One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved. \r\n\r\nTested with 2 gpus and 0.9 version of pytorch lightning.  This issue has been automatically marked as stale because it hasn't had any recent activity. This issue will be closed in 7 days if no further activity occurs. Thank you for your contributions, Pytorch Lightning Team!\n I have the same error. Training with a single gpu works fine but with multiple the error is raised > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n\r\nTraining with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\nSomething like:\r\n```python\r\n>>> model = MyModel(...)\r\n>>>\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\n\r\nIn my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled. > One more observation from my end. In multi gpu scenario when i save the model using `torch.save(trainer.model, PATH)` i get the above mentioned error . However, when i try to save the save dict `torch.save(trainer.model.state_dict(), PATH)` the state dict is successfully getting saved.\r\n> \r\n> Tested with 2 gpus and 0.9 version of pytorch lightning.\r\n\r\nPlease update to the latest version and let us know whether the error persist. > > I have the same error. Training with a single gpu works fine but with multiple the error is raised\r\n> \r\n> Training with single GPU is fine because there is no need to create multiple processes, so you model is not pickled. What happens if you simply load your model in a shell and try to pickle it?\r\n> Something like:\r\n> \r\n> ```python\r\n> >>> model = MyModel(...)\r\n> >>>\r\n> >>> import pickle\r\n> >>> pickle.dump(model, \"tmp_file.pk\")\r\n> ```\r\n> \r\n> In my little experience, most of the pickle errors are caused by lambda functions or non-global functions. See [here](https:\/\/docs.python.org\/3\/library\/pickle.html#what-can-be-pickled-and-unpickled) the list of what can be pickled.\r\n\r\nI tried adding as suggested:\r\n```\r\n>>> import pickle\r\n>>> pickle.dump(model, \"tmp_file.pk\")\r\n```\r\nHowever, this raise the error: TypeError: file must have a 'write' attribute\r\nI then tried with:\r\n```\r\n>>> import pickle\r\n>>> with open(\"tmp_file.pk\",\"wb\") as f:\r\n>>>         pickle.dump(model, f)\r\n```\r\nThis then again raised TypeError: can't pickle _thread.lock objects\r\nI am on the latest version of pytorch lightning This suggests that the problem is in your model. Can you post it here?\n\nA complete log of the error would also be useful. Thanks I ran into this as well. I wrote a little function to iterate through the model.__dict__.items() and check which caused pickle errors. It looks like there's a model attribute pointing to the trainer, which has that _thread.lock on it. Maybe there's a step that's supposed to clean this up that's being missed somehow? My quick work-around was to `delattr(model, \"trainer\")` before pickling the model, but I haven't actually tried loading the model again, so I this could cause other problems. same problem same problem on 1.2.4, works fine with 1.1.0. any ideas what might be causing the difference between the versions?",
        "Solution_gpt_summary":null,
        "Solution_link_count":2.0,
        "Solution_original_content":"contribut great schedul comment schedul definit return optim schedul comment schedul definit return optim repli updat schedul comment schedul definit return optim remov schedul ran experienc lucadiliello williamfalcon observ end multi gpu save model torch save trainer model path save save dict torch save trainer model state dict path state dict successfulli save test gpu version pytorch lightn automat mark stale hasn activ close dai activ contribut pytorch lightn team train singl gpu multipl rais train singl gpu multipl rais train singl gpu creat multipl process model pickl simpli load model shell pickl model mymodel import pickl pickl dump model tmp file littl pickl lambda function global function http doc org librari pickl html pickl unpickl list pickl observ end multi gpu save model torch save trainer model path save save dict torch save trainer model state dict path state dict successfulli save test gpu version pytorch lightn updat latest version persist train singl gpu multipl rais train singl gpu creat multipl process model pickl simpli load model shell pickl model mymodel import pickl pickl dump model tmp file littl pickl lambda function global function http doc org librari pickl html pickl unpickl list pickl tri import pickl pickl dump model tmp file rais typeerror file write attribut tri import pickl open tmp file pickl dump model rais typeerror pickl thread lock object latest version pytorch lightn model complet log ran wrote littl function iter model dict item pickl model attribut trainer thread lock mayb step suppos clean miss quick delattr model trainer pickl model haven tri load model idea version",
        "Solution_preprocessed_content":"contribut great schedul comment schedul definit return optim schedul comment schedul definit return optim repli updat schedul comment schedul definit return optim remov schedul experienc observ end multi gpu save model save save dict state dict successfulli save test gpu version pytorch lightn automat mark stale hasn activ close dai activ contribut pytorch lightn team train singl gpu multipl rais train singl gpu multipl rais train singl gpu creat multipl process model pickl simpli load model shell pickl littl pickl lambda function function list pickl observ end multi gpu save model save save dict state dict successfulli save test gpu version pytorch lightn updat latest version persist train singl gpu multipl rais train singl gpu creat multipl process model pickl simpli load model shell pickl littl pickl lambda function function list pickl tri rais typeerror file write attribut tri rais typeerror pickl object latest version pytorch lightn model complet log ran wrote littl function iter pickl model attribut trainer mayb step suppos clean miss quick pickl model haven tri load model idea version",
        "Solution_readability":7.2,
        "Solution_reading_time":51.46,
        "Solution_score_count":3.0,
        "Solution_sentence_count":63.0,
        "Solution_word_count":630.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":27.1641666667,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Challenge_closed_time":1599644307000,
        "Challenge_created_time":1599546516000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":59.64,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":54,
        "Challenge_solved_time":27.1641666667,
        "Challenge_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":532,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.\r\nI mean, your network is a single linear layer, you probably run through epochs super fast.\r\nI am not yet convinced it is a bug, but I'll try your example code hey @awaelchli, Thanks for replying!\r\nThe model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.\r\n\r\nI've done some debugging and added a solution section to the issue. It appears to be in in the `experiment` property of the MLFlowLogger. Each time `.experiment` is accessed, `self._mlflow_client.get_experiment_by_name(self._experiment_name)` is called, which communicates with the MLFlow server.\r\n\r\nIt seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger. oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.  yeah sure, I'll link it here shortly. Did you encounter this #3392 problem as well?",
        "Solution_gpt_summary":"identifi logger commun server train step metric log epoch modifi codebas elimin overhead store respons client avoid logger commun server train loop plan rais",
        "Solution_link_count":0.0,
        "Solution_original_content":"contribut great tri increas row log interv trainer flag control log sent logger network singl linear layer probabl run epoch fast convinc awaelchli repli model contriv test realis perform mflow logger tensorboard logger inher client debug section properti logger time access client call commun server store respons dramat tensorboard logger sens send test pass happi review yeah link shortli",
        "Solution_preprocessed_content":"contribut great tri increas trainer flag control log sent logger network singl linear layer probabl run epoch fast convinc repli model contriv test realis perform mflow logger tensorboard logger inher client debug section properti logger time access call commun server store respons dramat tensorboard logger sens send test pass happi review yeah link shortli",
        "Solution_readability":7.1,
        "Solution_reading_time":15.38,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":206.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":31.125,
        "Challenge_answer_count":8,
        "Challenge_body":"I think I'm logging correctly, this is my `training_step`\r\n\r\n        result = pl.TrainResult(loss)\r\n        result.log('loss\/train', loss)\r\n        return result\r\n\r\nand `validation_step`\r\n\r\n        result = pl.EvalResult(loss)\r\n        result.log('loss\/validation', loss)\r\n        return result\r\n\r\nThe validation loss is updated in mlflow each epoch, however the training loss isn't displayed until training has finished. Then it's available for every step. This may be a mlflow rather than pytorch-lighting issue - somewhere along the line it seems to be buffered?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/5028974\/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png)\r\n\r\nVersions:\r\n\r\npytorch-lightning==0.9.0\r\nmlflow==1.11.0\r\n\r\nEdit: logging TrainResult with on_epoch=True results in the metric appearing in mlflow during training, it's only the default train logging which gets delayed. i.e.\r\n\r\n        result.log('accuracy\/train', acc, on_epoch=True)\r\n\r\nis fine\r\n\r\n",
        "Challenge_closed_time":1599634019000,
        "Challenge_created_time":1599521969000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3392",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.5,
        "Challenge_reading_time":12.12,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":31.125,
        "Challenge_title":"mlflow training loss not reported until end of run",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":107,
        "Platform":"Github",
        "Solution_body":"When using the minimal example provided in the linked issue, and using the default training logging shown above, I don't see the behaviour described. \r\nI can sometimes see a discrepancy between the reported steps for each metrics, but I suspect this is to do with MLFlow and not the PL logger.\r\n![newplot](https:\/\/user-images.githubusercontent.com\/17157991\/92454951-7120fe00-f204-11ea-99f3-7d2ac09b0f5e.png)\r\n\r\n@david-waterworth could you elaborate on a few points?\r\n1. When you set up the MLFLowLogger, if your `tracking_uri` over `http:` or using `file:`?\r\n2. If `http`, is the tracking server remote?\r\n3. How long does a model training run typically take? \r\n4. does this behaviour consistently happen even when refreshing the MLFlow page?\r\n @patrickorlando \r\n\r\nWhen you set up the MLFLowLogger, if your tracking_uri over http: or using file:?\r\n\r\nI'm using file i.e. `mlflow = loggers.MLFlowLogger(\"Transformer\")`\r\n\r\nHow long does a model training run typically take?\r\n\r\n10-20 minutes\r\n\r\ndoes this behavior consistently happen even when refreshing the MLFlow page?\r\n\r\nYes I've tried to reproduce this but cant seem to. I can confirm that the MLFlow logger is logging metrics at the end of each epoch and for me they show up in the MLFlow UI as I refresh the page. \r\nDo you have a working code sample that can reproduce the issue?\r\n So I can actually see the behaviour you've described, but not when using the minimal example in #3393. I'll try to work out why. So I _think_ this is because of the default behaviour of the `TrainResult` and the way `row_log_interval` works. And it only appears if the number of batches per epoch is less than `row_log_interval`\r\n\r\nBy default TrainResult logs on step and not on epoch.\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/aaf26d70c4658e961192ba4c408558f1cf39bb18\/pytorch_lightning\/core\/step_result.py#L510-L517\r\n\r\nWhen logging only per step, the logger connector only logs when the `batch_idx` is a multiple of `row_log_interval`. However if you don't have more than `row_log_interval` batches, the metrics are not logged.\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/aaf26d70c4658e961192ba4c408558f1cf39bb18\/pytorch_lightning\/trainer\/logger_connector.py#L229-L237\r\n\r\n@david-waterworth Do you have less than 50 batches per epoch in your model? can you try setting `row_log_interval` to be less than the number of train batches to confirm whether the issue is caused by this?\r\n\r\n @patrickorlando yes I have 38 batches per epoch. I set `row_log_interval=1` and now the training step metrics are being displayed as they're generated. > yes I have 38 batches per epoch. I set row_log_interval=1 and now the training step metrics are being displayed as they're generated.\r\n\r\nThat makes sense now :) \r\n@david-waterworth Should we close this? or is there something left unresolved? Thanks for the assistance, no nothing unresolved.",
        "Solution_gpt_summary":"train loss displai end train run set row log interv default trainresult log trainresult log step epoch logger connector log batch idx multipl row log interv row log interv batch epoch metric log",
        "Solution_link_count":3.0,
        "Solution_original_content":"minim link default train log shown discrep report step metric logger newplot http imag githubusercont com dacbf png david waterworth elabor set logger track uri http file http track server remot model train run typic consist refresh page patrickorlando set logger track uri http file file logger logger transform model train run typic minut consist refresh page tri reproduc logger log metric end epoch refresh page sampl reproduc minim default trainresult row log interv batch epoch row log interv default trainresult log step epoch http github com pytorchlightn pytorch lightn blob aafdcebacfcfbb pytorch lightn core step log step logger connector log batch idx multipl row log interv row log interv batch metric log http github com pytorchlightn pytorch lightn blob aafdcebacfcfbb pytorch lightn trainer logger connector david waterworth batch epoch model set row log interv train batch patrickorlando batch epoch set row log interv train step metric displai gener batch epoch set row log interv train step metric displai gener sens david waterworth close left unresolv assist unresolv",
        "Solution_preprocessed_content":"minim link default train log shown discrep report step metric logger elabor set logger track server remot model train run typic consist refresh page set logger http file file model train run typic minut consist refresh page tri reproduc logger log metric end epoch refresh page sampl reproduc minim default batch epoch default trainresult log step epoch log step logger connector log multipl batch metric log batch epoch model set train batch batch epoch set train step metric displai gener batch epoch set train step metric displai gener sens close left unresolv assist unresolv",
        "Solution_readability":8.8,
        "Solution_reading_time":35.76,
        "Solution_score_count":2.0,
        "Solution_sentence_count":29.0,
        "Solution_word_count":405.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":9.3011111111,
        "Challenge_answer_count":3,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Challenge_closed_time":1597848054000,
        "Challenge_created_time":1597814570000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":16.5,
        "Challenge_reading_time":47.61,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":9.3011111111,
        "Challenge_title":"MLFlowLogger throws a JSONDecodeError",
        "Challenge_topic":"Kubernetes Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":299,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! Hi, thanks for submitting the bug. I don't know what's going on here. I cannot reproduce with your instructions. \r\n\r\nI'm running your sample code \r\n\r\n```python \r\n    mlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"http:\/\/127.0.0.1:5000\")\r\n    trainer = Trainer(logger=mlflow_logger)\r\n    trainer.logger.experiment_id\r\n```\r\nand the tracking uri I got from running \r\n```bash\r\nmlflow ui\r\n```\r\nThe experiment shows up in the UI and I get no errors. I verified this with the latest version of PL and mlflow, python 3.7.\r\n\r\nIs there any other information you can provide on the issue? Thanks for the quick response, @awaelchli! \r\n\r\nI did nothing different this morning and I am able to log metrics\/parameters to mlflow. I will close this now and in case I encounter this again, I will reopen this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"contribut great submit reproduc instruct run sampl logger logger test track uri http trainer trainer logger logger trainer logger track uri run bash verifi latest version quick respons awaelchli morn log metric paramet close reopen",
        "Solution_preprocessed_content":"contribut great submit reproduc instruct run sampl track uri run verifi latest version quick respons morn log close reopen",
        "Solution_readability":7.0,
        "Solution_reading_time":10.52,
        "Solution_score_count":1.0,
        "Solution_sentence_count":13.0,
        "Solution_word_count":124.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":59.9219444444,
        "Challenge_answer_count":3,
        "Challenge_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Challenge_closed_time":1597488847000,
        "Challenge_created_time":1597273128000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.5,
        "Challenge_reading_time":8.83,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":59.9219444444,
        "Challenge_title":"mlflow checkpoints in the wrong location ",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":82,
        "Platform":"Github",
        "Solution_body":"@david-waterworth mind try the latest 0.9rc12? It was fixed here: #2502 \r\nThe checkpoints subfolder will go here: `mlflow\\1{guid}\\checkpoints`, is that what you want @david-waterworth ?\r\n Thanks @awaelchli  yes that's what I want - thanks!",
        "Solution_gpt_summary":"latest version pytorch lightn version checkpoint subfold save locat guid checkpoint",
        "Solution_link_count":0.0,
        "Solution_original_content":"david waterworth latest checkpoint subfold guid checkpoint david waterworth awaelchli",
        "Solution_preprocessed_content":null,
        "Solution_readability":5.1,
        "Solution_reading_time":2.95,
        "Solution_score_count":1.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":32.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":487.0688888889,
        "Challenge_answer_count":2,
        "Challenge_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a `DictConfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#L177\r\n\r\n### To Reproduce\r\n\r\nUse Hydra and MLFlow together. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=None,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/utils\/validation.py\", line 120, in _validate_param_name\r\n    INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'\r\n```\r\n\r\n### Expected behavior\r\n\r\nCheck whether the instance if `dict` or `DictConfig` in the given line. \r\n",
        "Challenge_closed_time":1592925645000,
        "Challenge_created_time":1591172197000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2058",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":19.8,
        "Challenge_reading_time":50.14,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":487.0688888889,
        "Challenge_title":"Hydra MLFlow Clash",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":248,
        "Platform":"Github",
        "Solution_body":"Hi! thanks for your contribution!, great first issue! > Check whether the instance if `dict` or `DictConfig` in the given line.\r\n\r\n@ssakhavi that sounds reasonable solution, mind sending a PR - fix and its test?",
        "Solution_gpt_summary":"instanc dict dictconfig line send pull request test",
        "Solution_link_count":0.0,
        "Solution_original_content":"contribut great instanc dict dictconfig line ssakhavi reason send test",
        "Solution_preprocessed_content":"contribut great instanc line reason send test",
        "Solution_readability":4.2,
        "Solution_reading_time":2.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":33.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":1965.6686111111,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Challenge_closed_time":1583540837000,
        "Challenge_created_time":1576464430000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":14.0,
        "Challenge_reading_time":59.35,
        "Challenge_repo_contributor_count":449.0,
        "Challenge_repo_fork_count":2665.0,
        "Challenge_repo_issue_count":13532.0,
        "Challenge_repo_star_count":20903.0,
        "Challenge_repo_watch_count":227.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":64,
        "Challenge_solved_time":1965.6686111111,
        "Challenge_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":409,
        "Platform":"Github",
        "Solution_body":"Will investigate. We have a test that is supposed to prevent these problems from sneaking back in, but apparently it's not doing it's job. I imagine everyone is busy with the build failures - but for the record, I am  having a similar problem. Essentially, I cannot get a logger to work using ddp. It's gving me one of those days when I wonder why I ever wanted to write software ;)\r\n\r\nThis is Ubuntu 18.04.2LTS, on a 14 core, 7 gpu machine. Python 3.6.8, pytorch 1.3.1, pytorch-lightning 0.5.3.2, Tensorboard 2.1.0. Everything else standard except pillow isis 6.2.2 due to known bug in 7.0.\r\n\r\nI am working with a tried and true model and hyperparameters. The model and logging work fine as cpu, gpu, or dp - and ddp if I don't log. But not ddp with logging. I am not using SLURM.\r\n\r\nI have tried to get around this several ways: passing a custom logger, not using the logger created by Trainer(), etc. They either fail when called from one of the new processes, with an attribute error in Tensorboard TTDummyFileWriter.get_logdir(), or they fail with a pickle error about thread.locks when being copied to a new process\r\n\r\nI will detail these in a bug report if you think they are NOT due to the recent build issues.\r\n\r\nBut thought you'd want to know ...\r\n\r\ns\r\n @dbczumar,  @smurching? @neggert is this fixed now? Can this issue be re-opened? I'm currently working with Pytorch-Lightning==0.7.6 and am getting an identical pickle issue when using DDP with the MLFLowLogger.\r\n\r\n**Reproducing**\r\n\r\nUsing the script the OP gave led to some other errors (mostly to do with lightning version differences), so a new gist to reproduce in Pytorch-Lightning 0.7.6 can be found [here](https:\/\/gist.github.com\/Polyphenolx\/39424e5673fc029567f7f3ae3551fffb).\r\n\r\nThis is easily reproducible in other projects as well.\r\n\r\n**Error Output**\r\n\r\n```\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `logging` package has been renamed to `loggers` since v0.7.0 The deprecated package name will be removed in v0.9.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `mlflow_logger` module has been renamed to `mlflow` since v0.6.0. The deprecated module name will be removed in v0.8.0.\r\n  warnings.warn(*args, **kwargs)\r\n\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/utilities\/distributed.py:23: DeprecationWarning: `data_loader` decorator deprecated in v0.7.0. Will be removed v0.9.0\r\n  warnings.warn(*args, **kwargs)\r\nGPU available: True, used: True\r\nNo environment variable for node rank defined. Set as 0.\r\nCUDA_VISIBLE_DEVICES: [0,1,2,3]\r\nTraceback (most recent call last):\r\n  File \"mlflow_test.py\", line 65, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 844, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 200, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 149, in start_processes\r\n    process.start()\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/home\/user\/anaconda3\/envs\/pytorch\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n**Environment**\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t\t- GeForce GTX 1080 Ti\r\n\t- available:         True\r\n\t- version:           10.2\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.5.0\r\n\t- pytorch-lightning: 0.7.6\r\n\t- tensorboard:       2.2.2\r\n\t- tqdm:              4.46.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.8\r\n\t- version:           #102-Ubuntu SMP Mon May 11 10:07:26 UTC 2020\r\n``` To add to this, it appears to be a greater issue with MLFLow and how their tracking utilities are coded. They use a higher order function that causes issues with pickling in torches DDP backend. I've created an issue on MLFLow git, and submitted a PR to remedy the problem. \r\n\r\nIn the interim, feel free to implement the fix described in the issue in the MLFlow git as a temporary fix until\/if they review\/merge mine Following up on this: The pickling fix was merged into the master branch of MLFlow a couple days ago (see the bug mention above). Training using DDP is now functional on MLFLow versions installed from master, but it may take them some time to release the fix to PyPi Running into this same issue as are a few others here:\r\nhttps:\/\/github.com\/minimaxir\/aitextgen\/issues\/135\r\n![image](https:\/\/user-images.githubusercontent.com\/4674698\/121708545-8923f780-ca8c-11eb-9483-56740fd6d401.png)\r\n Hi,\r\n I am still getting the below error:\r\n![image](https:\/\/user-images.githubusercontent.com\/57705684\/131129141-fa483cb4-cb95-43a1-b1d3-62bf78711de2.png)\r\n\r\nI am using DP strategy and PT version '1.8.1+cu111' and PL version '1.3.8'.",
        "Solution_gpt_summary":"logger distribut backend ddp gpu slurm multiprocess pickl nest function function rest store implement git temporari review merg version instal master pickl merg master branch coupl dai ago",
        "Solution_link_count":4.0,
        "Solution_original_content":"test suppos prevent sneak appar job imagin busi build record essenti logger ddp gving dai write softwar ubuntu lt core gpu pytorch pytorch lightn tensorboard standard pillow isi tri model hyperparamet model log cpu gpu ddp log ddp log slurm tri pass logger logger creat trainer call process attribut tensorboard ttdummyfilewrit logdir pickl thread lock copi process report build dbczumar smurch neggert open pytorch lightn ident pickl ddp logger reproduc gave led lightn version gist reproduc pytorch lightn http gist github com polyphenolx efcffaeb easili reproduc output home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn log packag renam logger deprec packag remov warn warn arg kwarg home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn logger modul renam deprec modul remov warn warn arg kwarg home anaconda env pytorch lib site packag pytorch lightn util distribut deprecationwarn data loader decor deprec remov warn warn arg kwarg gpu environ variabl node rank defin set cuda visibl devic traceback file test line trainer fit model file home anaconda env pytorch lib site packag pytorch lightn trainer trainer line fit spawn ddp train nproc num process arg model file home anaconda env pytorch lib site packag torch multiprocess spawn line spawn return start process arg nproc daemon start spawn file home anaconda env pytorch lib site packag torch multiprocess spawn line start process process start file home anaconda env pytorch lib multiprocess process line start popen popen file home anaconda env pytorch lib multiprocess context line popen return popen process obj file home anaconda env pytorch lib multiprocess popen spawn posix line init init process obj file home anaconda env pytorch lib multiprocess popen fork line init launch process obj file home anaconda env pytorch lib multiprocess popen spawn posix line launch reduct dump process obj file home anaconda env pytorch lib multiprocess reduct line dump forkingpickl file protocol dump obj attributeerror pickl local object rest store default host cred environ cuda gpu geforc gtx geforc gtx geforc gtx geforc gtx version packag numpi pytorch debug pytorch version pytorch lightn tensorboard tqdm linux architectur bit processor version ubuntu smp mon utc add greater track util higher order function pickl torch ddp backend creat git submit remedi interim free implement git temporari review merg pickl merg master branch coupl dai ago train ddp function version instal master time releas pypi run http github com minimaxir aitextgen imag http imag githubusercont com cac fdd png imag http imag githubusercont com facb bfde png strategi version version",
        "Solution_preprocessed_content":"test suppos prevent sneak appar job imagin busi build record essenti logger ddp gving dai write softwar ubuntu core gpu pytorch tensorboard standard pillow isi tri model hyperparamet model log cpu gpu ddp log ddp log slurm tri pass logger logger creat trainer call process attribut tensorboard pickl copi process report build ident pickl ddp logger reproduc gave led gist reproduc easili reproduc output environ add greater track util higher order function pickl torch ddp backend creat git submit remedi interim free implement git temporari pickl merg master branch coupl dai ago train ddp function version instal master time releas pypi run strategi version version",
        "Solution_readability":9.3,
        "Solution_reading_time":75.14,
        "Solution_score_count":1.0,
        "Solution_sentence_count":72.0,
        "Solution_word_count":675.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0331806089,
        "Challenge_watch_issue_ratio":0.0167750517
    },
    {
        "Challenge_adjusted_solved_time":21.5194444444,
        "Challenge_answer_count":1,
        "Challenge_body":"```\r\nAttributeError: 'DatabaseServiceMetadataPipeline' object has no attribute 'mlModelFilterPattern'\r\n```\r\n\r\nWe need to review which configuration param is being sent here",
        "Challenge_closed_time":1662467440000,
        "Challenge_created_time":1662389970000,
        "Challenge_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/7232",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.3,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":126.0,
        "Challenge_repo_fork_count":360.0,
        "Challenge_repo_issue_count":9147.0,
        "Challenge_repo_star_count":1692.0,
        "Challenge_repo_watch_count":21.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":21.5194444444,
        "Challenge_title":"Mlflow UI deployment error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":22,
        "Platform":"Github",
        "Solution_body":"sourceConfig type missing to be sent from the UI",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"sourceconfig type miss sent",
        "Solution_preprocessed_content":"sourceconfig type miss sent",
        "Solution_readability":3.3,
        "Solution_reading_time":0.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":9.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0137750082,
        "Challenge_watch_issue_ratio":0.0022958347
    },
    {
        "Challenge_adjusted_solved_time":229.9927777778,
        "Challenge_answer_count":0,
        "Challenge_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Challenge_closed_time":1649449895000,
        "Challenge_created_time":1648621921000,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.7,
        "Challenge_reading_time":18.1,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":229.9927777778,
        "Challenge_title":"error creating a triton deployment mlflow plugin",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":159,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0183128775,
        "Challenge_watch_issue_ratio":0.0225988701
    },
    {
        "Challenge_adjusted_solved_time":745.5772222222,
        "Challenge_answer_count":2,
        "Challenge_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Challenge_closed_time":1650643135000,
        "Challenge_created_time":1647959057000,
        "Challenge_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":13.4,
        "Challenge_reading_time":34.23,
        "Challenge_repo_contributor_count":94.0,
        "Challenge_repo_fork_count":1046.0,
        "Challenge_repo_issue_count":5133.0,
        "Challenge_repo_star_count":4495.0,
        "Challenge_repo_watch_count":116.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":29,
        "Challenge_solved_time":745.5772222222,
        "Challenge_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":227,
        "Platform":"Github",
        "Solution_body":"It appears that the bug has been fixed by https:\/\/github.com\/triton-inference-server\/server\/pull\/3828 and I am not able to reproduce it using the model example for the plugin. Can you try the plugin from the latest codebase?\r\n```\r\npython `pwd`\/mlflow-triton-plugin\/scripts\/publish_model_to_mlflow.py \\\r\n    --model_name onnx_float32_int32_int32 \\\r\n    --model_directory `pwd`\/mlflow-triton-plugin\/examples\/onnx_float32_int32_int32\/ \\\r\n    --flavor triton\r\n```\r\nreturns:\r\n```\r\nRegistered model 'onnx_float32_int32_int32' already exists. Creating a new version of this model...\r\n2022\/04\/07 23:03:53 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: onnx_float32_int32_int32, version 3\r\nCreated version '3' of model 'onnx_float32_int32_int32'.\r\n.\/mlruns\/0\/945d5c5d6806470d889248cfc7f10b69\/artifacts\r\n``` Closing due to in-activity.",
        "Solution_gpt_summary":"latest codebas plugin latest codebas",
        "Solution_link_count":1.0,
        "Solution_original_content":"http github com triton infer server server pull reproduc model plugin plugin latest codebas pwd triton plugin publish model model onnx model directori pwd triton plugin onnx flavor triton return regist model onnx creat version model track model registri client wait model version finish creation model onnx version creat version model onnx dcddcfcfb artifact close activ",
        "Solution_preprocessed_content":"reproduc model plugin plugin latest codebas return close",
        "Solution_readability":12.9,
        "Solution_reading_time":11.49,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":86.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0183128775,
        "Challenge_watch_issue_ratio":0.0225988701
    },
    {
        "Challenge_adjusted_solved_time":2830.1302777778,
        "Challenge_answer_count":2,
        "Challenge_body":"At the moment, an mlflow byom predictor with arbitrary URLs can be created. We should first check whether an actual mlflow model is served at that URL before creating\/linking said model.",
        "Challenge_closed_time":1656947080000,
        "Challenge_created_time":1646758611000,
        "Challenge_link":"https:\/\/github.com\/mindsdb\/mindsdb\/issues\/2043",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":2.98,
        "Challenge_repo_contributor_count":241.0,
        "Challenge_repo_fork_count":1404.0,
        "Challenge_repo_issue_count":4035.0,
        "Challenge_repo_star_count":12007.0,
        "Challenge_repo_watch_count":327.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":2830.1302777778,
        "Challenge_title":"[ BYOM MLflow ] Check valid URL when creating predictor",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":"Can we close this @paxcema and @ea-rus  I think we need to merge the above PR after checking there are no conflicts (because it's a bit outdated by now), but once merged we can close this issue.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"close paxcema ru merg conflict bit outdat merg close",
        "Solution_preprocessed_content":"close merg conflict merg close",
        "Solution_readability":13.0,
        "Solution_reading_time":2.31,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":37.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0597273854,
        "Challenge_watch_issue_ratio":0.0810408922
    },
    {
        "Challenge_adjusted_solved_time":1488.5780555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Challenge_closed_time":1651060598000,
        "Challenge_created_time":1645701717000,
        "Challenge_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":9.2,
        "Challenge_reading_time":9.28,
        "Challenge_repo_contributor_count":148.0,
        "Challenge_repo_fork_count":956.0,
        "Challenge_repo_issue_count":3383.0,
        "Challenge_repo_star_count":6165.0,
        "Challenge_repo_watch_count":89.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":1488.5780555556,
        "Challenge_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":83,
        "Platform":"Github",
        "Solution_body":"fixed by https:\/\/github.com\/deepset-ai\/haystack\/pull\/2337",
        "Solution_gpt_summary":"pull request github repositori haystack librari involv remov logger disabl function initi inferenc addit",
        "Solution_link_count":1.0,
        "Solution_original_content":"http github com deepset haystack pull",
        "Solution_preprocessed_content":null,
        "Solution_readability":21.0,
        "Solution_reading_time":0.81,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0437481525,
        "Challenge_watch_issue_ratio":0.0263080106
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":14,
        "Challenge_body":"Trying to integrate Mlflow with my current bentoml workflow and following this example\r\n`https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch`\r\nBut getting error when i try to deploy model with docker, \r\n\r\nwhen i run  `bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\n```Building docker image for Bento(tag=\"mlflow_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")...\r\nERROR: failed to solve: executor failed running [\/bin\/sh -c bash <<EOF\r\nset -euxo pipefail\r\n\r\nif [ -f \/home\/bentoml\/bento\/env\/conda\/environment.yml ]; then\r\n   set pip_interop_enabled to improve conda-pip interoperability. Conda can use\r\n   pip-installed packages to satisfy dependencies.\r\n  echo \"Updating conda base environment with environment.yml\"\r\n  \/opt\/conda\/bin\/conda config --set pip_interop_enabled True\r\n  \/opt\/conda\/bin\/conda env update -n base -f \/home\/bentoml\/bento\/env\/conda\/environment.yml\r\n  \/opt\/conda\/bin\/conda clean --all\r\nfi\r\nEOF]: exit code: 1\r\nFailed building docker image: Command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', 'mlflow_pytfile', 'env\\\\docker\\\\Dockerfile', '--load', '.']' returned non-zero exit status 1.\r\n```\r\n### To reproduce\r\n\r\nBug recreation steps:\r\nClone the repo `https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch` \r\nGoto the folder `examples\/mlflow\/pytorch`\r\n`python mnist.py`\r\n`bentoml build`\r\n`bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\nP.S. `bentoml serve service.py:svc`  works fine`\r\n\r\n\r\n### Environment\r\n\r\nbentoml version 1.0.7\r\nPython version  3.9.12\r\nDocker Engine 20.10.17",
        "Challenge_closed_time":null,
        "Challenge_created_time":1666803000000,
        "Challenge_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/3146",
        "Challenge_link_count":2,
        "Challenge_open_time":679.1666666667,
        "Challenge_readability":12.8,
        "Challenge_reading_time":20.29,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":498.0,
        "Challenge_repo_issue_count":3155.0,
        "Challenge_repo_star_count":4302.0,
        "Challenge_repo_watch_count":60.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"bug: failed to containerize when using mlflow",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":156,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0427892235,
        "Challenge_watch_issue_ratio":0.0190174326
    },
    {
        "Challenge_adjusted_solved_time":341.4130555556,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nI can't load a *mlflow* model in the beta version of BentoML 1.0\r\n\r\n**To Reproduce**\r\n1. Train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport mlflow\r\n\r\n\r\n# Load training data\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Model Training\r\nclf = svm.SVC()\r\nclf.fit(X, y)\r\n\r\n# Wrap up as a custom pyfunc model\r\nclass ModelPyfunc(mlflow.pyfunc.PythonModel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# Log model\r\nwith mlflow.start_run() as run:\r\n    model = ModelPyfunc()\r\n    mlflow.pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. Load it into BentoML\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml.mlflow.import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml.mlflow.load(tag)\r\n```\r\n3. The model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml.mlflow.load(tag)` is failing to **AttributeError** `module 'mlflow.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**Expected behavior**\r\nPyfunc model should load without issues\r\n\r\n**Screenshots\/Logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"sandbox.py\", line 11, in <module>\r\n    bentoml.mlflow.load(tag)\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/mlflow.py\", line 85, in load\r\n    return loader_module.load_model(mlflow_folder)  # noqa\r\nAttributeError: module 'mlflow.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**Environment:**\r\n - OS: MacOS 11.6\r\n - Python Version Python 3.8.5\r\n - BentoML Version BentoML-1.0.0a1",
        "Challenge_closed_time":1642711286000,
        "Challenge_created_time":1641482199000,
        "Challenge_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/2160",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.7,
        "Challenge_reading_time":24.44,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":498.0,
        "Challenge_repo_issue_count":3155.0,
        "Challenge_repo_star_count":4302.0,
        "Challenge_repo_watch_count":60.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":341.4130555556,
        "Challenge_title":"MLflow pyfunc model can't be loaded",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":187,
        "Platform":"Github",
        "Solution_body":"I think this version is not yet up-to-date with our 1.0 branch. cc @parano Line 85 is different from `main` branch @alexdivet @aarnphm the new 1.0.0a2 was just released, could you help confirm the issue has been resolved? ![Screenshot 2022-01-20 at 15 39 52](https:\/\/user-images.githubusercontent.com\/29749331\/150418600-d75d6b01-679d-4fa7-9a66-395262c13569.png)\r\nWorks just fine for me\r\nRunning on M1 Max, Python 3.9.10, with Rosetta 2. Please let me know if you still run into problems @alexdivet It works on my end too. Thanks for resolving it \ud83d\ude4f\ud83c\udffb",
        "Solution_gpt_summary":"version date branch version releas rosetta end",
        "Solution_link_count":1.0,
        "Solution_original_content":"version date branch parano line branch alexdivet aarnphm releas screenshot http imag githubusercont com ddb png run rosetta run alexdivet end",
        "Solution_preprocessed_content":"version branch line branch releas run rosetta run end",
        "Solution_readability":4.8,
        "Solution_reading_time":6.86,
        "Solution_score_count":3.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":79.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0427892235,
        "Challenge_watch_issue_ratio":0.0190174326
    },
    {
        "Challenge_adjusted_solved_time":560.8433333333,
        "Challenge_answer_count":3,
        "Challenge_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Challenge_closed_time":1669124369000,
        "Challenge_created_time":1667105333000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_readability":8.4,
        "Challenge_reading_time":37.27,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":73,
        "Challenge_solved_time":560.8433333333,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":296,
        "Platform":"Github",
        "Solution_body":"In addition, runs of `compare_models `and `create_model` get nested recursively as well in pycaret > 2.3.6.\r\nSee screenshot below where the red line shows the behaviour in pycaret==2.3.6 (which is the wanted and expected behaviour) and in orange the nested unwanted behaviour in pycaret 2.3.8 , 2.3.9 and 2.3.10\r\n![image](https:\/\/user-images.githubusercontent.com\/50994394\/200543740-0883c8ba-9f4a-4d1f-8abc-560ae7dbd54e.png)\r\n @nagamatz @tdekelver-bd This issue was recently fixed on last rc release. Can you try installing pycaret with `pip install --pre pycaret` and let me know if you still face the issue. It's not yet fixed with 3.0.0rc4. This is the results with the code in the first post. Now, mlflow is 2.0.1\r\n![\u7121\u984c](https:\/\/user-images.githubusercontent.com\/1991802\/206426156-4b19a4cc-b865-4af3-8281-1b89fc099f28.png)\r\n",
        "Solution_gpt_summary":"releas pycaret instal pycaret pip instal pre pycaret",
        "Solution_link_count":2.0,
        "Solution_original_content":"addit run compar model creat model nest recurs pycaret screenshot red line pycaret orang nest unwant pycaret imag http imag githubusercont com cba abc aedbd png nagamatz tdekelv releas instal pycaret pip instal pre pycaret http imag githubusercont com bacc bfcf png",
        "Solution_preprocessed_content":"addit run nest recurs pycaret screenshot red line orang nest unwant pycaret releas instal pycaret",
        "Solution_readability":7.7,
        "Solution_reading_time":10.56,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":101.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":60.5666666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Challenge_closed_time":1663775400000,
        "Challenge_created_time":1663557360000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Challenge_link_count":6,
        "Challenge_open_time":null,
        "Challenge_readability":14.4,
        "Challenge_reading_time":20.35,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":60.5666666667,
        "Challenge_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":145,
        "Platform":"Github",
        "Solution_body":"Cannot reproduce on master. Please try again with `pip install -U --pre pycaret` @nagamatz and reopen the issue if it persists.",
        "Solution_gpt_summary":"instal latest version pycaret pip instal pre pycaret persist reopen",
        "Solution_link_count":0.0,
        "Solution_original_content":"reproduc master pip instal pre pycaret nagamatz reopen persist",
        "Solution_preprocessed_content":"reproduc master reopen persist",
        "Solution_readability":6.2,
        "Solution_reading_time":1.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":21.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":0.6102777778,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Challenge_closed_time":1660653306000,
        "Challenge_created_time":1660651109000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":0.6102777778,
        "Challenge_title":"MlFlow not logging metrics",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":161,
        "Platform":"Github",
        "Solution_body":"Update:\r\n\r\nWhen I write `exp.get_logs()` I can see some metrics there. But some runs still have the status \"RUNNING\", unsure why.\r\n\r\nAlso, all the runs that exist when I start the server using `!mlflow ui` are missing metrics. Edit: found issue, not on you! Sorry :)",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"updat write exp log metric run statu run run start server miss metric edit sorri",
        "Solution_preprocessed_content":"updat write metric run statu run run start server miss metric edit sorri",
        "Solution_readability":2.7,
        "Solution_reading_time":3.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":45.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":3142.3483333333,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nmlflow logs the name of both models \"Least Angle Regression\" and \"Lasso Least Angle Regression\" as \"Least Angle Regression\".\r\n\r\nWhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags.mlflow.runName`.\r\n\r\nPython Version: 3.9.5\r\nPyCaret Version: '3.0.0.rc3'\r\nPandas Version: 1.4.3\r\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nEXPERIMENT_NAME = 'diamond_experiment'\r\ns = setup(data=dataset, target='Price', log_experiment=True, experiment_name=EXPERIMENT_NAME, session_id=42, verbose=True)\r\n\r\nmodel = compare_models(verbose=False)\r\n\r\nprint(f\"Notice Least Angle Regression is not unique:\\n{get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'].value_counts()}\")\r\n\r\n# Loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# There should be a single unique value for each model\r\nfor model in pull().Model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == model])}\")\r\n\r\n# Further investigation: model Least Angle Regression has 2 instances (should be Lasso Least Angle Regression and Least Angle Regression)\r\nget_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == 'Least Angle Regression']\r\n```\n```\n\n\n### Expected Behavior\n\n`tags.mlflow.runName` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### Actual Results\n\n```python-traceback\nWhen looking into the `tags.mlflow.runName` you can see they are all unique but Least Angle Regression is there twice and Lasso Least Angle Regression isn't there at all. Could this be logged incorrectly?\r\n\r\nGradient Boosting Regressor - 1\r\nCatBoost Regressor - 1\r\nLight Gradient Boosting Machine - 1\r\nExtreme Gradient Boosting - 1\r\nLasso Regression - 1\r\nRidge Regression - 1\r\nLinear Regression - 1\r\nLasso Least Angle Regression - 0\r\nLeast Angle Regression - 2\r\nExtra Trees Regressor - 1\r\nRandom Forest Regressor - 1\r\nAdaBoost Regressor - 1\r\nDecision Tree Regressor - 1\r\nOrthogonal Matching Pursuit - 1\r\nElastic Net - 1\r\nHuber Regressor - 1\r\nBayesian Ridge - 1\r\nK Neighbors Regressor - 1\r\nDummy Regressor - 1\r\nPassive Aggressive Regressor - 1\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: PATH_TO_ENV\/venv\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1670522892000,
        "Challenge_created_time":1659210438000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2811",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":11.4,
        "Challenge_reading_time":47.39,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":49,
        "Challenge_solved_time":3142.3483333333,
        "Challenge_title":"[BUG]: mlflow incorrectly logging models \"Lasso Least Angle Regression\" and \"Least Angle Regression\"",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":420,
        "Platform":"Github",
        "Solution_body":"@Yard1 Can you give me a hand here? I ended up spending a lot of time in figuring out where is it coming from. The names inside `containers\/regression.py` seems to be fine but even then the run name is wrong.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/204138706-db0d0cc3-9a08-46d3-8037-fbaee414876b.png)\r\n\r\nAny ideas?",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"yard hand end spend time figur come insid regress run imag http imag githubusercont com dbdcc fbaeeb png idea",
        "Solution_preprocessed_content":"hand end spend time figur come insid run idea",
        "Solution_readability":6.6,
        "Solution_reading_time":4.25,
        "Solution_score_count":0.0,
        "Solution_sentence_count":5.0,
        "Solution_word_count":43.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":2889.2111111111,
        "Challenge_answer_count":2,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nWe have been using pycaret 2.2 for the model training procedure and registration to the mlflow server. (python based) My company uses a managed version of this in Azure Databricks. After the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. We would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. This is not working in pycaret and appears to be a bug of some sort. It is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. \n\n### Reproducible Example\n\n```python\n# import mlflow.sklearn\r\n# model = mlflow.sklearn.load_model(production_algorithm)\r\n# model.predict_prob(X)\n```\n\n\n### Expected Behavior\n\nwe should see the probabilities model.predict_prob(X) but this code errors out. Another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.DataFrame(X))\r\n\n\n### Actual Results\n\n```python-traceback\nthe end result of the prediction should be a numeric value between 0 and 1. Ex. 0.4278\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python\r\n   machine: Linux-5.4.0-1083-azure-x86_64-with-glibc2.29\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.0.1\r\n          setuptools: 52.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 7.22.0\r\n          ipywidgets: 7.7.1\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.6.2\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.8.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.1\r\n            requests: 2.28.1\r\n          matplotlib: 3.4.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.12.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.4\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Challenge_closed_time":1669247416000,
        "Challenge_created_time":1658846256000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2801",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":32.33,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":48,
        "Challenge_solved_time":2889.2111111111,
        "Challenge_title":"[BUG]: pycaret + mlflow integration does not allow probabilities for classification and binary response models",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":316,
        "Platform":"Github",
        "Solution_body":"@DerekKane We need more details than `code error out` to be able to help. Just tested this scenario with latest pycaret==3.0.0rc4 and it works fine.\r\n\r\nIf you are still facing issue, please feel free to open new ticket.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"derekkan test latest pycaret free open ticket",
        "Solution_preprocessed_content":"test latest free open ticket",
        "Solution_readability":3.5,
        "Solution_reading_time":2.63,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":38.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":28.55,
        "Challenge_answer_count":1,
        "Challenge_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Challenge_closed_time":1653501835000,
        "Challenge_created_time":1653399055000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":13.0,
        "Challenge_reading_time":21.61,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":28.55,
        "Challenge_title":"mlflow ui doesn't show any models",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":167,
        "Platform":"Github",
        "Solution_body":"Creating a clean env and installing pycaret again solved the issue.",
        "Solution_gpt_summary":"creat clean environ reinstal pycaret",
        "Solution_link_count":0.0,
        "Solution_original_content":"creat clean env instal pycaret",
        "Solution_preprocessed_content":"creat clean env instal pycaret",
        "Solution_readability":6.4,
        "Solution_reading_time":0.84,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":11.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":35.6138888889,
        "Challenge_answer_count":5,
        "Challenge_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Challenge_closed_time":1650470329000,
        "Challenge_created_time":1650342119000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.8,
        "Challenge_reading_time":1.48,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":35.6138888889,
        "Challenge_title":"plots are not saving through MLFLOW",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":18,
        "Platform":"Github",
        "Solution_body":"@akashg116414 We can't help you with this. Can you please describe a little bit more, show the code, show the complete error, etc. this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\nexample:\r\n\r\n```python\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('juice')\r\nfrom pycaret.classification import *\r\nclf1 = setup(data, target = 'Purchase', session_id=123, log_experiment=True, experiment_name='juice1',log_plots=True)\r\n``` > this error happens when i install pycaret[full]==2.3.9 and pycaret-ts-alpha both and tried basic classification example\r\n\r\nAre you installing both of them in the same environment? That should not be done as there may be conflicts at this time (until we officially release the 3.0.0 pycaret version which will have time-series integrated with the main pycaret package).\r\n\r\nTry installing one of them in a fresh (clean) environment and see if it works. @akashg116414 By the way, I am not able to recreate the issue with the information (code) you have provided. It works fine for me (see attached notebook below).\r\n\r\nhttps:\/\/gist.github.com\/ngupta23\/f7f33a5361928cac2f36f855f7398c88\r\n\r\nPlease provide a completely reproducible example that shows the steps to get to the error you are getting. Since we are unable to recreate this and information seems to be missing, we will close this for now. Feel free to create a new issue. We add a new Bug template that will guide you through the process of submitting a reproducible example. \r\n\r\nThanks!",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"akashg littl bit complet instal pycaret pycaret alpha tri classif pycaret dataset import data data data juic pycaret classif import clf setup data target purchas session log juic log plot instal pycaret pycaret alpha tri classif instal environ conflict time offici releas pycaret version time seri integr pycaret packag instal fresh clean environ akashg recreat attach notebook http gist github com ngupta ffacacfffc complet reproduc step recreat miss close free creat add templat guid process submit reproduc",
        "Solution_preprocessed_content":"littl bit complet instal tri classif instal tri classif instal environ conflict time instal fresh environ recreat complet reproduc step recreat miss close free creat add templat guid process submit reproduc",
        "Solution_readability":8.6,
        "Solution_reading_time":19.02,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":219.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":47.865,
        "Challenge_answer_count":6,
        "Challenge_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1650449956000,
        "Challenge_created_time":1650277642000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":47.865,
        "Challenge_title":"[BUG] mlflow ui never runs",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"@maverick-scientist there is not enough information here to recreate or debug the issue. Please provide a complete reproducible example so we can debug. Also, note that if the data is proprietary, you can create a fake dataset yourself to recreate the issue. Refer to the following for more details:\r\n\r\n\ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\r\n\ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\r\n\r\n- The do's and dont's have an example of how to create the fake data - minimal to reproduce the problem.\r\n- Alternately, you could try to reproduce the issue with a publicly available dataset or one available in pycaret itself.\r\n\r\nThanks!\r\n Hi Nikhil,\nCan we please discuss this over teams call? I can share a python file with\nyou for this issue but I feel if we can discuss this on a call it would\nsave me some time.\n\nWhat do you think?\n\nPlease advise.\n\nOn Mon, 18 Apr 2022 at 22:55, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> there is not\n> enough information here to recreate or debug the issue. Please provide a\n> complete reproducible example so we can debug. Also, note that if the data\n> is proprietary, you can create a fake dataset yourself to recreate the\n> issue. Refer to the following for more details:\n>\n> \ud83d\udccc Overview: http:\/\/ow.ly\/LoIv50IL5RQ\n> \ud83d\udccc Examples (Do's and Dont's): http:\/\/ow.ly\/AXKm50IL5RR\n>\n>    - The do's and dont's have an example of how to create the fake data -\n>    minimal to reproduce the problem.\n>    - Alternately, you could try to reproduce the issue with a publicly\n>    available dataset or one available in pycaret itself.\n>\n> Thanks!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1101586641>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRACMIA55LOVAGIZZKPLVFWLKHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n @maverick-scientist Honestly, I would prefer not to do that since it sets the wrong precedent for the open-source community and is not sustainable in the long run. The globally accepted best practice is to provide a minimal reproducible example and I would encourage you to do that.\r\n\r\nThanks! Hi Nikhil,\r\nAttaching the code file to reproduce this issue. Could you please check and tell me what's wrong with it or my machine?\r\n\r\nThanks & Regards,\r\nAbhinav\r\n\r\n[Simple MLflow.zip](https:\/\/github.com\/pycaret\/pycaret\/files\/8511837\/Simple.MLflow.zip)\r\n\r\n @maverick-scientist The example that you posted has no reference to pycaret. It is a generic MLFlow example. How is it related to pycaret and this repo? Hi Nikhil,\nThank you for your reply. However, if you read the issue carefully, I\u2019d\nclearly mentioned my dilemma whether it was the right forum to post this\nissue.\n\nAnyways, thanks for your support. I\u2019d try and see what\u2019s preventing the\nMLFlow to run properly on my machine.\n\nOn Wed, 20 Apr 2022 at 15:21, Nikhil Gupta ***@***.***> wrote:\n\n> @maverick-scientist <https:\/\/github.com\/maverick-scientist> The example\n> that you posted has no reference to pycaret. It is a generic MLFlow\n> example. How is it related to pycaret and this repo?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https:\/\/github.com\/pycaret\/pycaret\/issues\/2425#issuecomment-1103727978>,\n> or unsubscribe\n> <https:\/\/github.com\/notifications\/unsubscribe-auth\/AUVHRADHBVLOBH4SACXZHNLVF7HTHANCNFSM5TVQ4MTQ>\n> .\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":11.0,
        "Solution_original_content":"maverick scientist recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview http loivilrq http axkmilrr creat fake data minim reproduc reproduc publicli dataset pycaret nikhil team share file save time mon apr nikhil gupta wrote maverick scientist recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview http loivilrq http axkmilrr creat fake data minim reproduc reproduc publicli dataset pycaret repli email directli github unsubscrib receiv messag maverick scientist honestli prefer set preced open sourc commun sustain run global accept practic minim reproduc encourag nikhil attach file reproduc abhinav zip http github com pycaret pycaret file zip maverick scientist pycaret gener relat pycaret repo nikhil repli read carefulli clearli dilemma forum anywai what prevent run properli wed apr nikhil gupta wrote maverick scientist pycaret gener relat pycaret repo repli email directli github unsubscrib receiv messag",
        "Solution_preprocessed_content":"recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview creat fake data minim reproduc reproduc publicli dataset pycaret nikhil team share file save time mon apr nikhil gupta wrote recreat debug complet reproduc debug note data proprietari creat fake dataset recreat overview creat fake data minim reproduc reproduc publicli dataset pycaret repli email directli github unsubscrib receiv honestli prefer set preced commun sustain run global accept practic minim reproduc encourag nikhil attach file reproduc abhinav pycaret gener relat pycaret repo nikhil repli read carefulli clearli dilemma forum anywai what prevent run properli wed apr nikhil gupta wrote pycaret gener relat pycaret repo repli email directli github unsubscrib receiv",
        "Solution_readability":7.7,
        "Solution_reading_time":43.39,
        "Solution_score_count":0.0,
        "Solution_sentence_count":42.0,
        "Solution_word_count":477.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":276.9580555556,
        "Challenge_answer_count":0,
        "Challenge_body":"if in setup log_plot set True then it is giving error in self._mlflow_log_model() as \r\nfor plot in log_plots:\r\nTypeError: 'bool' object is not iterable",
        "Challenge_closed_time":1635811087000,
        "Challenge_created_time":1634814038000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1736",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":276.9580555556,
        "Challenge_title":"[BUG] Issue with Mlflow Timeseries_beta branch",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":29,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":375.8113888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Challenge_closed_time":1635405096000,
        "Challenge_created_time":1634052175000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":9.7,
        "Challenge_reading_time":27.37,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":375.8113888889,
        "Challenge_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":268,
        "Platform":"Github",
        "Solution_body":"@ejohnson-amerilife Thank you so much for bringing this up. Would you like to submit a PR for this? ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.3,
        "Solution_reading_time":1.2,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":3487.3783333333,
        "Challenge_answer_count":5,
        "Challenge_body":"I'm using clustering module of pycaret and the integration with mlflow but I have problems because I think it doesn't save all artifacs and the status is always failed.\r\n![image](https:\/\/user-images.githubusercontent.com\/12554263\/101971863-66f70d00-3c02-11eb-9710-01cf228fca1b.png)\r\n\r\nThis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=True,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=True,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=True,\r\n                     log_plots=True,\r\n                     log_profile=True,\r\n                     log_data=True,\r\n                     profile=False,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# Create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\nMy logs are the following\r\n\r\n```\r\n2020-12-11 22:39:07,118:INFO:PyCaret Supervised Module\r\n2020-12-11 22:39:07,118:INFO:ML Usecase: clustering\r\n2020-12-11 22:39:07,118:INFO:version 2.2.0\r\n2020-12-11 22:39:07,118:INFO:Initializing setup()\r\n2020-12-11 22:39:07,119:INFO:setup(target=None, ml_usecase=clustering, available_plots={'cluster': 'Cluster PCA Plot (2d)', 'tsne': 'Cluster TSnE (3d)', 'elbow': 'Elbow', 'silhouette': 'Silhouette', 'distance': 'Distance', 'distribution': 'Distribution'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=mode, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=['avg_dias_bancos_3m', 'avg_dias_app_pagos_3m', 'avg_dias_viajes_3m', 'avg_dias_compras_3m', 'avg_dias_mb_total_3m', 'avg_mb_total_3m', 'avg_q_apps_3m', 'ate_wh_sum_dias_3m', 'LEADs_tot_3m', 'tot_dias_appmov_movil_3m', 'avg_days_out_voice_tot_3m', 'meses_pagodig_3m'], numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['periodo', 'telefono', 'anexo', 'tot_dias_appmov_fija_3m', 'avg_dias_vid_mus_3m'], normalize=True, normalize_method=robust, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=False, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=kfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=True, session_id=123, log_experiment=True, experiment_name=pospatid_segmentation, log_plots=['cluster', 'distribution', 'elbow'], log_profile=True, log_data=True, silent=False, verbose=True, profile=False, display=None)\r\n2020-12-11 22:39:07,119:INFO:Checking environment\r\n2020-12-11 22:39:07,119:INFO:python_version: 3.8.5\r\n2020-12-11 22:39:07,119:INFO:python_build: ('default', 'Aug  5 2020 09:44:06')\r\n2020-12-11 22:39:07,119:INFO:machine: AMD64\r\n2020-12-11 22:39:07,120:INFO:platform: Windows-10-10.0.18362-SP0\r\n2020-12-11 22:39:07,121:WARNING:cannot find psutil installation. memory not traceable. Install psutil using pip to enable memory logging.\r\n2020-12-11 22:39:07,122:INFO:Checking libraries\r\n2020-12-11 22:39:07,122:INFO:pd==1.1.4\r\n2020-12-11 22:39:07,122:INFO:numpy==1.19.4\r\n2020-12-11 22:39:07,122:INFO:sklearn==0.23.2\r\n2020-12-11 22:39:07,156:INFO:xgboost==1.2.0\r\n2020-12-11 22:39:07,156:INFO:lightgbm==3.0.0\r\n2020-12-11 22:39:07,170:INFO:catboost==0.24.1\r\n2020-12-11 22:39:07,901:INFO:mlflow==1.11.0\r\n2020-12-11 22:39:07,901:INFO:Checking Exceptions\r\n2020-12-11 22:39:07,901:INFO:Declaring global variables\r\n2020-12-11 22:39:07,901:INFO:USI: cd5c\r\n2020-12-11 22:39:07,901:INFO:pycaret_globals: {'_available_plots', 'master_model_container', 'display_container', 'imputation_classifier', 'logging_param', 'seed', 'transform_target_param', 'experiment__', 'transform_target_method_param', 'iterative_imputation_iters_param', 'fold_groups_param', 'fix_imbalance_param', 'prep_pipe', 'exp_name_log', '_all_metrics', 'html_param', '_ml_usecase', 'USI', 'imputation_regressor', 'stratify_param', 'fold_generator', 'fix_imbalance_method_param', '_all_models', 'gpu_param', 'target_param', '_gpu_n_jobs_param', 'log_plots_param', 'pycaret_globals', 'fold_shuffle_param', '_all_models_internal', 'fold_param', 'create_model_container', 'data_before_preprocess', '_internal_pipeline', 'X', 'n_jobs_param'}\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,914:INFO:Importing libraries\r\n2020-12-11 22:39:07,914:INFO:Copying data for preprocessing\r\n2020-12-11 22:39:07,927:INFO:Declaring preprocessing parameters\r\n2020-12-11 22:39:07,940:INFO:Creating preprocessing pipeline\r\n2020-12-11 22:39:08,059:INFO:Preprocessing pipeline created successfully\r\n2020-12-11 22:39:08,060:ERROR:(Process Exit): setup has been interupted with user command 'quit'. setup must rerun.\r\n2020-12-11 22:39:08,060:INFO:Creating global containers\r\n2020-12-11 22:39:08,061:INFO:Internal pipeline: Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\r\n2020-12-11 22:39:10,064:INFO:Creating grid variables\r\n2020-12-11 22:39:10,101:INFO:Logging experiment in MLFlow\r\n2020-12-11 22:39:10,108:WARNING:Couldn't create mlflow experiment. Exception:\r\n2020-12-11 22:39:10,185:WARNING:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 1668, in setup\r\n    mlflow.create_experiment(exp_name_log)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 365, in create_experiment\r\n    return MlflowClient().create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 184, in create_experiment\r\n    return self._tracking_client.create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 142, in create_experiment\r\n    return self.store.create_experiment(name=name, artifact_location=artifact_location,)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 288, in create_experiment\r\n    self._validate_experiment_name(name)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 281, in _validate_experiment_name\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Experiment 'pospatid_segmentation' already exists.\r\n\r\n2020-12-11 22:39:10,490:INFO:SubProcess save_model() called ==================================\r\n2020-12-11 22:39:10,501:INFO:Initializing save_model()\r\n2020-12-11 22:39:10,501:INFO:save_model(model=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), model_name=Transformation Pipeline, prep_pipe_=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), verbose=False)\r\n2020-12-11 22:39:10,501:INFO:Adding model into prep_pipe\r\n2020-12-11 22:39:10,506:WARNING:Only Model saved as it was a pipeline.\r\n2020-12-11 22:39:10,530:INFO:Transformation Pipeline.pkl saved in current working directory\r\n2020-12-11 22:39:10,535:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:39:10,535:INFO:save_model() succesfully completed......................................\r\n2020-12-11 22:39:10,536:INFO:SubProcess save_model() end ==================================\r\n2020-12-11 22:40:03,332:INFO:create_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:master_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:display_container: 0\r\n2020-12-11 22:40:03,336:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:40:03,336:INFO:setup() succesfully completed......................................\r\n2020-12-11 22:40:07,628:INFO:Initializing create_model()\r\n2020-12-11 22:40:07,628:INFO:create_model(estimator=kmeans, num_clusters=6, fraction=0.05, ground_truth=None, round=4, fit_kwargs=None, verbose=True, system=True, raise_num_clusters=False, display=None, kwargs={})\r\n2020-12-11 22:40:07,628:INFO:Checking exceptions\r\n2020-12-11 22:40:07,629:INFO:Preparing display monitor\r\n2020-12-11 22:40:07,645:INFO:Importing libraries\r\n2020-12-11 22:40:07,652:INFO:Importing untrained model\r\n2020-12-11 22:40:07,662:INFO:K-Means Clustering Imported succesfully\r\n2020-12-11 22:40:07,670:INFO:Fitting Model\r\n2020-12-11 22:42:30,467:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:42:30,467:INFO:create_models() succesfully completed......................................\r\n2020-12-11 22:42:30,467:INFO:Creating MLFlow logs\r\n2020-12-11 22:42:30,481:INFO:Model: K-Means Clustering\r\n2020-12-11 22:42:30,518:INFO:logged params: {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 6, 'n_init': 10, 'n_jobs': -1, 'precompute_distances': 'deprecated', 'random_state': 123, 'tol': 0.0001, 'verbose': 0}\r\n2020-12-11 22:42:30,557:INFO:SubProcess plot_model() called ==================================\r\n2020-12-11 22:42:30,557:INFO:Initializing plot_model()\r\n2020-12-11 22:42:30,557:INFO:plot_model(plot=cluster, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:30,557:INFO:Checking exceptions\r\n2020-12-11 22:42:30,558:INFO:Preloading libraries\r\n2020-12-11 22:42:30,558:INFO:Copying training dataset\r\n2020-12-11 22:42:30,560:INFO:Plot type: cluster\r\n2020-12-11 22:42:31,493:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:31,494:INFO:Initializing assign_model()\r\n2020-12-11 22:42:31,494:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=True, score=True, verbose=False)\r\n2020-12-11 22:42:31,494:INFO:Checking exceptions\r\n2020-12-11 22:42:31,495:INFO:Determining Trained Model\r\n2020-12-11 22:42:31,495:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:31,495:INFO:Copying data\r\n2020-12-11 22:42:31,496:INFO:Transformation param set to True. Assigned clusters are attached on transformed dataset.\r\n2020-12-11 22:42:31,529:INFO:(90000, 12)\r\n2020-12-11 22:42:31,529:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:31,530:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:31,541:INFO:Fitting PCA()\r\n2020-12-11 22:42:31,908:INFO:Sorting dataframe\r\n2020-12-11 22:42:31,974:INFO:Rendering Visual\r\n2020-12-11 22:42:41,765:INFO:Saving 'Cluster PCA Plot (2d).html' in current active directory\r\n2020-12-11 22:42:41,765:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:42,286:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:42,739:INFO:Initializing plot_model()\r\n2020-12-11 22:42:42,739:INFO:plot_model(plot=distribution, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:42,739:INFO:Checking exceptions\r\n2020-12-11 22:42:42,739:INFO:Preloading libraries\r\n2020-12-11 22:42:42,739:INFO:Copying training dataset\r\n2020-12-11 22:42:42,741:INFO:Plot type: distribution\r\n2020-12-11 22:42:42,741:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:42,742:INFO:Initializing assign_model()\r\n2020-12-11 22:42:42,742:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=False, score=True, verbose=False)\r\n2020-12-11 22:42:42,742:INFO:Checking exceptions\r\n2020-12-11 22:42:42,742:INFO:Determining Trained Model\r\n2020-12-11 22:42:42,742:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:42,742:INFO:Copying data\r\n2020-12-11 22:42:42,793:INFO:(90000, 18)\r\n2020-12-11 22:42:42,793:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:42,794:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:42,794:INFO:Sorting dataframe\r\n2020-12-11 22:42:42,925:INFO:Rendering Visual\r\n2020-12-11 22:42:48,837:INFO:Saving 'Distribution.html' in current active directory\r\n2020-12-11 22:42:48,837:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:48,979:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:49,583:INFO:Initializing plot_model()\r\n2020-12-11 22:42:49,584:INFO:plot_model(plot=elbow, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:49,584:INFO:Checking exceptions\r\n2020-12-11 22:42:49,584:INFO:Preloading libraries\r\n2020-12-11 22:42:49,584:INFO:Copying training dataset\r\n2020-12-11 22:42:49,586:INFO:Plot type: elbow\r\n2020-12-11 22:42:49,690:INFO:Fitting Model\r\n2020-12-11 22:43:12,604:INFO:Saving 'Elbow.png' in current active directory\r\n2020-12-11 22:43:13,207:INFO:Visual Rendered Successfully\r\n2020-12-11 22:43:13,325:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:43:13,340:INFO:SubProcess plot_model() end ==================================\r\n2020-12-11 22:43:13,341:WARNING:Couldn't infer MLFlow signature.\r\n2020-12-11 22:43:13,352:ERROR:_mlflow_log_model() for KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0) raised an exception:\r\n2020-12-11 22:43:13,431:ERROR:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 2631, in create_model_unsupervised\r\n    _mlflow_log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 9942, in _mlflow_log_model\r\n    mlflow.sklearn.log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 290, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\model.py\", line 160, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 171, in save_model\r\n    _save_example(mlflow_model, input_example, path)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 131, in _save_example\r\n    example = _Example(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 67, in __init__\r\n    input_example = pd.DataFrame.from_dict(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 1309, in from_dict\r\n    return cls(data, index=index, columns=columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 468, in __init__\r\n    mgr = init_dict(data, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 283, in init_dict\r\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 78, in arrays_to_mgr\r\n    index = extract_index(arrays)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 387, in extract_index\r\n    raise ValueError(\"If using all scalar values, you must pass an index\")\r\nValueError: If using all scalar values, you must pass an index\r\n\r\n2020-12-11 22:43:13,432:INFO:Uploading results into container\r\n2020-12-11 22:43:13,435:INFO:Uploading model into container now\r\n2020-12-11 22:43:13,440:INFO:create_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:master_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:display_container: 1\r\n2020-12-11 22:43:13,440:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:43:13,440:INFO:create_model() succesfully completed......................................\r\n\r\n```\r\n\r\nI'm using Pycaret version : 2.2.0",
        "Challenge_closed_time":1620299767000,
        "Challenge_created_time":1607745205000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/931",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":25.9,
        "Challenge_reading_time":288.86,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":96,
        "Challenge_solved_time":3487.3783333333,
        "Challenge_title":"MLFlow doesn't save model artifact and some plots - Clustering",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":1212,
        "Platform":"Github",
        "Solution_body":"@DXcarlos I have tried to reproduce the error on `jewellery` dataset available on our GitHub repository and I couldn't reproduce this error.\r\n\r\nIs it possible for you to share the Notebook along with the dataset so we can reproduce the error and troubleshoot what is causing this?\r\n\r\nI am including @Yard1 in this thread to see if he can understand what's going on with the log file you shared above. Antoni, maybe something specific to the dataset.  @pycaret @Yard1 How can I share you privately? @DXcarlos You can private message on our Slack channel. If you are still not there, you can join using the following link:\r\n\r\nhttps:\/\/join.slack.com\/t\/pycaret\/shared_invite\/zt-kdoe7hee-yvNANPHXPM9VtK7R6Npx4Q\r\n\r\n Stale issue message @DXcarlos , we will close out this issue for now. Please feel free to reopen if you want.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"dxcarlo tri reproduc jewelleri dataset github repositori reproduc share notebook dataset reproduc troubleshoot yard thread log file share antoni mayb dataset pycaret yard share privat dxcarlo privat messag slack channel link http slack com pycaret share invit kdoehe yvnanphxpmvtkrnpxq stale messag dxcarlo close free reopen",
        "Solution_preprocessed_content":"tri reproduc dataset github repositori reproduc share notebook dataset reproduc troubleshoot thread log file share antoni mayb dataset share privat privat messag slack channel link stale messag close free reopen",
        "Solution_readability":7.6,
        "Solution_reading_time":9.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":128.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":24.5525,
        "Challenge_answer_count":9,
        "Challenge_body":"Hi. I just upgraded to pycaret 2.1. When I ran the compare_models function with the Titanic dataset, I got the following error:\r\n\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)\r\n\r\nThe same code worked fine in pycaret 2.0.",
        "Challenge_closed_time":1598806653000,
        "Challenge_created_time":1598718264000,
        "Challenge_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/566",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.9,
        "Challenge_reading_time":4.83,
        "Challenge_repo_contributor_count":93.0,
        "Challenge_repo_fork_count":1518.0,
        "Challenge_repo_issue_count":2643.0,
        "Challenge_repo_star_count":6633.0,
        "Challenge_repo_watch_count":124.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":24.5525,
        "Challenge_title":"Compare models MLFlowException",
        "Challenge_topic":"Remote Configuration",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":61,
        "Platform":"Github",
        "Solution_body":"@sagarnildass Hi, Thanks for reporting. This seems like an error from `MLFlow`. I tried to reproduce this out of `pycaret` and I was successful. See below code that throws an error:\r\n\r\n```\r\nimport pandas as pd\r\ndata = pd.read_csv('titanic.csv') #train data from Kaggle\r\nfrom mlflow.models.signature import infer_signature\r\ninfer_signature(data)\r\n```\r\nThis gives the following error:\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float).\r\n\r\nI will log an issue on MLFlow GitHub.\r\n\r\nHere is the issue I logged on MLFlow: https:\/\/github.com\/mlflow\/mlflow\/issues\/3362 Hey\r\n\r\nThanks for the quick reply. \r\n\r\nI found out that presence of null values are a problem. If the dataset contains null values, this error was raised. When I imputed the null values, this problem was solved. Can you also mention this when you log this issue?\r\n\r\nThanks! @sagarnildass Thanks. I have added that in my issue but I don't think so it's 100% True. I have worked with few missing datasets and it worked okay. For example, the `hepatitis` dataset on our repo works fine. Example code:\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('hepatitis')\r\nfrom pycaret.classification import *\r\ns = setup(data, target = 'Class', log_experiment=True, experiment_name = 'hepatitis1')\r\n```\r\n\r\nThis dataset has missing values but it just worked fine. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/58118658\/91642055-41991700-e9f6-11ea-9b6f-0f42a86401d9.png)\r\n\r\nCan you investigate more and add your comments on the original issue here: https:\/\/github.com\/mlflow\/mlflow\/issues\/3362\r\n\r\nThanks a lot for helping. @Yard1 I don't know how soon `MLFLow` will be able to fix this but in `2.2` we will have to create some kind of exceptions under `logging_param` chunks to not fail the process even when `infer_signature` fails, as it's not mandatory and has no impact other than the signature file that gets generated under `model` directory when `log_experiment` is set to `True`. @pycaret : I believe object datatypes are a problem. It clearly states: \"np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray), int, float)\". So do you think null values in a object datatype column might be the root  problem here? Because in hepatitis data, all the columns are numeric. @pycaret If we get MLFlow logging into a function then it will be easy to wrap it into a try except block.  @sagarnildass Thanks again for reporting. I am planning to do a bug fix release tomorrow `2.1.1`. For now, I have wrapped this inside `try` and `except` clause to avoid the error. I have tested it on the titanic dataset.\r\n\r\nCan you please sync the `master` and try to see if you can reproduce the error now?\r\n\r\nThanks Done...it's working as expected. @sagarnildass Thanks. I will publish the `2.1.1.` release today. @Yard1 FYI.\r\n\r\nThanks for your help @sagarnildass ",
        "Solution_gpt_summary":"report run compar model function titan dataset pycaret messag map object type datatyp valu ident data type byte byterrai log github presenc null valu object datatyp column root note miss dataset pycaret wrap insid claus avoid test titan dataset plan releas pycaret",
        "Solution_link_count":3.0,
        "Solution_original_content":"sagarnildass report tri reproduc pycaret throw import panda data read csv titan csv train data kaggl model signatur import infer signatur infer signatur data except map object type datatyp object canb map iff valu ident data type byte byterrai log github log http github com quick repli presenc null valu dataset null valu rais imput null valu log sagarnildass miss dataset okai hepat dataset repo pycaret dataset import data data data hepat pycaret classif import setup data target class log hepat dataset miss valu imag http imag githubusercont com fad png add comment origin http github com yard soon creat except log param chunk process infer signatur mandatori impact signatur file gener model directori log set pycaret believ object datatyp clearli state object canb map iff valu ident data type byte byterrai null valu object datatyp column root hepat data column numer pycaret log function wrap block sagarnildass report plan releas tomorrow wrap insid claus avoid test titan dataset sync master reproduc sagarnildass publish releas todai yard fyi sagarnildass",
        "Solution_preprocessed_content":"report creat except chunk process mandatori impact signatur file gener directori set believ object datatyp clearli state canb map iff valu ident data type null valu object datatyp column root hepat data column numer log function wrap block report plan releas tomorrow wrap insid claus avoid test titan dataset sync reproduc publish releas todai fyi",
        "Solution_readability":6.6,
        "Solution_reading_time":36.65,
        "Solution_score_count":0.0,
        "Solution_sentence_count":39.0,
        "Solution_word_count":449.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0351872872,
        "Challenge_watch_issue_ratio":0.0469163829
    },
    {
        "Challenge_adjusted_solved_time":1114.6180555556,
        "Challenge_answer_count":6,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Challenge_closed_time":1649939186000,
        "Challenge_created_time":1645926561000,
        "Challenge_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":13.3,
        "Challenge_reading_time":14.53,
        "Challenge_repo_contributor_count":47.0,
        "Challenge_repo_fork_count":181.0,
        "Challenge_repo_issue_count":2399.0,
        "Challenge_repo_star_count":2909.0,
        "Challenge_repo_watch_count":35.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1114.6180555556,
        "Challenge_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":81,
        "Platform":"Github",
        "Solution_body":"Hey @luisoala, thanks for reporting the issue!\r\n@devfox-se could you please take a look at this? Thanks for reporting this @luisoala, will take a look soon! Hey @luisoala! We've released `v3.6.2` containing the fix for mlflow converter. Please check it out and let me know if there are any issues. thanks @alberttorosyan working through a few other deadlines atm, aiming for a test ~ next tuesday, will share result here @luisoala Hi, have you had a chance to test this?:) Closing due to inactivity, feel free to reopen in case this still persists.",
        "Solution_gpt_summary":"releas version convert report",
        "Solution_link_count":0.0,
        "Solution_original_content":"luisoala report devfox report luisoala soon luisoala releas convert alberttorosyan deadlin atm ing test tuesdai share luisoala chanc test close inact free reopen persist",
        "Solution_preprocessed_content":"report report soon releas convert deadlin atm ing test tuesdai share chanc test close inact free reopen persist",
        "Solution_readability":4.2,
        "Solution_reading_time":6.68,
        "Solution_score_count":1.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":92.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0195914965,
        "Challenge_watch_issue_ratio":0.0145894123
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"### Operating System\n\nWindows\n\n### Version Information\n\nlatest cli v2 \n\n### Steps to reproduce\n\nhttps:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/cli\/endpoints\/online\/mlflow\/sklearn-deployment.yaml\r\n\r\nThis yaml is out of date, the model yaml config is wrong. \"name\" is no longer required when specifying model.\n\n### Expected behavior\n\nThat the deployment works based on sklearn-deployment.yml using the cli command `az create deployment`, but it fails. \n\n### Actual behavior\n\nIt fails.\n\n### Addition information\n\n_No response_",
        "Challenge_closed_time":null,
        "Challenge_created_time":1669043018000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1897",
        "Challenge_link_count":1,
        "Challenge_open_time":56.9394444444,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.9,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow cli endpoint example out of date with new syntax from breaking changes in yaml (last updated May 11)",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":78,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0687372709,
        "Challenge_watch_issue_ratio":1.4042769857
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"## Which example? Describe the issue\r\n\r\nexample: [CIFAR pytorch distributed](https:\/\/github.com\/Azure\/azureml-examples\/tree\/main\/cli\/jobs\/single-step\/pytorch\/cifar-distributed)\r\n\r\ndescription: model training shows completed, model is saved as well but driver logs (`70_driver_log..`.) for the model saving driver has:\r\n \r\n`ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: \/tmp\/tmpvthoxt0n\/model\/data, flavor: pytorch)\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/mlflow\/utils\/environment.py\", line 194, in infer_pip_requirements\r\n    return _infer_requirements(model_uri, flavor)\r\n  File \"\/azureml-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/mlflow\/utils\/requirements_utils.py\", line 306, in _infer_requirements\r\n    _MODULES_TO_PACKAGES = importlib_metadata.packages_distributions()\r\nAttributeError: module 'importlib_metadata' has no attribute 'packages_distributions'`\r\n\r\n## Additional context\r\n\r\nTried with variations to the environment in `job.yml: azureml:AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:11 and azureml:AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:6`. Same outcome. \r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1638959064000,
        "Challenge_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/937",
        "Challenge_link_count":1,
        "Challenge_open_time":8413.5933333333,
        "Challenge_readability":18.8,
        "Challenge_reading_time":16.56,
        "Challenge_repo_contributor_count":135.0,
        "Challenge_repo_fork_count":646.0,
        "Challenge_repo_issue_count":1964.0,
        "Challenge_repo_star_count":873.0,
        "Challenge_repo_watch_count":2758.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Mlflow error on pytorch.log_model but model is saved",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":96,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0687372709,
        "Challenge_watch_issue_ratio":1.4042769857
    },
    {
        "Challenge_adjusted_solved_time":20.83,
        "Challenge_answer_count":0,
        "Challenge_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Challenge_closed_time":1601658581000,
        "Challenge_created_time":1601583593000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":20.3,
        "Challenge_reading_time":69.38,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":46,
        "Challenge_solved_time":20.83,
        "Challenge_title":"mlflow.projects.run failing consistently with etag error ",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":338,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0296176629,
        "Challenge_watch_issue_ratio":1.0936995153
    },
    {
        "Challenge_adjusted_solved_time":0.4194444444,
        "Challenge_answer_count":0,
        "Challenge_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Challenge_closed_time":1581065545000,
        "Challenge_created_time":1581064035000,
        "Challenge_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":18.1,
        "Challenge_reading_time":9.23,
        "Challenge_repo_contributor_count":55.0,
        "Challenge_repo_fork_count":2291.0,
        "Challenge_repo_issue_count":1857.0,
        "Challenge_repo_star_count":3523.0,
        "Challenge_repo_watch_count":2031.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":0.4194444444,
        "Challenge_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":38,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0296176629,
        "Challenge_watch_issue_ratio":1.0936995153
    },
    {
        "Challenge_adjusted_solved_time":558.1344444444,
        "Challenge_answer_count":4,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Challenge_closed_time":1667718001000,
        "Challenge_created_time":1665708717000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.5,
        "Challenge_reading_time":92.22,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":558.1344444444,
        "Challenge_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":583,
        "Platform":"Github",
        "Solution_body":"I had the same problem TT Same for all the example in `benchmarks\/LightGBM`. This is because mlflow limits the length of params since 1.28.0.\r\nWhile waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution. > This is because mlflow limits the length of params since 1.28.0. While waiting the official qlib developers to find some way to accommodate this, downgrading mlflow to 1.27.0 can be a temp solution.\r\n\r\nThank you for help. Wish you have a good day.",
        "Solution_gpt_summary":"downgrad version temporari exceed length limit paramet valu offici qlib perman accommod",
        "Solution_link_count":0.0,
        "Solution_original_content":"benchmark lightgbm limit length param wait offici qlib accommod downgrad temp limit length param wait offici qlib accommod downgrad temp wish dai",
        "Solution_preprocessed_content":"limit length param wait offici qlib accommod downgrad temp limit length param wait offici qlib accommod downgrad temp wish dai",
        "Solution_readability":4.8,
        "Solution_reading_time":6.36,
        "Solution_score_count":4.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":89.0,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"when run workflow:\r\n```\r\nqrun ALSTM_workflow_config_alstm_Alpha158.yaml\r\n```  \r\nmlflow v1.27.0 work fine,but failed when with mlflow v1.28.0:\r\n```\r\nFile \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/pyqlib-0.8.6.99-py3.8-linux-x86_64.egg\/qlib\/workflow\/recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/client.py\", line 852, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/_tracking_service\/client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 778, which exceeded length limit of 500\r\n```\r\ni think the new mflow feature cause this bug.mlflow limit param valu lengh to 500,by read code ,it can not be overwrite.\r\nmaybe relate with this [issue](https:\/\/github.com\/mlflow\/mlflow\/commit\/d4109d00079355459a9a3df1821f0878877e42a8)\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1663557425000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1298",
        "Challenge_link_count":1,
        "Challenge_open_time":1580.7152777778,
        "Challenge_readability":11.8,
        "Challenge_reading_time":18.19,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"not compatible with mlflow v1.28.0",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":102,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"## \ud83d\udc1b Bug Description\r\nwhen I run the code below in qlib-main\/examples\/workflow_by_code.ipynb\uff0cit caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' \r\n###################################\r\n# train model\r\n###################################\r\ndata_handler_config = {\r\n    \"start_time\": \"2008-01-01\",\r\n    \"end_time\": \"2020-08-01\",\r\n    \"fit_start_time\": \"2008-01-01\",\r\n    \"fit_end_time\": \"2014-12-31\",\r\n    \"instruments\": market,\r\n}\r\n\r\ntask = {\r\n    \"model\": {\r\n        \"class\": \"LGBModel\",\r\n        \"module_path\": \"qlib.contrib.model.gbdt\",\r\n        \"kwargs\": {\r\n            \"loss\": \"mse\",\r\n            \"colsample_bytree\": 0.8879,\r\n            \"learning_rate\": 0.0421,\r\n            \"subsample\": 0.8789,\r\n            \"lambda_l1\": 205.6999,\r\n            \"lambda_l2\": 580.9768,\r\n            \"max_depth\": 8,\r\n            \"num_leaves\": 210,\r\n            \"num_threads\": 20,\r\n        },\r\n    },\r\n    \"dataset\": {\r\n        \"class\": \"DatasetH\",\r\n        \"module_path\": \"qlib.data.dataset\",\r\n        \"kwargs\": {\r\n            \"handler\": {\r\n                \"class\": \"Alpha158\",\r\n                \"module_path\": \"qlib.contrib.data.handler\",\r\n                \"kwargs\": data_handler_config,\r\n            },\r\n            \"segments\": {\r\n                \"train\": (\"2008-01-01\", \"2014-12-31\"),\r\n                \"valid\": (\"2015-01-01\", \"2016-12-31\"),\r\n                \"test\": (\"2017-01-01\", \"2020-08-01\"),\r\n            },\r\n        },\r\n    },\r\n}\r\n\r\n# model initiaiton\r\nmodel = init_instance_by_config(task[\"model\"])\r\ndataset = init_instance_by_config(task[\"dataset\"])\r\n\r\n# start exp to train model\r\nwith R.start(experiment_name=\"train_model\"):\r\n    R.log_params(**flatten_dict(task))\r\n    model.fit(dataset)\r\n    R.save_objects(trained_model=model)\r\n    rid = R.get_recorder().id\r\n\r\n=====================\r\nThe whole error message is below\uff1a\r\n[2607:MainThread](2022-04-06 17:38:12,377) INFO - qlib.timer - [log.py:113] - Time cost: 18.919s | Loading data Done\r\n[2607:MainThread](2022-04-06 17:38:12,737) INFO - qlib.timer - [log.py:113] - Time cost: 0.147s | DropnaLabel Done\r\n\/Users\/yzwu\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/data\/dataset\/processor.py:310: SettingWithCopyWarning: \r\nA value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_indexer,col_indexer] = value instead\r\n\r\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\r\n  df[cols] = df[cols].groupby(\"datetime\").apply(self.zscore_func)\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 1.650s | CSZScoreNorm Done\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 2.010s | fit & process data Done\r\n[2607:MainThread](2022-04-06 17:38:14,388) INFO - qlib.timer - [log.py:113] - Time cost: 20.930s | Init data Done\r\n[2607:MainThread](2022-04-06 17:38:14,399) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x2859099a0>\r\n[2607:MainThread](2022-04-06 17:38:14,402) WARNING - qlib.workflow - [expm.py:195] - No valid experiment found. Create a new experiment with name train_model.\r\n---------------------------------------------------------------------------\r\nMlflowException                           Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:391, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    390 try:\r\n--> 391     exp = self.client.get_experiment_by_name(experiment_name)\r\n    392     if exp is None or exp.lifecycle_stage.upper() == \"DELETED\":\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:462, in MlflowClient.get_experiment_by_name(self, name)\r\n    432 \"\"\"\r\n    433 Retrieve an experiment by experiment name from the backend store\r\n    434 \r\n   (...)\r\n    460     Lifecycle_stage: active\r\n    461 \"\"\"\r\n--> 462 return self._tracking_client.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:167, in TrackingServiceClient.get_experiment_by_name(self, name)\r\n    163 \"\"\"\r\n    164 :param name: The experiment name.\r\n    165 :return: :py:class:`mlflow.entities.Experiment`\r\n    166 \"\"\"\r\n--> 167 return self.store.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     67 \"\"\"\r\n     68 Fetch the experiment by name from the backend store.\r\n     69 This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74 :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75 \"\"\"\r\n---> 76 for experiment in self.list_experiments(ViewType.ALL):\r\n     77     if experiment.name == experiment_name:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    259 try:\r\n    260     # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261     experiment = self._get_experiment(exp_id, view_type)\r\n    262     if experiment:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    336 self._check_root_dir()\r\n--> 337 _validate_experiment_id(experiment_id)\r\n    338 experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:189, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    187 try:\r\n    188     return (\r\n--> 189         self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),\r\n    190         False,\r\n    191     )\r\n    192 except ValueError:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:397, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    396 except MlflowException as e:\r\n--> 397     raise ValueError(\r\n    398         \"No valid experiment has been found, please make sure the input experiment name is correct.\"\r\n    399     ) from e\r\n\r\nValueError: No valid experiment has been found, please make sure the input experiment name is correct.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMlflowException                           Traceback (most recent call last)\r\nInput In [6], in <cell line: 51>()\r\n     48 dataset = init_instance_by_config(task[\"dataset\"])\r\n     50 # start exp to train model\r\n---> 51 with R.start(experiment_name=\"train_model\"):\r\n     52     R.log_params(**flatten_dict(task))\r\n     53     model.fit(dataset)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/contextlib.py:113, in _GeneratorContextManager.__enter__(self)\r\n    111 del self.args, self.kwds, self.func\r\n    112 try:\r\n--> 113     return next(self.gen)\r\n    114 except StopIteration:\r\n    115     raise RuntimeError(\"generator didn't yield\") from None\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:69, in QlibRecorder.start(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     25 @contextmanager\r\n     26 def start(\r\n     27     self,\r\n   (...)\r\n     34     resume: bool = False,\r\n     35 ):\r\n     36     \"\"\"\r\n     37     Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\r\n     38 \r\n   (...)\r\n     67         whether to resume the specific recorder with given name under the given experiment.\r\n     68     \"\"\"\r\n---> 69     run = self.start_exp(\r\n     70         experiment_id=experiment_id,\r\n     71         experiment_name=experiment_name,\r\n     72         recorder_id=recorder_id,\r\n     73         recorder_name=recorder_name,\r\n     74         uri=uri,\r\n     75         resume=resume,\r\n     76     )\r\n     77     try:\r\n     78         yield run\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:125, in QlibRecorder.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     84 def start_exp(\r\n     85     self,\r\n     86     *,\r\n   (...)\r\n     92     resume=False,\r\n     93 ):\r\n     94     \"\"\"\r\n     95     Lower level method for starting an experiment. When use this method, one should end the experiment manually\r\n     96     and the status of the recorder may not be handled properly. Here is the example code:\r\n   (...)\r\n    123     An experiment instance being started.\r\n    124     \"\"\"\r\n--> 125     return self.exp_manager.start_exp(\r\n    126         experiment_id=experiment_id,\r\n    127         experiment_name=experiment_name,\r\n    128         recorder_id=recorder_id,\r\n    129         recorder_name=recorder_name,\r\n    130         uri=uri,\r\n    131         resume=resume,\r\n    132     )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:339, in MLflowExpManager.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n    337 if experiment_name is None:\r\n    338     experiment_name = self._default_exp_name\r\n--> 339 experiment, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)\r\n    340 # Set up active experiment\r\n    341 self.active_experiment = experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:202, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    200 if pr.scheme == \"file\":\r\n    201     with FileLock(os.path.join(pr.netloc, pr.path, \"filelock\")):  # pylint: disable=E0110\r\n--> 202         return self.create_exp(experiment_name), True\r\n    203 # NOTE: for other schemes like http, we double check to avoid create exp conflicts\r\n    204 try:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:362, in MLflowExpManager.create_exp(self, experiment_name)\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n    361         raise ExpAlreadyExistError() from e\r\n--> 362     raise e\r\n    364 experiment = MLflowExperiment(experiment_id, experiment_name, self.uri)\r\n    365 experiment._default_name = self._default_exp_name\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:358, in MLflowExpManager.create_exp(self, experiment_name)\r\n    356 # init experiment\r\n    357 try:\r\n--> 358     experiment_id = self.client.create_experiment(experiment_name)\r\n    359 except MlflowException as e:\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:507, in MlflowClient.create_experiment(self, name, artifact_location, tags)\r\n    464 def create_experiment(\r\n    465     self,\r\n    466     name: str,\r\n    467     artifact_location: Optional[str] = None,\r\n    468     tags: Optional[Dict[str, Any]] = None,\r\n    469 ) -> str:\r\n    470     \"\"\"Create an experiment.\r\n    471 \r\n    472     :param name: The experiment name. Must be unique.\r\n   (...)\r\n    505         Lifecycle_stage: active\r\n    506     \"\"\"\r\n--> 507     return self._tracking_client.create_experiment(name, artifact_location, tags)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:182, in TrackingServiceClient.create_experiment(self, name, artifact_location, tags)\r\n    179 _validate_experiment_name(name)\r\n    180 _validate_experiment_artifact_location(artifact_location)\r\n--> 182 return self.store.create_experiment(\r\n    183     name=name,\r\n    184     artifact_location=artifact_location,\r\n    185     tags=[ExperimentTag(key, value) for (key, value) in tags.items()] if tags else [],\r\n    186 )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:321, in FileStore.create_experiment(self, name, artifact_location, tags)\r\n    319 def create_experiment(self, name, artifact_location=None, tags=None):\r\n    320     self._check_root_dir()\r\n--> 321     self._validate_experiment_name(name)\r\n    322     # Get all existing experiments and find the one with largest numerical ID.\r\n    323     # len(list_all(..)) would not work when experiments are deleted.\r\n    324     experiments_ids = [\r\n    325         int(e.experiment_id)\r\n    326         for e in self.list_experiments(ViewType.ALL)\r\n    327         if e.experiment_id.isdigit()\r\n    328     ]\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:303, in FileStore._validate_experiment_name(self, name)\r\n    299 if name is None or name == \"\":\r\n    300     raise MlflowException(\r\n    301         \"Invalid experiment name '%s'\" % name, databricks_pb2.INVALID_PARAMETER_VALUE\r\n    302     )\r\n--> 303 experiment = self.get_experiment_by_name(name)\r\n    304 if experiment is not None:\r\n    305     if experiment.lifecycle_stage == LifecycleStage.DELETED:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     66 def get_experiment_by_name(self, experiment_name):\r\n     67     \"\"\"\r\n     68     Fetch the experiment by name from the backend store.\r\n     69     This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74     :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75     \"\"\"\r\n---> 76     for experiment in self.list_experiments(ViewType.ALL):\r\n     77         if experiment.name == experiment_name:\r\n     78             return experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    258 for exp_id in rsl:\r\n    259     try:\r\n    260         # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261         experiment = self._get_experiment(exp_id, view_type)\r\n    262         if experiment:\r\n    263             experiments.append(experiment)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    335 def _get_experiment(self, experiment_id, view_type=ViewType.ALL):\r\n    336     self._check_root_dir()\r\n--> 337     _validate_experiment_id(experiment_id)\r\n    338     experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n    339     if experiment_dir is None:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    265 \"\"\"Check that `experiment_id`is a valid string or None, raise an exception if it isn't.\"\"\"\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. just rerun the code in my envirment\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\nDarwin\r\narm64\r\nmacOS-12.2.1-arm64-arm-64bit\r\nDarwin Kernel Version 21.3.0: Wed Jan  5 21:37:58 PST 2022; root:xnu-8019.80.24~20\/RELEASE_ARM64_T6000\r\n\r\nPython version: 3.8.11 (default, Jul 29 2021, 14:57:32)  [Clang 12.0.0 ]\r\n\r\nQlib version: 0.8.4.99\r\nnumpy==1.22.3\r\npandas==1.4.2\r\nscipy==1.8.0\r\nrequests==2.25.1\r\nsacred==0.8.2\r\npython-socketio==5.5.2\r\nredis==4.2.2\r\npython-redis-lock==3.7.0\r\nschedule==1.1.0\r\ncvxpy==1.1.18\r\nhyperopt==0.1.2\r\nfire==0.4.0\r\nstatsmodels==0.13.2\r\nxlrd==2.0.1\r\nplotly==5.6.0\r\nmatplotlib==3.5.1\r\ntables==3.7.0\r\npyyaml==6.0\r\nmlflow==1.24.0\r\ntqdm==4.61.2\r\nloguru==0.6.0\r\nlightgbm==3.3.2\r\ntornado==6.1\r\njoblib==1.1.0\r\nfire==0.4.0\r\nruamel.yaml==0.17.21\r\n\r\n\r\n## Additional Notes\r\n\r\nI installed qlib from source, and my conda env is the version for arm64\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1649238776000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1035",
        "Challenge_link_count":1,
        "Challenge_open_time":5558.1177777778,
        "Challenge_readability":16.9,
        "Challenge_reading_time":205.01,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":169,
        "Challenge_solved_time":null,
        "Challenge_title":"run the example workflow_by_code.ipynb, caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' ",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":1288,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via setup.py and uninstalling \/ reinstalling numpy. but this one i do not know how to solve:\r\nMlflowException: Got invalid value Series([], dtype: float64) for metric 'IC' (timestamp=1616595157552). Please specify value as a valid double (64-bit floating point)\r\n\r\nif i have only sh000300 in my instruments, it's gonna produce the following value error:\r\nValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]).\r\nYou can drop duplicate edges by setting the 'duplicates' kwarg\r\n\r\ni followed instructions on data collector's markdown page to download cn data up until 03\/01\/2021. my yaml file looks like this:\r\nqlib_init:\r\n    provider_uri: \"\/content\/gdrive\/MyDrive\/qlib\/qlib_data\/qlib_cn_1d\"\r\n    region: cn\r\nmarket: &market \r\nbenchmark: &benchmark SH000300\r\ndata_handler_config: &data_handler_config\r\n    start_time: 2008-01-01\r\n    end_time: 2021-03-01\r\n    fit_start_time: 2008-01-01\r\n    fit_end_time: 2018-12-31\r\n    instruments: ['SH000300', 'SH000903']\r\nport_analysis_config: &port_analysis_config\r\n    strategy:\r\n        class: TopkDropoutStrategy\r\n        module_path: qlib.contrib.strategy.strategy\r\n        kwargs:\r\n            topk: 50\r\n            n_drop: 5\r\n    backtest:\r\n        verbose: True\r\n        limit_threshold: 0.095\r\n        account: 50000\r\n        benchmark: *benchmark\r\n        deal_price: close\r\n        open_cost: 0.0005\r\n        close_cost: 0.0015\r\n        min_cost: 5\r\ntask:\r\n    model:\r\n        class: DEnsembleModel\r\n        module_path: qlib.contrib.model.double_ensemble\r\n        kwargs:\r\n            base_model: \"gbm\"\r\n            loss: mse\r\n            num_models: 6\r\n            enable_sr: True\r\n            enable_fs: True\r\n            alpha1: 1\r\n            alpha2: 1\r\n            bins_sr: 10\r\n            bins_fs: 5\r\n            decay: 0.5\r\n            sample_ratios:\r\n                - 0.8\r\n                - 0.7\r\n                - 0.6\r\n                - 0.5\r\n                - 0.4\r\n            sub_weights:\r\n                - 1\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n            epochs: 28\r\n            colsample_bytree: 0.8879\r\n            learning_rate: 0.2\r\n            subsample: 0.8789\r\n            lambda_l1: 205.6999\r\n            lambda_l2: 580.9768\r\n            max_depth: 8\r\n            num_leaves: 210\r\n            num_threads: 20\r\n            verbosity: -1\r\n    dataset:\r\n        class: DatasetH\r\n        module_path: qlib.data.dataset\r\n        kwargs:\r\n            handler:\r\n                class: Alpha158\r\n                module_path: qlib.contrib.data.handler\r\n                kwargs: *data_handler_config\r\n            segments:\r\n                train: [2008-01-01, 2018-12-31]\r\n                valid: [2019-01-01, 2020-07-31]\r\n                test: [2020-08-01, 2020-03-01]\r\n    record:\r\n        - class: SignalRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs: {}\r\n        - class: SigAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            ana_long_short: False\r\n            ann_scaler: 252\r\n        - class: PortAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            config: *port_analysis_config\r\n\r\nthanks for answering in advance.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1616595419000,
        "Challenge_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/369",
        "Challenge_link_count":0,
        "Challenge_open_time":14625.7169444444,
        "Challenge_readability":8.9,
        "Challenge_reading_time":32.29,
        "Challenge_repo_contributor_count":101.0,
        "Challenge_repo_fork_count":1786.0,
        "Challenge_repo_issue_count":1390.0,
        "Challenge_repo_star_count":10030.0,
        "Challenge_repo_watch_count":243.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Double Ensemble MlflowException",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":294,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_contributor_issue_ratio":0.0726618705,
        "Challenge_watch_issue_ratio":0.1748201439
    },
    {
        "Challenge_adjusted_solved_time":19.5733333333,
        "Challenge_answer_count":3,
        "Challenge_body":"## Expected Behavior\r\nThe metadata service (using Neptune) to start successfully.\r\n\r\n## Current Behavior\r\nFlask application startup fails due to an import error - `ImportError: module 'metadata_service.config' has no attribute 'NeptuneConfig'`\r\n\r\n## Possible Solution\r\nMake the NeptuneConfig discoverable by the service.\r\n\r\n## Steps to Reproduce\r\n1. Deploy a container based on the amundsen-metadata image (latest)\r\n2. Follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-neptune.md) to set up the service to use Neptune i.e. configure env vars\r\n3. Start container and the app is unable to start\r\n\r\n## Screenshots (if appropriate)\r\n![Screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png)\r\n\r\n## Context\r\nI cannot start an ECS task based on this image and therefore can't connect to the Neptune cluster.\r\n\r\n## Your Environment\r\n",
        "Challenge_closed_time":1666184788000,
        "Challenge_created_time":1666114324000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/2013",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":11.6,
        "Challenge_reading_time":13.35,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":19.5733333333,
        "Challenge_title":"Bug Report: NeptuneConfig import failing - Flask",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":112,
        "Platform":"Github",
        "Solution_body":"Thanks for opening your first issue here!\n Any solution for this? > Any solution for this?\r\n\r\nThe error message was a bit of a red herring. The actual problem was amundsen-gremlin isn't installed as part of the base image creation `amundsendev\/amundsen-metadata`.\r\n\r\nSolution 1 - add the package to the requirements files and rebuild your own Amundsen image\r\n\r\nSolution 2 - build on the base image and add a `RUN pip install amundsen-gremlin` to your bespoke dockerfile. For my use case I've gone with the latter.",
        "Solution_gpt_summary":"add miss packag file rebuild amundsen imag build base imag add run pip instal amundsen gremlin bespok dockerfil",
        "Solution_link_count":0.0,
        "Solution_original_content":"open messag bit red her amundsen gremlin isn instal base imag creation amundsendev amundsen metadata add packag file rebuild amundsen imag build base imag add run pip instal amundsen gremlin bespok dockerfil gone",
        "Solution_preprocessed_content":"open messag bit red her isn instal base imag creation add packag file rebuild amundsen imag build base imag add bespok dockerfil gone",
        "Solution_readability":6.7,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":82.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":1322.0291666667,
        "Challenge_answer_count":4,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Challenge_closed_time":1664069854000,
        "Challenge_created_time":1659310549000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":31.58,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":1322.0291666667,
        "Challenge_title":"Neptune MalformedQueryException",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":345,
        "Platform":"Github",
        "Solution_body":"Thanks for opening your first issue here!\n The PR that solves the issue in my case https:\/\/github.com\/amundsen-io\/amundsen\/pull\/1947\/files This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n This issue has been automatically closed for inactivity. If you still wish to make these changes, please open a new pull request or reopen this one.\n",
        "Solution_gpt_summary":"initi tornadotransport class properli move order decr order desc function gremlin proxi file http github com amundsen amundsen pull file",
        "Solution_link_count":1.0,
        "Solution_original_content":"open http github com amundsen amundsen pull file automat mark stale activ close activ automat close inact wish open pull request reopen",
        "Solution_preprocessed_content":"open automat mark stale activ close activ automat close inact wish open pull request reopen",
        "Solution_readability":7.7,
        "Solution_reading_time":5.27,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":67.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":766.1991666667,
        "Challenge_answer_count":7,
        "Challenge_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Expected Behavior\r\n<WARNING:elasticsearch:PUT https:\/\/my aws ES endpoint\/table_a54a9a96-c246-4bcd-b417-2d8c005c3290 [status:400 request:0.069s]\r\nINFO:databuilder.callback.call_back:No callbacks to notify\r\nTraceback (most recent call last):\r\n  File \"example\/scripts\/sample_data_loader_neptune.py\", line 403, in <module>\r\n    job_es_table.launch()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 76, in launch\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 72, in launch\r\n    self.publisher.publish()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n    self.publish_impl()\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/elasticsearch_publisher.py\", line 93, in publish_impl\r\n    self.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/utils.py\", line 347, in _wrapped\r\n    return func(*args, params=params, headers=headers, **kwargs)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/indices.py\", line 146, in create\r\n    \"PUT\", _make_path(index), params=params, headers=headers, body=body\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 466, in perform_request\r\n    raise e\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 434, in perform_request\r\n    timeout=timeout,\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/http_requests.py\", line 216, in perform_request\r\n    self._raise_error(response.status_code, raw_data)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/base.py\", line 329, in _raise_error\r\n    status_code, error_message, additional_info\r\n\r\n\r\nelasticsearch.exceptions.RequestError: RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [schema : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [cluster : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [description : {analyzer=simple, type=text}] [display_name : {type=keyword}] [column_descriptions : {analyzer=simple, type=text}] [programmatic_descriptions : {analyzer=simple, type=text}] [tags : {type=keyword}] [badges : {type=keyword}] [database : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [total_usage : {type=long}] [name : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [last_updated_timestamp : {format=epoch_second, type=date}] [unique_usage : {type=long}] [column_names : {analyzer=simple, type=text, fields={raw={normalizer=column_names_normalizer, type=keyword}}}] [key : {type=keyword}]')->\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n* Amunsen version used: Databuilder: 6.7.1 Common 0.26.0 Amundsen-Gremlin 0.0.13 AWS ES : 6.8\r\n",
        "Challenge_closed_time":1649071896000,
        "Challenge_created_time":1646313579000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1748",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":22.0,
        "Challenge_reading_time":43.86,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":766.1991666667,
        "Challenge_title":"Bug Report elasticsearch exception for sample_neptune_loader",
        "Challenge_topic":"Git Tracking",
        "Challenge_topic_macro":"Code Management",
        "Challenge_word_count":216,
        "Platform":"Github",
        "Solution_body":"Thanks for opening your first issue here!\n This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n Same problem here, does you solved? @amandeep848 could you fix the problem? @amandeep848 could you fix the problem? Hello!\r\n\r\nI have been fixed the problem by putting the version of amundsen-common to 0.24.1\r\n\r\n- My `requirements.txt` file is setup as shown below:\r\n\r\n```text\r\namundsen-databuilder==6.5.2\r\namundsen-gremlin==0.0.13\r\ngremlinpython==3.4.10\r\nrequests-aws4auth==1.1.1\r\nboto3==1.21.23\r\nbotocore==1.24.23\r\ntyping-extensions==4.1.1\r\noverrides==6.1.0\r\namundsen-common==0.24.1\r\n```\r\n\r\n- My Glue databuilder script:\r\n\r\n```python\r\nimport logging\r\nimport os\r\nimport uuid\r\nimport boto3\r\nimport textwrap\r\nimport json\r\n\r\nfrom datetime import date\r\n\r\nfrom elasticsearch import Elasticsearch\r\nfrom pyhocon import ConfigFactory\r\n\r\nfrom databuilder.clients.neptune_client import NeptuneSessionClient\r\nfrom databuilder.extractor.es_last_updated_extractor import EsLastUpdatedExtractor\r\nfrom databuilder.extractor.neptune_search_data_extractor import NeptuneSearchDataExtractor\r\n\r\nfrom databuilder.job.job import DefaultJob\r\nfrom databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader\r\nfrom databuilder.loader.file_system_neptune_csv_loader import FSNeptuneCSVLoader\r\nfrom databuilder.publisher.elasticsearch_constants import (\r\n    DASHBOARD_ELASTICSEARCH_INDEX_MAPPING, USER_ELASTICSEARCH_INDEX_MAPPING,\r\n)\r\nfrom databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher\r\nfrom databuilder.publisher.neptune_csv_publisher import NeptuneCSVPublisher\r\nfrom databuilder.task.task import DefaultTask\r\nfrom databuilder.transformer.base_transformer import ChainedTransformer, NoopTransformer\r\nfrom databuilder.transformer.dict_to_model import MODEL_CLASS, DictToModel\r\nfrom databuilder.transformer.generic_transformer import (\r\n    CALLBACK_FUNCTION, FIELD_NAME, GenericTransformer,\r\n)\r\n\r\nfrom databuilder.extractor.glue_extractor import GlueExtractor\r\nfrom databuilder.task.neptune_staleness_removal_task import NeptuneStalenessRemovalTask\r\n\r\n\r\nes_host = os.getenv('ES_HOST')\r\n\r\nneptune_host = os.getenv('NEPTUNE_HOST')\r\nneptune_port = os.getenv('NEPTUNE_PORT', 8182)\r\nneptune_iam_role_name = os.getenv('NEPTUNE_IAM_ROLE')\r\n\r\nS3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME')\r\ntoday = date.today()\r\nS3_DATA_PATH = f'amundsen_data\/glue_extractor\/year={today.year}\/month={today.month}\/day={today.day}'\r\n\r\nAWS_REGION = os.getenv('AWS_REGION')\r\nGLUE_DATABASE_IDENTIFIER = os.getenv('GLUE_DATABASE_IDENTIFIER')\r\n\r\nes = Elasticsearch(\r\n    '{}'.format(es_host),\r\n    scheme=\"https\",\r\n    port=443,\r\n)\r\n\r\nNEPTUNE_ENDPOINT = '{}:{}'.format(neptune_host, neptune_port)\r\n\r\nLOGGER = logging.getLogger(__name__)\r\n\r\n\r\ndef run_glue_job(job_name):\r\n    \"\"\"Run Glue metadata extraction\r\n\r\n    Args:\r\n        job_name (string): job name\r\n    \"\"\"\r\n\r\n    tmp_folder = '\/var\/tmp\/amundsen\/{job_name}'.format(job_name=job_name)\r\n    node_files_folder = '{tmp_folder}\/nodes'.format(tmp_folder=tmp_folder)\r\n    relationship_files_folder = '{tmp_folder}\/relationships'.format(tmp_folder=tmp_folder)\r\n\r\n    loader = FSNeptuneCSVLoader()\r\n    publisher = NeptuneCSVPublisher()\r\n\r\n    with open(\"databases.json\") as jsonFile:\r\n\r\n        filters = json.load(jsonFile)\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        f'extractor.glue.{GlueExtractor.CLUSTER_KEY}': GLUE_DATABASE_IDENTIFIER,\r\n        f'extractor.glue.{GlueExtractor.FILTER_KEY}': filters,\r\n        loader.get_scope(): {\r\n            FSNeptuneCSVLoader.NODE_DIR_PATH: node_files_folder,\r\n            FSNeptuneCSVLoader.RELATION_DIR_PATH: relationship_files_folder,\r\n            FSNeptuneCSVLoader.SHOULD_DELETE_CREATED_DIR: True,\r\n            FSNeptuneCSVLoader.JOB_PUBLISHER_TAG: 'unique_tag'\r\n        },\r\n        publisher.get_scope(): {\r\n            NeptuneCSVPublisher.NODE_FILES_DIR: node_files_folder,\r\n            NeptuneCSVPublisher.RELATION_FILES_DIR: relationship_files_folder,\r\n            NeptuneCSVPublisher.AWS_S3_BUCKET_NAME: S3_BUCKET_NAME,\r\n            NeptuneCSVPublisher.AWS_BASE_S3_DATA_PATH: S3_DATA_PATH,\r\n            NeptuneCSVPublisher.NEPTUNE_HOST: NEPTUNE_ENDPOINT,\r\n            NeptuneCSVPublisher.AWS_IAM_ROLE_NAME: neptune_iam_role_name,\r\n            NeptuneCSVPublisher.AWS_REGION: AWS_REGION\r\n        },\r\n    })\r\n\r\n    DefaultJob(\r\n        conf=job_config,\r\n        task=DefaultTask(\r\n            extractor=GlueExtractor(),\r\n            loader=loader,\r\n            transformer=NoopTransformer()\r\n        ),\r\n        publisher=publisher\r\n    ).launch()\r\n\r\ndef create_remove_stale_data_job():\r\n    \"\"\"Run remove stale data from Neptune\r\n\r\n    Returns:\r\n        NeptuneStalenessRemovalTask: Neptune stateleness data job\r\n    \"\"\"\r\n\r\n    target_relations = ['DESCRIPTION', 'DESCRIPTION_OF', 'COLUMN', 'COLUMN_OF', 'TABLE', 'TABLE_OF']\r\n    target_nodes = ['Table', 'Column', 'Programmatic_Description', \"Schema\"]\r\n\r\n    staleness_max_pct = 5\r\n\r\n    while True:\r\n\r\n        try:\r\n\r\n            LOGGER.info(f'Delete stale data at threshold - {staleness_max_pct}%')\r\n\r\n            job_config = ConfigFactory.from_dict({\r\n                'task.remove_stale_data': {\r\n                    NeptuneStalenessRemovalTask.TARGET_RELATIONS: target_relations,\r\n                    NeptuneStalenessRemovalTask.TARGET_NODES: target_nodes,\r\n                    NeptuneStalenessRemovalTask.STALENESS_CUT_OFF_IN_SECONDS: 86400,  # 1 day\r\n                    NeptuneStalenessRemovalTask.STALENESS_MAX_PCT: staleness_max_pct,\r\n                    'neptune.client': {\r\n                        NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                        NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                    }\r\n                }\r\n            })\r\n\r\n            job = DefaultJob(\r\n                conf=job_config,\r\n                task=NeptuneStalenessRemovalTask()\r\n            )\r\n\r\n            job.launch()\r\n\r\n            break\r\n\r\n        except Exception as ex:\r\n\r\n            LOGGER.error(ex)\r\n            LOGGER.info(f'Increase stale data threshold')\r\n\r\n            staleness_max_pct += 5\r\n\r\n            if staleness_max_pct == 105:\r\n\r\n                break\r\n\r\n\r\ndef create_es_publisher_job(elasticsearch_index_alias='table_search_index',\r\n                            elasticsearch_doc_type_key='table',\r\n                            model_name='databuilder.models.table_elasticsearch_document.TableESDocument',\r\n                            entity_type='table',\r\n                            elasticsearch_mapping=None):\r\n    \"\"\"\r\n    :param elasticsearch_index_alias:  alias for Elasticsearch used in\r\n                                       amundsensearchlibrary\/search_service\/config.py as an index\r\n    :param elasticsearch_doc_type_key: name the ElasticSearch index is prepended with. Defaults to `table` resulting in\r\n                                       `table_{uuid}`\r\n    :param model_name:                 the Databuilder model class used in transporting between Extractor and Loader\r\n    :param entity_type:                Entity type handed to the `Neo4jSearchDataExtractor` class, used to determine\r\n                                       Cypher query to extract data from Neo4j. Defaults to `table`.\r\n    :param elasticsearch_mapping:      Elasticsearch field mapping \"DDL\" handed to the `ElasticsearchPublisher` class,\r\n                                       if None is given (default) it uses the `Table` query baked into the Publisher\r\n    \"\"\"\r\n    # loader saves data to this location and publisher reads it from here\r\n    extracted_search_data_path = '\/var\/tmp\/amundsen\/search_data.json'\r\n    loader = FSElasticsearchJSONLoader()\r\n    extractor = NeptuneSearchDataExtractor()\r\n\r\n    task = DefaultTask(\r\n        loader=loader,\r\n        extractor=extractor,\r\n        transformer=NoopTransformer()\r\n    )\r\n\r\n    # elastic search client instance\r\n    elasticsearch_client = es\r\n\r\n    # unique name of new index in Elasticsearch\r\n    elasticsearch_new_index_key = '{}_'.format(elasticsearch_doc_type_key) + str(uuid.uuid4())\r\n\r\n    publisher = ElasticsearchPublisher()\r\n\r\n    session = boto3.Session(region_name=AWS_REGION)\r\n\r\n    aws_creds = session.get_credentials()\r\n    aws_access_key = aws_creds.access_key\r\n    aws_access_secret = aws_creds.secret_key\r\n    aws_token = aws_creds.token\r\n\r\n    job_config = ConfigFactory.from_dict({\r\n        extractor.get_scope(): {\r\n            NeptuneSearchDataExtractor.ENTITY_TYPE_CONFIG_KEY: entity_type,\r\n            NeptuneSearchDataExtractor.MODEL_CLASS_CONFIG_KEY: model_name,\r\n            'neptune.client': {\r\n                NeptuneSessionClient.NEPTUNE_HOST_NAME: NEPTUNE_ENDPOINT,\r\n                NeptuneSessionClient.AWS_REGION: AWS_REGION,\r\n                NeptuneSessionClient.AWS_ACCESS_KEY: aws_access_key,\r\n                NeptuneSessionClient.AWS_SECRET_ACCESS_KEY: aws_access_secret,\r\n                NeptuneSessionClient.AWS_SESSION_TOKEN: aws_token\r\n            }\r\n        },\r\n        'loader.filesystem.elasticsearch.file_path': extracted_search_data_path,\r\n        'loader.filesystem.elasticsearch.mode': 'w',\r\n        publisher.get_scope(): {\r\n            'file_path': extracted_search_data_path,\r\n            'mode': 'r',\r\n            'client': elasticsearch_client,\r\n            'new_index': elasticsearch_new_index_key,\r\n            'doc_type': elasticsearch_doc_type_key,\r\n            'alias': elasticsearch_index_alias\r\n        }\r\n    })\r\n\r\n    # only optionally add these keys, so need to dynamically `put` them\r\n    if elasticsearch_mapping:\r\n        job_config.put('publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_MAPPING_CONFIG_KEY),\r\n                       elasticsearch_mapping)\r\n\r\n    job = DefaultJob(\r\n        conf=job_config,\r\n        task=task,\r\n        publisher=ElasticsearchPublisher()\r\n    )\r\n\r\n    return job\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    LOGGER.info('ES Host: ' +  es_host)\r\n    LOGGER.info('Neptune Host: ' + neptune_host)\r\n    LOGGER.info('Neptune Port: ' + str(neptune_port))\r\n    LOGGER.info('Neptune IAM Role Name: ' + neptune_iam_role_name)\r\n    LOGGER.info('S3 Bucket Name: ' + S3_BUCKET_NAME)\r\n    LOGGER.info('S3 Data Path: ' + S3_DATA_PATH)\r\n    LOGGER.info('AWS Region: ' + AWS_REGION)\r\n\r\n    logging.info('>>> Running Remove Stale Data Job <<<')\r\n\r\n    create_remove_stale_data_job()\r\n\r\n    logging.info('>>> Running Glue Extractor <<<')\r\n\r\n    run_glue_job('amundsen_glue_extractor')\r\n\r\n    logging.info('>>> Running ES Publisher <<<')\r\n\r\n    job_es_table = create_es_publisher_job(\r\n        elasticsearch_index_alias='table_search_index',\r\n        elasticsearch_doc_type_key='table',\r\n        entity_type='table',\r\n        model_name='databuilder.models.table_elasticsearch_document.TableESDocument'\r\n    )\r\n    job_es_table.launch()\r\n```\r\n\r\n- databases.json\r\n\r\n```json\r\n[]\r\n```\r\n\r\n- .env\r\n\r\n```env\r\nES_HOST=<ES_HOST>\r\nNEPTUNE_HOST=<NEPTUNE_HOST>\r\nNEPTUNE_PORT=8182\r\nNEPTUNE_IAM_ROLE=<NEPTUNE_IAM_ROLE>\r\nS3_BUCKET_NAME=<S3_BUCKET_NAME>\r\nAWS_REGION=<AWS_REGION>\r\nSECRET_NAME=<SECRET_NAME>\r\nGLUE_DATABASE_IDENTIFIER=<GLUE_DATABASE_IDENTIFIER>\r\n```\r\n\r\n\r\nHope this help!\r\n\r\nBest Regards.\r\nBill\r\n I encountered this issue, too, as I installed data builder from codebase with `python setup.py install`, and after rebase with the main branch, the previous version was not clean up when we simply rerun `python setup.py install`, the way out was to do `pip uninstall amundsen-databuilder` and `pip uninstall amundsen-common` until non of packages existed(there could be multiple versions left, more than once per each package could be required).\r\n\r\nThen the expected elastic-related code is up to date w\/o this error anymore.",
        "Solution_gpt_summary":"put version amundsen common txt file glue databuild uninstal amundsen databuild amundsen common packag",
        "Solution_link_count":0.0,
        "Solution_original_content":"open automat mark stale activ close activ amandeep amandeep put version amundsen common txt file setup shown text amundsen databuild amundsen gremlin gremlinpython request awsauth boto botocor type extens overrid amundsen common glue databuild import log import import uuid import boto import textwrap import json datetim import date elasticsearch import elasticsearch pyhocon import configfactori databuild client client import sessioncli databuild extractor updat extractor import eslastupdatedextractor databuild extractor search data extractor import searchdataextractor databuild job job import defaultjob databuild loader file elasticsearch json loader import fselasticsearchjsonload databuild loader file csv loader import fscsvloader databuild publish elasticsearch constant import dashboard elasticsearch index map elasticsearch index map databuild publish elasticsearch publish import elasticsearchpublish databuild publish csv publish import csvpublish databuild task task import defaulttask databuild transform base transform import chainedtransform nooptransform databuild transform dict model import model class dicttomodel databuild transform gener transform import callback function field generictransform databuild extractor glue extractor import glueextractor databuild task stale remov task import stalenessremovaltask host getenv host host getenv host port getenv port iam role getenv iam role bucket getenv bucket todai date todai data path amundsen data glue extractor year todai year month todai month dai todai dai region getenv region glue databas identifi getenv glue databas identifi elasticsearch format host scheme http port endpoint format host port logger log getlogg run glue job job run glue metadata extract arg job job tmp folder var tmp amundsen job format job job node file folder tmp folder node format tmp folder tmp folder relationship file folder tmp folder relationship format tmp folder tmp folder loader fscsvloader publish csvpublish open databas json jsonfil filter json load jsonfil job config configfactori dict extractor glue glueextractor cluster kei glue databas identifi extractor glue glueextractor filter kei filter loader scope fscsvloader node dir path node file folder fscsvloader relat dir path relationship file folder fscsvloader delet creat dir fscsvloader job publish tag uniqu tag publish scope csvpublish node file dir node file folder csvpublish relat file dir relationship file folder csvpublish bucket bucket csvpublish base data path data path csvpublish host endpoint csvpublish iam role iam role csvpublish region region defaultjob conf job config task defaulttask extractor glueextractor loader loader transform nooptransform publish publish launch creat remov stale data job run remov stale data return stalenessremovaltask statel data job target relat descript descript column column tabl tabl target node tabl column programmat descript schema stale pct logger delet stale data threshold stale pct job config configfactori dict task remov stale data stalenessremovaltask target relat target relat stalenessremovaltask target node target node stalenessremovaltask stale cut dai stalenessremovaltask stale pct stale pct client sessioncli host endpoint sessioncli region region job defaultjob conf job config task stalenessremovaltask job launch break except logger logger increas stale data threshold stale pct stale pct break creat publish job elasticsearch index alia tabl search index elasticsearch doc type kei tabl model databuild model tabl elasticsearch document tableesdocu entiti type tabl elasticsearch map param elasticsearch index alia alia elasticsearch amundsensearchlibrari search servic config index param elasticsearch doc type kei elasticsearch index prepend default tabl tabl uuid param model databuild model class transport extractor loader param entiti type entiti type hand neojsearchdataextractor class determin cypher queri extract data neoj default tabl param elasticsearch map elasticsearch field map ddl hand elasticsearchpublish class default tabl queri bake publish loader save data locat publish read extract search data path var tmp amundsen search data json loader fselasticsearchjsonload extractor searchdataextractor task defaulttask loader loader extractor extractor transform nooptransform elast search client instanc elasticsearch client uniqu index elasticsearch elasticsearch index kei format elasticsearch doc type kei str uuid uuid publish elasticsearchpublish session boto session region region cred session credenti access kei cred access kei access secret cred secret kei token cred token job config configfactori dict extractor scope searchdataextractor entiti type config kei entiti type searchdataextractor model class config kei model client sessioncli host endpoint sessioncli region region sessioncli access kei access kei sessioncli secret access kei access secret sessioncli session token token loader filesystem elasticsearch file path extract search data path loader filesystem elasticsearch mode publish scope file path extract search data path mode client elasticsearch client index elasticsearch index kei doc type elasticsearch doc type kei alia elasticsearch index alia option add kei dynam elasticsearch map job config publish elasticsearch format elasticsearchpublish elasticsearch map config kei elasticsearch map job defaultjob conf job config task task publish elasticsearchpublish return job log basicconfig level log logger host host logger host host logger port str port logger iam role iam role logger bucket bucket logger data path data path logger region region log run remov stale data job run glue extractor run publish host port iam role bucket region secret glue databas identifi hope instal data builder codebas setup instal rebas branch previou version clean simpli rerun setup instal pip uninstal amundsen databuild pip uninstal amundsen common packag multipl version left packag elast relat date anymor",
        "Solution_preprocessed_content":"open automat mark stale activ close activ put version file setup shown glue databuild env hope instal data builder codebas rebas branch previou version clean simpli rerun packag date anymor",
        "Solution_readability":20.1,
        "Solution_reading_time":132.24,
        "Solution_score_count":1.0,
        "Solution_sentence_count":103.0,
        "Solution_word_count":698.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":11799.0105555556,
        "Challenge_answer_count":1,
        "Challenge_body":"For uploading data to AWS Neptune we use `NeptuneCSVPublisher`, which internally uses `NeptuneBulkLoaderApi`. The current configuration uses config key `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME`, which provides name of IAM role for the loader to be able to use S3 and Neptune. The issue is that `NeptuneBulkLoaderApi` constructs IAM role ARN from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['Account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of ARN aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -AWS Regions\r\n* `aws-cn` - China Regions\r\n* `aws-us-gov` - AWS GovCloud (US) Regions\r\n\r\nSince we use Amundsen also in AWS China, the above ARN is not valid. \r\n\r\n## Expected Behavior\r\n\r\nIAM role ARN either takes into account AWS partition or there is a possibility of passing IAM role ARN instead of name directly.\r\n\r\n## Current Behavior\r\n\r\nIAM role ARN is constructed incorrectly outside of AWS Global.\r\n\r\n## Possible Solutions\r\n\r\nIAM role ARN should take partition into account. There are two solutions:\r\n1. Add partition into current code\r\n2. Add option of passing IAM role ARN directly which supersedes IAM role name \r\n\r\n### Solution 1\r\n\r\nSince I didn't know or found any good way to get the AWS partition, we can use caller identity and ARN there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['Account']\r\npartition = identity['Arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nThis is smaller fix but it is a bit hacky and I'm not sure it'll work in all situation, but it should I guess.\r\n\r\n### Solution 2\r\n\r\nAdd config key `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN` which either supersedes `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nOr even replace `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` with `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN`, which is IMO cleaner, but would be not backward compatible. \r\n\r\n## Steps to Reproduce\r\nDeploy Amundsen in AWS China with Neptune and try to use Databuilder to upload CSV data from S3. \r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nCurrently we are unable to load data into Neptune as the IAM role ARN setting is hidden and we get an error:\r\n\r\n```\r\n[ERROR] Exception: Failed to load csv. Response: {'detailedMessage': \"Failed to start new load from the source s3:\/\/amundsenBucket\/amundsen\/2021_08_10_01_01_28. Couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/RoleForNeptune111111-2222\", 'code': 'InvalidParameterException', 'requestId': 'xxx'}\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to_neptune_job.launch()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/neptune_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(\"Failed to load csv. Response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## Your Environment\r\n* Amunsen version used: `amundsen-databuilder==4.3.1`\r\n* Data warehouse stores: AWS Neptune\r\n* Deployment (k8s or native): AWS Step Functions (k8s for backend but unrelated for now)\r\n* Link to your fork or repository:",
        "Challenge_closed_time":1671067447000,
        "Challenge_created_time":1628591009000,
        "Challenge_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1430",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.2,
        "Challenge_reading_time":49.7,
        "Challenge_repo_contributor_count":207.0,
        "Challenge_repo_fork_count":890.0,
        "Challenge_repo_issue_count":2023.0,
        "Challenge_repo_star_count":3674.0,
        "Challenge_repo_watch_count":245.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":38,
        "Challenge_solved_time":11799.0105555556,
        "Challenge_title":"Databuilder `NeptuneBulkLoaderApi` constructs wrong IAM role ARN for AWS other than global",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":441,
        "Platform":"Github",
        "Solution_body":"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"automat mark stale activ close activ",
        "Solution_preprocessed_content":"automat mark stale activ close activ",
        "Solution_readability":9.2,
        "Solution_reading_time":1.69,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":24.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.1023232823,
        "Challenge_watch_issue_ratio":0.1211072664
    },
    {
        "Challenge_adjusted_solved_time":29.6355555556,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Challenge_closed_time":1635986654000,
        "Challenge_created_time":1635879966000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":18.5,
        "Challenge_reading_time":16.11,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":29.6355555556,
        "Challenge_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Challenge_topic":"Role-based Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":103,
        "Platform":"Github",
        "Solution_body":"Resolved as of release 3.0.8",
        "Solution_gpt_summary":"misidentif endpoint cluster region config option auth mode region set releas amazonaw com",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":2.9,
        "Solution_reading_time":0.35,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":5.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":216.3194444444,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\nWhen using the Neptune ML widget to export data like the command below from the 01- Node Classification notebook:\r\n```\r\n%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nThe following error is thrown\r\n```\r\n{\r\n  \"message\": \"Credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**Expected behavior**\r\nThe export should run to completion\r\n\r\n\r\n",
        "Challenge_closed_time":1628716798000,
        "Challenge_created_time":1627938048000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/167",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.7,
        "Challenge_reading_time":6.46,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":216.3194444444,
        "Challenge_title":"[BUG] Neptune ML Export widget throwing error",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":60,
        "Platform":"Github",
        "Solution_body":"This issue occurs on a cluster created using the default CFN script with IAM disabled\r\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"cluster creat default cfn iam disabl",
        "Solution_preprocessed_content":"cluster creat default cfn iam disabl",
        "Solution_readability":8.0,
        "Solution_reading_time":1.04,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":15.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":1679.0761111111,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Challenge_closed_time":1632957644000,
        "Challenge_created_time":1626912970000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.1,
        "Challenge_reading_time":9.62,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1679.0761111111,
        "Challenge_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":76,
        "Platform":"Github",
        "Solution_body":"Hi @Roshin29, thank you for the bug report! \r\n\r\nThe machine learning sample notebooks received substantial revisions in [Release 3.0.1](https:\/\/github.com\/aws\/graph-notebook\/releases\/tag\/v3.0.1). This release also included a number of changes under the hood to support the general availability release of Amazon Neptune ML.\r\n\r\nThe Gremlin query listed is only seen in older versions of the `Neptune-ML-01-Introduction-to-Node-Classification-Gremlin` sample notebook, and is now replaced by the one below:\r\n```\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"${endpoint}\").\r\n  with(\"Neptune#ml.limit\",3).\r\n  V().has('title', 'Apollo 13 (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\n```\r\nI am not able to reproduce the listed exception when running this query using graph-notebook v3.0.6, so the issue appears to have been resolved with the latest changes.\r\n\r\nClosing this issue out, as there are no further action items at this time. Please feel free to re-open if you have any further questions.",
        "Solution_gpt_summary":"set limit latest version introduct node classif gremlin sampl notebook updat gremlin queri",
        "Solution_link_count":1.0,
        "Solution_original_content":"roshin report sampl notebook receiv substanti revis releas http github com graph notebook releas tag releas hood gener releas gremlin queri list older version introduct node classif gremlin sampl notebook replac gremlin endpoint endpoint limit titl apollo properti genr classif valu reproduc list except run queri graph notebook latest close action item time free open",
        "Solution_preprocessed_content":"report sampl notebook receiv substanti revis releas hood gener releas gremlin queri list older version sampl notebook replac reproduc list except run queri latest close action item time free",
        "Solution_readability":12.2,
        "Solution_reading_time":12.78,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":123.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"For the  01 notebooks for Neptune ML the text in the notebook incorrectly specifies that the genre returned for a node classification task on `Toy Story` is `Comedy` when it should be `Drama`",
        "Challenge_closed_time":null,
        "Challenge_created_time":1619195852000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/116",
        "Challenge_link_count":0,
        "Challenge_open_time":13903.3744444444,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.16,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] Neptune ML notebooks have incorrect Genre stated in the text",
        "Challenge_topic":"Batch Processing",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":43,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":1339.2047222222,
        "Challenge_answer_count":25,
        "Challenge_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Challenge_closed_time":1620330541000,
        "Challenge_created_time":1615509404000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.2,
        "Challenge_reading_time":6.48,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1339.2047222222,
        "Challenge_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":74,
        "Platform":"Github",
        "Solution_body":"This appears to be an issue with the versions of `ipython` that SageMaker is using.  If you update the Lifecycle start script by putting the following code at the bottom (just before EOF) and stopping and starting the notebook.\r\n```\r\nsource activate JupyterSystemEnv\r\npip install --upgrade ipython==7.16.1\r\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\r\n``` Hi, i have updated the Lifecycle scripts as suggested and that works - but then it fails on the training:\r\n\r\n`\"status\": \"Failed\",\r\n    \"failureReason\": \"ClientError: Failed to download data`\r\n\r\n...\r\npreloading-2021-04-05-17-33-3910000\/preloading-output\/graph.bin has an illegal char sub-sequence '\/\/' in it\"`\r\n\r\ni just used the movie lens database and steps in the notebook. it adds an extra '\\' in the \"outputLocation\"...?\r\n\r\ncan you help?  \r\n Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?  > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\n\r\n<img width=\"1103\" alt=\"error_train_screen\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n > > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n> \r\n> <img alt=\"error_train_screen\" width=\"1103\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705359-4123d580-96d5-11eb-9b65-59e38f3e5140.png\">\r\n\r\n<img width=\"1117\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113705546-73cdce00-96d5-11eb-81fa-633c14942847.png\">\r\n > Hi @Kristof-Neys, can you give a screenshot of the error so we can confirm\/reproduce?\r\n\r\nhi @austinkline  - thanks for helping me out. So as you can see from the screenshots, it fails to download the data and seems to be adding an extra slash...\r\n\r\nso I changed the script: `--s3-processed-uri {str(s3_bucket_uri)}preloading \"\"\"` \r\nand it then ran fine.... perhaps you want to correct that in the notebook?\r\n\r\nbut when making the prediction I am getting:\r\n\r\n<img width=\"1120\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/113713442-42f29680-96df-11eb-8dc8-131e8377fa4c.png\">\r\n\r\n\r\nso Toy Story comes up as 'Thriller\" and not 'Comedy' as  per the notebook\r\n\r\n\r\nhow can I see which actual model the classification is using? Is it a graph convolutional network, I recall seeing that in the notebooks in the repository. It would be good to see the actual DGL model & code. \r\n\r\nThanks!!\r\n Thanks for the info. I'll spend some time reproducing and get back to you I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n\r\n@Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore > I was not able to reproduce this issue after running a fresh notebook created via cloud-formation found in our public docs\r\n> \r\n> ![image](https:\/\/user-images.githubusercontent.com\/8711160\/113755017-afac6780-96c4-11eb-86ee-42798d595609.png)\r\n> \r\n> @Kristof-Neys I wonder if the state of the notebook got mixed up somehow? I would suggest creating a fresh notebook instance and trying again. The bug which needed the workaround lifecycle configuration has been resolved and released to pypi so that is not needed anymore\r\n\r\nthank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?  > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n\r\nChecked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console. \r\n > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> \r\n> Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n\r\nyeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network > > > thank you @austinkline . I'll re-run everything from fresh... - meanwhile, how can I figure out which GNN model is actually used and the model specifics in DGL?\r\n> > \r\n> > \r\n> > Checked with the team about this, you should be able to find this information in cloudwatch logs for that particular job in the Sagemaker console.\r\n> \r\n> yeah thanks - just found it in the S3, says rgcn which presumably stands for the relational graph convolutional network\r\n\r\nyes. that's correct. Hi @Kristof-Neys and updates? Did recreating work for you? Hi @austinkline - thanks for reaching out. I have been caught up in another project but was just about to look at it. I'll update you guys probably tomorrow.  hi @austinkline & Team, i am finally getting around to this. I started everything new but now I cannot export the configuration any more, I get the following error:\r\n`{'error': ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='none', port=443): Max retries exceeded with url: \/neptune-export (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f28f5e81748>: Failed to establish a new connection: [Errno -2] Name or service not known',))\",),)}`\r\n\r\nUPdate: when I re-started everything and used the notebook of last week... i get\r\n\r\n`403 \"Missing Authentication Token\" `\r\n\r\n\r\n\r\nany ideas? Thanks!!\r\n     Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n\r\nCan you provide your notebook version and configuration by running the following:\r\n\r\n1. What cell did you execute that gave you the above mentioned error?\r\n\r\n2. What version of `graph-notebook` are you running?\r\n```\r\n%graph_notebook_version\r\n```\r\n\r\n3. What is your configuration? Really we just care about the authentication setting\r\n**NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n\r\n```\r\n%graph_notebook_config\r\n```\r\n > Let's start by gathering what version you're running again and what your configuration looks like. What we want to figure out is whether the exporter or Neptune is throwing the exception provided. That is to say, was the exporter unable to be called due to a missing auth token, or did the exporter start and then it was unable to communicate with Neptune. You also could take a look at cloudwatch logs for your api gateway on the corresponding exporter resource and see if it has any additional info you can point to. I'll go ahead and provision a fresh stack and see if I get the same issue once we've confirmed your auth setting.\r\n> \r\n> Can you provide your notebook version and configuration by running the following:\r\n> \r\n>     1. What cell did you execute that gave you the above mentioned error?\r\n> \r\n>     2. What version of `graph-notebook` are you running?\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_version\r\n> ```\r\n> \r\n>     1. What is your configuration? Really we just care about the authentication setting\r\n>        **NOTE: PLEASE ERASE OR BLOCK OUT YOUR HOST ENDPOINT FROM YOUR CONFIGURATION WHEN PROVIDING THIS INFO**\r\n> \r\n> \r\n> ```\r\n> %graph_notebook_config\r\n> ```\r\n\r\n@austinkline thank you! Very much appreciate taking time & effort. Ok, so these are the detail:\r\n\r\ncell that I am running:\r\n`%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}`\r\n=> this gives me error: \r\n`{\r\n  \"message\": \"Missing Authentication Token\"\r\n}`\r\n\r\n\r\nVersion graph-notebook: 2.1.0\r\n\r\n%graph_notebook_config:\r\n`{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}`\r\n\r\nThe strange thing is that all worked well two weeks ago, altho I did get wrong predictions, but at least the export worked and I could train model and get predictions etc. Now I cannot get beyond the export.... \r\n\r\nthank you again\r\n\r\n @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n\r\n```\r\n%%graph_notebook_config\r\n{\r\n  \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"IAM\",\r\n  \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n  \"ssl\": true,\r\n  \"aws_region\": \"us-east-1\",\r\n  \"sparql\": {\r\n    \"path\": \"sparql\"\r\n  }\r\n}\r\n```\r\n\r\nNote that we're changing the auth mode to IAM > @Kristof-Neys I believe I found the bug we're dealing with. Can you flip IAM auth on in your config and see if the exporter\/other components work?\r\n> \r\n> ```\r\n> %%graph_notebook_config\r\n> {\r\n>   \"host\": \"neptunedbcluster-xxxxxx.....xxxxxx.us-east-1.neptune.amazonaws.com\",\r\n>   \"port\": 8182,\r\n>   \"auth_mode\": \"IAM\",\r\n>   \"load_from_s3_arn\": \"arn:aws:iam::504028651370:role\/neptuneml-NeptuneBaseStack-Y-NeptuneLoadFromS3Role-1UBUI982ZI077\",\r\n>   \"ssl\": true,\r\n>   \"aws_region\": \"us-east-1\",\r\n>   \"sparql\": {\r\n>     \"path\": \"sparql\"\r\n>   }\r\n> }\r\n> ```\r\n> \r\n> Note that we're changing the auth mode to IAM\r\nhey @austinkline  - that worked!, export and training went fine....but still predicting the wrong genre.... - how can this be??\r\n\r\n<img width=\"904\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/10049871\/115867858-82d1b180-a433-11eb-809c-a1aa733e5d90.png\">\r\n\r\n\r\n @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect.  Drama is what is coming back from the model that is generated .  I have created an issue to track this https:\/\/github.com\/aws\/graph-notebook\/issues\/116 and will address this with the additional feedback on those notebooks in the near future.  > @Kristof-Neys The issue you are seeing is actually one where the text in the notebook is incorrect. Drama is what is coming back from the model that is generated . I have created an issue to track this #116 and will address this with the additional feedback on those notebooks in the near future.\r\n\r\nok understood - thank you\r\n Closing this out since we're tracking the reported issue of notebooks being out of date in #116. Please cut us a new ticket if you run into any further issues! Hi guys, I'm facing a similar issue, I applied your fix(setting \"auth_mode\": \"IAM\") but did not work, any suggestions? Hi @llealgt , is this referring to the same issue mentioned at https:\/\/github.com\/aws\/graph-notebook\/issues\/445#issuecomment-1426192856? Hi @michaelnchin, nope, it's not the same, this happens when running notebook \r\nNeptune-ML-01-Introduction-to-Node-Classification-Gremlin\r\nThe other errors happen in notebook \r\nNeptune-ML-00-Getting-Started-with-Neptune-ML-Gremlin\r\nI guess it is related but they are different errors in different notebooks.",
        "Solution_gpt_summary":"widget updat ipython version lifecycl start aros train remov extra slash predict text notebook tri auth mode iam",
        "Solution_link_count":9.0,
        "Solution_original_content":"version ipython updat lifecycl start put eof stop start notebook sourc activ jupytersystemenv pip instal upgrad ipython sourc home anaconda bin deactiv updat lifecycl train statu failurereason clienterror download data preload preload output graph bin illeg char sub sequenc movi len databas step notebook add extra outputloc kristof nei screenshot reproduc kristof nei screenshot reproduc kristof nei screenshot reproduc kristof nei screenshot reproduc austinklin screenshot download data extra slash process uri str bucket uri preload ran notebook predict toi stori come thriller comedi notebook model classif graph convolut network recal see notebook repositori dgl model spend time reproduc reproduc run fresh notebook creat cloud format public doc imag http imag githubusercont com afac png kristof nei state notebook mix creat fresh notebook instanc workaround lifecycl configur releas pypi anymor reproduc run fresh notebook creat cloud format public doc imag http imag githubusercont com afac png kristof nei state notebook mix creat fresh notebook instanc workaround lifecycl configur releas pypi anymor austinklin run fresh figur gnn model model dgl austinklin run fresh figur gnn model model dgl team cloudwatch log job consol austinklin run fresh figur gnn model model dgl team cloudwatch log job consol yeah sai rgcn presum stand relat graph convolut network austinklin run fresh figur gnn model model dgl team cloudwatch log job consol yeah sai rgcn presum stand relat graph convolut network kristof nei updat recreat austinklin reach caught updat gui probabl tomorrow austinklin team final start export configur connectionerror maxretryerror httpsconnectionpool host port retri exceed url export newconnectionerror establish connect errno servic updat start notebook week miss authent token idea start gather version run configur figur export throw except export call miss auth token export start commun cloudwatch log api gatewai export resourc addit ahead provis fresh stack auth set notebook version configur run cell execut gave version graph notebook run graph notebook version configur care authent set note eras block host endpoint configur graph notebook config start gather version run configur figur export throw except export call miss auth token export start commun cloudwatch log api gatewai export resourc addit ahead provis fresh stack auth set notebook version configur run cell execut gave version graph notebook run graph notebook version configur care authent set note eras block host endpoint configur graph notebook config austinklin time effort cell run export start export url export servic host export iam wait store export export param messag miss authent token version graph notebook graph notebook config host dbcluster amazonaw com port auth mode default load arn arn iam role basestack loadfromsrol ubuizi ssl region sparql path sparql week ago altho predict export train model predict export kristof nei believ deal flip iam auth config export compon graph notebook config host dbcluster amazonaw com port auth mode iam load arn arn iam role basestack loadfromsrol ubuizi ssl region sparql path sparql note auth mode iam kristof nei believ deal flip iam auth config export compon graph notebook config host dbcluster amazonaw com port auth mode iam load arn arn iam role basestack loadfromsrol ubuizi ssl region sparql path sparql note auth mode iam austinklin export train went predict genr kristof nei see text notebook drama come model gener creat track http github com graph notebook address addit feedback notebook futur kristof nei see text notebook drama come model gener creat track address addit feedback notebook futur understood close track report notebook date cut ticket run gui appli set auth mode iam llealgt http github com graph notebook issuecom michaelnchin nope run notebook introduct node classif gremlin notebook start gremlin guess relat notebook",
        "Solution_preprocessed_content":"version updat lifecycl start put stop start notebook updat lifecycl train illeg char preload miss authent token run configur care authent set note eras block host endpoint configur start gather version run configur figur export throw except export call miss auth token export start commun cloudwatch log api gatewai export resourc addit ahead provis fresh stack auth set notebook version configur run cell execut gave version run configur care authent set note eras block host endpoint configur time effort cell run version week ago altho predict export train model predict believ deal flip iam auth config compon note auth mode iam believ deal flip iam auth config compon note auth mode iam export train went predict img width alt imag see text notebook drama come model gener creat track address addit feedback notebook futur see text notebook drama come model gener creat track address addit feedback notebook futur understood close track report notebook date cut ticket run gui appli nope run notebook notebook guess relat notebook",
        "Solution_readability":9.2,
        "Solution_reading_time":145.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":102.0,
        "Solution_word_count":1555.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":431.6069444444,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Challenge_closed_time":1608658157000,
        "Challenge_created_time":1607104372000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.8,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":431.6069444444,
        "Challenge_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":81,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":43.3186111111,
        "Challenge_answer_count":3,
        "Challenge_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Challenge_closed_time":1607104425000,
        "Challenge_created_time":1606948478000,
        "Challenge_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.1,
        "Challenge_reading_time":20.85,
        "Challenge_repo_contributor_count":22.0,
        "Challenge_repo_fork_count":115.0,
        "Challenge_repo_issue_count":411.0,
        "Challenge_repo_star_count":500.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":43.3186111111,
        "Challenge_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":192,
        "Platform":"Github",
        "Solution_body":"Looks like we have a missing piece in our walkthrough for connecting to Neptune:\r\n\r\nhttps:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune\r\n\r\nCould you set your hostname from localhost to your Neptune endpoint:\r\n\r\n```\r\n%graph_notebook_host <your endpoint here>\r\n```\r\n\r\nAnd give it a try? I  updated \/etc\/hosts on my Mac and added an alias for localhost as\r\n\r\n> _27.0.0.1       localhost    yourneptuneendpoint_\r\n\r\nFlushed DNS cache.\r\nSet the hostname in Jupyter notebook graph_notebook_config command to **yourneptuneendpoint**.\r\nRan sparql query and it successfully completed.\r\n\r\n\r\n Glad it worked! I have filed an issue for us to expand our documentation to cover the steps you had to take. Closing this but feel free to open or submit a new issue if you need further assistance",
        "Solution_gpt_summary":"set hostnam localhost endpoint graph notebook host updat host file add alia localhost localhost yourendpoint flush dn cach set hostnam jupyt notebook graph notebook config yourendpoint step run sparql queri complet successfulli document updat cover step",
        "Solution_link_count":1.0,
        "Solution_original_content":"miss piec walkthrough connect http github com graph notebook tree addit databas set hostnam localhost endpoint graph notebook host updat host mac alia localhost localhost yourendpoint flush dn cach set hostnam jupyt notebook graph notebook config yourendpoint ran sparql queri successfulli complet glad file expand document cover step close free open submit assist",
        "Solution_preprocessed_content":"miss piec walkthrough connect set hostnam localhost endpoint updat mac alia localhost localhost flush dn cach set hostnam jupyt notebook yourendpoint ran sparql queri successfulli complet glad file expand document cover step close free open submit assist",
        "Solution_readability":6.9,
        "Solution_reading_time":9.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":110.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0535279805,
        "Challenge_watch_issue_ratio":0.0802919708
    },
    {
        "Challenge_adjusted_solved_time":38.1544444444,
        "Challenge_answer_count":3,
        "Challenge_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Challenge_closed_time":1625030635000,
        "Challenge_created_time":1624893279000,
        "Challenge_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":12.0,
        "Challenge_reading_time":3.03,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":160.0,
        "Challenge_repo_star_count":28.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":38.1544444444,
        "Challenge_title":"Neptune_catalyst.ipynb fails",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":22,
        "Platform":"Github",
        "Solution_body":"on it. #43 fixing here fixed in #43 ",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":0.5,
        "Solution_reading_time":0.41,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":8.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.06875,
        "Challenge_watch_issue_ratio":0.06875
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\nNeptune guide's quicklaunch link points to the full launcher\r\n\r\n**Expected behavior**\r\nShould point to the minimal launcher\r\n\r\n",
        "Challenge_closed_time":null,
        "Challenge_created_time":1625774761000,
        "Challenge_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/57",
        "Challenge_link_count":0,
        "Challenge_open_time":12075.8997222222,
        "Challenge_readability":7.6,
        "Challenge_reading_time":2.54,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":16.0,
        "Challenge_repo_issue_count":82.0,
        "Challenge_repo_star_count":102.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"[BUG] neptune minimal launcher link points to full launcher",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":28,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0487804878,
        "Challenge_watch_issue_ratio":0.1463414634
    },
    {
        "Challenge_adjusted_solved_time":4.2694444444,
        "Challenge_answer_count":2,
        "Challenge_body":"**Describe the bug**\r\n\r\nCloud formation for neptune fails on a p3.2 and p3.16 yet succeeds on a g4dn\r\n\r\nReported by a Neptune user\r\n\r\n**To Reproduce**\r\n\r\nRun through Neptune tutorial and use a p3.16\r\n\r\n**Expected behavior**\r\nIt launches\r\n\r\n**Actual behavior**\r\nFormation template stalls out and auto-deletes\r\n\r\nWorking on getting logs. After 10min, GPU services (forge-etl-python + streamgl) failed to start. V100 issue?\r\n\r\n**Screenshots**\r\n\r\n**Browser environment (please complete the following information):**\r\nall\r\n\r\n**PyGraphistry environment**\r\nAll\r\n\r\n**Additional context**\r\nCurrent graph-app-kit",
        "Challenge_closed_time":1615109545000,
        "Challenge_created_time":1615094175000,
        "Challenge_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/45",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":7.89,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":16.0,
        "Challenge_repo_issue_count":82.0,
        "Challenge_repo_star_count":102.0,
        "Challenge_repo_watch_count":12.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":4.2694444444,
        "Challenge_title":"[BUG] AWS neptune templates for p3.2, p3.16 fail to start",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":85,
        "Platform":"Github",
        "Solution_body":"Slow start, and wrong docker images load, with manual restart required to reach a healthy state:\r\n\r\n```\r\nCONTAINER ID        IMAGE                                    COMMAND                  CREATED             STATUS                             PORTS                                                NAMES\r\n0e32dfece83d        graphistry\/graphistry-nexus:v2.32.4      \"\/entrypoint \/bin\/ba\u2026\"   28 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_nexus_1\r\ndae68b0726e0        graphistry\/graphistry-pivot:v2.32.4      \"\/tini -- \/entrypoin\u2026\"   28 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_pivot_1\r\nffea7ceeb047        graphistry\/etl-server:v2.32.4            \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_forge-etl_1\r\nfbbb7924272e        graphistry\/streamgl-gpu:v2.32.4          \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 39 seconds (health: starting)   8080\/tcp                                             graphistry_streamgl-gpu_1\r\n6c1501fe0317        graphistry\/streamgl-sessions:v2.32.4     \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_streamgl-sessions_1\r\n485ef6374082        willfarrell\/autoheal:v0.7.0              \"\/docker-entrypoint \u2026\"   30 minutes ago      Up 28 minutes (healthy)            8080\/tcp                                             graphistry_autoheal_1\r\n24e6fd022e1e        graphistry\/graphistry-postgres:v2.32.4   \"docker-entrypoint.s\u2026\"   30 minutes ago      Up 28 minutes (healthy)            5432\/tcp, 8080\/tcp                                   graphistry_postgres_1\r\n6bd0cc9c7a58        graphistry\/streamgl-vgraph-etl:v2.32.4   \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_streamgl-vgraph-etl_1\r\n097054def985        graphistry\/etl-server-python:v2.32.4     \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_forge-etl-python_1\r\n2e516287f6e5        graphistry\/streamgl-viz:v2.32.4          \"\/tini -- \/entrypoin\u2026\"   30 minutes ago      Up 27 minutes (healthy)            8080\/tcp                                             graphistry_streamgl-viz_1\r\nf16cd890cb6b        graphistry\/caddy:v2.30.28                \"\/usr\/bin\/caddy --co\u2026\"   30 minutes ago      Up 40 seconds (health: starting)   0.0.0.0:80->80\/tcp, 0.0.0.0:443->443\/tcp, 2015\/tcp   graphistry_caddy_1\r\nd14e375955c8        redis:6.0.5                              \"docker-entrypoint.s\u2026\"   30 minutes ago      Up 28 minutes (healthy)            6379\/tcp, 8080\/tcp                                   graphistry_redis_1\r\nub\r\n```\r\n\r\nAMI:\r\n\r\n* ami-088aaa8746bde2e21\r\n* graphistry-standalone-2020-10-14T22-32-56Z-v2.32.4-062c9b47-e144-4bbe-8623-fbf14199f760-ami-0cbffaeee3c800e7a.4\r\n* aws-marketplace\/graphistry-standalone-2020-10-14T22-32-56Z-v2.32.4-062c9b47-e144-4bbe-8623-fbf14199f760-ami-0cbffaeee3c800e7a.4 Likely fixed by https:\/\/github.com\/graphistry\/graph-app-kit\/pull\/49 . Reopen if needed.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"slow start docker imag load manual restart reach healthi state imag creat statu port edfec graphistri graphistri nexu entrypoint bin minut ago minut healthi tcp graphistri nexu daeb graphistri graphistri pivot tini entrypoin minut ago minut healthi tcp graphistri pivot ffeaceeb graphistri etl server tini entrypoin minut ago minut healthi tcp graphistri forg etl graphistri streamgl gpu tini entrypoin minut ago health start tcp graphistri streamgl gpu cfe graphistri streamgl session tini entrypoin minut ago minut healthi tcp graphistri streamgl session willfarrel autoh docker entrypoint minut ago minut healthi tcp graphistri autoh efde graphistri graphistri postgr docker entrypoint minut ago minut healthi tcp tcp graphistri postgr bdccca graphistri streamgl vgraph etl tini entrypoin minut ago minut healthi tcp graphistri streamgl vgraph etl graphistri etl server tini entrypoin minut ago minut healthi tcp graphistri forg etl ef graphistri streamgl viz tini entrypoin minut ago minut healthi tcp graphistri streamgl viz fcdcbb graphistri caddi usr bin caddi minut ago health start tcp tcp tcp graphistri caddi dec redi docker entrypoint minut ago minut healthi tcp tcp graphistri redi ami ami bdee graphistri standalon bbe fbff ami cbffacea marketplac graphistri standalon bbe fbff ami cbffacea http github com graphistri graph app kit pull reopen",
        "Solution_preprocessed_content":"slow start docker imag load manual restart reach healthi state ami reopen",
        "Solution_readability":10.4,
        "Solution_reading_time":30.48,
        "Solution_score_count":0.0,
        "Solution_sentence_count":16.0,
        "Solution_word_count":195.0,
        "Tool":"Neptune",
        "Challenge_contributor_issue_ratio":0.0487804878,
        "Challenge_watch_issue_ratio":0.1463414634
    },
    {
        "Challenge_adjusted_solved_time":1434.0155555556,
        "Challenge_answer_count":9,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of the bug. -->\r\n\r\nWhen using `PyTorchLightningPruningCallback` to search best hyperparams, it reports `AttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'`\r\n\r\n### To Reproduce\r\n\r\n```python\r\nfrom typing import List, Optional\r\n\r\nimport optuna\r\nimport pytorch_lightning as pl\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchmetrics\r\nimport torchvision\r\nfrom optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback\r\nfrom torch.utils.data import random_split, DataLoader\r\n\r\n\r\nclass FashionDataModule(pl.LightningDataModule):\r\n    def __init__(self, data_dir: str, batch_size: int):\r\n        super().__init__()\r\n        self.data_dir = data_dir\r\n        self.batch_size = batch_size\r\n\r\n    def setup(self, stage: Optional[str] = None):\r\n        self.train_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=True, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.test_set = torchvision.datasets.FashionMNIST(\r\n            self.data_dir, train=False, download=True, transform=torchvision.transforms.ToTensor()\r\n        )\r\n        self.train_set, self.valid_set = random_split(self.train_set, [55000, 5000])\r\n\r\n    def train_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=4)\r\n\r\n    def val_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.valid_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n    def test_dataloader(self) -> DataLoader:\r\n        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=4)\r\n\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super(SimpleNet, self).__init__()\r\n\r\n        hidden_layers = []\r\n        d_inp = 28 * 28\r\n        for d_hid in d_hids:\r\n            hidden_layers.append(nn.Linear(d_inp, d_hid))\r\n            hidden_layers.append(nn.ReLU())\r\n            hidden_layers.append(nn.Dropout(p_drop))\r\n            d_inp = d_hid\r\n        hidden_layers.append(nn.Linear(d_inp, 10))\r\n\r\n        self.layers = nn.Sequential(*hidden_layers)\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.layers(inputs)\r\n\r\n\r\nclass LitSimpleNet(pl.LightningModule):\r\n    def __init__(self, d_hids: List[int], p_drop: float):\r\n        super().__init__()\r\n        self.model = SimpleNet(d_hids, p_drop)\r\n        self.criterion = nn.CrossEntropyLoss()\r\n        self.accuracy = torchmetrics.Accuracy()\r\n\r\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\r\n        return self.model(inputs.view(-1, 28 * 28))\r\n\r\n    def training_step(self, batch, batch_idx) -> torch.Tensor:\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        return self.criterion(outputs, targets)\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        inputs, targets = batch\r\n        outputs = self(inputs)\r\n        self.accuracy(outputs, targets)\r\n        self.log(\"valid_acc\", self.accuracy, on_step=False, on_epoch=True, prog_bar=True)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=3e-4, weight_decay=1e-5)\r\n\r\n\r\ndef objective(trial: optuna.trial.Trial) -> float:\r\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\r\n    p_drop = trial.suggest_float(\"p_drop\", 0.1, 0.5)\r\n    d_hids = [trial.suggest_int(f\"d_hid_{i}\", 16, 128, log=True) for i in range(n_layers)]\r\n\r\n    datamodule = FashionDataModule(\".\", 128)\r\n    model = LitSimpleNet(d_hids, p_drop)\r\n    trainer = pl.Trainer(\r\n        max_epochs=20,\r\n        accelerator=\"gpu\",\r\n        devices=1,\r\n        enable_checkpointing=False,\r\n        logger=True,\r\n        default_root_dir=\".\",\r\n        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"valid_acc\")]\r\n    )\r\n\r\n    hparams = dict(n_layers=n_layers, d_hids=d_hids, p_drop=p_drop)\r\n    trainer.logger.log_hyperparams(hparams)\r\n    trainer.fit(model, datamodule=datamodule)\r\n    return trainer.callback_metrics[\"valid_acc\"].item()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    pruner = optuna.pruners.MedianPruner()\r\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\r\n    study.optimize(objective, n_trials=100, timeout=1000)\r\n\r\n    print(\"Number of Finished Trials:\", len(study.trials))\r\n\r\n    trial = study.best_trial\r\n    print(\"Best Trial:\")\r\n    print(\"\\tValue:\", trial.value)\r\n    print(\"\\tParams:\")\r\n    for key, value in trial.params.items():\r\n        print(f\"\\t\\t{key}: {value}\")\r\n\r\n```\r\n\r\n```bash\r\n[W 2022-09-08 20:14:45,294] Trial 0 failed because of the following error: AttributeError(\"'AcceleratorConnector' object has no attribute 'distributed_backend'\")\r\nTraceback (most recent call last):\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/study\/_optimize.py\", line 196, in _run_trial\r\n    value_or_values = func(trial)\r\n  File \"optuna_examples\/optuna_lightning_example.py\", line 89, in objective\r\n    trainer = pl.Trainer(\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/argparse.py\", line 345, in insert_env_defaults\r\n    return fn(self, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 497, in __init__\r\n    self._call_callback_hooks(\"on_init_start\")\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1585, in _call_callback_hooks\r\n    fn(self, *args, **kwargs)\r\n  File \"\/home\/wyn\/miniconda3\/envs\/wyn\/lib\/python3.8\/site-packages\/optuna\/integration\/pytorch_lightning.py\", line 61, in on_init_start\r\n    trainer._accelerator_connector.distributed_backend is not None  # type: ignore\r\nAttributeError: 'AcceleratorConnector' object has no attribute 'distributed_backend'\r\n```\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/github\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.ipynb\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/Lightning-AI\/lightning\/blob\/master\/examples\/pl_bug_report\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n\r\n### Expected behavior\r\n\r\nShould not report any errors.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/Lightning-AI\/lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n\r\n```\r\n\r\n\r\n<details>\r\n  <summary>Details<\/summary>\r\n    Paste the output here and move this toggle outside of the comment block.\r\n<\/details>\r\n\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- Lightning Component:  Trainer\r\n- PyTorch Lightning Version:  1.7.5\r\n- PyTorch Version:  1.12.1\r\n- Python version: 3.8.13\r\n- OS: Linux (Ubuntu 20.04)\r\n- CUDA\/cuDNN version: 11.3.1\r\n- How you installed PyTorch: conda\r\n\r\n\n\ncc @akihironitta",
        "Challenge_closed_time":1667802669000,
        "Challenge_created_time":1662640213000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/14604",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":17.8,
        "Challenge_reading_time":88.49,
        "Challenge_repo_contributor_count":447.0,
        "Challenge_repo_fork_count":2788.0,
        "Challenge_repo_issue_count":14589.0,
        "Challenge_repo_star_count":22027.0,
        "Challenge_repo_watch_count":231.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":75,
        "Challenge_solved_time":1434.0155555556,
        "Challenge_title":"Optuna integration reports AttributeError",
        "Challenge_topic":"Artifact Management",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":522,
        "Platform":"Github",
        "Solution_body":"Hey, @RegiusQuant. \r\n\r\nSide answer, you might be interested by Lightning HPO: https:\/\/github.com\/Lightning-AI\/lightning-hpo. This enables to run Optuna with PyTorch Lightning without friction and scalable in the cloud.\r\n\r\n Hey, @RegiusQuant - Thanks for the question. Can you please point me to the version of `optuna` that you are using?  For reference: https:\/\/github.com\/optuna\/optuna\/issues\/3978 @krshrimali Optuna version\uff1a3.0.0 I'm observing the same issue with Optuna 3.0.2 @hrzn Hi, I'm from the Optuna-dev team. Optuna's pytorch-lightning (PL) integration module doesn't support PL>=1.6 because it broke backwards-compatibility as investigated in https:\/\/github.com\/optuna\/optuna\/issues\/3418. Unfortunately, Optuna team doesn't have time to fix the module soon to support recent PL; we would like to wait for a PR from optuna and PL users.\r\n\r\n@tchaton I believe you can close this issue because the issue comes from Optuna... With Optuna==3.0.2 with lightning==1.5.10, I got \r\n`ValueError: optuna.integration.PyTorchLightningPruningCallback supports only optuna.storages.RDBStorage in DDP.`\r\nAfter downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.  @mikiotada Again, the error does not relate to PL. As the error message said, Optuna's integration does not support DDP without RDBStorage. \r\n\r\n> After downgrading Optuna to 2.0.0 (arbitrary version) while keeping lightning==1.5.10, it ran without any error.\r\n\r\nIn my understanding, Optuna 2.x didn't officially support DDP; it does not work as you expected, I'm afraid even though there was no error. Closing this issue as there seems nothing we can address from our side. Please refer to https:\/\/github.com\/optuna\/optuna\/issues\/3418.",
        "Solution_gpt_summary":"pytorch lightn integr modul broke backward compat downgrad keep lightn offici ddp team time modul soon",
        "Solution_link_count":4.0,
        "Solution_original_content":"regiusqu lightn hpo http github com lightn lightn hpo enabl run pytorch lightn friction scalabl cloud regiusqu version http github com krshrimali version observ hrzn dev team pytorch lightn integr modul broke backward compat http github com unfortun team time modul soon wait tchaton believ close come lightn valueerror integr pytorchlightningpruningcallback storag rdbstorag ddp downgrad arbitrari version keep lightn ran mikiotada relat messag said integr ddp rdbstorag downgrad arbitrari version keep lightn ran offici ddp afraid close address http github com",
        "Solution_preprocessed_content":"lightn hpo enabl run pytorch lightn friction scalabl cloud version valueerror downgrad keep ran relat messag said integr ddp rdbstorag downgrad keep ran offici ddp afraid close address",
        "Solution_readability":8.9,
        "Solution_reading_time":21.93,
        "Solution_score_count":5.0,
        "Solution_sentence_count":28.0,
        "Solution_word_count":232.0,
        "Tool":"Optuna",
        "Challenge_contributor_issue_ratio":0.0306395229,
        "Challenge_watch_issue_ratio":0.0158338474
    },
    {
        "Challenge_adjusted_solved_time":4.5216666667,
        "Challenge_answer_count":1,
        "Challenge_body":"### What happened + What you expected to happen\n\nNotebook is broken due to missing permission first, maybe more issues down the road. @Yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### Versions \/ Dependencies\n\nmaster\n\n### Reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:sigopt_example`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Challenge_closed_time":1659051271000,
        "Challenge_created_time":1659034993000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/27203",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":12.8,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":4.5216666667,
        "Challenge_title":"[AIR] Fix  \/\/doc\/source\/tune\/examples:sigopt_example",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":61,
        "Platform":"Github",
        "Solution_body":"Duplicate of https:\/\/github.com\/ray-project\/ray\/issues\/26567",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"duplic http github com rai rai",
        "Solution_preprocessed_content":null,
        "Solution_readability":29.2,
        "Solution_reading_time":0.85,
        "Solution_score_count":1.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"### What happened + What you expected to happen\n\nI tried to run a ray tune job using the Sigopt suggester on a remote cluster. The sigopt suggester object was later found to be unserialisable however the stack trace gave no indication of this.\r\n\r\nThe stack trace looks like this\r\n\r\ndiscussion around this issue can be found here https:\/\/ray-distributed.slack.com\/archives\/CNECXMW22\/p1652417782100299\r\n\r\nThanks to Matthew Deng for finding the issue on this one!\n\n### Versions \/ Dependencies\n\nPython 3.8.12\r\nray==1.12.0\n\n### Reproduction script\n\n```\r\nimport ray\r\nimport numpy as np\r\nimport os\r\nos.environ['SIGOPT_KEY'] = APIKEYHERE\r\n\r\nfrom ray.tune.suggest.sigopt import SigOptSearch\r\nfrom ray import tune\r\nWORKING_DIR = os.getcwd()\r\n\r\n\r\n\r\ndef main():\r\n\r\n\tray.init(\r\n\t\taddress = \"ray:\/\/127.0.0.1:10001\",\r\n\t\t# address = \"auto\",\r\n\t\truntime_env = {\r\n\t\t\t\"working_dir\": WORKING_DIR,\r\n\t\t\t\"pip\": [\"sigopt==5.7.0\"]\r\n\t\t}\r\n\t)\r\n\t\r\n\tn_observations = 20\r\n\r\n\thyperparameter_space = [\r\n          {\r\n              'name': 'learning_rate',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'max': np.log(0.01),\r\n                  'min': np.log(0.0001)\r\n              },\r\n          },\r\n          {\r\n              'name': 'momentum',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'min': 0.85,\r\n                  'max': 0.99\r\n              },\r\n          },\r\n      ]\r\n\t\r\n\tsigopt_search = SigOptSearch(\r\n\t\t# OmegaConf.to_container(config.search_space),\r\n        hyperparameter_space,\r\n\t\tname=\"Tune distributed\",\r\n\t\tmax_concurrent=2, \r\n\t\tobservation_budget=n_observations,\r\n\t\tproject=\"sigopt-ray-integration\",\r\n\t\tmetric=[\"val_loss\"],\r\n\t\tmode=[\"min\"]\r\n\t\t# metric=[\"val_loss\", \"training_loss\"],\r\n\t\t# mode=[\"max\", \"min\"]\r\n\t)\r\n\r\n\ttune_config = {\r\n\t\t# \"config\": config\r\n\t}\r\n\tanalysis = tune.run(\r\n\t\ttrain_model,\r\n\t\tmetric=\"val_loss\",\r\n\t\tmode=\"min\",\r\n\t\tconfig=tune_config,\r\n\t\tnum_samples=n_observations,\r\n\t\tname=\"Tune distributed\",\r\n\t\tresources_per_trial={'gpu': 1},\r\n\t\tsearch_alg=sigopt_search,\r\n\t\t# scheduler=FIFOScheduler(),\r\n\t)\r\n\r\n\r\n\r\n\r\ndef train_model(config):\r\n    pass\r\n\r\nmain()\r\n\r\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1652740461000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/24864",
        "Challenge_link_count":1,
        "Challenge_open_time":4585.4275,
        "Challenge_readability":11.8,
        "Challenge_reading_time":23.77,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"[tune] SigOptSearch suggester is not serialisable",
        "Challenge_topic":"Data Visualization",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":183,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":5.2522222222,
        "Challenge_answer_count":3,
        "Challenge_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Challenge_closed_time":1603498330000,
        "Challenge_created_time":1603479422000,
        "Challenge_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.8,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":425.0,
        "Challenge_repo_fork_count":4048.0,
        "Challenge_repo_issue_count":30819.0,
        "Challenge_repo_star_count":23050.0,
        "Challenge_repo_watch_count":435.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":5.2522222222,
        "Challenge_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":102,
        "Platform":"Github",
        "Solution_body":"This is a consequence of search metric being able to be multi-metric. cc @krfricke \r\n\r\nAlso, let me ping the sigopt folks for a working API key... Should be fixed on #11583 . We'll pick this onto the release.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"consequ search metric multi metric krfrick ping folk api kei pick releas",
        "Solution_preprocessed_content":"consequ search metric ping folk api pick releas",
        "Solution_readability":3.4,
        "Solution_reading_time":2.45,
        "Solution_score_count":1.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":37.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0137901944,
        "Challenge_watch_issue_ratio":0.0141146695
    },
    {
        "Challenge_adjusted_solved_time":76.3483333333,
        "Challenge_answer_count":0,
        "Challenge_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Challenge_closed_time":1658150380000,
        "Challenge_created_time":1657875526000,
        "Challenge_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.1,
        "Challenge_reading_time":14.47,
        "Challenge_repo_contributor_count":439.0,
        "Challenge_repo_fork_count":17206.0,
        "Challenge_repo_issue_count":20680.0,
        "Challenge_repo_star_count":76088.0,
        "Challenge_repo_watch_count":860.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":21,
        "Challenge_solved_time":76.3483333333,
        "Challenge_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":153,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"SigOpt",
        "Challenge_contributor_issue_ratio":0.0212282398,
        "Challenge_watch_issue_ratio":0.0415860735
    },
    {
        "Challenge_adjusted_solved_time":341.7419444444,
        "Challenge_answer_count":8,
        "Challenge_body":"Hi,\r\n\r\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get **explainable** predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\r\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\r\n\r\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\r\n\r\n\r\n#### Environment details\r\n\r\n  - Google Cloud Notebook\r\n  - Python version: 3.7.12\r\n  - pip version: 21.3.1\r\n  - `google-cloud-aiplatform` version: 1.15.0\r\n\r\n#### Reference\r\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\r\n\r\n#### explanation-metadata.json\r\n(_Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key._)\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"A\": {},\r\n        \"B\": {}\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\n#### Model upload with explanation parameters and metadata\r\n```\r\n! gcloud ai models upload \\\r\n  --region=$REGION \\\r\n  --display-name=$MODEL_NAME \\\r\n  --container-image-uri=$PRED_IMAGE_URI \\\r\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\r\n  --explanation-method=sampled-shapley \\\r\n  --explanation-path-count=10 \\\r\n  --explanation-metadata-file=explanation-metadata.json\r\n```\r\n\r\n#### Prediction\/Explanation Input\r\n```\r\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\r\n\r\n# Another example\r\ninstances_2 = [[1.1,20], [2.2,21]]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances_2)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances_2) # Returns error\r\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\r\n```\r\n#### Prediction Server (Flask)\r\n```python\r\n# Custom Flask server to serve online predictions\r\n# Input for prediction\r\nraw_input = request.get_json()\r\ninput = raw_input['instances']\r\ndf = pd.DataFrame(input, columns = ['A', 'B'])\r\n# Prediction from model (loaded from GCP bucket)\r\npredictions = model.predict(df).tolist() # [0, 1]\r\nresponse = jsonify({\"predictions\": predictions})\r\nreturn response\r\n```\r\n\r\n#### Stack trace of error (1)\r\n```\r\n---------------------------------------------------------------------------\r\n_InactiveRpcError                         Traceback (most recent call last)\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     49         try:\r\n---> 50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\r\n    945                                       wait_for_ready, compression)\r\n--> 946         return _end_unary_response_blocking(state, call, False, None)\r\n    947 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\r\n    848     else:\r\n--> 849         raise _InactiveRpcError(state)\r\n    850 \r\n\r\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\r\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\r\n>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\r\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\r\n   1563             parameters=parameters,\r\n   1564             deployed_model_id=deployed_model_id,\r\n-> 1565             timeout=timeout,\r\n   1566         )\r\n   1567 \r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\r\n    917             retry=retry,\r\n    918             timeout=timeout,\r\n--> 919             metadata=metadata,\r\n    920         )\r\n    921 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\r\n    152             kwargs[\"metadata\"] = metadata\r\n    153 \r\n--> 154         return wrapped_func(*args, **kwargs)\r\n    155 \r\n    156 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n---> 52             raise exceptions.from_grpc_error(exc) from exc\r\n     53 \r\n     54     return error_remapped_callable\r\n\r\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\r\n---------------------------------------------------------------------------\r\n```",
        "Challenge_closed_time":1659550833000,
        "Challenge_created_time":1658320562000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.2,
        "Challenge_reading_time":79.22,
        "Challenge_repo_contributor_count":75.0,
        "Challenge_repo_fork_count":188.0,
        "Challenge_repo_issue_count":1846.0,
        "Challenge_repo_star_count":283.0,
        "Challenge_repo_watch_count":53.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":45,
        "Challenge_solved_time":341.7419444444,
        "Challenge_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI (Prediction without explanation works fine)",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":578,
        "Platform":"Github",
        "Solution_body":"Hi @jaycee-li,\r\nAny update on this? Would really appreciate your inputs! Hi @pankajrsingla, sorry for the late reply!\r\n\r\nSince instance_2 prediction works for your model, looks like your model takes unkeyed input. Could you please try this metadata setting:\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"X\": {},\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\nThen update the model, endpoint, and try:\r\n```\r\ninstances = [[1.1,20], [2.2,21]]\r\nendpoint.explain(instances=instances)\r\n```\r\n\r\nPlease let me know if this works for you. Hi @jaycee-li,\r\nThank you so much for your response.\r\nI tried your suggestion, but I got the same error as before.\r\n`Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"`\r\n\r\nIf you see the code for my prediction server, it can take both unkeyed as well as keyed input (prediction works fine for both cases), since it converts the input to a dataframe. The output is definitely unkeyed. However, I am still confused as to what should be the contents of the explanation-metadata.json file.\r\n\r\nAlso, just to be sure - the same API (predict) in the flask server is supposed to work for both predictions and explanations, right? Or do I need to create a separate API for 'explain'?\r\n\r\nIf you have any other suggestions, I would be more than happy to try them out. \r\n(If that would help, I can also send you the full contents of the Jupyter notebook - all code one place - if you share your email id.)\r\n\r\nPlease let me know!\r\n\r\nThank you! It would be helpful if you can share the notebook to jayceeli@google.com\r\n\r\nThank you very much! Done!\r\nThanks! :) Hi @pankajrsingla ,\r\n\r\nI got `AttributeError: 'Blob' object has no attribute 'open'` for `with blob.open(\"wb\") as f:` in your TRAIN_IMAGE_URI. So I was stuck here and didn't reproduce the error you got. \r\n\r\nYou mixed CLI, gapic API, and SDK in your code. Since I'm not familiar with CLI tool, I'm not very sure what the problem is. Maybe it's due to your PRED_IMAGE_URI? I would suggest you to try a pre-built container(`us-docker.pkg.dev\/vertex-ai\/prediction\/sklearn-cpu.1-0:latest`) when uploading the model.\r\n\r\nI drafted a notebook that used SDK only to train, upload, deploy a same model as yours. And it can successfully make predictions and explanations. I've shared the notebook with you for your reference.\r\n\r\nPlease let me know if you still get the error. Thanks! Hi @pankajrsingla ,\r\n\r\nPlease check this [notebook](https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/community\/ml_ops\/stage6\/get_started_with_xai_and_custom_server.ipynb) (Specifically **Create the model server** and **Build a FastAPI HTTP server** sections) for how to use XAI with a custom container. Thanks a lot, @jaycee-li! This is exactly what I was looking for!\r\nI will give this a try for my model, and will update you once I have the results. This should work.\r\n\r\nThank you!",
        "Solution_gpt_summary":"metadata set model endpoint instanc pre built upload model notebook share direct notebook explain xai",
        "Solution_link_count":1.0,
        "Solution_original_content":"jayce updat input pankajrsingla sorri late repli instanc predict model model unkei input metadata set input output updat model endpoint instanc endpoint explain instanc instanc jayce respons tri respons predict server respons field predict miss respons request browser proxi sent request server predict server unkei kei input predict convert input datafram output definit unkei explan metadata json file api predict flask server suppos predict explan creat separ api explain happi send jupyt notebook share email share notebook jayce com pankajrsingla attributeerror blob object attribut open blob open train imag uri stuck reproduc mix cli gapic api sdk familiar cli mayb pred imag uri pre built docker pkg dev vertex predict sklearn cpu latest upload model draft notebook sdk train upload deploi model successfulli predict explan share notebook pankajrsingla notebook http github com googlecloudplatform vertex sampl blob notebook commun op stage start xai server ipynb creat model server build fastapi http server section xai jayce exactli model updat",
        "Solution_preprocessed_content":"updat input sorri late repli predict model model unkei input metadata set updat model endpoint respons tri predict server unkei kei input convert input datafram output definit unkei file api flask server suppos predict explan creat separ api explain happi send jupyt notebook share email share notebook stuck reproduc mix cli gapic api sdk familiar cli mayb upload model draft notebook sdk train upload deploi model successfulli predict explan share notebook xai exactli model updat",
        "Solution_readability":5.5,
        "Solution_reading_time":35.96,
        "Solution_score_count":0.0,
        "Solution_sentence_count":38.0,
        "Solution_word_count":449.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0406283857,
        "Challenge_watch_issue_ratio":0.0287107259
    },
    {
        "Challenge_adjusted_solved_time":675.9419444444,
        "Challenge_answer_count":1,
        "Challenge_body":"### Issue in Vertex AI Automl training python API while using vertex dataset with source as foreign project's BQ table\r\n- What would you like to achieve: I would like to run automl training job(using automl api python) with vertex dataset created from foreign project's BigQuery table.\r\n- Even though 'vertex AI service agent' and 'Vertex AI Custom code service agent' added to respective bigquery dataset with 'bigquery data editor' role, we are receiving bigquery.tables.get persmission denied error when we try run automl training job. \r\n\r\nError:\r\n`INFO:google.cloud.aiplatform.training_jobs:No column transformations provided, so now retrieving columns from dataset in order to set default column transformations. \r\nTraceback (most recent call last): \r\nFile \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 190, in <module> \r\nautoml_tabular = create_training_pipeline_tabular(train_type, project_id, display_name, int(dataset_id), location, model_display_name, float(training_fraction_split), float(validation_fraction_split), float(test_fraction_split), int(budget_milli_node_hours), disable_early_stopping, sync, target_column) \r\nFile \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 57, in create_training_pipeline_tabular \r\nmodel = tabular_job.run( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3461, in run \r\nreturn self._run( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/base.py\", line 730, in wrapper \r\nreturn method(*args, **kwargs) \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3645, in _run \r\n) = column_transformations_utils.get_default_column_transformations( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/utils\/column_transformations_utils.py\", line 42, in get_default_column_transformations \r\nfor column_name in dataset.column_names \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 81, in column_names \r\nself._retrieve_bq_source_columns( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 241, in _retrieve_bq_source_columns \r\ntable = client.get_table(bq_table_uri) \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 1034, in get_table \r\napi_response = self._call_api( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 782, in _call_api \r\nreturn call() \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 283, in retry_wrapped_func \r\nreturn retry_target( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 190, in retry_target \r\nreturn target() \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/_http\/__init__.py\", line 480, in api_request \r\nraise exceptions.from_http_response(response) \r\ngoogle.api_core.exceptions.Forbidden: 403 GET https:\/\/bigquery.googleapis.com\/bigquery\/v2\/projects\/<<projectID>>\/datasets\/<<datasetID>>\/tables\/<<tableID>>?prettyPrint=false: Access Denied: Table <<TableID>>: Permission bigquery.tables.get denied on table <<TableID>> (or it may not exist). \r\n`\r\n\r\nAfter raising google support, we got a temporary workaround by adding column specification in API call (reference case# 29535929, 29310889).\r\n**Sample API call of the workaround:**\r\n`job = training_jobs.AutoMLTabularTrainingJob( \r\ndisplay_name=\"my_display_name\", \r\noptimization_prediction_type=\"classification\", \r\noptimization_objective=\"minimize-log-loss\", \r\ncolumn_specs={\"column_1\": \"auto\", \"column_2\": \"numeric\"}, \r\nlabels={'key': 'value'}, \r\n) \r\n`\r\n\r\nWe would want to have this fixed in in SDK. If possible, we would like to have a gcloud command for automl modules.\r\n",
        "Challenge_closed_time":1649696673000,
        "Challenge_created_time":1647263282000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1078",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":15.3,
        "Challenge_reading_time":53.0,
        "Challenge_repo_contributor_count":75.0,
        "Challenge_repo_fork_count":188.0,
        "Challenge_repo_issue_count":1846.0,
        "Challenge_repo_star_count":283.0,
        "Challenge_repo_watch_count":53.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":33,
        "Challenge_solved_time":675.9419444444,
        "Challenge_title":"Vertex AI Automl training error when training dataset with foreign project's BQ table as source",
        "Challenge_topic":"Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":316,
        "Platform":"Github",
        "Solution_body":"I've updated the docstring to clarify that the service account needs certain permissions when neither `column_transformations` or `column_specs` is set.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"updat docstr clarifi servic account permiss column transform column spec set",
        "Solution_preprocessed_content":"updat docstr clarifi servic account permiss set",
        "Solution_readability":14.6,
        "Solution_reading_time":1.95,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":20.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0406283857,
        "Challenge_watch_issue_ratio":0.0287107259
    },
    {
        "Challenge_adjusted_solved_time":52.535,
        "Challenge_answer_count":6,
        "Challenge_body":"## Expected Behavior\r\nCode example  from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended.\r\n\r\n## Actual Behavior\r\nCode example below from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work\r\n\r\n```\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\n    from google.cloud import aiplatform\r\n    aiplatform.init(project=project, location=region)\r\n\r\n    # THIS IS THE METHOD THAT DOESN'T APPEAR TO WORK\r\n    model_upload_op = gcc_aip.ModelUploadOp(\r\n            project=project,\r\n            location=region,\r\n            display_name=model_display_name,\r\n            artifact_uri=model.uri,\r\n            serving_container_image_uri=serving_container_image_uri\r\n            )\r\n```\r\nOn the other hand, the method below worked:\r\n```\r\n # THIS METHOD DOES WORK\r\n    # aiplatform.Model.upload(\r\n    #     display_name=model_display_name,\r\n    #     artifact_uri=model.uri,\r\n    #     serving_container_image_uri=serving_container_image_uri,\r\n    # )\r\n```\r\n\r\nI'm currently using Vertex AI Pipelines to train a model and upload to Vertex AI. Currently in the pipeline, I'm attempting to use the ModelUploadOp class to upload a custom model to Vertex AI models. The logs show the job is succeeding, but the model never actually gets uploaded.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n## Specifications\r\n\r\nVersion: \r\n- Pipeline SDK (Kubeflow Pipelines\/TFX) Version: kfp\r\n- Pipelines Version: kfp==1.8.11\r\n- Platform: Google Cloud Vertex AI \r\n\r\n[1]: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Challenge_closed_time":1646371495000,
        "Challenge_created_time":1646182369000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/349",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":15.2,
        "Challenge_reading_time":22.2,
        "Challenge_repo_contributor_count":84.0,
        "Challenge_repo_fork_count":365.0,
        "Challenge_repo_issue_count":1349.0,
        "Challenge_repo_star_count":544.0,
        "Challenge_repo_watch_count":33.0,
        "Challenge_score_count":2.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":52.535,
        "Challenge_title":"ModelUploadOp from \"Vertex AI Pipelines: model upload using google-cloud-pipeline-components\"  does not work",
        "Challenge_topic":"TensorBoard Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":174,
        "Platform":"Github",
        "Solution_body":"There was a recent breaking change. Will update notebook accordingly. Notebook has been updated, tested and merged. @andrewferlitsch can we close this issue since the notebook is merged ? yes, I will close it. I thought I had. Hi all,\r\nI saw this thread and the updated notebook - thanks for fixing it. \r\n\r\nI can't help but think that using `artifact_uri=WORKING_DIR` in the Importer Node seems misaligned with Kubeflow's focus on Artifacts and Artifact management. Is it possible to set `artifact_uri` to the Artifact location without hardcoding `WORKING_DIR`?\r\n\r\n```\r\nimport_unmanaged_model_task = importer_node.importer(\r\n        artifact_uri=WORKING_DIR,        <--- Can we set this without hardcoding WORKING_DIR?\r\n        artifact_class=artifact_types.UnmanagedContainerModel,\r\n        metadata={\r\n            \"containerSpec\": {\r\n                \"imageUri\": \"us-docker.pkg.dev\/cloud-aiplatform\/prediction\/tf2-cpu.2-3:latest\",\r\n            },\r\n        },\r\n    )\r\n```\r\n\r\nTo make things simple, let's say that instead of putting the training code in CustomTrainingJobOp, you define a custom function (below). In this case wouldn't it be more Kubeflow-ish to replace save the file to `Output[Model].path` instead of hard coding `WORKING_DIR` , like the following? \r\n\r\n```\r\ndef train_model(dataset: Input[Dataset],  model: Output[Model]):\r\n      ...\r\n\r\n       # then save model\r\n       # model.save(WORKING_DIR)   <---- This is the way outlined in the notebook\r\n       model.save(model.path)         <--- This seems more aligned with KFP than above\r\n```\r\n\r\nThen, when you wanted to upload the model, you would again replace `WORKING_DIR` with the location of the Artifact set by Kubeflow.\r\n\r\n```\r\n@kfp.dsl.pipeline(...)\r\ndef pipeline(...):\r\n        ...\r\n\r\n        # train model\r\n        train_model_op = train_model(...)\r\n\r\n        import_unmanaged_model_task = importer_node.importer(\r\n                artifact_uri=train_model_op.outputs[\"model\"].uri,         <---- USING ARTIFACT LOCATION\r\n                artifact_class=artifact_types.UnmanagedContainerModel,\r\n                metadata={\r\n                     \"containerSpec\": {\r\n                      \"imageUri\": serving_container_image_uri,\r\n                 },\r\n               },\r\n           )\r\n```\r\nBut unfortunately you can't actually do this because you get an error: \"AttributeError: 'PipelinParam' object has no attribute uri'\". To avoid this error, you could also nest the Importer Node into a custom Component that has Input[Model] as one of the parameters. Then you could set `artifact_uri=model.uri`. \r\n\r\n```\r\n@component(...)\r\ndef custom_importer(trained_model: Input[Model], vertex_model: Output[Model]):\r\n     import_unmanaged_model_task = importer_node.importer(\r\n                artifact_uri=trained_model.uri,         <---- USING ARTIFACT LOCATION\r\n                artifact_class=artifact_types.UnmanagedContainerModel,\r\n                metadata={\r\n                     \"containerSpec\": {\r\n                      \"imageUri\": serving_container_image_uri,\r\n                 },\r\n               },\r\n           )\r\n```\r\nUnfortunately, you can't do this as you get \"TypeError: There are no registered serializers for type \"google.UnmanagedContainerModel\".\" @natetsang If you want an easy way to upload models to Vertex Model Registry, you can use my components: https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/KFPv2_hell\/components\/google-cloud\/Vertex_AI\/Models \r\nExample usage: https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/05b2f255f8ccd7d8588f8143a76536bf83c2c7c7\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI\/pipeline.py#L51\r\n\r\n```Python\r\nupload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https:\/\/raw.githubusercontent.com\/Ark-kun\/pipeline_components\/719783ef44c04348ea23e247a93021d91cfe602d\/components\/google-cloud\/Vertex_AI\/Models\/Upload_Tensorflow_model\/component.yaml\")\r\n\r\n...\r\n    vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\r\n        model=model,\r\n    ).outputs[\"model_name\"]\r\n```\r\nWhere `model` is a `TensorflowSavedModel` artifact that was produced by `model.save(model_path)`.\r\n\r\nPlease open an issue in my repo if you have any issues or feature requests for the components I've linked.",
        "Solution_gpt_summary":null,
        "Solution_link_count":3.0,
        "Solution_original_content":"break updat notebook accordingli notebook updat test merg andrewferlitsch close notebook merg close thread updat notebook artifact uri dir import node misalign kubeflow focu artifact artifact set artifact uri artifact locat hardcod dir import unmanag model task import node import artifact uri dir set hardcod dir artifact class artifact type unmanagedcontainermodel metadata containerspec imageuri docker pkg dev cloud aiplatform predict cpu latest put train customtrainingjobop defin function kubeflow ish replac save file output model path dir train model dataset input dataset model output model save model model save dir outlin notebook model save model path align kfp upload model replac dir locat artifact set kubeflow kfp dsl pipelin pipelin train model train model train model import unmanag model task import node import artifact uri train model output model uri artifact locat artifact class artifact type unmanagedcontainermodel metadata containerspec imageuri serv imag uri unfortun attributeerror pipelinparam object attribut uri avoid nest import node compon input model paramet set artifact uri model uri compon import train model input model vertex model output model import unmanag model task import node import artifact uri train model uri artifact locat artifact class artifact type unmanagedcontainermodel metadata containerspec imageuri serv imag uri unfortun typeerror regist serial type unmanagedcontainermodel natetsang upload model vertex model registri compon http github com ark kun pipelin compon tree kfpv hell compon cloud vertex model usag http github com ark kun pipelin compon blob bffccddfabfccc sampl cloud vertex train tabular regress model tensorflow import vertex pipelin upload tensorflow model cloud vertex compon load compon url http raw githubusercont com ark kun pipelin compon efceaeadcf compon cloud vertex model upload tensorflow model compon yaml vertex model upload tensorflow model cloud vertex model model output model model tensorflowsavedmodel artifact produc model save model path open repo featur request compon link",
        "Solution_preprocessed_content":"break updat notebook accordingli notebook updat test merg close notebook merg close thread updat notebook import node misalign kubeflow focu artifact artifact set artifact locat hardcod put train customtrainingjobop defin function replac save file upload model replac locat artifact set kubeflow unfortun attributeerror pipelinparam object attribut uri avoid nest import node compon input paramet set unfortun typeerror regist serial type upload model vertex model registri compon usag artifact produc open repo featur request compon link",
        "Solution_readability":15.8,
        "Solution_reading_time":49.27,
        "Solution_score_count":5.0,
        "Solution_sentence_count":32.0,
        "Solution_word_count":353.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0622683469,
        "Challenge_watch_issue_ratio":0.0244625649
    },
    {
        "Challenge_adjusted_solved_time":234.8477777778,
        "Challenge_answer_count":7,
        "Challenge_body":"### Contact Details [Optional]\n\nfrancogbocci@gmail.com\n\n### System Information\n\nZenML version: 0.20.5\r\nInstall path: \/Users\/f.bocci\/Library\/Caches\/pypoetry\/virtualenvs\/banana-bMSm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\nPython version: 3.9.6\r\nPlatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nEnvironment: native\r\nIntegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### What happened?\n\nTrying to follow the [guide to run a pipeline using Vertex AI](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because ZenML does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nStack Components:\r\n      alerter                 Commands to interact with alerters.\r\n      annotator               Commands to interact with annotators.\r\n      artifact-store          Commands to interact with artifact stores.\r\n      container-registry      Commands to interact with container registries.\r\n      data-validator          Commands to interact with data validators.\r\n      experiment-tracker      Commands to interact with experiment trackers.\r\n      feature-store           Commands to interact with feature stores.\r\n      model-deployer          Commands to interact with model deployers.\r\n      orchestrator            Commands to interact with orchestrators.\r\n      secrets-manager         Commands to interact with secrets managers.\r\n      step-operator           Commands to interact with step operators.\r\n$ zenml metadata-store\r\nError: No such command 'metadata-store'.\r\n```\n\n### Reproduction steps\n\n1. zenml metadata-store\r\n\r\nIf I don't add it and run the Vertex AI pipeline, it fails.\r\n\n\n### Relevant log output\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Challenge_closed_time":1667472145000,
        "Challenge_created_time":1666626693000,
        "Challenge_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/1001",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":20.77,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":246.0,
        "Challenge_repo_issue_count":1160.0,
        "Challenge_repo_star_count":2571.0,
        "Challenge_repo_watch_count":37.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":24,
        "Challenge_solved_time":234.8477777778,
        "Challenge_title":"[BUG]: Vertex AI blogpost is outdated after 0.20.0 release",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":186,
        "Platform":"Github",
        "Solution_body":"@francobocciDH Thanks for reporting the issue. We have recently undergone a [big architectural shift](https:\/\/blog.zenml.io\/zenml-revamped\/) and therefore a lot of the blog is a bit outdated! In particular, the metadata store is no longer a required stack component.\r\n\r\nIn order to make the vertex orchestrator work, I would suggest either taking a look at the [updated docs page](https:\/\/docs.zenml.io\/component-gallery\/orchestrators\/gcloud-vertexai), or taking a look at the [migration guide](https:\/\/docs.zenml.io\/guidelines\/migration-zero-twenty) that will help you update that blog's code to  the 0.20.5 world. Hey! Thanks for the quick reply. I followed the updated docs page. I checked the post as well to see if there is something different, but following the docs I'm still getting the error\r\n```\r\nMaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nConnectionError: HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: \/api\/v1\/login (Caused by \r\nNewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f46e31ea0a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\n```\r\n\r\nFor what I saw following the traceback, it's something related to:\r\n`\/usr\/local\/lib\/python3.9\/site-packages\/zenml\/zen_stores\/base_zen_store.py:104`\r\nbut I haven't solved it yet.\r\n\r\nCould you follow the steps on the guide and make it work? I downloaded the image, got into the container and launch the entrypoint being used in Vertex AI:\r\n`python -m zenml.entrypoints.entrypoint --entrypoint_config_source zenml.integrations.gcp.orchestrators.vertex_entrypoint_configuration.VertexEntrypointConfiguration@zenml_0.20.5 --step_name importer --vertex_job_id test1234`\r\n\r\nAnd I got the same error. After that, I ran the ZenML Server (`zenml up`), and I got a different error (so apparently, something's missing?)\r\n\r\nThe error I'm getting now comes from `tfx` package and it's:\r\n```\r\nThe filesystem scheme 'gs:\/\/' is not available for use. For expanded filesystem scheme support, install the `tensorflow` package to enable additional filesystem plugins\r\n```\r\n\r\n I made it work locally. I had to:\r\n1) Register the `artifact-store` using GCS\r\n2) Set it as the artifact-store in the \"default\" stack\r\n3) Start zenml server\r\n\r\nShould this be done in some specific way by the user? @francobocciDH I think the main problem you are suffering from is that you have not deployed ZenML on Google before doing all this. Its our fault as I see that the Vertex orchestrator guide does not make this clear at all (only if you read the docs from the top, it does).\r\n\r\nPlease try [deploying ZenML](https:\/\/docs.zenml.io\/getting-started\/deploying-zenml) to google first. The easiest way to do it is to do:\r\n\r\n```\r\nzenml deploy\r\n```\r\n\r\nAfter you have done this, you can connect to the remote ZenML deployemnt, and re-register your stack as described in the Vertex AI docs, and then run your pipeline. It should work then! @francobocciDH Did this work out? Hey @htahir1 , yes, I deployed it and it worked. It could be clearer in the Vertex AI section of the docs, but it is clearly mentioned in other places of the documentation, so it's my fault for missing this. We can close this from my side. Let me know if there is anything I can help with \ud83d\udc4d ",
        "Solution_gpt_summary":"run pipelin outdat guid zenml metadata store stack metadata store longer stack compon updat doc page migrat guid updat world updat doc page deploi zenml run pipelin deploi zenml regist artifact store gc set artifact store default stack start zenml server pipelin",
        "Solution_link_count":4.0,
        "Solution_original_content":"francoboccidh report undergon big architectur shift http blog zenml zenml revamp blog bit outdat metadata store longer stack compon order vertex orchestr updat doc page http doc zenml compon galleri orchestr gcloud vertexai migrat guid http doc zenml guidelin migrat updat blog world quick repli updat doc page doc maxretryerror httpconnectionpool host port retri exceed url api login newconnectionerror establish connect errno connect refus connectionerror httpconnectionpool host port retri exceed url api login newconnectionerror establish connect errno connect refus traceback relat usr local lib site packag zenml zen store base zen store haven step guid download imag launch entrypoint zenml entrypoint entrypoint entrypoint config sourc zenml integr orchestr vertex entrypoint configur vertexentrypointconfigur zenml step import vertex job test ran zenml server zenml appar miss come tfx packag filesystem scheme expand filesystem scheme instal tensorflow packag enabl addit filesystem plugin local regist artifact store gc set artifact store default stack start zenml server francoboccidh suffer deploi zenml fault vertex orchestr guid clear read doc deploi zenml http doc zenml start deploi zenml easiest zenml deploi connect remot zenml deployemnt regist stack doc run pipelin francoboccidh htahir deploi clearer section doc clearli document fault miss close",
        "Solution_preprocessed_content":"report undergon blog bit outdat metadata store longer stack compon order vertex orchestr updat blog world quick repli updat doc page doc traceback relat haven step guid download imag launch entrypoint ran zenml server come packag local regist gc set default stack start zenml server suffer deploi zenml fault vertex orchestr guid clear easiest connect remot zenml deployemnt stack doc run pipelin deploi clearer section doc clearli document fault miss close",
        "Solution_readability":10.0,
        "Solution_reading_time":43.38,
        "Solution_score_count":1.0,
        "Solution_sentence_count":34.0,
        "Solution_word_count":480.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0482758621,
        "Challenge_watch_issue_ratio":0.0318965517
    },
    {
        "Challenge_adjusted_solved_time":0.3547222222,
        "Challenge_answer_count":1,
        "Challenge_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Challenge_closed_time":1664935217000,
        "Challenge_created_time":1664933940000,
        "Challenge_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.8,
        "Challenge_reading_time":6.83,
        "Challenge_repo_contributor_count":20.0,
        "Challenge_repo_fork_count":14.0,
        "Challenge_repo_issue_count":558.0,
        "Challenge_repo_star_count":29.0,
        "Challenge_repo_watch_count":42.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":0.3547222222,
        "Challenge_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Challenge_topic":"Pandas Dataframe",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":53,
        "Platform":"Github",
        "Solution_body":"> \r\n\r\nWhen I upgrade the nodejs to v16.17.1 and add a call_option\r\n`\r\n      const call_options = {\r\n        timeout: 200000 \/\/ millis\r\n      }\r\n`\r\nproblem solved.\r\n",
        "Solution_gpt_summary":"endpoint predict upgrad node version option timeout millisecond",
        "Solution_link_count":0.0,
        "Solution_original_content":"upgrad nodej add option const option timeout milli",
        "Solution_preprocessed_content":null,
        "Solution_readability":6.8,
        "Solution_reading_time":1.59,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":18.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.0358422939,
        "Challenge_watch_issue_ratio":0.0752688172
    },
    {
        "Challenge_adjusted_solved_time":453.4755555556,
        "Challenge_answer_count":0,
        "Challenge_body":"## Description\r\n\r\nScikit Learn model in [`kubeflow_pipelines\/pipelines` directory](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/master\/notebooks\/kubeflow_pipelines\/pipelines\/solutions\/trainer_image\/train.py#L46) doesn't work in Vertex AI prediction environment, since it assumes the input as Pandas Dataframe and cannot handle JSON from Web API.\r\n\r\nAfter deploying the model following the labs, this issue can be reproduced with this code snippet.\r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [{'Elevation': [2841.0]},\r\n {'Aspect': [45, 0]},\r\n {'Slope': [0, 0]},\r\n {'Horizontal_Distance_To_Hydrology': [644.0]},\r\n {'Vertical_Distance_To_Hydrology': [282.0]},\r\n {'Horizontal_Distance_To_Roadways': [1376.0]},\r\n {'Hillshade_9am': [218.0]},\r\n {'Hillshade_Noon': [237.0]},\r\n {'Hillshade_3pm': [156.0]},\r\n {'Horizontal_Distance_To_Fire_Points': [1003.0]},\r\n {'Wilderness_Area': ['Commanche']},\r\n {'Soil_Type': ['C4758']}]\r\n\r\nendpoint.predict([instance])\r\n```\r\n\r\nreturns:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/6895245\/156965179-92e4e873-8f60-411c-86b7-df0685509e4c.png)\r\n\r\n## Approach\r\nRewrite feature definition part of `train.py` from:\r\nhttps:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/e87f3514dda440fb381a78f563bda177aa38ad80\/notebooks\/kubeflow_pipelines\/cicd\/solutions\/trainer_image_vertex\/train.py#L43-L63\r\n\r\nto:\r\n```python\r\n    numeric_feature_indexes = slice(0, 10)\r\n    categorical_feature_indexes = slice(10, 12)\r\n\r\n    preprocessor = ColumnTransformer(\r\n    transformers=[\r\n        ('num', StandardScaler(), numeric_feature_indexes),\r\n        ('cat', OneHotEncoder(), categorical_feature_indexes) \r\n    ])\r\n```\r\n\r\nAnd it should run with this \r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [\r\n    2841.0,\r\n    45.0,\r\n    0.0,\r\n    644.0,\r\n    282.0,\r\n    1376.0,\r\n    218.0,\r\n    237.0,\r\n    156.0,\r\n    1003.0,\r\n    \"Commanche\",\r\n    \"C4758\",\r\n]\r\nendpoint.predict([instance])\r\n```\r\n\r\nOutput:\r\n```\r\nPrediction(predictions=[1.0], deployed_model_id='4516996077043318784', explanations=None)\r\n```\r\n\r\n## Target Files\r\n[These 8 files ](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/search?q=numeric_features+%3D+%5B+++++++++%22Elevation%22%2C) should be update.",
        "Challenge_closed_time":1648259081000,
        "Challenge_created_time":1646626569000,
        "Challenge_link":"https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/issues\/171",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":17.8,
        "Challenge_reading_time":29.22,
        "Challenge_repo_contributor_count":14.0,
        "Challenge_repo_fork_count":220.0,
        "Challenge_repo_issue_count":286.0,
        "Challenge_repo_star_count":41.0,
        "Challenge_repo_watch_count":11.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":453.4755555556,
        "Challenge_title":"[Bug] scikit learn model feature definition doesn't work on Vertex AI Prediction.",
        "Challenge_topic":"Pipeline Configuration",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_word_count":152,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Vertex AI",
        "Challenge_contributor_issue_ratio":0.048951049,
        "Challenge_watch_issue_ratio":0.0384615385
    },
    {
        "Challenge_adjusted_solved_time":2751.6177777778,
        "Challenge_answer_count":2,
        "Challenge_body":"Synced astral-sweep-1: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/peg6pn8y\r\nRun peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: ERROR Run peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: Agent Starting Run: e8d1m877 with config:\r\nwandb: \tlayer_0-6: 2.581652533230976e-05\r\nwandb: \tlayer_12-18: 3.584294374584665e-05\r\nwandb: \tlayer_18-24: 4.488348372658677e-05\r\nwandb: \tlayer_6-12: 1.0161197251306803e-05\r\nwandb: \tnum_train_epochs: 40\r\nwandb: \tparams_classifier.dense.bias: 0.0005874506018709628\r\nwandb: \tparams_classifier.dense.weight: 0.0003389591868569285\r\nwandb: \tparams_classifier.out_proj.bias: 0.0003078179192499977\r\nwandb: \tparams_classifier.out_proj.weight: 0.0006868779346654171\r\nTracking run with wandb version 0.10.19\r\nSyncing run peach-sweep-2 to Weights & Biases (Documentation).\r\nProject page: https:\/\/wandb.ai\/sakrah\/humorize\r\nSweep page: https:\/\/wandb.ai\/sakrah\/humorize\/sweeps\/4sl6uygs\r\nRun page: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/e8d1m877\r\nRun data is saved locally in \/content\/wandb\/run-20210215_055312-e8d1m877",
        "Challenge_closed_time":1623275505000,
        "Challenge_created_time":1613369681000,
        "Challenge_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/993",
        "Challenge_link_count":4,
        "Challenge_open_time":null,
        "Challenge_readability":9.7,
        "Challenge_reading_time":16.88,
        "Challenge_repo_contributor_count":88.0,
        "Challenge_repo_fork_count":686.0,
        "Challenge_repo_issue_count":1416.0,
        "Challenge_repo_star_count":3418.0,
        "Challenge_repo_watch_count":58.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":2751.6177777778,
        "Challenge_title":"Getting Errors with wandb sweeps ",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":117,
        "Platform":"Github",
        "Solution_body":"I got this error when I tried to run wandb sweeps for a regressionn classifcation. It complained when I included the required num_labels. After removing it, the error is what I get. Are there some additional settings required besides setting regression=True. This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"tri run sweep regressionn classifc complain num label remov addit set set regress automat mark stale activ close activ contribut",
        "Solution_preprocessed_content":"tri run sweep regressionn classifc complain remov addit set set regress automat mark stale activ close activ contribut",
        "Solution_readability":7.2,
        "Solution_reading_time":5.32,
        "Solution_score_count":0.0,
        "Solution_sentence_count":7.0,
        "Solution_word_count":70.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0621468927,
        "Challenge_watch_issue_ratio":0.040960452
    },
    {
        "Challenge_adjusted_solved_time":829.3705555556,
        "Challenge_answer_count":4,
        "Challenge_body":"**Describe the bug**\r\nAfter running a model evaluation suite and exprorint to wandb using \"to_wandb\" function, the confusion matrix appears in the w&b page without the values\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n\r\n**Expected behavior**\r\nThe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: linux\r\n - Python Version:3.7.1\r\n - Deepchecks Version:0.7.2\r\n\r\n",
        "Challenge_closed_time":1657703576000,
        "Challenge_created_time":1654717842000,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1592",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":13.0,
        "Challenge_reading_time":8.52,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":829.3705555556,
        "Challenge_title":"[BUG] Weird behavior with \"to_wandb\" and confusion matrix",
        "Challenge_topic":"Spark Distributed Processing",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":75,
        "Platform":"Github",
        "Solution_body":"Hey @DL1992,\r\n\r\nFor me the export works fine, can you provide us with some more info about what you did?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/9868530\/173320868-74292589-5da2-4a15-9583-0855f592a602.png)\r\n hmmm, literally just result = suite.run follow by result.to_wandb.\r\nmy wandb version is 0.12.9\r\n what is the wandb and plotly version on the wandb server? This issue is stale and we couldn't reproduce it.\r\n@DL1992 feel free to reach out to us if this problem persists and we will try to help personally. Closing for now",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"export imag http imag githubusercont com png liter suit run version plotli version server stale reproduc free reach persist close",
        "Solution_preprocessed_content":"export liter version plotli version server stale reproduc free reach persist close",
        "Solution_readability":5.8,
        "Solution_reading_time":6.64,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":76.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0161216029,
        "Challenge_watch_issue_ratio":0.005988024
    },
    {
        "Challenge_adjusted_solved_time":72.2088888889,
        "Challenge_answer_count":1,
        "Challenge_body":"**Describe the bug**\r\n to_wandb not sectioning by train\/test and overrides runs by checks\r\n\r\n**To Reproduce**\r\nrun a suite with train\/test checks and duplicate checks in suite\r\n\r\n**Expected behavior**\r\nsections for each dataset and being able to run a suite with a couple of checks\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n\r\n",
        "Challenge_closed_time":1649580744000,
        "Challenge_created_time":1649320792000,
        "Challenge_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1210",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":14.6,
        "Challenge_reading_time":5.26,
        "Challenge_repo_contributor_count":35.0,
        "Challenge_repo_fork_count":159.0,
        "Challenge_repo_issue_count":2171.0,
        "Challenge_repo_star_count":2280.0,
        "Challenge_repo_watch_count":13.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":72.2088888889,
        "Challenge_title":"[BUG] to_wandb not sectioning by train\/test and overrides runs by checks",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":64,
        "Platform":"Github",
        "Solution_body":"Initial fix -  from 'name' to 'header', already applied in my local deepchecks environment (screenshots after that fix)\r\n\r\nSee example (note both DataDuplicates and CalibrationScore behavior):\r\n\r\nCode that ran:\r\n`custom_suite = Suite('Custom Evaluation',CalibrationScore(), CalibrationScore(),\r\n                     DataDuplicates(), DataDuplicates(columns=['Total Value']))\r\nsuite_res = custom_suite.run(train_ds, test_ds, rf_clf)\r\nsuite_res.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"my_run\")`\r\n\r\n### Suite Result\r\n#### Calibration Metric\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162167715-0db6398d-4822-4e35-a886-741753982ab5.png)\r\n\r\n#### Data Duplicates - \r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162167770-5e0450ce-a2ff-493f-8495-8779379d7a86.png)\r\n\r\n### W&B Logging - Suite Result\r\n#### Data Duplicates - appears 3 times (??)\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162168073-f79f6f3b-33fc-4453-8aff-4ae47652e979.png)\r\n\r\n\r\n#### Calibration Metric\r\nOnly one result (for each, after the above fix applied):\r\n![image](https:\/\/user-images.githubusercontent.com\/33841818\/162169354-b810fb42-2422-4ab5-8e5e-1bd377609fa0.png)\r\n\r\n\r\n\r\n",
        "Solution_gpt_summary":"initi appli local deepcheck environ involv header function properli section train test overrid run",
        "Solution_link_count":4.0,
        "Solution_original_content":"initi header appli local deepcheck environ screenshot note datadupl calibrationscor ran suit suit evalu calibrationscor calibrationscor datadupl datadupl column valu suit re suit run train test clf suit re entiti entiti run suit calibr metric imag http imag githubusercont com dbd png data duplic imag http imag githubusercont com ec aff png log suit data duplic time imag http imag githubusercont com fffb aff aee png calibr metric appli imag http imag githubusercont com bfb bdfa png",
        "Solution_preprocessed_content":"initi header appli local deepcheck environ ran suit calibr metric data duplic log suit data duplic time calibr metric",
        "Solution_readability":21.0,
        "Solution_reading_time":15.57,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":73.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0161216029,
        "Challenge_watch_issue_ratio":0.005988024
    },
    {
        "Challenge_adjusted_solved_time":1.9119444444,
        "Challenge_answer_count":1,
        "Challenge_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Challenge_closed_time":1583456108000,
        "Challenge_created_time":1583449225000,
        "Challenge_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.2,
        "Challenge_reading_time":2.73,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":88.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1.9119444444,
        "Challenge_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":25,
        "Platform":"Github",
        "Solution_body":"@koustuvsinha Thanks for bringing this up. Could you try the new version?\r\n\r\nWhen constructing the logbook, pass an additional parameter:\r\n\r\n```\r\nfrom ml_logger import logbook as ml_logbook\r\nlogbook_config = ml_logbook.make_config(\r\n    logger_file_path = <path to write logs>,\r\n    wandb_config = <wandb config or None>,\r\n    wandb_prefix_key = \"mode\",\r\n)\r\n```",
        "Solution_gpt_summary":"pass addit paramet construct logbook train test mode involv config function logger librari pass prefix kei paramet valu mode",
        "Solution_link_count":0.0,
        "Solution_original_content":"koustuvsinha version construct logbook pass addit paramet logger import logbook logbook logbook config logbook config logger file path config prefix kei mode",
        "Solution_preprocessed_content":"version construct logbook pass addit paramet",
        "Solution_readability":10.7,
        "Solution_reading_time":4.26,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":40.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0113636364,
        "Challenge_watch_issue_ratio":0.0227272727
    },
    {
        "Challenge_adjusted_solved_time":20.8108333333,
        "Challenge_answer_count":0,
        "Challenge_body":"When using wandb, it shows step as X and not episode.\r\n\r\nHence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Challenge_closed_time":1605698611000,
        "Challenge_created_time":1605623692000,
        "Challenge_link":"https:\/\/github.com\/MathisFederico\/LearnRL\/issues\/96",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":7.1,
        "Challenge_reading_time":3.89,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":4.0,
        "Challenge_repo_issue_count":137.0,
        "Challenge_repo_star_count":17.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":20.8108333333,
        "Challenge_title":"Add episode to wandb",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":29,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0072992701,
        "Challenge_watch_issue_ratio":0.0145985401
    },
    {
        "Challenge_adjusted_solved_time":305.2888888889,
        "Challenge_answer_count":9,
        "Challenge_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Challenge_closed_time":1644017877000,
        "Challenge_created_time":1642918837000,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":3.1,
        "Challenge_reading_time":2.07,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":305.2888888889,
        "Challenge_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"This is expected. If, for example, you are checkpointing every 50 steps and your training run crashes after step 212, the last checkpoint will have been at step 200. So when you resume training, you start again from step 201, and W&B will warn you about logging duplicate steps until you get to step 213.  Why do we even try to log those earlier steps again? Wandb has an option for resuming runs. Because the W&B callback that was restored from the checkpoint at step 200 does not know that we actually got to step 212 before crashing.\r\n\r\nAnd we are using the `resume` option. Although after just rereading their docs just now, I think we should set `resume` to \"allow\" instead of \"auto\". https:\/\/github.com\/allenai\/tango\/pull\/155\r\n\r\nFrom their [docs](https:\/\/docs.wandb.ai\/ref\/python\/init):\r\n\r\n> \"auto\" (or True): if the preivous run on this machine crashed, automatically resume it. Otherwise, start a new run. - \"allow\": if id is set with init(id=\"UNIQUE_ID\") or WANDB_RUN_ID=\"UNIQUE_ID\" and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. \r\n\r\n\"allow\" seems a little more robust for our use case, because maybe W&B won't always know when a run crashed (resulting in the \"auto\" option not working correctly). I'm assuming that what wandb wants is to have `resume=auto`, and then the next step we input into wandb is 201. But I think what happens now is that we resume with step 201 correctly, but we tell wandb that it's step 1 (because it's the first step we're actually running). > but we tell wandb that it's step 1\r\n\r\nNo, we tell W&B that it's step 201. W&B complains for the next 12 steps until we get to step 213. Ah, interesting. The documentation also says that new values will overwrite the old ones (which would be the right behavior), but the warning message clearly says it's dropping the new information. We could probably suppress those warnings though Is there a way we can make it actually overwrite the values? As it is, the values in the gap will be wrong (or at least might be wrong, if there is any non-determinism). > Is there a way we can make it actually overwrite the values?\r\n\r\nI don't think so \ud83d\ude15",
        "Solution_gpt_summary":"resum option set allow auto avoid log duplic step suppress warn messag overwrit valu gap",
        "Solution_link_count":2.0,
        "Solution_original_content":"checkpoint step train run crash step checkpoint step resum train start step warn log duplic step step log earlier step option resum run callback restor checkpoint step step crash resum option reread doc set resum allow auto http github com allenai tango pull doc http doc ref init auto preivou run crash automat resum start run allow set init uniqu run uniqu ident previou run automat resum run start run allow littl robust mayb run crash auto option resum auto step input resum step step step run step step complain step step document sai valu overwrit on warn messag clearli sai drop probabl suppress warn overwrit valu valu gap determin overwrit valu",
        "Solution_preprocessed_content":"checkpoint step train run crash step checkpoint step resum train start step warn log duplic step step log earlier step option resum run callback restor checkpoint step step crash option reread doc set allow auto auto preivou run crash automat resum start run allow set ident previou run automat resum run start run allow littl robust mayb run crash step input resum step step step step complain step step document sai valu overwrit on warn messag clearli sai drop probabl suppress warn overwrit valu valu gap overwrit valu",
        "Solution_readability":7.3,
        "Solution_reading_time":26.5,
        "Solution_score_count":0.0,
        "Solution_sentence_count":23.0,
        "Solution_word_count":376.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0304878049,
        "Challenge_watch_issue_ratio":0.012195122
    },
    {
        "Challenge_adjusted_solved_time":2728.7016666667,
        "Challenge_answer_count":1,
        "Challenge_body":"",
        "Challenge_closed_time":1652740320000,
        "Challenge_created_time":1642916994000,
        "Challenge_link":"https:\/\/github.com\/allenai\/tango\/issues\/151",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":2.9,
        "Challenge_reading_time":1.03,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":24.0,
        "Challenge_repo_issue_count":492.0,
        "Challenge_repo_star_count":255.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":2728.7016666667,
        "Challenge_title":"WandB callback changes the train step's unique ID, but does not change the results",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":14,
        "Platform":"Github",
        "Solution_body":"Actually, callbacks can change the result. So we'll close this.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":4.1,
        "Solution_reading_time":0.79,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0304878049,
        "Challenge_watch_issue_ratio":0.012195122
    },
    {
        "Challenge_adjusted_solved_time":123.815,
        "Challenge_answer_count":2,
        "Challenge_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Challenge_closed_time":1649731891000,
        "Challenge_created_time":1649286157000,
        "Challenge_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":14.5,
        "Challenge_reading_time":51.74,
        "Challenge_repo_contributor_count":3.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":41.0,
        "Challenge_repo_star_count":24.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":123.815,
        "Challenge_title":"Resume error in WandB.",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":326,
        "Platform":"Github",
        "Solution_body":"I will look into this soon. Crazily busy at the moment. This is not a bug, this happended because WandbCallbacks were used in the wrong way\r\n\r\nIn `pipeline.yaml`\r\n```python\r\n\"name\": \"WandbCallbacks\",\r\n\"args\": {\r\n    \"username\": \"lannguyen\",\r\n    \"project_name\": \"theseus_classification\",\r\n    \"resume\": true # <----- you didnt have to specify this\r\n}\r\n```\r\n\r\nThe repo havent been fully-well documented therefore it will be confusing sometimes.",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"soon crazili busi moment happend callback pipelin yaml callback arg usernam lannguyen theseu classif resum didnt specifi repo havent fulli document",
        "Solution_preprocessed_content":"soon crazili busi moment happend callback repo havent document",
        "Solution_readability":9.9,
        "Solution_reading_time":5.24,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":56.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0731707317,
        "Challenge_watch_issue_ratio":0.0731707317
    },
    {
        "Challenge_adjusted_solved_time":24.8108333333,
        "Challenge_answer_count":1,
        "Challenge_body":"## What\r\n\r\nA clear and concise description of what the bug is.\r\n\r\n## How to reproduce\r\n\r\nReproduce by starting a non-dry run via a notebook\r\n\r\n1. Start the run\r\n2. Look at files on the wandb interface. There are no checkpoints\r\n\r\n## Expected\r\n\r\nCheckpoints should be uploaded to wandb whenever there is a better one available during training.\r\n\r\n## Additional context\r\n\r\nI thought I fixed wandb, but it seems that I don't understand the symlinking model of wandb. Apparently you need to have checkpoints under the project root? But this would mean that you can't run multiple experiements at the same time. ",
        "Challenge_closed_time":1624957138000,
        "Challenge_created_time":1624867819000,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/9",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.5,
        "Challenge_reading_time":7.51,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":24.8108333333,
        "Challenge_title":"Fix writing of checkpoints to wandb",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":104,
        "Platform":"Github",
        "Solution_body":"Fixed. See https:\/\/github.com\/feldberlin\/wavenet\/commit\/1125dcc5ce5004386160f3dfe0e1d1dc1e5aed98.",
        "Solution_gpt_summary":null,
        "Solution_link_count":1.0,
        "Solution_original_content":"http github com feldberlin wavenet commit dcccefdfeeddcea",
        "Solution_preprocessed_content":null,
        "Solution_readability":44.6,
        "Solution_reading_time":1.4,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":3.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0285714286,
        "Challenge_watch_issue_ratio":0.0857142857
    },
    {
        "Challenge_adjusted_solved_time":52.5838888889,
        "Challenge_answer_count":0,
        "Challenge_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Challenge_closed_time":1624287268000,
        "Challenge_created_time":1624097966000,
        "Challenge_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":11.1,
        "Challenge_reading_time":10.49,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":0.0,
        "Challenge_repo_issue_count":35.0,
        "Challenge_repo_star_count":3.0,
        "Challenge_repo_watch_count":3.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":52.5838888889,
        "Challenge_title":"Loading configs from wandb yields incorrect parameters",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":99,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0285714286,
        "Challenge_watch_issue_ratio":0.0857142857
    },
    {
        "Challenge_adjusted_solved_time":18.5230555556,
        "Challenge_answer_count":2,
        "Challenge_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Challenge_closed_time":1635948747000,
        "Challenge_created_time":1635882064000,
        "Challenge_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.51,
        "Challenge_repo_contributor_count":56.0,
        "Challenge_repo_fork_count":208.0,
        "Challenge_repo_issue_count":1067.0,
        "Challenge_repo_star_count":1173.0,
        "Challenge_repo_watch_count":30.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":18.5230555556,
        "Challenge_title":"Config type in WandBLogger",
        "Challenge_topic":"Runtime Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":87,
        "Platform":"Github",
        "Solution_body":"I agree, we can easily add support for plain dictionary. @digantamisra98 are you still working on the logger right? Can you take care of this? I've made a simple fix to it by removing the conversion inside WandBLogger. It works with plain dictionaries now. I also made a PR just in case.",
        "Solution_gpt_summary":"convers config paramet logger outsid logger gener term config input agre plain dictionari logger remov convers insid logger plain dictionari pull request",
        "Solution_link_count":0.0,
        "Solution_original_content":"agre easili add plain dictionari digantamisra logger care remov convers insid logger plain dictionari",
        "Solution_preprocessed_content":"agre easili add plain dictionari logger care remov convers insid logger plain dictionari",
        "Solution_readability":4.3,
        "Solution_reading_time":3.47,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":52.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0524835989,
        "Challenge_watch_issue_ratio":0.0281162137
    },
    {
        "Challenge_adjusted_solved_time":97.4961111111,
        "Challenge_answer_count":0,
        "Challenge_body":"- [x] wandb index name modify\r\n\r\nwandb create index name error and  change name to \"modelname + save_folder_name\"",
        "Challenge_closed_time":1635155013000,
        "Challenge_created_time":1634804027000,
        "Challenge_link":"https:\/\/github.com\/boostcampaitech2\/semantic-segmentation-level2-cv-02\/issues\/21",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":5.0,
        "Challenge_repo_issue_count":65.0,
        "Challenge_repo_star_count":5.0,
        "Challenge_repo_watch_count":5.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":97.4961111111,
        "Challenge_title":"wandb create index name error",
        "Challenge_topic":"GPU Acceleration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_word_count":21,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0923076923,
        "Challenge_watch_issue_ratio":0.0769230769
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"```Problem Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.``` I'm running into this issue with a specific model (e.g. DA-RNN w\/meta-data sweep). If runs truly aren't cleared then sweeps could be corrupting subsequent runs. This behavior hasn't been observed previously however.",
        "Challenge_closed_time":null,
        "Challenge_created_time":1600976683000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/162",
        "Challenge_link_count":0,
        "Challenge_open_time":18964.2547222222,
        "Challenge_readability":8.1,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Weird memory problem with sweeps Colab Wandb",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":65,
        "Platform":"Github",
        "Solution_body":null,
        "Solution_gpt_summary":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score_count":null,
        "Solution_sentence_count":null,
        "Solution_word_count":null,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":124.4336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"Wandb sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgH78bNb9A5JP6NcfFHB189TIjy5c#scrollTo=sTDGweZ0d0QP) advance instead they just stall after the first part of the sweep completes. This is causing problems.",
        "Challenge_closed_time":1600665871000,
        "Challenge_created_time":1600217910000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/154",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.48,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":124.4336111111,
        "Challenge_title":"Wandb Run stalling",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":26,
        "Platform":"Github",
        "Solution_body":"So this appears to be a problem on the Weights and Biases end of things. https:\/\/github.com\/wandb\/client\/issues\/1243 This is fixed see original issue.",
        "Solution_gpt_summary":"sweep stall sweep complet identifi end origin updat reflect",
        "Solution_link_count":1.0,
        "Solution_original_content":"end http github com client origin",
        "Solution_preprocessed_content":null,
        "Solution_readability":7.6,
        "Solution_reading_time":1.9,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":22.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":45.9877777778,
        "Challenge_answer_count":3,
        "Challenge_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Challenge_closed_time":1578199794000,
        "Challenge_created_time":1578034238000,
        "Challenge_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Challenge_link_count":2,
        "Challenge_open_time":null,
        "Challenge_readability":9.5,
        "Challenge_reading_time":7.42,
        "Challenge_repo_contributor_count":13.0,
        "Challenge_repo_fork_count":209.0,
        "Challenge_repo_issue_count":605.0,
        "Challenge_repo_star_count":1230.0,
        "Challenge_repo_watch_count":20.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":45.9877777778,
        "Challenge_title":"Wandb bug when running train long",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":59,
        "Platform":"Github",
        "Solution_body":"Currently working on this should be fixed. Just need to test it So it seems to not be doing steps anymore on the same model run chart, but the runs are still not terminating until the loop ends. Don't really know if this is a problem or not. Going to close this for now Wandb supports up to 50 concurrent runs. So as long as aren't training on more than 50 rivers at a time this shouldn't be an issue. If it becomes one will revert to the subprocess thing but don't want otherwise as with that it doesn't log debugging. ",
        "Solution_gpt_summary":"test entir train session run termin loop end unclear concurr run train river time revert subprocess log debug",
        "Solution_link_count":0.0,
        "Solution_original_content":"test step anymor model run chart run termin loop end close concurr run aren train river time revert subprocess log debug",
        "Solution_preprocessed_content":"test step anymor model run chart run termin loop end close concurr run aren train river time revert subprocess log debug",
        "Solution_readability":6.3,
        "Solution_reading_time":6.16,
        "Solution_score_count":0.0,
        "Solution_sentence_count":6.0,
        "Solution_word_count":101.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0214876033,
        "Challenge_watch_issue_ratio":0.0330578512
    },
    {
        "Challenge_adjusted_solved_time":18.9641666667,
        "Challenge_answer_count":1,
        "Challenge_body":"![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Challenge_closed_time":1622440667000,
        "Challenge_created_time":1622372396000,
        "Challenge_link":"https:\/\/github.com\/pstage-ocr-team6\/ocr-teamcode\/issues\/5",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":8.2,
        "Challenge_reading_time":3.11,
        "Challenge_repo_contributor_count":6.0,
        "Challenge_repo_fork_count":6.0,
        "Challenge_repo_issue_count":43.0,
        "Challenge_repo_star_count":11.0,
        "Challenge_repo_watch_count":0.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":18.9641666667,
        "Challenge_title":"[BUG] wandb value doesn't update",
        "Challenge_topic":"YAML Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":25,
        "Platform":"Github",
        "Solution_body":"![image](https:\/\/user-images.githubusercontent.com\/26226101\/120102333-71439e00-c185-11eb-8a20-b113112b0b3f.png)\r\n\r\n\uc544\uc774\uac70 \uc65c \uadf8\ub7f0\uac8c \ud588\ub124\uc694.... \uc218\uc815\ud574 \uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4!",
        "Solution_gpt_summary":"index grow list list",
        "Solution_link_count":1.0,
        "Solution_original_content":"imag http imag githubusercont com bbbf png",
        "Solution_preprocessed_content":null,
        "Solution_readability":15.5,
        "Solution_reading_time":2.01,
        "Solution_score_count":1.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":8.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.1395348837,
        "Challenge_watch_issue_ratio":0.0
    },
    {
        "Challenge_adjusted_solved_time":64.4975,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n\r\nAt model train completion, the test set loss is written as iteration 0 to the TensorBoard \/ W&B chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. As the validation loss and perplexity has already been written to this chart, this results in TensorBoard deleting all the validation metrics, overwriting them with the test loss and perplexity values. W&B refuses to add the test metrics to the charts at all, throwing a warning that looks like `wandb: WARNING Step must only increase in log calls.  Step 0 < 32000; dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Pip install and setup TensorBoard and W&B\r\n2. Begin training a model with a train, validation, and test set\r\n3. Observe in both TensorBoard and W&B that validation metrics are being logged\r\n4. Allow the model to train to completion\r\n5. Observe that the TensorBoard validation metrics are now gone, overwritten by the test set metrics\r\n6. Observe the W&B error in the text logs \/ program output\r\n\r\n**Expected behavior**\r\nTest metrics should be written to their own charts.\r\n\r\n**Proposed solution**\r\nTest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**Environment (please complete the following information):**\r\n - GPUs: 4x A100 80 GB\r\n- Configs: (configs that I used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# GPT-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"Adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": True,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": True,\r\n    \"reduce_scatter\": True,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": True,\r\n    \"cpu_offload\": False\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # Checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# Suggested data paths when using GPT-NeoX locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": False,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4TBNVME\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4TBNVME\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_wandb\": True,\r\n  \"wandb_host\": \"https:\/\/api.wandb.ai\",\r\n  \"wandb_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# Add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**Additional context**\r\n\r\nI have a bug fix ready, will follow up with it.",
        "Challenge_closed_time":1663248037000,
        "Challenge_created_time":1663015846000,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/669",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":16.2,
        "Challenge_reading_time":63.87,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":64.4975,
        "Challenge_title":"Test set metrics overwrite validation set metrics in TensorBoard and are rejected for logging by Weights and Biases (W&B)",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":522,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0617977528,
        "Challenge_watch_issue_ratio":0.1053370787
    },
    {
        "Challenge_adjusted_solved_time":1618.0830555556,
        "Challenge_answer_count":0,
        "Challenge_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Challenge_closed_time":1624218172000,
        "Challenge_created_time":1618393073000,
        "Challenge_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.5,
        "Challenge_reading_time":2.51,
        "Challenge_repo_contributor_count":44.0,
        "Challenge_repo_fork_count":385.0,
        "Challenge_repo_issue_count":712.0,
        "Challenge_repo_star_count":2929.0,
        "Challenge_repo_watch_count":75.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":1618.0830555556,
        "Challenge_title":"Local wandb logging is borked",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":35,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0617977528,
        "Challenge_watch_issue_ratio":0.1053370787
    },
    {
        "Challenge_adjusted_solved_time":170.0166666667,
        "Challenge_answer_count":0,
        "Challenge_body":"## \ud83d\udc1b Bug\r\n\r\nRefused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### To Reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"Screen Shot 2022-07-23 at 10 23 34 AM\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n\r\n### Additional context\r\n",
        "Challenge_closed_time":1659198312000,
        "Challenge_created_time":1658586252000,
        "Challenge_link":"https:\/\/github.com\/Lightning-AI\/lightning-hpo\/issues\/17",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":11.5,
        "Challenge_reading_time":7.99,
        "Challenge_repo_contributor_count":15.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":261.0,
        "Challenge_repo_star_count":46.0,
        "Challenge_repo_watch_count":18.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":170.0166666667,
        "Challenge_title":"Refused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".",
        "Challenge_topic":"Dataset Versioning",
        "Challenge_topic_macro":"Data Management",
        "Challenge_word_count":62,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0574712644,
        "Challenge_watch_issue_ratio":0.0689655172
    },
    {
        "Challenge_adjusted_solved_time":50.4269444444,
        "Challenge_answer_count":0,
        "Challenge_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Challenge_closed_time":1615408051000,
        "Challenge_created_time":1615226514000,
        "Challenge_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Challenge_link_count":1,
        "Challenge_open_time":null,
        "Challenge_readability":6.0,
        "Challenge_reading_time":5.98,
        "Challenge_repo_contributor_count":2.0,
        "Challenge_repo_fork_count":2.0,
        "Challenge_repo_issue_count":47.0,
        "Challenge_repo_star_count":6.0,
        "Challenge_repo_watch_count":4.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":50.4269444444,
        "Challenge_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Challenge_topic":"Web Service Deployment",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":70,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0425531915,
        "Challenge_watch_issue_ratio":0.085106383
    },
    {
        "Challenge_adjusted_solved_time":1.3086111111,
        "Challenge_answer_count":1,
        "Challenge_body":"## TL;DR\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## WHY?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## WHAT?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## TODOs\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Challenge_closed_time":1634915290000,
        "Challenge_created_time":1634910579000,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/90",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":2.4,
        "Challenge_reading_time":3.53,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1.3086111111,
        "Challenge_title":"wrong wandb dataset file name",
        "Challenge_topic":"Quota Management",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":49,
        "Platform":"Github",
        "Solution_body":"feature_68\uc5d0 \uc5c5\ub370\uc774\ud2b8\ub41c \uba54\uc778 \ube0c\ub79c\uce58\uac00 \uc801\uc6a9 \uc548\ub418\uc11c \uadf8\ub7f0\uac83.\r\nPR \uc0dd\uc131\ud574\uc11c \uc218\uc815\ud558\uc790",
        "Solution_gpt_summary":"creat pull request updat branch file data load file train tsv data load",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-0.8,
        "Solution_reading_time":0.6,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0322580645,
        "Challenge_watch_issue_ratio":0.0161290323
    },
    {
        "Challenge_adjusted_solved_time":167.0766666667,
        "Challenge_answer_count":0,
        "Challenge_body":"**Describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import Mapping, Sequence \r\nModuleNotFoundError: No module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**To Reproduce**\r\nRun on @ohsuz 's server.\r\n(Cannot reproduce on Intel i7 based local condition.)\r\n\r\n**Expected behavior**\r\nwandb should be properly imported.\r\n\r\n**Server (please complete the following information):**\r\n - OS: centOS\r\n",
        "Challenge_closed_time":1635333937000,
        "Challenge_created_time":1634732461000,
        "Challenge_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/89",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.5,
        "Challenge_reading_time":5.07,
        "Challenge_repo_contributor_count":4.0,
        "Challenge_repo_fork_count":9.0,
        "Challenge_repo_issue_count":124.0,
        "Challenge_repo_star_count":94.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":167.0766666667,
        "Challenge_title":"wandb import failure",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":45,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0322580645,
        "Challenge_watch_issue_ratio":0.0161290323
    },
    {
        "Challenge_adjusted_solved_time":46.9786111111,
        "Challenge_answer_count":0,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nWandbLogger throws error while import if etna[torch] is not installed.\n\n### Expected behavior\n\nWandb Logger should work no matter pytorch installation \n\n### How To Reproduce\n\n1. Create new env\r\n2. install etna and etna[wandb]\r\n3. import WandbLogger\r\n\n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version",
        "Challenge_closed_time":1638449992000,
        "Challenge_created_time":1638280869000,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/335",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":8.6,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":46.9786111111,
        "Challenge_title":"[BUG] Wandb Logger does not work unless pytorch is installed ",
        "Challenge_topic":"Notebook Environment Configuration",
        "Challenge_topic_macro":"Enrivonment Management",
        "Challenge_word_count":63,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0173410405,
        "Challenge_watch_issue_ratio":0.0057803468
    },
    {
        "Challenge_adjusted_solved_time":360.0336111111,
        "Challenge_answer_count":3,
        "Challenge_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Challenge_closed_time":1635943713000,
        "Challenge_created_time":1634647592000,
        "Challenge_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":11.8,
        "Challenge_reading_time":10.77,
        "Challenge_repo_contributor_count":18.0,
        "Challenge_repo_fork_count":60.0,
        "Challenge_repo_issue_count":1038.0,
        "Challenge_repo_star_count":652.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":360.0336111111,
        "Challenge_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":97,
        "Platform":"Github",
        "Solution_body":"The key to solve the bug can be [here](https:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/d99573326eb9acc3b4dd3148b9e63d2144acc917\/etna\/loggers\/wandb_logger.py#L149) lets discuss it  check that `fold_number` in df.column before drop\r\nhttps:\/\/github.com\/tinkoff-ai\/etna-ts\/blob\/master\/etna\/loggers\/wandb_logger.py#L175",
        "Solution_gpt_summary":"fold column drop modifi line logger modifi line file",
        "Solution_link_count":2.0,
        "Solution_original_content":"kei http github com tinkoff etna blob debaccbddbedacc etna logger logger fold column drop http github com tinkoff etna blob master etna logger logger",
        "Solution_preprocessed_content":null,
        "Solution_readability":18.9,
        "Solution_reading_time":4.23,
        "Solution_score_count":0.0,
        "Solution_sentence_count":3.0,
        "Solution_word_count":20.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0173410405,
        "Challenge_watch_issue_ratio":0.0057803468
    },
    {
        "Challenge_adjusted_solved_time":367.1658333333,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Challenge_closed_time":1657912525000,
        "Challenge_created_time":1656590728000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Challenge_link_count":5,
        "Challenge_open_time":null,
        "Challenge_readability":17.7,
        "Challenge_reading_time":99.61,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":79,
        "Challenge_solved_time":367.1658333333,
        "Challenge_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":523,
        "Platform":"Github",
        "Solution_body":"Hi @GillianGrayson \r\n\r\nFor **1. hydra-optuna-sweeper problem**, it has been modified in release_1.4. You can find [here](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/7e67c4692590550e7b703655845e59508eb071bb\/configs\/hparams_search\/mnist_optuna.yaml#L49)\r\n\r\n @GillianGrayson ty for reporting, the problems have been fixed on the current `main` branch.",
        "Solution_gpt_summary":"hydra sweeper releas link logger run train process",
        "Solution_link_count":1.0,
        "Solution_original_content":"gilliangrayson hydra sweeper modifi releas http github com ashlev lightn hydra templat blob ecebeebbb config hparam search mnist yaml gilliangrayson report branch",
        "Solution_preprocessed_content":null,
        "Solution_readability":12.1,
        "Solution_reading_time":4.76,
        "Solution_score_count":0.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":30.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":26.0741666667,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Challenge_closed_time":1654420353000,
        "Challenge_created_time":1654326486000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":4.4,
        "Challenge_reading_time":61.14,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":210,
        "Challenge_solved_time":26.0741666667,
        "Challenge_title":"wandb logger not working",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":584,
        "Platform":"Github",
        "Solution_body":"`wandb-callbacks` haven't been maintained for a while and it might not work correctly with recent lightning and hydra releases. \r\n\r\nHave you trained using the `main` branch?\r\n\r\nI'm preparing new release and will fix the callbacks when it's ready https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/308\r\n So i managed to get it working using a fresh conda environment: \r\ntorch==1.10.0 with CUDA10.2\r\npytorch-lightning==1.6.4\r\nwandb == 0.12.17\r\n\r\nI doesnt check if all the callbacks work properly but my initial problem is solved. Thank you for your help! ",
        "Solution_gpt_summary":"callback branch lightn hydra releas train branch creat fresh conda environ version torch pytorch lightn torch cuda pytorch lightn releas prepar callback",
        "Solution_link_count":1.0,
        "Solution_original_content":"callback haven maintain lightn hydra releas train branch prepar releas callback readi http github com ashlev lightn hydra templat fresh conda environ torch cuda pytorch lightn doesnt callback properli initi",
        "Solution_preprocessed_content":"haven maintain lightn hydra releas train branch prepar releas callback readi fresh conda environ doesnt callback properli initi",
        "Solution_readability":7.0,
        "Solution_reading_time":6.86,
        "Solution_score_count":0.0,
        "Solution_sentence_count":8.0,
        "Solution_word_count":77.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":1666.9041666667,
        "Challenge_answer_count":1,
        "Challenge_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Challenge_closed_time":1657910798000,
        "Challenge_created_time":1651909943000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.94,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":1666.9041666667,
        "Challenge_title":"wandb log only 1 run when using ddp and multirun",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":37,
        "Platform":"Github",
        "Solution_body":"Try adding `wandb.finish()` after testing to make sure it has closed properly",
        "Solution_gpt_summary":"add finish test close properli",
        "Solution_link_count":0.0,
        "Solution_original_content":"finish test close properli",
        "Solution_preprocessed_content":null,
        "Solution_readability":3.3,
        "Solution_reading_time":0.97,
        "Solution_score_count":0.0,
        "Solution_sentence_count":2.0,
        "Solution_word_count":12.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":1043.1541666667,
        "Challenge_answer_count":3,
        "Challenge_body":"Hi,\r\n\r\nThere may be version conflict between wandb and PL 1.6.1\r\n\r\n**OS:** Ubuntu20.04\r\n**Python:** 3.8.13\r\n**Pytorch:**  1.11.0\r\n**PL:** 1.6.1\r\n**Wandb:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen I use the Hyperparameter Search, it produces the following error:\r\n\r\n```python\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/wandb\/offline-run-20*\/logs\/debug-internal.log'\r\nProblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py 357 experiment\r\n```\r\n",
        "Challenge_closed_time":1654689077000,
        "Challenge_created_time":1650933722000,
        "Challenge_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/285",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":10.0,
        "Challenge_reading_time":7.37,
        "Challenge_repo_contributor_count":24.0,
        "Challenge_repo_fork_count":341.0,
        "Challenge_repo_issue_count":412.0,
        "Challenge_repo_star_count":2043.0,
        "Challenge_repo_watch_count":19.0,
        "Challenge_score_count":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":1043.1541666667,
        "Challenge_title":"Wandb is not compatible with PL 1.6.1",
        "Challenge_topic":"Hyperparameter Tuning",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_word_count":55,
        "Platform":"Github",
        "Solution_body":"I have met the same problem. > I have met the same problem.\r\n\r\nInstall PL=1.5.10 for me it's working with 1.6.3 \r\nonly update wandb 0.12.16",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":"met met instal updat",
        "Solution_preprocessed_content":"met met instal updat",
        "Solution_readability":2.1,
        "Solution_reading_time":1.62,
        "Solution_score_count":2.0,
        "Solution_sentence_count":4.0,
        "Solution_word_count":24.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0582524272,
        "Challenge_watch_issue_ratio":0.0461165049
    },
    {
        "Challenge_adjusted_solved_time":166.3613888889,
        "Challenge_answer_count":1,
        "Challenge_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Challenge_closed_time":1666770135000,
        "Challenge_created_time":1666171234000,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Challenge_link_count":3,
        "Challenge_open_time":null,
        "Challenge_readability":18.9,
        "Challenge_reading_time":171.15,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":126,
        "Challenge_solved_time":166.3613888889,
        "Challenge_title":"WandB fails when config is too large",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":858,
        "Platform":"Github",
        "Solution_body":"Yeah, I wouldn't call this a bug _per se_. It's just that `WandbLogger` has some limitations that we need to navigate.\r\n\r\nI think your options are to:\r\n\r\n1. not log the training selection; \r\n2. log the test selection instead, as it should be considerably smaller; \r\n3. encode the selection in the data pipeline such that the train\/test label is a column in your database rather than a separate array, and then just log this column name; or \r\n4. implement and log the selection as a reproducible prescription (e.g., `test = event_no % 5 == 0` and `train = not test`) rather than as an explicit array of indices. \r\n\r\nI don't think (1) is a good option, but (2-4) could all work and I think they are all pretty straightforward to do.",
        "Solution_gpt_summary":"log train select log test select consider smaller encod select data pipelin train test label column databas separ arrai log column implement log select reproduc prescript test event train test explicit arrai",
        "Solution_link_count":0.0,
        "Solution_original_content":"yeah logger limit navig option log train select log test select consider smaller encod select data pipelin train test label column databas separ arrai log column implement log select reproduc prescript test event train test explicit arrai option pretti straightforward",
        "Solution_preprocessed_content":"yeah limit navig option log train select log test select consider smaller encod select data pipelin label column databas separ arrai log column implement log select reproduc prescript explicit arrai option pretti straightforward",
        "Solution_readability":6.4,
        "Solution_reading_time":8.58,
        "Solution_score_count":0.0,
        "Solution_sentence_count":9.0,
        "Solution_word_count":127.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0291777188,
        "Challenge_watch_issue_ratio":0.0159151194
    },
    {
        "Challenge_adjusted_solved_time":24.1527777778,
        "Challenge_answer_count":0,
        "Challenge_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Challenge_closed_time":1661948109000,
        "Challenge_created_time":1661861159000,
        "Challenge_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":15.9,
        "Challenge_reading_time":59.04,
        "Challenge_repo_contributor_count":11.0,
        "Challenge_repo_fork_count":20.0,
        "Challenge_repo_issue_count":377.0,
        "Challenge_repo_star_count":23.0,
        "Challenge_repo_watch_count":6.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":41,
        "Challenge_solved_time":24.1527777778,
        "Challenge_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Challenge_topic":"Dataset Access Control",
        "Challenge_topic_macro":"Identity Management",
        "Challenge_word_count":330,
        "Platform":"Github",
        "Solution_body":"",
        "Solution_gpt_summary":null,
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":-15.7,
        "Solution_reading_time":0.0,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":0.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0291777188,
        "Challenge_watch_issue_ratio":0.0159151194
    },
    {
        "Challenge_adjusted_solved_time":28.0155555556,
        "Challenge_answer_count":1,
        "Challenge_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Challenge_closed_time":1650034426000,
        "Challenge_created_time":1649933570000,
        "Challenge_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Challenge_link_count":0,
        "Challenge_open_time":null,
        "Challenge_readability":6.5,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":1.0,
        "Challenge_repo_fork_count":3.0,
        "Challenge_repo_issue_count":102.0,
        "Challenge_repo_star_count":20.0,
        "Challenge_repo_watch_count":2.0,
        "Challenge_score_count":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":28.0155555556,
        "Challenge_title":"In random agent script wandb full episode data logging skips a few steps",
        "Challenge_topic":"API Endpoint Configuration",
        "Challenge_topic_macro":"Service Management",
        "Challenge_word_count":57,
        "Platform":"Github",
        "Solution_body":"This has been implemented and will be shipped with v0.4.4 \ud83d\ude80",
        "Solution_gpt_summary":"add metric log timestep dai proportion implement ship",
        "Solution_link_count":0.0,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":3.7,
        "Solution_reading_time":0.72,
        "Solution_score_count":0.0,
        "Solution_sentence_count":1.0,
        "Solution_word_count":10.0,
        "Tool":"Weights & Biases",
        "Challenge_contributor_issue_ratio":0.0098039216,
        "Challenge_watch_issue_ratio":0.0196078431
    }
]