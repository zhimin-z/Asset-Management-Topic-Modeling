[
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to use Azure ML to find related products using information from receipts from a store.<\/p>\n\n<p>I got a file of reciepts:<\/p>\n\n<pre><code>44366,136778\n79619,88975\n78861,78864\n53395,78129,78786,79295,79353,79406,79408,79417,85829,136712\n32340,33973\n31897,32905\n32476,32697,33202,33344,33879,34237,34422,48175,55486,55490,55498\n17800\n32476,32697,33202,33344,33879,34237,34422,48175,55490,55497,55498,55503\n47098\n136974\n85832\n<\/code><\/pre>\n\n<p>Each row represent one receipt and each number is a product id.<\/p>\n\n<p>Given a product id I want to get a list of similar products, i.e. products that was bought together by other customers.<\/p>\n\n<p>Can anyone point me in the right direction of how do to this?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1407506662160,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"relat receipt store file receipt row repres receipt list bought guidanc accomplish task",
        "Challenge_last_edit_time":1448443169168,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/25205371",
        "Challenge_link_count":0,
        "Challenge_original_content":"relat receipt store file reciept row repres receipt list bought direct",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"relat receipt store file reciept row repres receipt list bought direct",
        "Challenge_readability":6.5,
        "Challenge_reading_time":9.56,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Recommendations",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":539.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":261741337840
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Does anyone have an example of doing matching using the new <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/machine-learning\/\" rel=\"nofollow\">Machine Learning functionality<\/a> in Microsoft Azure? <\/p>\n\n<p>The examples of doing classification make sense, and I was wondering if there was an example of doing matching using the built in tools. This would be instead of using classification and comparing those with my own custom codes. Either way an example would be nice. <\/p>\n\n<p>I want to match two different entities based on location, demographic data, etc.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1412797648427,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"match function match entiti base locat demograph data match built classif compar",
        "Challenge_last_edit_time":1454300747150,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/26265181",
        "Challenge_link_count":1,
        "Challenge_original_content":"match match function classif sens match built classif compar nice match entiti base locat demograph data",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"match match function classif sens match built classif compar nice match entiti base locat demograph data",
        "Challenge_readability":10.4,
        "Challenge_reading_time":7.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Matching using the new Azure Machine Learning",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":954.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":256450351573
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>all,<\/p>\n\n<p>I am using R on the Azure machine learning, and I have some problems.<\/p>\n\n<p>I want to use program R to calculate the difference between two date, for example, 2014\/11\/01 and 2014\/11\/03.<\/p>\n\n<p>I using the function \"strptime\" in R to do this thing, it can work on my own computer, but when I want to run the same code on Azure ml, it came out the error.<\/p>\n\n<p>The error is : <\/p>\n\n<pre><code>[ModuleOutput] 1: In strptime(x, format, tz = tz) :\n[ModuleOutput] \n[ModuleOutput]   unable to identify current timezone 'C':\n[ModuleOutput] \n[ModuleOutput] please set environment variable 'TZ'\n[ModuleOutput] \n[ModuleOutput] 2: In strptime(x, format, tz = tz) : unknown timezone 'localtime'\n<\/code><\/pre>\n\n<p>I think the problem is that it can't detect the timezone on Azure ml, but I'm not sure.<\/p>\n\n<p>Is there any way to solve this problem?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1415262044470,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"strptime function calcul date messag timezon identifi set environ variabl relat inabl detect timezon",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/26774724",
        "Challenge_link_count":0,
        "Challenge_original_content":"identifi timezon program calcul date function strptime run came moduleoutput strptime format moduleoutput moduleoutput identifi timezon moduleoutput moduleoutput set environ variabl moduleoutput moduleoutput strptime format unknown timezon localtim detect timezon advanc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"identifi timezon program calcul date function strptime run came detect timezon advanc",
        "Challenge_readability":9.9,
        "Challenge_reading_time":11.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"unable to identify current timezone 'C'",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1456.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":253985955530
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are trying to create an Azure ML web-service that will receive a (.csv) data file, do some processing, and return two similar files. The Python support recently added to the azure ML platform was very helpful and we were able to successfully port our code, run it in experiment mode and publish the web-service.<\/p>\n\n<p>Using the \"batch processing\" API, we are now able to direct a file from blob-storage to the service and get the desired output. However, run-time for small files (a few KB) is significantly slower than on a local machine, and more importantly, the process seems to never return for slightly larger input data files (40MB). Processing time on my local machine for the same file is under 1 minute. <\/p>\n\n<p>My question is if you can see anything we are doing wrong, or if there is a way to get this to speed up. Here is the DAG representation of the experiment:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/MalhQ.png\" alt=\"The DAG representation of the experiment\"><\/p>\n\n<p>Is this the way the experiment should be set up? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1425837068693,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"speed creat web servic process csv file return file successfulli port run mode run time small file slower local process return slightli larger input data file advic speed process",
        "Challenge_last_edit_time":1425890468080,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/28929813",
        "Challenge_link_count":1,
        "Challenge_original_content":"speed creat web servic receiv csv data file process return file platform successfulli port run mode publish web servic batch process api direct file blob storag servic desir output run time small file significantli slower local importantli process return slightli larger input data file process time local file minut speed dag represent set",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"speed creat receiv data file process return file platform successfulli port run mode publish batch process api direct file servic desir output small file significantli slower local importantli process return slightli larger input data file process time local file minut speed dag represent set",
        "Challenge_readability":9.2,
        "Challenge_reading_time":13.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Python support for Azure ML -- speed issue",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":491.0,
        "Challenge_word_count":182,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":243410931307
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I understand that Office 365 is on separate domain and live id (Microsoft account) is used for consumer applications.<\/p>\n\n<p>But can an Office 365 account get live\/Microsoft services?<\/p>\n\n<p>The issue is we trying to SSO Office 365 applications and Azure ML (used with Microsoft account) but as the domains are different I am unable to find any proper help or process on the web.<\/p>\n\n<p>We can create a live account with our company domain but can we create a federation on Live account ? For e.g. on Office 365 we created a @.com federation and were able to SSO it, how can we do the same with a live account ?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1426122077520,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"organiz account offic live servic implement sso offic process domain creat live account compani domain creat feder live account",
        "Challenge_last_edit_time":1426276822120,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29000199",
        "Challenge_link_count":0,
        "Challenge_original_content":"organiz account offic live servic offic separ domain live account consum offic account live servic sso offic account domain process web creat live account compani domain creat feder live account offic creat com feder sso live account",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"organiz account servic offic separ domain live consum offic account servic sso offic domain process web creat live account compani domain creat feder live account offic creat feder sso live account",
        "Challenge_readability":8.8,
        "Challenge_reading_time":8.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Can an organizational account (office 365) be used for live\/Microsoft services?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":120,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":243125922480
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've got a dataset at hand with a column of DateTime in String format, eg.<\/p>\n\n<pre><code>a = 'Tue Sep 22 1998 00:00:00 GMT+0000 (Coordinated Universal Time)'\n<\/code><\/pre>\n\n<p>and a is just a value from the column.<\/p>\n\n<p>If I use Metadata Editor in Azure Machine Learning Studio, it won't work and will complain that it can't do the conversion (from String to DateTime). I guess it's something to do with the format. So I'm trying the following:<\/p>\n\n<pre><code>a = str(a)[:10]+','+str(a)[10:15]\n#'Tue Sep 22, 1998'\n<\/code><\/pre>\n\n<p>Now .NET surely can do the conversion, I mean by method like Convert.ToDateTime(). However, when I visualized the output of the Python script, I found the String has been changed into 'Tue Sep 22, 1998 None,', which is quite weird. Anyone knows what's wrong with it? I'm attaching the excerpt of python code down below:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n\n  dataframe1['timestamp'] = dataframe1['timestamp'].apply(lambda a: str(a)[:10]+','+str(a)[10:15])\n\n  return dataframe1,\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1426922780947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"convert column datetim format datetim studio metadata editor studio convers format convert datetim output",
        "Challenge_last_edit_time":1453514644060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29180287",
        "Challenge_link_count":0,
        "Challenge_original_content":"panda convert datetim dataset hand column datetim format tue sep gmt coordin univers time valu column metadata editor studio complain convers datetim guess format str str tue sep net convers convert todatetim visual output tue sep attach excerpt datafram datafram datafram timestamp datafram timestamp appli lambda str str return datafram",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"panda convert datetim dataset hand column datetim format valu column metadata editor studio complain convers guess format net convers visual output tue sep attach excerpt",
        "Challenge_readability":6.5,
        "Challenge_reading_time":13.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML & Pandas: How to convert String to DateTime",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2509.0,
        "Challenge_word_count":157,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":242325219053
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've looked all over the google and stackoverflow for the answer but I cant seem to find it. I'm trying to get the output from an azure experiment to an app. I've made the app using <code>ibuildapp<\/code> and google forms. How can I use the inputs from the google form, pass it to azure and get an output to display on the app?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1429093608083,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat rest servic consum mobil app stackoverflow creat app ibuildapp pass input displai output app",
        "Challenge_last_edit_time":1429213713288,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29647782",
        "Challenge_link_count":0,
        "Challenge_original_content":"web servic creat rest servic consum mobil app stackoverflow output app app ibuildapp input pass output displai app",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic creat rest servic consum mobil app stackoverflow output app app input pass output displai app",
        "Challenge_readability":6.1,
        "Challenge_reading_time":5.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azureml Web Service - How to create a Rest Service from an Experiment to be consumed by a mobile app?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":603.0,
        "Challenge_word_count":81,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":240154391917
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>If we have installed nltk zip file and then we want to install the setup of the same using the python script then how can we do it?<\/p>\n\n<p>I have created  the zip file and kept all the packages in it and then importing it from python script <\/p>\n\n<pre><code>from mynltk.corpus import *\nfrom mynltk.corpus.reader import *\nbrown.words()[0:10]\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1429858050847,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal setup nltk zip file creat zip file packag import",
        "Challenge_last_edit_time":1429860049860,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/29840532",
        "Challenge_link_count":0,
        "Challenge_original_content":"instal nltk zip file instal setup instal nltk zip file instal setup creat zip file kept packag import mynltk corpu import mynltk corpu reader import brown",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"instal nltk zip file instal setup instal nltk zip file instal setup creat zip file kept packag import",
        "Challenge_readability":5.8,
        "Challenge_reading_time":5.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"If we have installed nltk zip file and then we want to install the setup of the same using the python script",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":239389949153
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>We are facing an error when we have a column which have datatype as string and the value like \ncol1  col2 \n1      .89<\/p>\n\n<p>So, when we are using <\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n\n    # Execution logic goes here\n    print('Input pandas.DataFrame #1:')\n    import pandas as pd\n    import numpy as np\n    from sklearn.kernel_approximation import RBFSampler\n    x =dataframe1.iloc[:,2:1080]\n    print x\n    df1 = dataframe1[['colname']]\n\n    change = np.array(df1)\n    b = change.ravel()\n    print b\n    rbf_feature = RBFSampler(gamma=1, n_components=100,random_state=1)\n    print rbf_feature\n    print \"test\"\n    X_features = rbf_feature.fit_transform(x)\n<\/code><\/pre>\n\n<p>After this we are getting error as cannt convert non int into type float<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1431079640547,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"convert column valu valu datafram messag state convert valu type",
        "Challenge_last_edit_time":1431079747740,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30121181",
        "Challenge_link_count":0,
        "Challenge_original_content":"convert valu datafram column datatyp valu col col datafram datafram execut logic goe print input panda datafram import panda import numpi sklearn kernel approxim import rbfsampler datafram iloc print datafram colnam arrai ravel print rbf featur rbfsampler gamma compon random state print rbf featur print test featur rbf featur fit transform cannt convert type",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"convert valu datafram column datatyp valu col col cannt convert type",
        "Challenge_readability":8.7,
        "Challenge_reading_time":9.84,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to convert string into float value in the dataframe",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":27214.0,
        "Challenge_word_count":98,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":238168359453
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I want to 'display' the results of my machine learning dataset in Microsoft Power BI. MPBI is telling me that the way to connect to Azure is through SQL database. Should I somehow put my data from machine learning into an SQL database, and if so how?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1434876356187,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"connect dataset power connect sql databas transfer data sql databas",
        "Challenge_last_edit_time":1456849948427,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/30962756",
        "Challenge_link_count":0,
        "Challenge_original_content":"connect dataset sql databas displai dataset power mpbi connect sql databas data sql databas",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"connect dataset sql databas displai dataset power mpbi connect sql databas data sql databas",
        "Challenge_readability":5.5,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I connect datasets from machine learning to SQL database in Azure?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":579.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":234371643813
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've built a ML model in Azure and most of my data transformation \/ feature generation steps have been carried out in R. \nAt first, I transformed the data in R itself (works faster) and uploaded the created csv to ML studio.<\/p>\n\n<p>Now that my model is finished I tried to run the R-code in ML studio itself, instead of manually uploading a transformed dataset. It runs flawlessly. However, when I compare the resulting datasets (the one uploaded and the one created) they differ. The columns have different means, formats and the model performs less. The actual data entries\/cells look fine. <\/p>\n\n<p>I feel it has to do with the format of the columns so I tried stuff like converted the columns to character type, or converting the resulting dataset to csv (in ML studio) and letting ML studio figure out how to format them.<\/p>\n\n<p>So far, no results.<\/p>\n\n<p>Has anyone already faced this problem? What is the solution?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1438076433613,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"model run studio manual upload transform dataset dataset term column format perform relat format column tri convert charact type studio format advic",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31672656",
        "Challenge_link_count":0,
        "Challenge_original_content":"compil built model data transform featur gener step carri transform data faster upload creat csv studio model finish tri run studio manual upload transform dataset run flawlessli compar dataset upload creat column format model perform data entri cell format column tri stuff convert column charact type convert dataset csv studio studio figur format",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"compil built model data transform featur gener step carri transform data upload creat csv studio model finish tri run studio manual upload transform dataset run flawlessli compar dataset column format model perform data format column tri stuff convert column charact type convert dataset csv studio figur format",
        "Challenge_readability":7.4,
        "Challenge_reading_time":12.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure-machine-learning: Compiling R-script, but different results",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":241.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":231171566387
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm facing a most annoying behavior where a R script works juts fine in R Studio and generates an error in Azure ML.<\/p>\n\n<p>I first I thought it was about inputs and outputs difference, but as you can see in the script below, I removed dependencies to input and outputs.<\/p>\n\n<p>The error is generated by the call to <code>chartr<\/code>: \"old\" is longer than \"new\".<\/p>\n\n<p>Any input is appreciated.<\/p>\n\n<pre><code>accented_characters &lt;- list('\u0160'='S', '\u0161'='s', '\u017d'='Z', '\u017e'='z', '\u00c0'='A', '\u00c1'='A', '\u00c2'='A', '\u00c3'='A', '\u00c4'='A', '\u00c5'='A', '\u00c6'='A', '\u00c7'='C', '\u00c8'='E', '\u00c9'='E',\n                        '\u00ca'='E', '\u00cb'='E', '\u00cc'='I', '\u00cd'='I', '\u00ce'='I', '\u00cf'='I', '\u00d1'='N', '\u00d2'='O', '\u00d3'='O', '\u00d4'='O', '\u00d5'='O', '\u00d6'='O', '\u00d8'='O', '\u00d9'='U',\n                        '\u00da'='U', '\u00db'='U', '\u00dc'='U', '\u00dd'='Y', '\u00de'='B', '\u00df'='Ss', '\u00e0'='a', '\u00e1'='a', '\u00e2'='a', '\u00e3'='a', '\u00e4'='a', '\u00e5'='a', '\u00e6'='a', '\u00e7'='c',\n                        '\u00e8'='e', '\u00e9'='e', '\u00ea'='e', '\u00eb'='e', '\u00ec'='i', '\u00ed'='i', '\u00ee'='i', '\u00ef'='i', '\u00f0'='o', '\u00f1'='n', '\u00f2'='o', '\u00f3'='o', '\u00f4'='o', '\u00f5'='o',\n                        '\u00f6'='o', '\u00f8'='o', '\u00f9'='u', '\u00fa'='u', '\u00fb'='u', '\u00fd'='y', '\u00fd'='y', '\u00fe'='b', '\u00ff'='y' )\n\ninput &lt;- data.frame(text = c(\"some piZza\u00e9 pizZa word a to : here $\",\"or there \u20ac with 28'89.5\"))\nstop_words &lt;- data.frame(international = c('pizza'))\n\nstop_words &lt;- as.character(stop_words$international)\nstop_words &lt;- gsub(\"^\\\\s+|\\\\s+$\", \"\", stop_words) # trim\nstop_words &lt;- tolower(stop_words) # lowercase\n\ninput &lt;- as.character(input$text)\ninput &lt;- gsub(\"[[:space:]]+\", ' ', input) # remove multiple spaces\ninput &lt;- gsub(\"[1-9!\\\"#$\u20ac%&amp;'()*+,.\/:;&lt;=&gt;@}~^_|`\\\\?\\\\[\\\\{]+\", '', input) # remove punctuation, numbers and some others. Note, does not remove closing bracket, can't figure out why\ninput &lt;- chartr(paste(names(accented_characters), collapse = ''),\n            paste(accented_characters, collapse = ''), input) # remove accents\ninput &lt;- tolower(input) # lowercase everything         \ninput &lt;- gsub(\"\\\\b[a-z]{1,2}\\\\b\", '', input) #remove too short words\ninput &lt;- gsub(paste(stop_words, \"|\"), '', input) # remove stop words\n\ninput &lt;- data.frame(input) # set as data.frame class\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1438768012707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"studio gener gener chartr longer remov depend input output input",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31828926",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio annoi jut studio gener input output remov depend input output gener chartr longer input accent charact input remov punctuat note remov close bracket figur input chartr past accent charact collaps past accent charact collaps input remov accent input tolow input lowercas input gsub input remov short input gsub past stop input remov stop input data frame input set data frame class",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"studio annoi jut studio gener input output remov depend input output gener longer input",
        "Challenge_readability":9.6,
        "Challenge_reading_time":27.01,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"R error in Azure Machine Learning, not in R Studio",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":453.0,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":230479987293
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>How can one access the global parameters (\"GlobalParameters\") sent from a web service in a Python script on Azure ML?<\/p>\n\n<p>I tried:<\/p>\n\n<pre><code>if 'GlobalParameters' in globals():\n    myparam = GlobalParameters['myparam']\n<\/code><\/pre>\n\n<p>but with no success. <\/p>\n\n<h2>EDIT: Example<\/h2>\n\n<p>In my case, I'm sending a sound file over the web service (as a list of samples). I would also like to send a sample rate and the number of bits per sample. I've successfully configured the web service (I think) to take these parameters, so the GlobalParameters now look like:<\/p>\n\n<pre><code>\"GlobalParameters\": {\n     \"sampleRate\": \"44100\",\n     \"bitsPerSample\": \"16\",\n}\n<\/code><\/pre>\n\n<p>However, I cannot access these variables from the Python script, neither as <code>GlobalParameters[\"sampleRate\"]<\/code> nor as <code>sampleRate<\/code>. Is it possible? Where are they stored?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1438792035673,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"access global paramet sent web servic tri access paramet global paramet sent access variabl",
        "Challenge_last_edit_time":1438967109632,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/31838017",
        "Challenge_link_count":0,
        "Challenge_original_content":"access globalparamet access global paramet globalparamet sent web servic tri globalparamet global myparam globalparamet myparam edit send file web servic list sampl send sampl rate bit sampl successfulli configur web servic paramet globalparamet globalparamet sampler bitspersampl access variabl globalparamet sampler sampler store",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"access globalparamet access global paramet sent web servic tri edit send file web servic send sampl rate bit sampl successfulli configur web servic paramet globalparamet access variabl store",
        "Challenge_readability":11.6,
        "Challenge_reading_time":11.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Access GlobalParameters in Azure ML Python script",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":673.0,
        "Challenge_word_count":122,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":230455964327
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am facing a problem while running my R script in Azure Machine Learning where I am getting the error for Rcpp package:<\/p>\n\n<p>object '.rcpp_warning_recorder' not found <\/p>\n\n<p>Searching on SO it seems like it could be an issue to be resolved by upgrading Rcpp tp latst version. AML's Rcpp version is 11.2 while latest on CRAN is 12.0. Does anyone know if pre-installed packages in AML can be updated to latest verson? <\/p>\n\n<p>Thanks and Regards<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1440739441067,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run outdat rcpp packag pre instal rcpp version aml latest version cran pre instal packag aml updat latest version",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32263842",
        "Challenge_link_count":0,
        "Challenge_original_content":"remov pre instal packag run rcpp packag object rcpp warn record search upgrad rcpp latst version aml rcpp version latest cran pre instal packag aml updat latest verson",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"remov packag run rcpp packag object search upgrad rcpp latst version aml rcpp version latest cran packag aml updat latest verson",
        "Challenge_readability":5.6,
        "Challenge_reading_time":6.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning: Remove pre-installed R packages",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":246.0,
        "Challenge_word_count":83,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":228508558933
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way using Azure ML to edit a column of DateTime values so that the dates are set relative to a different date, e.g. the min date in the data set?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1441318942047,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"edit column datetim valu set date rel date minimum date dataset",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32386351",
        "Challenge_link_count":0,
        "Challenge_original_content":"edit datetim rel date edit column datetim valu date set rel date date data set",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"edit datetim rel date edit column datetim valu date set rel date date data set",
        "Challenge_readability":6.3,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Edit DateTime to be relative to a different date",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":227929057953
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have successfully deployed a web service using Azure ML and am able to get output both on Azure ML as well as a sample R client application.<\/p>\n\n<p>I would like to however get response using the firefox poster.<\/p>\n\n<p>I have followed the instructions from the Azure page on deploying the web service and tried using the same request headers and parameters as follows<\/p>\n\n<p>Instructions from azure page\n<a href=\"https:\/\/i.stack.imgur.com\/YRq2c.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRq2c.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>this is what I've tried on Poster\n<a href=\"https:\/\/i.stack.imgur.com\/zH220.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zH220.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/QY2oc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QY2oc.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Error message\n<a href=\"https:\/\/i.stack.imgur.com\/VqhGH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/VqhGH.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My R Code which works<\/p>\n\n<pre><code>library(\"RCurl\")\nlibrary(\"rjson\")\n\n# Accept SSL certificates issued by public Certificate Authorities\noptions(RCurlOptions = list(cainfo = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\")))\n\nh = basicTextGatherer()\nhdr = basicHeaderGatherer()\n\n\nreq = list(\n\n        Inputs = list(\n\n\n            \"input1\" = list(\n                \"ColumnNames\" = list(\"Smoker\", \"GenderCD\", \"Age\"),\n                \"Values\" = list( list( \"1\", \"M\", \"8\" ),  list( \"1\", \"M\", \"8\" )  )\n            )                ),\n        GlobalParameters = setNames(fromJSON('{}'), character(0))\n)\n\nbody = enc2utf8(toJSON(req))\napi_key = \"hHlKbffejMGohso5yiJFke0D9yCKwvcXHG8tfIL2d8ccWZz8DN8nqxh9M4h727uVWPz+jmBgm0tKBLxnPO4RyA==\"\nauthz_hdr = paste('Bearer', api_key, sep=' ')\n\nh$reset()\ncurlPerform(url = \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/79f267a884464b6a95f5819870787918\/services\/e3490c06c73849f8a78ff320f7e5ffbc\/execute?api-version=2.0&amp;details=true\",\n            httpheader=c('Content-Type' = \"application\/json\", 'Authorization' = authz_hdr),\n            postfields=body,\n            writefunction = h$update,\n            headerfunction = hdr$update,\n            verbose = TRUE\n            )\n\nheaders = hdr$value()\nhttpStatus = headers[\"status\"]\nif (httpStatus &gt;= 400)\n{\n    print(paste(\"The request failed with status code:\", httpStatus, sep=\" \"))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(headers)\n}\n\nprint(\"Result:\")\nresult = h$value()\nprint(fromJSON(result))\n<\/code><\/pre>\n\n<p>My API key<\/p>\n\n<pre><code>hHlKbffejMGohso5yiJFke0D9yCKwvcXHG8tfIL2d8ccWZz8DN8nqxh9M4h727uVWPz+jmBgm0tKBLxnPO4RyA==\n<\/code><\/pre>\n\n<p>How can I form a correct URL which works?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1442532286663,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"respons firefox poster send request web servic instruct page deploi web servic tri request header paramet receiv messag api kei url",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32641508",
        "Challenge_link_count":9,
        "Challenge_original_content":"send request web servic poster successfulli deploi web servic output sampl client respons firefox poster instruct page deploi web servic tri request header paramet instruct page tri poster messag librari rcurl librari rjson accept ssl certif public certif author option rcurlopt list cainfo file curlssl cacert pem packag rcurl basictextgather hdr basicheadergather req list input list input list columnnam list smoker gendercd ag valu list list list globalparamet setnam fromjson charact bodi encutf tojson req api kei hhlkbffejmgohsoyijfkedyckwvcxhgtfildccwzzdnnqxhmhuvwpz jmbgmtkblxnporya authz hdr past bearer api kei sep reset curlperform url http ussouthcentr servic net workspac fabaf servic eccfafffeffbc execut api version httpheader type json author authz hdr postfield bodi writefunct updat headerfunct hdr updat verbos header hdr valu httpstatu header statu httpstatu print past request statu httpstatu sep print header requert timestamp debug print header print valu print fromjson api kei hhlkbffejmgohsoyijfkedyckwvcxhgtfildccwzzdnnqxhmhuvwpz jmbgmtkblxnporya url",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"send request web servic poster successfulli deploi web servic output sampl client respons firefox poster instruct page deploi web servic tri request header paramet instruct page tri poster messag api kei url",
        "Challenge_readability":17.2,
        "Challenge_reading_time":36.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"sending POST request to azure ML Web service using poster",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":345.0,
        "Challenge_word_count":246,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":226715713337
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a web service using Azure ML and deployed it. It works, but when I hit the Test button to test the web service, I am not being able to enter a different set of input values in the screen which asks for input. See screenshot below. As you can see, it's not a textbox where I can enter values, but a dropdown where the values are the ones in my script.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/O1c4f.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/O1c4f.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also, note how the instructions page shows allowed values as just those values\n<a href=\"https:\/\/i.stack.imgur.com\/62HDR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/62HDR.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>These values are from my initial script where I do the following<\/p>\n\n<pre><code>## ------- User-Defined Parameters ------ ##\n\nIDinput&lt;- data.frame(\nGenderCD=\"M\",\nAge=\"8\",\n..,\n..\n)\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"IDinput\");\n<\/code><\/pre>\n\n<p>I then have a script which reads these variables using POST as<\/p>\n\n<pre><code># Map 1-based optional input ports to variables# Map 1-based optional input ports to variables\nPOST &lt;- maml.mapInputPort(1) # class: data.frame\n\n#getting data from POST\nmytestrow = NULL\nmytestrow$GenderCD=POST$GenderCD\nmytestrow$Age=POST$Age\n\n#perform logic and store in a data frame called outputdf\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"outputdf\");\n<\/code><\/pre>\n\n<p>My overall architecture looks as\n<a href=\"https:\/\/i.stack.imgur.com\/F0fuo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/F0fuo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1442689076097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"test web servic creat input valu shown dropdown constant valu initi textbox enter valu instruct page allow valu valu architectur involv read variabl perform logic store output data frame",
        "Challenge_last_edit_time":1456850044128,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32672079",
        "Challenge_link_count":6,
        "Challenge_original_content":"allow valu shown constant input web servic creat web servic deploi hit test button test web servic enter set input valu screen input screenshot textbox enter valu dropdown valu on note instruct page allow valu valu valu initi defin paramet idinput data frame gendercd ag select data frame sent output dataset port maml mapoutputport idinput read variabl map base option input port variabl map base option input port variabl maml mapinputport class data frame data mytestrow null mytestrow gendercd gendercd mytestrow ag ag perform logic store data frame call outputdf select data frame sent output dataset port maml mapoutputport outputdf overal architectur",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"allow valu shown constant input web servic creat web servic deploi hit test button test web servic enter set input valu screen input screenshot textbox enter valu dropdown valu on note instruct page allow valu valu valu initi read variabl overal architectur",
        "Challenge_readability":9.0,
        "Challenge_reading_time":23.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Allowed values shown as constant input in azure ml web service",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":222,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":226558923903
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is it possible to use the parameters incoming from the webservice ( named 'Query' in my case ), in the SQL in the reader module.<\/p>\n\n<p>I think it's possible, but I cannot find anywhere how to template these properties in the SQL query so that it becomes dynamic.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oLWmr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oLWmr.png\" alt=\"dynamic query\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1442949113010,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"paramet webservic queri paramet reader modul templat properti sql queri dynam",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32725010",
        "Challenge_link_count":2,
        "Challenge_original_content":"webservic input queri paramet reader modul paramet incom webservic queri sql reader modul templat properti sql queri dynam",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"webservic input queri paramet reader modul paramet incom webservic sql reader modul templat properti sql queri dynam",
        "Challenge_readability":9.8,
        "Challenge_reading_time":6.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Use WebService input as query parameter in reader module in Azure ML?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":513.0,
        "Challenge_word_count":64,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":226298886990
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have tried to create a predictive webservice (following the movie recommender tutorial) but when I run the predective experiment I get an error:<\/p>\n\n<p><strong>Model could not be deserialized because it is likely serialized with an older serialization format. Please retrain and re-save the model. . ( Error 0082 )<\/strong><\/p>\n\n<p>Have you any idea?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3BwSe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3BwSe.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1443829276443,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"serial creat predict webservic messag model serial older format retrain resav model",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/32917356",
        "Challenge_link_count":2,
        "Challenge_original_content":"serial tri creat predict webservic movi tutori run predect model deseri serial older serial format retrain save model idea",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"serial tri creat predict webservic run predect model deseri serial older serial format retrain model idea",
        "Challenge_readability":11.6,
        "Challenge_reading_time":7.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning - Serialization error",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":157.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":225418723557
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>All of these dates that I\u2019ve manipulated in Execute R module in Azure Machine Learning write out as blank in the output \u2013 that is, these date columns exist, but there is no value in those columns. <\/p>\n\n<p>The source variables which contain date information that I\u2019m reading into the data frame have two different date formats.  They are as follows:<\/p>\n\n<pre><code>usage$Date1=c(\u20188\/6\/2015\u2019   \u20188\/20\/2015\u2019  \u20187\/9\/2015\u2019)\nusage$Date2=c(\u20184\/16\/2015 0:00\u2019,  \u20187\/1\/2015 0:00\u2019, \u20187\/1\/2015 0:00\u2019) \n<\/code><\/pre>\n\n<p>I inspected the log file in AML, and AML can't find the local time zone. \nThe log file warnings specifically:\n    [ModuleOutput] 1: In strptime(x, format, tz = tz) :\n    [ModuleOutput] unable to identify current timezone 'C':\n    [ModuleOutput] please set environment variable 'TZ' [ModuleOutput]\n    [ModuleOutput] 2: In strptime(x, format, tz = tz) : unknown timezone 'localtime'<\/p>\n\n<p>I referred to another answer regarding setting default time zone for strptime here<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/4047188\/unknown-timezone-name-in-r-strptime-as-posixct\">unknown timezone name in R strptime\/as.POSIXct<\/a><\/p>\n\n<p>I changed my code to explicitly define the global environment time variable.<\/p>\n\n<pre><code>Sys.setenv(TZ='GMT')\n\n\n####Data frame usage cleanup, format and labeling\nusage&lt;-as.data.frame(usage)\nusage$Date1&lt;-as.character(usage$Date1)\nusage$Date1&lt;-as.POSIXct(usage$Date1, \"%m\/%d\/%Y\",tz=\"GMT\")\nusage$Date1&lt;-format(usage$Date1, \"%m\/%d\/%Y\")\nusage$Date1&lt;-as.Date(usage$Date1, \"%m\/%d\/%Y\")\nusage&lt;-as.data.frame(usage)\n\nusage$Date2&lt;- as.POSIXct(usage$Date2, \"%m\/%d\/%Y\",tz=\"GMT\")\nusage$Date2&lt;- format(usage$Date2,\"%m\/%d\/%Y\")\nusage$Date2&lt;-as.Date(usage$Date2, \"%m\/%d\/%Y\")\nusage&lt;-as.data.frame(usage)\n<\/code><\/pre>\n\n<p>The problem persists -as a result AzureML does not write these variables out, rather writing out these columns as blanks.<br>\n(This code works in R studio, where I presume the local time is taken from the system.)<\/p>\n\n<p>After reading two blog posts on this problem, it seems that Azure ML doesn't support some date time formats:<\/p>\n\n<p><a href=\"http:\/\/blogs.msdn.com\/b\/andreasderuiter\/archive\/2015\/02\/03\/troubleshooting-error-1000-rpackage-library-exception-failed-to-convert-robject-to-dataset-when-running-r-scripts-in-azure-ml.aspx\" rel=\"nofollow noreferrer\">http:\/\/blogs.msdn.com\/b\/andreasderuiter\/archive\/2015\/02\/03\/troubleshooting-error-1000-rpackage-library-exception-failed-to-convert-robject-to-dataset-when-running-r-scripts-in-azure-ml.aspx<\/a><\/p>\n\n<p><a href=\"http:\/\/www.mikelanzetta.com\/2015\/01\/data-cleaning-with-azureml-and-r-dates\/\" rel=\"nofollow noreferrer\">http:\/\/www.mikelanzetta.com\/2015\/01\/data-cleaning-with-azureml-and-r-dates\/<\/a><\/p>\n\n<p>So I tried to convert to POSIXct before sending it to the output stream, which I've done as follows:\n    tenantusage$Date1 = as.POSIXct(tenantusage$Date1 , \"%m\/%d\/%Y\",tz = \"EST5EDT\");\n    tenantusage$Date2 = as.POSIXct(tenantusage$Date2 , \"%m\/%d\/%Y\",tz = \"EST5EDT\");<\/p>\n\n<p>But encounter the same problem.  The information in these variables refuses to write out to the output.  Date1 and Date2 columns are blank.<\/p>\n\n<p>Please advise!<\/p>\n\n<p>thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1446079421807,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"defin time zone strptime function date column valu column log file warn aml local time zone explicitli defin global environ time variabl persist date time format tri convert posixct send output stream variabl refus write output date date column blank",
        "Challenge_last_edit_time":1495541637147,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33404099",
        "Challenge_link_count":5,
        "Challenge_original_content":"defin time zone strptime function unknown timezon localtim date iv manipul execut modul write blank output date column valu column sourc variabl date read data frame date format usag date usag date inspect log file aml aml local time zone log file warn moduleoutput strptime format moduleoutput identifi timezon moduleoutput set environ variabl moduleoutput moduleoutput strptime format unknown timezon localtim set default time zone strptime unknown timezon strptime posixct explicitli defin global environ time variabl sy setenv gmt data frame usag cleanup format label usag data frame usag usag date charact usag date usag date posixct usag date gmt usag date format usag date usag date date usag date usag data frame usag usag date posixct usag date gmt usag date format usag date usag date date usag date usag data frame usag persist write variabl write column blank studio presum local time taken read blog date time format http blog msdn com andreasderuit archiv troubleshoot rpackag librari except convert robject dataset run aspx http mikelanzetta com data clean date tri convert posixct send output stream tenantusag date posixct tenantusag date estedt tenantusag date posixct tenantusag date estedt variabl refus write output date date column blank",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"defin time zone strptime function unknown timezon localtim date iv manipul execut modul write blank output date column valu column sourc variabl date read data frame date format inspect log file aml aml local time zone log file warn strptime identifi timezon set environ variabl strptime unknown timezon localtim set default time zone strptime unknown timezon explicitli defin global environ time variabl persist write variabl write column blank studio presum local time taken read blog date time format tri convert posixct send output stream tenantusag date est edt tenantusag date est edt variabl refus write output date date column blank",
        "Challenge_readability":12.8,
        "Challenge_reading_time":43.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":7.0,
        "Challenge_sentence_count":25,
        "Challenge_solved_time":null,
        "Challenge_title":"How to define current time zone in Azure ML for strptime function, unknown timezone 'localtime'",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1738.0,
        "Challenge_word_count":310,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":223168578193
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>How does Azure ML handle categorical columns during training a linear regression model? A linear regression model takes continuous values. However, even though I haven't changed anything of those categorical columns, Azure ML trains linear and logistic regression without error. So I would like to know how Azure ML manages to process categorical columns behind the scene. Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1446222013497,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"categor column train linear logist regress model process categor column train process",
        "Challenge_last_edit_time":1483479351347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33440519",
        "Challenge_link_count":0,
        "Challenge_original_content":"categor column train linear logist regress model categor column train linear regress model linear regress model valu haven categor column train linear logist regress process categor column scene",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"categor column train linear logist regress model categor column train linear regress model linear regress model valu haven categor column train linear logist regress process categor column scene",
        "Challenge_readability":11.3,
        "Challenge_reading_time":6.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How does Azure ML handle categorical columns during training a linear or logistic regression model?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":793.0,
        "Challenge_word_count":72,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":223025986503
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi I am trying to begin an Azure ML algorithm by executing a python script that queries data from a table storage account. I do it using this:<\/p>\n<pre><code>entities_Azure=table_session.query_entities(table_name=table_name, \n                                                filter=&quot;PartitionKey eq '&quot; + partitionKey + &quot;'&quot;,\n                                                select='PartitionKey,RowKey,Timestamp,value',\n                                                next_partition_key = next_pk,\n                                                next_row_key = next_rk, top=1000)  \n<\/code><\/pre>\n<p>I pass in the variables needed when calling the function that this bit of code sits in, and I include the function by including a zip file in Azure ML.<\/p>\n<p>I assume the error is due to the query taking too long, or something like that, but it has to take a long time because I might have to query loads of data.... I looked at this SO post <a href=\"https:\/\/stackoverflow.com\/questions\/18825567\/windows-azure-storage-table-connection-timed-out\">Windows Azure Storage Table connection timed out<\/a> which is a similar issue I think with regard to hitting specified thresholds for these queries, but I don't know how I'd be able to avoid it. The run time of the program is only about 1.5 mins before timing out..<\/p>\n<p>Any ideas as to why this is happening and how I might be able to solve it?<\/p>\n<h3>Edit:<\/h3>\n<p>As per <a href=\"https:\/\/stackoverflow.com\/users\/4989676\/peter-pan-msft\">Peter Pan - MSFT<\/a>'s advice I ran a query that was more specific:<\/p>\n<pre><code>entities_Azure=table_service.query_entities(table_name='#######',select='PartitionKey,RowKey,Timestamp,value', next_partition_key = None, next_row_key = None, top=2)\n<\/code><\/pre>\n<p>This returned the following error log:<\/p>\n<pre><code>Error 0085: The following error occurred during script evaluation, please view the output log for more information:  \n---------- Start of error message from Python interpreter ----------  \ndata:text\/plain,Caught exception while executing function: Traceback (most recent call last):    \n\nFile &quot;C:\\server\\invokepy.py&quot;, line 169, in \nbatch odfs = mod.azureml_main(*idfs)    \n\nFile &quot;C:\\temp\\azuremod.py&quot;, line 61, in \nazureml_main entities_Azure=table_service.query_entities(table_name='######',select='PartitionKey,RowKey,Timestamp,value', next_partition_key = None, next_row_key = None, top=2)    \n\nFile &quot;.\/Script Bundle\\azure\\storage\\table\\tableservice.py&quot;, line 421, in query_entities\n response = self._perform_request(request)    \n\nFile &quot;.\/Script Bundle\\azure\\storage\\storageclient.py&quot;, line 171, in _perform_request\n resp = self._filter(request)    \n\nFile &quot;.\/Script Bundle\\azure\\storage\\table\\tableservice.py&quot;, line 664, in _perform_request_worker\n return self._httpclient.perform_request(request)    \n\nFile &quot;.\/Script Bundle\\azure\\storage\\_http\\httpclient.py&quot;, line 181, in perform_request\n self.send_request_body(connection, request.body)    \n\nFile &quot;.\/Script Bundle\\azure\\storage\\_http\\httpclient.py&quot;, line 145, in send_request_body\n connection.send(None)    \n\nFile &quot;.\/Script Bundle\\azure\\storage\\_http\\requestsclient.py&quot;, line 81, in send\n self.response = self.session.request(self.method, self.uri, data=request_body, headers=self.headers, timeout=self.timeout)    \n\nFile &quot;C:\\pyhome\\lib\\site-packages\\requests\\sessions.py&quot;, line 456, in request\n resp = self.send(prep, **send_kwargs)    \n\nFile &quot;C:\\pyhome\\lib\\site-packages\\requests\\sessions.py&quot;, line 559, in send\n r = adapter.send(request, **kwargs)    \n\nFile &quot;C:\\pyhome\\lib\\site-packages\\requests\\adapters.py&quot;, line 382, in send\n raise SSLError(e, request=request) \n\nSSLError: The write operation timed out    \n---------- End of error message from Python  interpreter \n---------- Start time: UTC 11\/18\/2015 11:39:32 End time: UTC 11\/18\/2015 11:40:53\n<\/code><\/pre>\n<p>Hopefully this brings more insight to the situation!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1447768980763,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"sslerror execut queri data tabl storag account queri program time minut tri run queri sslerror write oper timeout advic",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33758922",
        "Challenge_link_count":2,
        "Challenge_original_content":"sslerror sslerror write oper time tabl storag entiti queri begin algorithm execut queri data tabl storag account entiti tabl session queri entiti tabl tabl filter partitionkei partitionkei select partitionkei rowkei timestamp valu partit kei row kei pass variabl call function bit sit function zip file queri time queri load data window storag tabl connect time hit specifi threshold queri avoid run time program time idea edit peter pan msft advic ran queri entiti tabl servic queri entiti tabl select partitionkei rowkei timestamp valu partit kei row kei return log evalu output log start messag interpret data text plain caught except execut function traceback file server invokepi line batch odf mod idf file temp azuremod line entiti tabl servic queri entiti tabl select partitionkei rowkei timestamp valu partit kei row kei file bundl storag tabl tableservic line queri entiti respons perform request request file bundl storag storagecli line perform request resp filter request file bundl storag tabl tableservic line perform request worker return httpclient perform request request file bundl storag http httpclient line perform request send request bodi connect request bodi file bundl storag http httpclient line send request bodi connect send file bundl storag http requestscli line send respons session request uri data request bodi header header timeout timeout file pyhom lib site packag request session line request resp send prep send kwarg file pyhom lib site packag request session line send adapt send request kwarg file pyhom lib site packag request adapt line send rais sslerror request request sslerror write oper time end messag interpret start time utc end time utc hopefulli",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"tabl storag entiti queri begin algorithm execut queri data tabl storag account pass variabl call function bit sit function zip file queri time queri load window storag tabl connect time hit specifi threshold queri avoid run time program time idea edit peter pan msft advic ran queri return log hopefulli",
        "Challenge_readability":11.2,
        "Challenge_reading_time":50.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":35,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML with python - (SSLError(SSLError('The write operation timed out',),),) when doing a table storage entity query",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":782.0,
        "Challenge_word_count":390,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":221479019237
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using an \"execute R script module\" in Azure-ml studio, when I plot to an rgl device, I get a broken image icon under the graphics section of the R Device output.  <\/p>\n\n<p>Is there some way to view (and even interact with) the resulting rgl device?  If not is there some way to transfer the rgl output to a standard R graphics device?  <\/p>\n\n<p>Simple example:<\/p>\n\n<pre><code># put this code inside the execute R script module\nlibrary(rgl)\nrgl.spheres(0,0,0, radius=1, col=\"red\")\n<\/code><\/pre>\n\n<p>To be clear, I know about <code>rgl.snapshot<\/code> and <code>rgl.postscript<\/code> and how to save and \/or view an rgl device in a standard R session, but have not been able to make these standard approaches work in azure-ml.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1448680195707,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"interact rgl plot studio plot rgl devic broken imag icon graphic section devic output transfer rgl output standard graphic devic awar standard rgl snapshot rgl postscript",
        "Challenge_last_edit_time":1456849933912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/33967251",
        "Challenge_link_count":0,
        "Challenge_original_content":"rgl plot execut modul studio plot rgl devic broken imag icon graphic section devic output interact rgl devic transfer rgl output standard graphic devic insid execut modul librari rgl rgl sphere radiu col red clear rgl snapshot rgl postscript save rgl devic standard session standard",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"rgl plot execut modul studio plot rgl devic broken imag icon graphic section devic output rgl devic transfer rgl output standard graphic devic clear save rgl devic standard session standard",
        "Challenge_readability":7.1,
        "Challenge_reading_time":9.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":8.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"View an rgl plot using Microsoft Azure Machine Learning",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":508.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":220567804293
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've recently created new experiment and set \"Azure SQL Database\" as data source. I've typed all necessary information to connect to database and I've run the experiment. Everything seems to be correct, but when I try to \"Visualise\" my data, some prompt at the bottom of the screen says:<\/p>\n\n<blockquote>\n  <p>Error producing the visualization of the output<\/p>\n<\/blockquote>\n\n<p>And that's all. As well when I click \"Visualise\", a window appears with empty table (column names are correct, so it's sign that it's connected to database, because it had to get names of columns from db).<\/p>\n\n<p>Where should I look for some additional information about why it doesn't work? What do you think could cause a problem? How to fix it?<\/p>\n\n<p><strong>EDIT:\nMore info:<\/strong><\/p>\n\n<p>What I've already done is creating blank experiment and drag&amp;drop \"Reader\" from \"Data Input and Output\". In \"Reader\" I've set \"Data source\" to \"Azure SQL Database\" and I've put login credentials. Database query: \"SELECT * FROM Words\" (Words is one of db tables).<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1449076498530,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"visual data creat sql databas data sourc messag displai produc visual output advic addit",
        "Challenge_last_edit_time":1454300469516,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34048905",
        "Challenge_link_count":0,
        "Challenge_original_content":"produc visual output creat set sql databas data sourc type connect databas run visualis data prompt screen sai produc visual output click visualis window tabl column sign connect databas column addit edit creat blank drag drop reader data input output reader set data sourc sql databas login credenti databas queri select tabl",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"produc visual output creat set sql databas data sourc type connect databas run visualis data prompt screen sai produc visual output click visualis window tabl addit edit creat blank drag drop reader data input output reader set data sourc sql databas login credenti databas queri select",
        "Challenge_readability":8.3,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning - Error producing the visualization of the output",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":740.0,
        "Challenge_word_count":174,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":220171501470
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In my Azure ML experiment I am using a writer to write data into a table in Azure SQL Database. However, I would like to truncate the data in that table before each insert. Is there any way that I can achieve this through the experiment itself? Any inbuilt module through which I can achieve this?<\/p>\n\n<p>I know from sql using triggers I can achieve this. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1449593595153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"truncat data tabl sql databas insert achiev awar trigger sql",
        "Challenge_last_edit_time":1449684018627,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34161557",
        "Challenge_link_count":0,
        "Challenge_original_content":"truncat tabl sql databas writer write data tabl sql databas truncat data tabl insert achiev inbuilt modul achiev sql trigger achiev",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"truncat tabl sql databas writer write data tabl sql databas truncat data tabl insert achiev inbuilt modul achiev sql trigger achiev",
        "Challenge_readability":5.9,
        "Challenge_reading_time":5.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Truncate Table in Azure SQL Database for Azure ML Experiment",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":436.0,
        "Challenge_word_count":76,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":219654404847
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created an azure ml experiment which fetches data from API and updates it in sql azure database. My power bi report picks data from this database and displays the report. The data from the source is changing frequently. So I need something like a checkbox in power bi which when checked will trigger the azure ml experiment and update the database with latest data.<\/p>\n\n<p>I know that we can schedule it to run in Rstudio pipeline but we are not thinking of this approach as it is not financially viable.\nThanks in Advance.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1450204682987,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"updat sql databas latest data api frequent trigger power checkbox schedul run rstudio pipelin financi constraint",
        "Challenge_last_edit_time":1456849941007,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34296921",
        "Challenge_link_count":0,
        "Challenge_original_content":"trigger powerbi creat fetch data api updat sql databas power report pick data databas displai report data sourc frequent checkbox power trigger updat databas latest data schedul run rstudio pipelin financi viabl advanc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"trigger powerbi creat fetch data api updat sql databas power report pick data databas displai report data sourc frequent checkbox power trigger updat databas latest data schedul run rstudio pipelin financi viabl advanc",
        "Challenge_readability":7.7,
        "Challenge_reading_time":7.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"trigger azure ml experiment from powerbi",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":292.0,
        "Challenge_word_count":100,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":219043317013
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am new to Azure Machine Learning. I have created a training experiment where training data has some missing values. The logic for handling missing data and few other transformations is in Python code which works on this data.<\/p>\n\n<p>Now I want the same for test data. I have deployed the experiment as web service. So, the schema is produced for input and output data (all are Numeric fields).<\/p>\n\n<p>Two questions:\n1. It asks me to define the label for test data as well, otherwise it gives inconsistent number of columns error since label column is missing in the test data<br>\n2. I have some missing data in test data, which ideally Python script in the experiment should take care. But it gives me the following error because of schema.<\/p>\n\n<pre><code>The request failed with status code: 400\nContent-Length: 323\nContent-Type: application\/json; charset=utf-8\nServer: Microsoft-HTTPAPI\/2.0\nDate: Thu, 21 Jan 2016 11:44:49 GMT\nConnection: close\n\n{u'error': {u'message': u'Invalid argument provided.', u'code': 'BadArgument', u'details': [{u'message': u'Parsing of input vector failed.  Verify the input vector has the correct number of columns and data types.  Additional details: Value was either too large or too small for an Int32..', u'code': u'InputParseError', u'target': u'input1'}]}}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1453377464100,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"miss data test data creat train miss valu miss data deploi web servic defin label test data miss data test data",
        "Challenge_last_edit_time":1454300414052,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/34923196",
        "Challenge_link_count":0,
        "Challenge_original_content":"miss data test data machineformachin creat train train data miss valu logic miss data transform data test data deploi web servic schema produc input output data numer field defin label test data inconsist column label column miss test data miss data test data ideal care schema request statu length type json charset utf server httpapi date thu jan gmt connect close messag argument badargu messag pars input vector verifi input vector column data type addit valu larg small inputparseerror target input",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"miss data test data machineformachin creat train train data miss valu logic miss data transform data test data deploi web servic schema produc input output data defin label test data inconsist column label column miss test data miss data test data ideal care schema",
        "Challenge_readability":8.0,
        "Challenge_reading_time":17.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Missing data in test data for Azure MachineforMachine Learning",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":796.0,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":215870535900
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>After reading this post here: <a href=\"https:\/\/stackoverflow.com\/questions\/34990561\/azure-machine-learning-request-response-latency\/35020997#35020997?newreg=18fbd305056d4e6ba239783ebefb9629\">Azure Machine Learning Request Response latency<\/a>\nand the article mentioned in the comments I was wondering if this behavior is also true when a published webservice is called in batch mode. \nEspecially since I have read somewhere (sorry, can't find the link at the moment) that the batch calls are not influenced by the \"concurrent calls\" config...<\/p>\n\n<p>In our scenario we have a custom R module uploaded to our workspace which includes some libraries that are not available on aML by default. The module takes a dataset, trains a binary tree, creates some plots and encodes them in base64 before returning those as a dataset. Locally that does not take more than 5s. But in the aML webservice it takes approx. 90s and it seems that the runtime in batchmode does not improve when calling the service multiple times.<\/p>\n\n<p>Additionally it would be nice to know for how long the containers, mentioned in the linked post, will stay warm.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":5,
        "Challenge_created_time":1453972229010,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"batch execut latenc upload modul workspac dataset train binari tree creat plot encod base return dataset local webservic approxim publish webservic call batch mode durat link stai warm",
        "Challenge_last_edit_time":1495540755230,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35057156",
        "Challenge_link_count":1,
        "Challenge_original_content":"batch execut latenc read request respons latenc articl comment publish webservic call batch mode read sorri link moment batch call influenc concurr call config modul upload workspac librari aml default modul dataset train binari tree creat plot encod base return dataset local aml webservic approx runtim batchmod improv call servic multipl time nice link stai warm",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"batch execut latenc read request respons latenc articl comment publish webservic call batch mode read batch call influenc concurr call modul upload workspac librari aml default modul dataset train binari tree creat plot encod base return dataset local aml webservic approx runtim batchmod improv call servic multipl time nice link stai warm",
        "Challenge_readability":8.7,
        "Challenge_reading_time":14.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML batch execution latency",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":212.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":215275770990
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new in Azure and I would like to run my R Code, I read how it's work but I have problem to run my packages but I always get errors like this . <\/p>\n\n<blockquote>\n  <p>package 'ParamHelpers' required by 'mlr' could not be found<\/p>\n<\/blockquote>\n\n<p>And when I run in Rstudio i don't have the same errors <\/p>\n\n<p>I have a zip file with inside the zip packages <\/p>\n\n<pre><code>install.packages(\"src\/Metrics_0.1.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/checkmate_1.7.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/mlr_2.7.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xgboost_0.4-2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/BBmisc_1.9.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(xgboost)\nlibrary(Metrics, lib.loc=\".\", verbose=TRUE)\nlibrary(checkmate, lib.loc=\".\", verbose=TRUE)\nlibrary(BBmisc, lib.loc=\".\", verbose=TRUE)\nlibrary(mlr, lib.loc=\".\", verbose=TRUE)\nlibrary(Hmisc)\npackageVersion(\"mlr\")\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1454502020573,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run troubl run packag receiv packag paramhelp mlr instal packag zip file tri run rstudio",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35176930",
        "Challenge_link_count":0,
        "Challenge_original_content":"packag run read run packag packag paramhelp mlr run rstudio zip file insid zip packag instal packag src metric zip lib repo null verbos instal packag src checkmat zip lib repo null verbos instal packag src mlr zip lib repo null verbos instal packag src zip lib repo null verbos instal packag src bbmisc zip lib repo null verbos librari librari metric lib loc verbos librari checkmat lib loc verbos librari bbmisc lib loc verbos librari mlr lib loc verbos librari hmisc packagevers mlr",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"packag run read run packag packag paramhelp mlr run rstudio zip file insid zip packag",
        "Challenge_readability":8.3,
        "Challenge_reading_time":13.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Packages R Azure Machine Learning",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":685.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":214745979427
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I want to deploy a basic trained R model as a webservice to AzureML. Similar to what is done here:\n<a href=\"http:\/\/www.r-bloggers.com\/deploying-a-car-price-model-using-r-and-azureml\/\" rel=\"nofollow\">http:\/\/www.r-bloggers.com\/deploying-a-car-price-model-using-r-and-azureml\/<\/a><\/p>\n\n<p>Since that post the publishWebService function in the R AzureML package was has changed it now requires me to have a workspace object as first parameter thus my R code looks as follows:<\/p>\n\n<pre><code>    library(MASS)\n    library(AzureML)\n\n    PredictionModel = lm( medv ~ lstat , data = Boston )\n\n    PricePredFunktion = function(percent)\n    {return(predict(PredictionModel, data.frame(lstat =percent)))}\n\n    myWsID = \"&lt;my Workspace ID&gt;\"\n    myAuth = \"&lt;my Authorization code\"\n\n    ws = workspace(myWsID, myAuth, api_endpoint = \"https:\/\/studio.azureml.net\/\", .validate = TRUE)\n\n    # publish the R function to AzureML\n    PricePredService = publishWebService(\n      ws,\n\n      \"PricePredFunktion\",\n      \"PricePredOnline\",\n      list(\"lstat\" = \"float\"),\n      list(\"mdev\" = \"float\"),\n      myWsID,\n      myAuth\n    )\n<\/code><\/pre>\n\n<p>But every time I execute the code I get the following error:<\/p>\n\n<pre><code>    Error in publishWebService(ws, \"PricePredFunktion\", \"PricePredOnline\",  : \n    Requires external zip utility. Please install zip, ensure it's on your path and try again.\n<\/code><\/pre>\n\n<p>I tried installing programs that handle zip files (like 7zip) on my machine as well as calling the <code>utils<\/code> library in R which allows R to directly interact with zip files. But I couldn't get rid of the error.<\/p>\n\n<p>I also found the R package code that is throwing the error, it is on line 154 on this page:\n<a href=\"https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/R\/internal.R\" rel=\"nofollow\">https:\/\/github.com\/RevolutionAnalytics\/AzureML\/blob\/master\/R\/internal.R<\/a> <\/p>\n\n<p>but it didn't help me in figuring out what to do.<\/p>\n\n<p>Thanks in advance for any Help!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1454506823477,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi train model webservic publishwebservic function packag function workspac object paramet messag messag state extern zip util tri instal program zip file call util librari persist packag throw figur",
        "Challenge_last_edit_time":1456850085647,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35178688",
        "Challenge_link_count":5,
        "Challenge_original_content":"publish webservic extern zip util deploi train model webservic http blogger com deploi car price model publishwebservic function packag workspac object paramet librari mass librari predictionmodel medv lstat data boston pricepredfunkt function percent return predict predictionmodel data frame lstat percent mywsid myauth author workspac mywsid myauth api endpoint http studio net publish function pricepredservic publishwebservic pricepredfunkt pricepredonlin list lstat list mdev mywsid myauth time execut publishwebservic pricepredfunkt pricepredonlin extern zip util instal zip path tri instal program zip file zip call util librari allow directli interact zip file rid packag throw line page http github com revolutionanalyt blob master intern figur advanc",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"publish webservic extern zip util deploi train model webservic publishwebservic function packag workspac object paramet time execut tri instal program zip file call librari allow directli interact zip file rid packag throw line page figur advanc",
        "Challenge_readability":14.2,
        "Challenge_reading_time":25.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Publishing AzureML Webservice from R requires external zip utility",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":962.0,
        "Challenge_word_count":225,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":214741176523
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the code with a URL and API key but every time i will get the some error of 405 or 400. Is there any proper way to implement Azure ML API in Rails.<\/p>\n\n<p>The code as below :- <\/p>\n\n<p>data =  {<\/p>\n\n<pre><code>\"Inputs\" =&gt; {\n\n        \"input1\" =&gt;\n        {\n            \"ColumnNames\" =&gt; @a,\n            \"Values\" =&gt; [ @writer ]\n        },        },\n    \"GlobalParameters\" =&gt; {\n}\n}\n\nbody = data.to_json\nputs \"adssssssssssssssssssssssssssssss#{body}\"\nurl = \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/5aecd8f887e64999a9c854d724e5\/services\/5f350fa1b48647ce95c5279eee2170d0\/execute?api-version=2.0&amp;details=true\"\napi_key = 'wGMMQGYlo4tttV+oTjrR\/tyt6xYSmWskCezNKkbGwvAVt0wsessJUORQ==' # Replace this with the API key for the web service\nheaders = {'Content-Type' =&gt; 'application\/json', 'Authorization' =&gt; ('Bearer '+ api_key)}\n\n\n\nurl = URI.parse(url)\nreq = Net::HTTP::Get.new(url.request_uri,headers)\nhttp = Net::HTTP.new(url.host, url.port)\nres = http.request(req)\n\n{\"Inputs\":{\"input1\":{\"ColumnNames\":[\"encounter_id\",\"patient_nbr\",\"Fname\",\"Lname\",\"Email\",\"Type\",\"race\",\"gender\",\"Birth Date\",\"Birth Year\",\"age\",\"Age Min\",\"Age Max\",\"weight\",\"admission_type_id\",\"discharge_disposition_id\",\"admission_source_id\",\"time_in_hospital\",\"payer_code\",\"medical_specialty\",\"num_lab_procedures\",\"num_procedures\",\"num_medications\",\"number_outpatient\",\"number_emergency\",\"number_inpatient\",\"number_diagnoses\",\"max_glu_serum\",\"A1Cresult\",\"metformin\",\"repaglinide\",\"nateglinide\",\"chlorpropamide\",\"glimepiride\",\"acetohexamide\",\"glipizide\",\"glyburide\",\"tolbutamide\",\"pioglitazone\",\"rosiglitazone\",\"acarbose\",\"miglitol\",\"troglitazone\",\"tolazamide\",\"examide\",\"citoglipton\",\"insulin\",\"glyburide-metformin\",\"glipizide-metformin\",\"glimepiride-pioglitazone\",\"metformin-rosiglitazone\",\"metformin-pioglitazone\",\"change\",\"diabetesMed\",\"readmitted\"],\"Values\":[[[{\"$oid\":\"56b1ab886e75720ba23b5400\"},\"\",\"Rana\",\"Warhurst\",null,\"Patient\",\"Caucasian\",\"Male\",\"2012-10-23\",\"\",3,\"\",\"\",\"\",\"\",null,null,null,\"\",null,\"\",\"\",\"\",\"\",\"\",\"\",null,\"\",\"No\",\"NO\"]]]}},\"GlobalParameters\":{}}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1454934570380,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"implement api rail url api kei json data structur header type author",
        "Challenge_last_edit_time":1520314007140,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35269720",
        "Challenge_link_count":1,
        "Challenge_original_content":"api rubi rail url api kei time implement api rail data input input columnnam valu writer globalparamet bodi data json put bodi url http ussouthcentr servic net workspac aecdfeacd servic ffabcecd execut api version api kei wgmmqgylov otjrr tytxysmwskceznkkbgwvavtwsessjuorq replac api kei web servic header type json author bearer api kei url uri pars url req net http url request uri header http net http url host url port re http request req input input columnnam patient nbr fname lname email type race gender birth date birth year ag ag ag weight admiss type discharg disposit admiss sourc time hospit payer medic specialti num lab procedur num procedur num medic outpati inpati diagnos glu serum acresult metformin repaglinid nateglinid chlorpropamid glimepirid acetohexamid glipizid glyburid tolbutamid pioglitazon rosiglitazon acarbos miglitol troglitazon tolazamid examid citoglipton insulin glyburid metformin glipizid metformin glimepirid pioglitazon metformin rosiglitazon metformin pioglitazon diabetesm readmit valu oid babebab rana warhurst null patient caucasian male null null null null null globalparamet",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"api rubi rail url api kei time implement api rail data",
        "Challenge_readability":25.1,
        "Challenge_reading_time":28.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use Azure ML API with Ruby on Rails",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":171.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":214313429620
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can machine learning be used to transform\/modifiy a list of numbers.<\/p>\n\n<p>I have many pairs of binary files read from vehicle ECUs, an original or stock file before the vehicle was tuned, and a modified file which has the engine parameters altered.  The files are basically lists of little or big endian 16 bit numbers.<\/p>\n\n<p>I was wondering if it is at all possible to feed these pairs of files into machine learning, and for it to take a new stock file and attempt to transform or tune that stock file.<\/p>\n\n<p>I would appreciate it if somebody could tell me if this is something which is at all possible.  All of the examples I've found appear to make decisions on data rather than do any sort of a transformation.<\/p>\n\n<p>Also I'm hoping to use azure for this.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1455630592703,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"transform list pair binari file read vehicl ecu purpos transform stock file base pair file decis data transform",
        "Challenge_last_edit_time":1456850005583,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35434371",
        "Challenge_link_count":0,
        "Challenge_original_content":"data transform transform modifii list pair binari file read vehicl ecu origin stock file vehicl tune modifi file engin paramet alter file list littl big endian bit feed pair file stock file transform tune stock file somebodi decis data sort transform hope",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"data transform list pair binari file read vehicl ecu origin stock file vehicl tune modifi file engin paramet alter file list littl big endian bit feed pair file stock file transform tune stock file somebodi decis data sort transform hope",
        "Challenge_readability":9.0,
        "Challenge_reading_time":9.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Data Transformation",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":113.0,
        "Challenge_word_count":141,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":213617407297
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an experiment within Azure ML Studio and published as a web service. I need the experiment to run nightly or possible several times a day. I currently have azure mobile services and azure web jobs as part of the application and need to create an endpoint to retrieve data from the published web service. Obviously, the whole point is to make sure I have updated data.<\/p>\n\n<p>I see answers like use azure data factory but I need specifics as in how to actually set up the scheduler.<\/p>\n\n<p>I explain my dilemma further @ <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/e7126c6e-b43e-474a-b461-191f0e27eb74\/scheduling-a-machine-learning-experiment-and-publishing-nightly?forum=AzureDataFactory\" rel=\"nofollow\">https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/e7126c6e-b43e-474a-b461-191f0e27eb74\/scheduling-a-machine-learning-experiment-and-publishing-nightly?forum=AzureDataFactory<\/a><\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1456360366850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set schedul run nightli time dai creat studio publish web servic creat endpoint retriev updat data guidanc set schedul dilemma forum assist",
        "Challenge_last_edit_time":1456812982912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35616017",
        "Challenge_link_count":2,
        "Challenge_original_content":"kick base schedul creat studio publish web servic run nightli time dai mobil servic web job creat endpoint retriev data publish web servic obvious updat data data factori set schedul explain dilemma http social msdn com forum ec feeb schedul publish nightli forum azuredatafactori",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"kick base schedul creat studio publish web servic run nightli time dai mobil servic web job creat endpoint retriev data publish web servic obvious updat data data factori set schedul explain dilemma",
        "Challenge_readability":14.1,
        "Challenge_reading_time":12.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How do you kick off an Azure ML experiment based on a scheduler?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":192.0,
        "Challenge_word_count":110,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":212887633150
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have created data sets of various sizes say 1GB, 2GB, 3GB, 4GB (&lt; 10 GB) and executing various machine learning models on Azure ML. <\/p>\n\n<p>1) Can I know what is the server specifications (RAM, CPU) that is provided in the Azure ML service.<\/p>\n\n<p>2) Also at times the reader says \"Memory exhaust\" for >4GB of data.Though azure ml should be able to handle 10GB of data as per documentation.<\/p>\n\n<p>3) If I run multiple experiments(in different tabs of browser) in parallel, its taking more time.<\/p>\n\n<p>4) Is there any way to set the RAM, CPU cores in Azure ML<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1456387015953,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"execut model server experienc memori exhaust data larger run multipl parallel time set ram cpu core",
        "Challenge_last_edit_time":1457425395532,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/35621424",
        "Challenge_link_count":0,
        "Challenge_original_content":"time test creat data set size data data document run multipl tab browser parallel time set ram cpu core",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"time test creat data set size execut model server servic time reader sai memori exhaust data document run multipl parallel time set ram cpu core",
        "Challenge_readability":6.8,
        "Challenge_reading_time":7.23,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Timing test on azure ml",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":155.0,
        "Challenge_word_count":105,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":212860984047
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have multiple columns in Azure Machine Learning that each have an hour, year, day, minute, etc for a date. I need to convert this hour from UTC to EDT, and then make it a date string such as<\/p>\n\n<blockquote>\n  <p>YYYY\/MM\/DD HH:SS<\/p>\n<\/blockquote>\n\n<p>This way, I can do an inner join. I've tried using CAST, CONVERT, and other SQLite functions, but none of these combos work. Here is where I am now: <\/p>\n\n<pre><code>select *\nCAST([Col11] as int) -4 as EDTHour\n\n([Col8] || '\/' || [Col9] || '\/' || [Col10] || ' ' || EDTHour|| ':' || [Col12]) as WeatherTime from t1\n\nselect 'Time Stamp' as secondTableTime from t2\n\nSELECT *\nFROM t1\nINNER JOIN t2\nON t1.WeatherTime=t2.secondTableTime\n<\/code><\/pre>\n\n<p>However, It never lets me cast the varchar column Col11 to a integer or decimal. What am I missing? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458224867397,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"convert hour utc edt date format tri sqlite function cast convert cast varchar column col integ decim",
        "Challenge_last_edit_time":1458336187940,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36063443",
        "Challenge_link_count":0,
        "Challenge_original_content":"sqlite queri cast integ subtract multipl column hour year dai minut date convert hour utc edt date tri cast convert sqlite function combo select cast col edthour col col col edthour col weathertim select time stamp secondtabletim select weathertim secondtabletim cast varchar column col integ decim miss",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"sqlite queri cast integ subtract multipl column hour year dai minut date convert hour utc edt date tri cast convert sqlite function combo cast varchar column col integ decim miss",
        "Challenge_readability":7.4,
        "Challenge_reading_time":10.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is this SQLite query not letting me cast the integer and subtract?",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":64.0,
        "Challenge_word_count":137,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":211023132603
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am planning to build a model in Azure Ml and there are certain parameters that needs to passed to the model ,before the model could be run.These parameters should come from PowerBi,maybe based on some filters . Is it possible ?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1458418332563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pass paramet powerbi order run model paramet base filter",
        "Challenge_last_edit_time":1458425260707,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36106614",
        "Challenge_link_count":0,
        "Challenge_original_content":"push paramet powerbi plan build model paramet pass model model run paramet come powerbi mayb base filter",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"push paramet powerbi plan build model paramet pass model model paramet come powerbi mayb base filter",
        "Challenge_readability":5.5,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Push parameters from powerBi to Azure ML",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":102.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":210829667437
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use cyrillic symbols in my IPython notebooks. It works fine when I work in ML studio.<\/p>\n\n<p>But when I download notebooks and open them (for example on <a href=\"http:\/\/try.jupyter.org\" rel=\"nofollow\">http:\/\/try.jupyter.org<\/a> ), I see strange characters.<\/p>\n\n<p>Bad notebook (created on Azure ML Studio):<\/p>\n\n<pre><code>{\"nbformat_minor\": 0, \"cells\": [{\"source\": \"\\u00d1\\u0082\\u00d0\\u00b5\\u00d1\\u0081\\u00d1\\u0082\", \"cell_type\": \"markdown\", \"metadata\": {\"collapsed\": true}}], \"nbformat\": 4, \"metadata\": {\"kernelspec\": {\"display_name\": \"Python 2\", \"name\": \"python2\", \"language\": \"python\"}, \"language_info\": {\"mimetype\": \"text\/x-python\", \"nbconvert_exporter\": \"python\", \"version\": \"2.7.11\", \"name\": \"python\", \"file_extension\": \".py\", \"pygments_lexer\": \"ipython2\", \"codemirror_mode\": {\"version\": 2, \"name\": \"ipython\"}}}}\n\n$ file bad.ipynb \nbad.ipynb: ASCII text, with very long lines, with no line terminators\n<\/code><\/pre>\n\n<p>\"Good\" version, created on <a href=\"http:\/\/try.jupyter.org\" rel=\"nofollow\">http:\/\/try.jupyter.org<\/a>:<\/p>\n\n<pre><code>{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"\u0442\u0435\u0441\u0442\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 2\",\n   \"language\": \"python\",\n   \"name\": \"python2\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 2\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text\/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython2\",\n   \"version\": \"2.7.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}\n\n$ file good.ipynb \ngood.ipynb: UTF-8 Unicode text\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1458803950260,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"displai cyril symbol ipython notebook download notebook open platform charact cyril symbol highlight comparison version creat studio version creat http jupyt org",
        "Challenge_last_edit_time":1458825814968,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36194987",
        "Challenge_link_count":4,
        "Challenge_original_content":"symbol ipython notebook cyril symbol ipython notebook studio download notebook open http jupyt org charact notebook creat studio nbformat minor cell sourc cell type markdown metadata collaps nbformat metadata kernelspec displai languag languag mimetyp text nbconvert export version file extens pygment lexer ipython codemirror mode version ipython file ipynb ipynb ascii text line line termin version creat http jupyt org cell cell type markdown metadata sourc metadata kernelspec displai languag languag codemirror mode ipython version file extens mimetyp text nbconvert export pygment lexer ipython version nbformat nbformat minor file ipynb ipynb utf unicod text",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"symbol ipython notebook cyril symbol ipython notebook studio download notebook open charact notebook version creat",
        "Challenge_readability":9.0,
        "Challenge_reading_time":20.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Weird symbols in IPython Notebook",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1275.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":210444049740
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using AngelList DB to categorize startups based on their industries since these startups are categorized based on community input which is misleading most of the time.<\/p>\n\n<p>My business objective is to extract keywords that indicate to which industry this specific startup belongs to then map it to one of the industries specified in LinkedIn sheet <a href=\"https:\/\/developer.linkedin.com\/docs\/reference\/industry-codes\" rel=\"nofollow\">https:\/\/developer.linkedin.com\/docs\/reference\/industry-codes<\/a><\/p>\n\n<p>I experimented with Azure Machine learning, where I pushed 300 startups descriptions and analyzed the keyword extraction was pretty bad and was not even close to what I am trying to achieve.<\/p>\n\n<p>I would like to know how data scientists will approach this problem? where should I look? and where I should not? is keyword analysis tools (like Google Adwords keyword planner is a viable option)<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1459274273800,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"categor startup base industri angellist mislead commun input aim extract keyword startup descript map industri specifi linkedin sheet achiev desir advic data scientist keyword analysi adword keyword planner viabl option",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36291712",
        "Challenge_link_count":2,
        "Challenge_original_content":"startup industri descript angellist categor startup base industri startup categor base commun input mislead time busi object extract keyword industri startup belong map industri specifi linkedin sheet http linkedin com doc industri push startup descript analyz keyword extract pretti close achiev data scientist keyword analysi adword keyword planner viabl option",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"startup industri descript angellist categor startup base industri startup categor base commun input mislead time busi object extract keyword industri startup belong map industri specifi linkedin sheet push startup descript analyz keyword extract pretti close achiev data scientist keyword analysi",
        "Challenge_readability":12.0,
        "Challenge_reading_time":12.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Find startup's industry from its description",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":100.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":209973726200
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I deployed a web service of multiclass decision forest, but when i tested it, just kept getting this returned:<\/p>\n\n<p>Error code: InternalError, Http status code: 500<\/p>\n\n<p>How can I solve it?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1461225573297,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"intern test deploi web servic multiclass decis forest internalerror http statu",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36763348",
        "Challenge_link_count":0,
        "Challenge_original_content":"web servic return intern deploi web servic multiclass decis forest test kept return internalerror http statu",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic return intern deploi web servic multiclass decis forest test kept return internalerror http statu",
        "Challenge_readability":10.5,
        "Challenge_reading_time":3.04,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"azure ml web service return internal error",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":322.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":208022426703
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is it possible to call Cognitive Services API in Azure ML studio when build model?\u201d any document our sample experiment can be reference?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1461813240840,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"call cognit servic api studio build model document sampl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/36904609",
        "Challenge_link_count":0,
        "Challenge_original_content":"cognit servic api cognit servic api studio build model document sampl advanc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"cognit servic api cognit servic api studio build model document sampl advanc",
        "Challenge_readability":8.9,
        "Challenge_reading_time":2.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Microsoft Azure Machine Learning and Cognitive Services API",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":877.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":207434759160
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Could anyone please let me know what is the purpose of making some variable as Categorical in EditMetadata module of Machine Learning? Would appreciate if explained with some example. Also is it applicable on both features as well as label?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1462604751650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"clarif purpos variabl categor editmetadata modul request explan appli featur label",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37085588",
        "Challenge_link_count":0,
        "Challenge_original_content":"categor variabl editmetadata modul purpos variabl categor editmetadata modul explain featur label",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"categor variabl editmetadata modul purpos variabl categor editmetadata modul explain featur label",
        "Challenge_readability":10.3,
        "Challenge_reading_time":3.78,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Categorical Variable in EditMetadata module of Azure ML",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":892.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":206643248350
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've been experimenting a bit with the AzureML package. It weems works fine <em>unless<\/em> there is a need for external libraries.<\/p>\n\n<p>Consider the following code (the function <em>fun<\/em> usually does quite a bit more):<\/p>\n\n<pre><code>fun&lt;- function (b5) {\n    res &lt;- require(rmarkdown)\n    res\n}\n\ntest &lt;- as.data.frame(\n    cbind(\n        c(0.0,  0.3,  0.0,  0.0,  0.0),\n        c(0.0,  0.0,  0.0, -0.4,  0.0),\n        c(0,      0,    0,    0,    0))\n)\n\n\napi &lt;- publishWebService (\n  ws,\n  fun = fun,\n  name = \"Talection-fun\",\n  inputSchema = test,\n  packages = c(\"talection\",\"psych\",\"jsonlite\",\"rmarkdown\",\"knitr\")\n)\n<\/code><\/pre>\n\n<p>The service returns FALSE<\/p>\n\n<pre><code>Created new folder: \/var\/folders\/zf\/587__ss15z7_tq240vtpb68c0000gn\/T\/\/Rtmpyu2qRC\/dir138e46cbc778f\/packages\/bin\/windows\/contrib\/3.1\nRequest failed with status 401. Waiting 9.7 seconds before retry\n..........    ans\n1 FALSE\nSourced file '\/Users\/roffe\/Documents\/talections\/code\/Web Services\/WebServices.R'\n<\/code><\/pre>\n\n<p>It seems that <em>knitr<\/em>, <em>psych<\/em> and <em>jsonlite<\/em> work OK, whereas <em>rmarkdown<\/em> and <em>talection<\/em> (all of which are binary packages in a miniCRAN repository) apparently are located and uploaded, but not installed. Because there's an error message if I remove them from the miniCRAN repository.<\/p>\n\n<p>Is there a way to trace what happens to the libraries? Or anything else I can do to make this work?<\/p>\n\n<p>All suggestions and comments appreciated.<\/p>\n\n<p>Thank you,<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1463066071353,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"upload librari packag tri upload extern librari rmarkdown talect psych jsonlit knitr knitr psych jsonlit instal rmarkdown talect binari packag minicran repositori servic return trace librari",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37191043",
        "Challenge_link_count":0,
        "Challenge_original_content":"upload librari bit packag weem extern librari function fun bit fun function re rmarkdown re test data frame cbind api publishwebservic fun fun talect fun inputschema test packag talect psych jsonlit rmarkdown knitr servic return creat folder var folder ssz tqvtpbcgn rtmpyuqrc direcbcf packag bin window contrib request statu wait retri an sourc file roff document talect web servic webservic knitr psych jsonlit rmarkdown talect binari packag minicran repositori appar locat upload instal messag remov minicran repositori trace librari comment",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"upload librari bit packag weem extern librari servic return knitr psych jsonlit rmarkdown talect appar locat upload instal messag remov minicran repositori trace librari comment",
        "Challenge_readability":9.0,
        "Challenge_reading_time":19.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Uploading libraries to Azure using AzureML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":134.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":206181928647
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Training a simple convolutional network to recognize MNIST digits on Microsoft Azure (in Machine Learning Studio) takes many many times longer than it does for (already very slow) learning of exactly the same model locally, on a CPU (MacBook Pro, with limited memory) with TensorFlow.<\/p>\n\n<p>Is there a way \u2014 perhaps purchasing resources or connecting virtual GPUs \u2014 to improve performance of Azure Machine Learning?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1463083181150,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"slow perform studio compar local macbook pro tensorflow improv perform purchas resourc connect virtual gpu",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37196368",
        "Challenge_link_count":0,
        "Challenge_original_content":"improv perform train convolut network recogn mnist digit studio time longer slow exactli model local cpu macbook pro limit memori tensorflow purchas resourc connect virtual gpu improv perform",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"improv perform train convolut network recogn mnist digit time longer exactli model local cpu tensorflow purchas resourc connect virtual gpu improv perform",
        "Challenge_readability":13.6,
        "Challenge_reading_time":5.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I improve Azure ML learning performance?",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1048.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":206164818850
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have already set up an <strong>Azure SQL Database<\/strong> and loaded results into it form my local machine via <strong>R (RODBC)<\/strong> successfully.  I can do queries in R Studio with no problem. <\/p>\n\n<p>However when I use the same code in <strong>Execute R script<\/strong> module in the  ML studio, I get an error that the connection is not open. <\/p>\n\n<p>What do I need to change? Have tried different strings for the driver with no avail.   <\/p>\n\n<p><em>The reason Reader or Import Data module is not working for my case is that I am creating an API that provides me with the information to query the database before doing analytics. The database is very big and I do not want to load whole table and then use project columns, etc.<\/em><\/p>\n\n<p>Any help is really appreciated<\/p>\n\n<p>Thanks all<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1463678195617,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"connect sql databas execut modul studio successfulli load databas rodbc local studio tri driver creat api queri databas analyt databas big load tabl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37329999",
        "Challenge_link_count":0,
        "Challenge_original_content":"connect sql databas execut modul studio set sql databas load local rodbc successfulli queri studio execut modul studio connect open tri driver reason reader import data modul creat api queri databas analyt databas big load tabl column",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"connect sql databas execut modul studio set sql databas load local successfulli queri studio execut modul studio connect open tri driver reason reader import data modul creat api queri databas analyt databas big load tabl column",
        "Challenge_readability":7.5,
        "Challenge_reading_time":11.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Connecting to Azure SQL database from \"Execute R Script\" module in \"Azure Machine Learning Studio\"",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1136.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":205569804383
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried integrating Azure ML API with PHP but unfortunately getting an error in response.<\/p>\n\n<p>Updated: I have used request response API sending through json response<\/p>\n\n<p>Below is the response obtained on executing PHP script:<\/p>\n\n<pre><code>array(1) { [\"error\"]=&gt; array(3) { [\"code\"]=&gt; string(11) \"BadArgument\" \n    [\"message\"]=&gt; string(26) \"Invalid argument provided.\" [\"details\"]=&gt; array(1)\n    {[0]=&gt; array(2) { [\"code\"]=&gt; string(18) \"RequestBodyInvalid\" [\"message\"]=&gt;\n    string(68) \"No request body provided or error in deserializing the request\n    body.\" } } } }\n<\/code><\/pre>\n\n<p>PHP Script:<\/p>\n\n<pre><code>$url = 'URL';\n$api_key = 'API';\n$data = array(\n    'Inputs'=&gt; array(\n        'My Experiment Name'=&gt; array(\n            \"ColumnNames\" =&gt; [['Column1'],\n                              ['Column2'],\n                              ['Column3'],\n                              ['Column4'],\n                              ['Column5'],\n                              ['Column6'],\n                              ['Column7']],\n            \"Values\" =&gt; [ ['Value1'],\n                          ['Value2'],\n                          ['Value3'],\n                          ['Value4'],\n                          ['Value5'],\n                          ['Value6'],\n                          ['Value7']]\n            ),\n        ),\n        'GlobalParameters' =&gt; new StdClass(),\n    );\n\n$body = json_encode($data);\n\n$ch = curl_init();\ncurl_setopt($ch, CURLOPT_URL, $url);\ncurl_setopt($ch, CURLOPT_HTTPHEADER, array('Content-Type: application\/json', 'Authorization: Bearer '.$api_key, 'Accept: application\/json'));\ncurl_setopt($ch, CURLOPT_POST, 1);\ncurl_setopt($ch, CURLOPT_POSTFIELDS, $body);\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, true);\ncurl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);\n\n$response  = json_decode(curl_exec($ch), true);\n\/\/echo 'Curl error: ' . curl_error($ch);\ncurl_close($ch);\n\nvar_dump ($response);\n<\/code><\/pre>\n\n<p>I have followed few examples, still unable to crack it. Please let me know the solution for this.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1464591071917,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"integr api php receiv respons execut php messag request bodi deseri request bodi tri",
        "Challenge_last_edit_time":1464872708576,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37519307",
        "Challenge_link_count":0,
        "Challenge_original_content":"integr api php tri integr api php unfortun respons updat request respons api send json respons respons execut php arrai arrai badargu messag argument arrai arrai requestbodyinvalid messag request bodi deseri request bodi php url url api kei api data arrai input arrai arrai columnnam column column column column column column column valu valu valu valu valu valu valu valu globalparamet stdclass bodi json encod data curl init curl setopt curlopt url url curl setopt curlopt httpheader arrai type json author bearer api kei accept json curl setopt curlopt curl setopt curlopt postfield bodi curl setopt curlopt returntransf curl setopt curlopt ssl verifyp respons json decod curl exec echo curl curl curl close var dump respons crack",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"integr api php tri integr api php unfortun respons updat request respons api send json respons respons execut php php crack",
        "Challenge_readability":16.2,
        "Challenge_reading_time":21.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to integrate Azure ML API with PHP",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":463.0,
        "Challenge_word_count":167,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":204656928083
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I'm trying to import <code>rmarkdown<\/code> into <code>AzureML<\/code> for one of my projects.<\/p>\n\n<p>This is the function I'm trying to upload into <code>AzureML<\/code>. <\/p>\n\n<p>The <code>R.version<\/code> check is because the function is evaluated in the local environment before uploaded to <code>AzureML<\/code>. <\/p>\n\n<pre><code>fun &lt;- function (b5) {\n    if (R.version[[\"os\"]]==\"mingw32\" &amp;&amp; ! require(talection)) {\n        install.packages(\n            \"src\/rmarkdown_0.9.6.zip\",\n            lib=\".\",\n            type=\"win.binary\",\n            repos=NULL,\n            verbose=TRUE)\n    }\n    ans &lt;- as.data.frame(c(\"Finished\"))\n}\n<\/code><\/pre>\n\n<p><code>rmarkdown_0.9.6.zip<\/code> is in a <code>miniCRAN<\/code> library. <\/p>\n\n<p>The following code, is the code that uploads <code>rmarkwodn<\/code> to <code>Azure ML<\/code>. Please note the line <code>packages<\/code>, which tells R to upload <code>rmarkdown<\/code> to <code>Azure ML<\/code>. <\/p>\n\n<pre><code>test &lt;- as.data.frame(\n    cbind(\n        c(0.0,  0.3,  0.0,  0.0,  0.0),\n        c(0.0,  0.0,  0.0, -0.4,  0.0),\n        c(0,      0,    0,    0,    0))\n)\n\nep &lt;- publishWebService (\n  ws,\n  fun = fun,\n  name = \"Talection-fun\",\n  inputSchema = test,\n  outputSchema = list(\n    ans = \"character\"\n  ),\n  packages = c(\"rmarkdown\")\n)\n\nprint(consume(ep,test))\n<\/code><\/pre>\n\n<p>The code returns <\/p>\n\n<blockquote>\n  <p>Request failed with status 400. Waiting 12.7 seconds before retry<br>\n  Request failed with status 400. Waiting 33.6 seconds before retry<br>\n  Request failed with status 400. Waiting 76.7 seconds before retry<br>\n  Request failed with status 400. Waiting 234.3 seconds before retry<br>\n  Request failed with status 400. Waiting 123.1 seconds before retry<br>\n  Error: AzureML returns error code:<br>\n  HTTP status code : 400<br>\n  AzureML error code  : LibraryExecutionError  <\/p>\n<\/blockquote>\n\n<p>Any and all relevant suggestions welcome. Thank you.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1464707847620,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"import rmarkdown librari function upload librari packag line return messag statu libraryexecutionerror",
        "Challenge_last_edit_time":1478175571603,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/37549591",
        "Challenge_link_count":0,
        "Challenge_original_content":"import librari rmarkdown packag import rmarkdown function upload version function evalu local environ upload fun function version mingw talect instal packag src rmarkdown zip lib type win binari repo null verbos an data frame finish rmarkdown zip minicran librari upload rmarkwodn note line packag upload rmarkdown test data frame cbind publishwebservic fun fun talect fun inputschema test outputschema list an charact packag rmarkdown print consum test return request statu wait retri request statu wait retri request statu wait retri request statu wait retri request statu wait retri return http statu libraryexecutionerror relev welcom",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"import librari rmarkdown packag import function upload function evalu local environ upload librari upload note line upload return request statu wait retri request statu wait retri request statu wait retri request statu wait retri request statu wait retri return http statu libraryexecutionerror relev welcom",
        "Challenge_readability":8.9,
        "Challenge_reading_time":23.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to import a library \u201crmarkdown\u201d using the AzureML package?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":147.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":204540152380
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an Azure ML webservice as an example and face an unknown error when it comes to deploy a web service. The error comes without an explanation, so it's hard to trace. <\/p>\n\n<p>When running the experiment within the studio, the experiment was running without any issue. However, when deploy to webservice, the test function has failed with the same input as in the studio.<\/p>\n\n<p>I have also published a sample of the service to see if anyone can see what the issue is.<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/mywebservice-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/mywebservice-1<\/a><\/p>\n\n<p>Some info about the service:<\/p>\n\n<p>The service takes input as a string represented for a sparse feature vector of svmlight format. It will return the predicted class for the input feature vector. The error fails when running the test function from the deployed service while the experiment within the studio is running without any issue.<\/p>\n\n<p>Hope anyone has an idea how it went wrong.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1467335799167,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"unknown deploi webservic trace run studio test function deploi input webservic input repres spars featur vector svmlight format return predict class input featur vector publish sampl servic identifi",
        "Challenge_last_edit_time":1467350970220,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38135604",
        "Challenge_link_count":2,
        "Challenge_original_content":"web servic input data creat webservic unknown come deploi web servic come explan trace run studio run deploi webservic test function input studio publish sampl servic http galleri cortanaintellig com mywebservic servic servic input repres spars featur vector svmlight format return predict class input featur vector run test function deploi servic studio run hope idea went",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic input data creat webservic unknown come deploi web servic come explan trace run studio run deploi webservic test function input studio publish sampl servic servic servic input repres spars featur vector svmlight format return predict class input featur vector run test function deploi servic studio run hope idea went",
        "Challenge_readability":10.1,
        "Challenge_reading_time":13.87,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Web service Input Data Issue",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":738.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":201912200833
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have designed a little experiment on AzureML.\nSuppose my dataset has <em>column1<\/em> (A, B, C are the \"unique\" elements) and <em>column2<\/em> (D, E, F are the unique elements). <\/p>\n\n<p>In the way shown in picture I am able to take as input to the \"Execute R Script\" module the whole dataset with both the columns (port 1) and make a filtering with the parameters passed in the port 2. In this way, when I create the Web Service and I have to insert the values for the <em>column1<\/em> it automatically creates a picklist with the values \"A\", \"B\" and \"C\". So the user cannot choose a value different from them, and I like it!<\/p>\n\n<p>Now, I want another obvious choice too: I want the user having the possibility to choose \"no filtering\" as fourth option (such that no filtering is done on the column1). There exists an easy way to force Azure to give this possibility?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CJ2T1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CJ2T1.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1467795302130,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"design execut modul filter data base paramet pass port option filter fourth option column",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38219932",
        "Challenge_link_count":2,
        "Challenge_original_content":"web servic filter choic design littl suppos dataset column uniqu element column uniqu element shown pictur input execut modul dataset column port filter paramet pass port creat web servic insert valu column automat creat picklist valu choos valu obviou choic choos filter fourth option filter column forc",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic filter choic design littl suppos dataset column column shown pictur input execut modul dataset column filter paramet pass port creat web servic insert valu column automat creat picklist valu choos valu obviou choic choos filter fourth option forc",
        "Challenge_readability":8.2,
        "Challenge_reading_time":13.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure, Web Service: \"no filtering\" choice",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":49.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":201452697870
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I develop an experiment on Azure ML I have the chance to insert the \"Execute R Script\" module. When I have runned it, I can explore the outputs produced by the module itself.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/RzLGZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RzLGZ.png\" alt=\"The output window\"><\/a><\/p>\n\n<p>My problem is that I have two modules: I do a filtering on a dataset in the first and use the resulting dataset in the second.\nThen I create a web service with it.\nProblem: when the filtering gives a null dataset this possibly create problems in the functions on the second module.<\/p>\n\n<p>I want to find a way to \"write\" in the \"Standard Error\" space. I have tried to use:<\/p>\n\n<pre><code>if (length(dataset$column1)==0) {warning(\"Empty filtering!!!!\")}\n<\/code><\/pre>\n\n<p>but it does not work. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1467796520573,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"execut modul modul filter dataset dataset filter null dataset creat function modul write standard space address",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/38220335",
        "Challenge_link_count":2,
        "Challenge_original_content":"standard chanc insert execut modul run explor output produc modul modul filter dataset dataset creat web servic filter null dataset creat function modul write standard space tri length dataset column warn filter",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"standard chanc insert execut modul run explor output produc modul modul filter dataset dataset creat web servic filter null dataset creat function modul write standard space tri",
        "Challenge_readability":7.9,
        "Challenge_reading_time":11.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure, R: showing the Standard Error",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":129,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":201451479427
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I can run the following code in a Jupyter notebook (Python 3.5) on my PC using Anaconda, and it works fine. But when I run the same code in an Azure ML notebook, I get the plot, but also the error message described below. Does anyone know how to use Bokeh in Azure ML notebooks ? Is there perhaps a way to import the seemingly missing module 'ipykernel'<\/p>\n\n<pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.sampledata.iris import flowers\n\ncolormap = {'setosa': 'red', 'versicolor': 'green', 'virginica': 'blue'}\ncolors = [colormap[x] for x in flowers['species']]\n\np = figure(title = \"Iris Morphology\")\np.xaxis.axis_label = 'Petal Length'\np.yaxis.axis_label = 'Petal Width'\n\np.circle(flowers[\"petal_length\"], flowers[\"petal_width\"],\n     color=colors, fill_alpha=0.2, size=10)\n\noutput_notebook()\nshow(p)\n<\/code><\/pre>\n\n<p>produces the plot, but also the following errors<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-17-c50d1a94007e&gt; in &lt;module&gt;()\n 13 \n 14 output_notebook()\n---&gt; 15 show(p)\n\n\/home\/nbuser\/env3\/lib\/python3.4\/site-packages\/bokeh\/io.py in show(obj,        browser, new)\n    299 \n    300     '''\n--&gt; 301     return _show_with_state(obj, _state, browser, new)\n    302 \n    303 def _show_with_state(obj, state, browser, new):\n\n\/home\/nbuser\/env3\/lib\/python3.4\/site-packages\/bokeh\/io.py in     _show_with_state(obj, state, browser, new)\n    307 \n    308     if state.notebook:\n--&gt; 309         comms_handle = _show_notebook_with_state(obj, state)\n    310 \n    311     elif state.server_enabled:\n\n\/home\/nbuser\/env3\/lib\/python3.4\/site-packages\/bokeh\/io.py in     _show_notebook_with_state(obj, state)\n    329         comms_target = make_id()\n    330         publish_display_data({'text\/html': notebook_div(obj,   comms_target)})\n--&gt; 331         handle = _CommsHandle(get_comms(comms_target), state.document,     state.document.to_json())\n    332         state.last_comms_handle = handle\n    333         return handle\n\n\/home\/nbuser\/env3\/lib\/python3.4\/site-packages\/bokeh\/util\/notebook.py in   get_comms(target_name)\n    109 \n    110     '''\n--&gt; 111     from ipykernel.comm import Comm \n    112     return Comm(target_name=target_name, data={})\n    113 \n\nImportError: No module named 'ipykernel'\n\nIn [16]:\n<\/code><\/pre>\n\n<p>\u200b<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1472380764853,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"bokeh notebook jupyt notebook anaconda produc messag run notebook messag ipykernel modul miss",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39190415",
        "Challenge_link_count":0,
        "Challenge_original_content":"bokeh notebook run jupyt notebook anaconda run notebook plot messag bokeh notebook import seemingli miss modul ipykernel bokeh plot import figur output notebook bokeh sampledata iri import flower colormap setosa red versicolor green virginica blue color colormap flower speci figur titl iri morpholog xaxi axi label petal length yaxi axi label petal width circl flower petal length flower petal width color color alpha size output notebook produc plot importerror traceback output notebook home nbuser env lib site packag bokeh obj browser return state obj state browser state obj state browser home nbuser env lib site packag bokeh state obj state browser state notebook comm notebook state obj state elif state server enabl home nbuser env lib site packag bokeh notebook state obj state comm target publish displai data text html notebook div obj comm target commshandl comm comm target state document state document json state comm return home nbuser env lib site packag bokeh util notebook comm target ipykernel comm import comm return comm target target data importerror modul ipykernel",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"bokeh notebook run jupyt notebook anaconda run notebook plot messag bokeh notebook import seemingli miss modul ipykernel produc plot",
        "Challenge_readability":11.3,
        "Challenge_reading_time":29.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I use Bokeh in an Azure ML notebook",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":575.0,
        "Challenge_word_count":229,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":196867235147
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to install qdap package in Azure ML. Rest of the dependent packages get installed without any issues. When it comes to qdapTools, I get this error , though the version that I try to install is 1.3.1 ( Verified this from the Decription file that comes with the R package)<\/p>\n\n<pre><code>package 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap\n<\/code><\/pre>\n\n<p>The code in \"Execute R Script\" :<\/p>\n\n<pre><code>install.packages(\"src\/qdapTools.zip\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapDictionaries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapRegex.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdap.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(stringr, lib.loc=\".\", verbose=TRUE)\nlibrary(qdap, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>And the log : <\/p>\n\n<pre><code>[ModuleOutput] End R Execution: 9\/22\/2016 6:44:44 AM\n[Stop]     DllModuleMethod::Execute. Duration = 00:00:16.7828106\n[Critical]     Error: Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\n\n\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\n----------- End of error message from R -----------\n[Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":2,\"Columns\":1,\"estimatedSize\":11767808,\"ColumnTypes\":{\"System.String\":1},\"IsComplete\":true,\"Statistics\":{\"0\":[2,0]}}],\"Generic\":{\"bundlePath\":\"..\\\\..\\\\Script Bundle\\\\Script Bundle.zip\",\"rLibVersion\":\"R310\"},\"Unknown\":[\"Key: rStreamReader, ValueType : System.IO.StreamReader\"]},\"OutputParameters\":[],\"ModuleType\":\"LanguageWorker\",\"ModuleVersion\":\" Version=6.0.0.0\",\"AdditionalModuleInfo\":\"LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR\",\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 0063: The following error occurred during evaluation of R script:\\r\\n---------- Start of error message from R ----------\\r\\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\\r\\n\\r\\n\\r\\npackage 'qdapTools' 1.1.0 was found, but &gt;= 1.3.1 is required by 'qdap'\\r\\n----------- End of error message from R -----------\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.ExecuteR(NewRWorker worker, DataTable dataset1, DataTable dataset2, IEnumerable`1 bundlePath, StreamReader rStreamReader, Nullable`1 seed) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 287\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS._RunImpl(NewRWorker worker, DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptExternalResource source, String url, ExecuteRScriptGitHubRepositoryType githubRepoType, SecureString accountToken) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 207\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.RunRSNR(DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptRVersion rLibVersion) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\REntryPoint.cs:line 105\",\"Warnings\":[],\"Duration\":\"00:00:16.7752607\"}\nModule finished after a runtime of 00:00:17.1411124 with exit code -2\nModule failed due to negative exit code of -2\n\nRecord Ends at UTC 09\/22\/2016 06:44:44.\n<\/code><\/pre>\n\n<p>Editing the code to : <\/p>\n\n<pre><code>install.packages(\"src\/qdapTools.zip\",lib=\".\" , repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapDictionaries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdapRegex.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/qdap.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(qdapTools, lib.loc=\".\", verbose=TRUE)\nlibrary(qdap, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>throws the following error :- <\/p>\n\n<pre><code>[ModuleOutput] 4: package 'qdapTools' was built under R version 3.3.1 \n[ModuleOutput] \n[ModuleOutput] End R Execution: 9\/22\/2016 7:11:05 AM\n[Stop]     DllModuleMethod::Execute. Duration = 00:00:17.0656414\n[Critical]     Error: Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage or namespace load failed for 'qdapTools'\n\n\npackage or namespace load failed for 'qdapTools'\n----------- End of error message from R -----------\n<\/code><\/pre>\n\n<p>Not sure how to proceed, can someone help please.<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":8,
        "Challenge_created_time":1473835293153,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal qdap packag depend packag instal qdaptool packag version detect tri instal version qdaptool throw proce",
        "Challenge_last_edit_time":1474530017472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39483984",
        "Challenge_link_count":0,
        "Challenge_original_content":"packag qdaptool version detect instal qdap packag rest depend packag instal come qdaptool version instal verifi decript file come packag packag qdaptool qdap execut instal packag src qdaptool zip repo null verbos instal packag src magrittr zip lib repo null verbos instal packag src stringi zip lib repo null verbos instal packag src stringr zip lib repo null verbos instal packag src qdapdictionari zip lib repo null verbos instal packag src qdapregex zip lib repo null verbos instal packag src rcolorbrew zip lib repo null verbos instal packag src qdap zip lib repo null verbos librari stringr lib loc verbos librari qdap lib loc verbos log moduleoutput end execut stop dllmodulemethod execut durat critic evalu start messag packag qdaptool qdap packag qdaptool qdap end messag critic inputparamet datat row column estimateds columntyp iscomplet statist gener bundlepath bundl bundl zip rlibvers unknown kei rstreamread valuetyp streamread outputparamet moduletyp languagework modulevers version additionalmoduleinfo languagework version cultur neutral publickeytoken cefca metaanalyt languagework languageworkerclientr runrsnr analyt except errormap moduleexcept evalu start messag npackag qdaptool qdap npackag qdaptool qdap end messag metaanalyt languagework languageworkerclientr execut newrwork worker datat dataset datat dataset ienumer bundlepath streamread rstreamread nullabl seed bld sourc sourc modul languagework languagework dll entrypoint rmodul line metaanalyt languagework languageworkerclientr runimpl newrwork worker datat dataset datat dataset bundlepath streamread rstreamread nullabl seed executerscriptexternalresourc sourc url executerscriptgithubrepositorytyp githubrepotyp securestr accounttoken bld sourc sourc modul languagework languagework dll entrypoint rmodul line metaanalyt languagework languageworkerclientr runrsnr datat dataset datat dataset bundlepath streamread rstreamread nullabl seed executerscriptrvers rlibvers bld sourc sourc modul languagework languagework dll entrypoint rentrypoint line warn durat modul finish runtim exit modul neg exit record end utc edit instal packag src qdaptool zip lib repo null verbos instal packag src qdapdictionari zip lib repo null verbos instal packag src qdapregex zip lib repo null verbos instal packag src rcolorbrew zip lib repo null verbos instal packag src qdap zip lib repo null verbos librari qdaptool lib loc verbos librari qdap lib loc verbos throw moduleoutput packag qdaptool built version moduleoutput moduleoutput end execut stop dllmodulemethod execut durat critic evalu start messag packag namespac load qdaptool packag namespac load qdaptool end messag proce",
        "Challenge_participation_count":9,
        "Challenge_preprocessed_content":"packag version detect instal qdap packag rest depend packag instal come qdaptool version instal execut log edit throw proce",
        "Challenge_readability":15.3,
        "Challenge_reading_time":69.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":10.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":null,
        "Challenge_title":"R package ( qdapTools) version not getting detected correctly in Azure ML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":443.0,
        "Challenge_word_count":457,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":195412706847
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I am trying to use the <a href=\"https:\/\/CRAN.R-project.org\/package=unbalanced\" rel=\"nofollow noreferrer\">unbalanced package in R<\/a> in an azure machine learning classification experiment.<\/p>\n\n<p>The problem I'm running into is getting the package to be imported in R, specifically getting the mlr package dependency to be installed too.<\/p>\n\n<p>I use the following R script to import the unbalanced package:<\/p>\n\n<pre><code>install.packages(\"src\/colorspace_1.2-6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer_1.1-2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/dichromat_2.0-0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/munsell_0.4.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/labeling_0.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tibble_1.2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/DBI_0.5-1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/BH_1.60.0-2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/digest_0.6.10.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/gtable_0.2.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/scales_0.4.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/backports_1.0.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/chron_2.3-47.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/assertthat_0.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/jsonlite_1.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr_1.5.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/dplyr_0.5.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lazyeval_0.2.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/htmltools_0.3.5.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/Rcpp_0.12.7.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr_1.1.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/httpuv_1.3.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/mime_0.5.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xtable_1.8-2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/R6_2.1.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/sourcetools_0.1.5.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/BBmisc_1.10.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2_2.1.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ParamHelpers_1.9.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi_1.1.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/checkmate_1.8.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/data.table_1.9.6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggvis_0.4.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/parallelMap_1.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/plyr_1.8.4.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/reshape2_1.4.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/shiny_0.14.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/iterators_1.0.8.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/mlr_2.9.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/foreach_1.4.3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/doParallel_1.0.10.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/FNN_1.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RANN_2.5.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/unbalanced_2.0.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(unbalanced, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>I have a zip archive of those zipped packages connected to the Execute R script module in Azure ML like this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9mvAQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9mvAQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Also, the archive contains the mlr package and its dependencies, as the archive was created on the archived packages in<\/p>\n\n<pre><code>C:\\Users\\my_user\\AppData\\Local\\Temp\\RtmpU3WpI6\\downloaded_packages\n<\/code><\/pre>\n\n<p>I am using CRAN R 3.1.0 R version in Azure ML.<\/p>\n\n<p>I get the following error:<\/p>\n\n<pre><code>Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\npackage 'mlr' could not be loaded\n<\/code><\/pre>\n\n<p>Changing to Microsoft R Open 3.2.2 doesn't work at all, too, as the error code says that the mlr package cannot be found.<\/p>\n\n<p>The order I install those packages is the same error in which they are installed in R Studio whenever I perform a fresh<\/p>\n\n<pre><code>install.packages(\"unbalanced\")\n<\/code><\/pre>\n\n<p>I really need to use this package for my classification. Any ideas about solving this error?<\/p>\n\n<p>Thank you!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1474015937750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"import mlr packag depend classif tri instal packag depend zip archiv receiv messag state mlr packag load cran version open idea mlr packag classif",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39527347",
        "Challenge_link_count":3,
        "Challenge_original_content":"packag mlr load unbalanc packag classif run packag import mlr packag depend instal import unbalanc packag instal packag src colorspac zip lib repo null verbos instal packag src rcolorbrew zip lib repo null verbos instal packag src dichromat zip lib repo null verbos instal packag src munsel zip lib repo null verbos instal packag src label zip lib repo null verbos instal packag src tibbl zip lib repo null verbos instal packag src dbi zip lib repo null verbos instal packag src zip lib repo null verbos instal packag src digest zip lib repo null verbos instal packag src gtabl zip lib repo null verbos instal packag src scale zip lib repo null verbos instal packag src backport zip lib repo null verbos instal packag src chron zip lib repo null verbos instal packag src assertthat zip lib repo null verbos instal packag src jsonlit zip lib repo null verbos instal packag src magrittr zip lib repo null verbos instal packag src dplyr zip lib repo null verbos instal packag src lazyev zip lib repo null verbos instal packag src htmltool zip lib repo null verbos instal packag src rcpp zip lib repo null verbos instal packag src stringr zip lib repo null verbos instal packag src httpuv zip lib repo null verbos instal packag src mime zip lib repo null verbos instal packag src xtabl zip lib repo null verbos instal packag src zip lib repo null verbos instal packag src sourcetool zip lib repo null verbos instal packag src bbmisc zip lib repo null verbos instal packag src ggplot zip lib repo null verbos instal packag src paramhelp zip lib repo null verbos instal packag src stringi zip lib repo null verbos instal packag src checkmat zip lib repo null verbos instal packag src data tabl zip lib repo null verbos instal packag src ggvi zip lib repo null verbos instal packag src parallelmap zip lib repo null verbos instal packag src plyr zip lib repo null verbos instal packag src reshap zip lib repo null verbos instal packag src shini zip lib repo null verbos instal packag src iter zip lib repo null verbos instal packag src mlr zip lib repo null verbos instal packag src foreach zip lib repo null verbos instal packag src doparallel zip lib repo null verbos instal packag src fnn zip lib repo null verbos instal packag src rann zip lib repo null verbos instal packag src unbalanc zip lib repo null verbos librari unbalanc lib loc verbos zip archiv zip packag connect execut modul archiv mlr packag depend archiv creat archiv packag appdata local temp rtmpuwpi download packag cran version evalu start messag packag mlr load open sai mlr packag order instal packag instal studio perform fresh instal packag unbalanc packag classif idea",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"packag mlr load unbalanc packag classif run packag import mlr packag depend instal import unbalanc packag zip archiv zip packag connect execut modul archiv mlr packag depend archiv creat archiv packag cran version open sai mlr packag order instal packag instal studio perform fresh packag classif idea",
        "Challenge_readability":12.4,
        "Challenge_reading_time":69.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":59,
        "Challenge_solved_time":null,
        "Challenge_title":"package 'mlr' could not be loaded in azure machine learning",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":314.0,
        "Challenge_word_count":490,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":195232062250
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm going crazy!\nI'm using Azure Machine Learning and R Script. I deploy it as Web Service. I use sample code based on HttpClient.<\/p>\n\n<pre><code>            using (var client = new HttpClient())\n        {\n            var scoreRequest = new\n            {\n                Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"input1\",\n                        new StringTable()\n                        {\n                            ColumnNames = new string[] {\n                                    \"experts_estimates\",\n                                    \"experts_share_of_unique_information\",\n                                    \"avg_correlation\",\n                                    \"point_a\",\n                                    \"point_b\",\n                                    \"is_export_mode\"\n                            },\n                            Values = new string[,] {\n                                {\n                                    expertsEstimatesStr,\n                                    expertsShareOfUniqueInformationStr,\n                                    avgCorrelationStr,\n                                    pointAStr,\n                                    pointBStr,\n                                    isExportModeStr\n                                },\n                            }\n                        }\n                    },\n                },\n                GlobalParameters = new Dictionary&lt;string, string&gt;()\n                {\n                }\n            };\n\n            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n            client.BaseAddress = new Uri(apiUrl);\n\n            \/\/ WARNING: The 'await' statement below can result in a deadlock\n            \/\/ if you are calling this code from the UI thread of an ASP.Net application.\n            \/\/ One way to address this would be to call ConfigureAwait(false)\n            \/\/ so that the execution does not attempt to resume on the original context.\n            \/\/ For instance, replace code such as:\n            \/\/      result = await DoSomeTask()\n            \/\/ with the following:\n            \/\/      result = await DoSomeTask().ConfigureAwait(false)\n\n            HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n\n            if (response.IsSuccessStatusCode)\n            {\n                string result = await response.Content.ReadAsStringAsync();\n                return result;\n            }\n            else\n            {\n                \/\/ Print the headers - they include the requert ID and the timestamp,\n                \/\/ which are useful for debugging the failure\n                var headers = response.Headers.ToString();\n                string responseContent = await response.Content.ReadAsStringAsync();\n                throw new Exception(responseContent, new Exception(headers));\n            }\n        }\n<\/code><\/pre>\n\n<p>and when I run code from Visual Studio I get:\n<a href=\"https:\/\/i.stack.imgur.com\/KHHpN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KHHpN.png\" alt=\"enter image description here\"><\/a>\nbut when I run code from Azure App Service I get:\n<a href=\"https:\/\/i.stack.imgur.com\/oYIzx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oYIzx.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Any ideas?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1474982413217,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi web servic run successfulli visual studio run app servic",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39725764",
        "Challenge_link_count":4,
        "Challenge_original_content":"columnnam crazi deploi web servic sampl base httpclient var client httpclient var scorerequest input dictionari input stringtabl columnnam expert estim expert share uniqu avg correl export mode valu expertsestimatesstr expertsshareofuniqueinformationstr avgcorrelationstr pointastr pointbstr isexportmodestr globalparamet dictionari client defaultrequesthead author authenticationheadervalu bearer apikei client baseaddress uri apiurl warn await statement deadlock call thread asp net address configureawait execut resum origin context instanc replac await dosometask await dosometask configureawait httpresponsemessag respons await client postasjsonasync scorerequest respons issuccetatuscod await respons readasstringasync return print header requert timestamp debug var header respons header tostr responsecont await respons readasstringasync throw except responsecont except header run visual studio run app servic idea",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"columnnam crazi deploi web servic sampl base httpclient run visual studio run app servic idea",
        "Challenge_readability":12.4,
        "Challenge_reading_time":28.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Result of the \"R Script\" without ColumnNames",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":63.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":194265586783
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In azure ml if we select a train algorithm(for ex \"Two Class Logistic Regression\") we can then have a set of parameters to do a parameter sweep while training.But how can I know how they change values of parameters in the training? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1475120461813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"paramet sweep valu paramet train",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/39761025",
        "Challenge_link_count":0,
        "Challenge_original_content":"paramet sweep select train algorithm class logist regress set paramet paramet sweep train valu paramet train",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"paramet sweep select train algorithm set paramet paramet sweep valu paramet train",
        "Challenge_readability":9.2,
        "Challenge_reading_time":3.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"I don't understand how parameter sweep is done in the Azure Machine Learning?",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":697.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":194127538187
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to increase the processing power of the Azure ML? I've deployed a neural network on a huge dataset (8000+ retina images, and Azure is taking an impossible amount of time to run the programme. Is it possible to deploy the ML workspace from a Virtual Machine, so that I can leverage increased processing speeds? Help!!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1476428657303,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"process power workspac run neural network larg dataset retina imag increas process power deploi workspac virtual",
        "Challenge_last_edit_time":1510175957023,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40036956",
        "Challenge_link_count":0,
        "Challenge_original_content":"increas process power workspac increas process power deploi neural network huge dataset retina imag imposs time run programm deploi workspac virtual leverag increas process speed",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"increas process power workspac increas process power deploi neural network huge dataset retina imag imposs time run programm deploi workspac virtual leverag increas process speed",
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Increasing processing power of Azure Machine Learning workspace",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":706.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":192819342697
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using text analysis with Azure ML. So in my python script I want to create a bag of word model and then calculate TFIDF of each words. For that I am using gensim model, It's not working on Azure ML. So is there any options for me? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477453917853,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"gensim packag text analysi creat bag model calcul tfidf gensim option",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40253448",
        "Challenge_link_count":0,
        "Challenge_original_content":"gensim packag text analysi creat bag model calcul tfidf gensim model option",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"gensim packag text analysi creat bag model calcul tfidf gensim model option",
        "Challenge_readability":3.1,
        "Challenge_reading_time":3.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How Can I use gensim package in Azure ML?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1024.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":191794082147
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am just trying out training a simple logistic regression model using SGD in python in Azure ML, but when I run the code it keep getting an error. What is more confusing is that the error shows up in Epoch 8  only and not in any of the epochs. I ll appreciate if anyone and can let me know why I would get an error like this and how to avoid it. I have included the code and error below. <\/p>\n\n<pre><code>from sklearn.linear_model import SGDClassifier\n    #Import data\n    cadd_dir = '.\\\\Script Bundle\\\\theano\\\\data\\\\'\n    ClinVar_ESP_dir = '.\\\\Script Bundle\\\\theano\\\\data\\\\'\n    #load data    \n    X_tr = numpy.load(os.path.join(cadd_dir, 'training.X.npz'))\n    X_tr = scipy.sparse.csr_matrix((X_tr['data'], X_tr['indices'], X_tr['indptr']), shape=X_tr['shape'])\n    y_tr = numpy.load(os.path.join(cadd_dir, 'training.y.npy'))\n    #Train model\n    print('Train SGD Logistic Regression')\n    alpha = 1e-2\n    clf = SGDClassifier(loss=\"log\", penalty='l2', alpha=alpha, random_state=None, shuffle=False, n_iter=10, verbose=1, n_jobs=1)\n    clf.fit(X_tr, y_tr)\n\n\n\n\n#Error\n\"[Information]         -- Epoch 7\n[Information]         Norm: 0.40, NNZs: 641, Bias: 0.000623, T: 186214000, Avg. loss: 0.670200\n[Information]         Total training time: 43.97 seconds.\n\n[Information]         -- Epoch 8\n[Error]         Caught exception while executing function: Traceback (most recent call last):\n[Error]           File \"C:\\server\\invokepy.py\", line 211, in batch\n[Error]             xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\n[Error]           File \"C:\\server\\XDRReader\\xdrutils.py\", line 51, in DataFrameToRFile\n[Error]             attributes = XDRBridge.DataFrameToRObject(dataframe)\n[Error]           File \"C:\\server\\XDRReader\\xdrbridge.py\", line 40, in DataFrameToRObject\n[Error]             if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):\n[Error]         TypeError: object of type 'NoneType' has no len()\n[Information]         Norm: 0.40, NNZs: 641, Bias: 0.000623, T: 212816000, Avg. loss: 0.669797\n[Information]         Total training time: 50.21 seconds.\n\n[Information]         -- Epoch 9\n[Information]         Norm: 0.40, NNZs: 641, Bias: 0.000622, T: 239418000, Avg. loss: 0.669482\n[Information]         Total training time: 56.46 seconds.\"\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1477893065637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"train logist regress model sgd epoch epoch messag object type nonetyp len avoid",
        "Challenge_last_edit_time":1477921870856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40337201",
        "Challenge_link_count":0,
        "Challenge_original_content":"scikit sgdclassifi object type nonetyp len train logist regress model sgd run epoch epoch avoid sklearn linear model import sgdclassifi import data cadd dir bundl theano data clinvar esp dir bundl theano data load data numpi load path cadd dir train npz scipi spars csr matrix data indptr shape shape numpi load path cadd dir train npy train model print train sgd logist regress alpha clf sgdclassifi loss log penalti alpha alpha random state shuffl iter verbos job clf fit epoch norm nnz bia avg loss train time epoch caught except execut function traceback file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil file server xdrreader xdrutil line dataframetorfil attribut xdrbridg dataframetorobject datafram file server xdrreader xdrbridg line dataframetorobject len datafram type datafram datafram typeerror object type nonetyp len norm nnz bia avg loss train time epoch norm nnz bia avg loss train time",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"sgdclassifi object type nonetyp len train logist regress model sgd run epoch epoch avoid",
        "Challenge_readability":9.0,
        "Challenge_reading_time":27.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"scikit-learn SGDClassifier object of type 'NoneType' has no len()",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":305.0,
        "Challenge_word_count":255,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":191354934363
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am facing a issue while doing R Script in Azure MIL and error is\ni.e. \"Error: (list) object cannot be coerced to type double\"<\/p>\n\n<p>My code is<\/p>\n\n<pre><code>dataset1 &lt;-maml.mapInputPort(2)\ndataset3 &lt;-maml.mapInputPort(1)\nZ &lt;- as.numeric((dataset3),stringsAsFactors=TRUE)\nY &lt;- mdBinaryDesign(Z,4,dataset1)\nY.aggregate=mdBinaryToAggregateDesign(Y)\nsurvey.design=mdDesignNames(Y.aggregate, dataset1)\ndata.set &lt;- as.data.frame(survey.design)\nmaml.mapOutputPort(\"data.set\")\n<\/code><\/pre>\n\n<p>The issue in coming while assigning value to Z variable. dataset3 has simple numeric data i.e. \"5\" , which acts as a input to my model.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YwVDR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YwVDR.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1478082143523,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"mil messag list object coerc type doubl come assign valu variabl suppos numer input dataset convert dataset numer type numer function",
        "Challenge_last_edit_time":1478083135856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40377504",
        "Challenge_link_count":2,
        "Challenge_original_content":"list object coerc type doubl mil list object coerc type doubl dataset maml mapinputport dataset maml mapinputport numer dataset stringsasfactor mdbinarydesign dataset aggreg mdbinarytoaggregatedesign survei design mddesignnam aggreg dataset data set data frame survei design maml mapoutputport data set come assign valu variabl dataset numer data act input model",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"object coerc type doubl mil object coerc type doubl come assign valu variabl dataset numer data act input model",
        "Challenge_readability":11.7,
        "Challenge_reading_time":11.59,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"R Script : \"Error: (list) object cannot be coerced to type double\"",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":41537.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":191165856477
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to develop a survey App and using R Script with Azure ML for same.<\/p>\n\n<p>I have developed below code for the same and it works perfectly fine on Local Machine:<\/p>\n\n<pre><code>dataset1 &lt;-maml.mapInputPort(2)\ndataset3 &lt;-maml.mapInputPort(1)\nZ &lt;- as.numeric((dataset3),stringsAsFactors=TRUE)\nY &lt;- mdBinaryDesign(Z,4,dataset1)\nY.aggregate=mdBinaryToAggregateDesign(Y)\nsurvey.design=mdDesignNames(Y.aggregate, dataset1)\ndata.set &lt;- as.data.frame(survey.design)\nmaml.mapOutputPort(\"data.set\")\n<\/code><\/pre>\n\n<p>Now we plan to deploy this Application on server , for which we are using Azure MIL .<\/p>\n\n<p>Now my Dataset1 and Dataset3 are coming using Input port in R Model , by using the above code , I get error \"missing value where TRUE\/FALSE needed\". My Dataset3 contains a simple number eg: \"5\" .<\/p>\n\n<p>Since my model will only run having three dynamic inputs (e.g. a, b, c), is there a way I can call a web service which will give me three output parameters via JSON and I can assign same to my model?<\/p>\n\n<p>The part where I want to dynamically apply parameters are:<\/p>\n\n<pre><code>Y &lt;- mdBinaryDesign(parameter_1,parameter_2,parameters3)\n<\/code><\/pre>\n\n<p>Since I am new to R , Please suggest we what library to use as well how to assigns value to parameter_1 and so on.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1478087664743,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi survei app local deploi server miss valu dataset model run dynam input web servic output paramet json assign model guidanc librari assign valu paramet",
        "Challenge_last_edit_time":1483531517736,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40379325",
        "Challenge_link_count":0,
        "Challenge_original_content":"survei app perfectli local dataset maml mapinputport dataset maml mapinputport numer dataset stringsasfactor mdbinarydesign dataset aggreg mdbinarytoaggregatedesign survei design mddesignnam aggreg dataset data set data frame survei design maml mapoutputport data set plan deploi server mil dataset dataset come input port model miss valu dataset model run dynam input web servic output paramet json assign model dynam appli paramet mdbinarydesign paramet paramet paramet librari assign valu paramet",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"survei app perfectli local plan deploi server mil dataset dataset come input port model miss valu dataset model run dynam input web servic output paramet json assign model dynam appli paramet librari assign valu",
        "Challenge_readability":9.6,
        "Challenge_reading_time":16.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"R Script in AzureML",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":203.0,
        "Challenge_word_count":185,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":191160335257
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is it possible to run more than one training models in the same time in            Azure ML?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1478088448113,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"determin run multipl train model simultan",
        "Challenge_last_edit_time":1483486889187,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40379578",
        "Challenge_link_count":0,
        "Challenge_original_content":"run train model time run train model time",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"run train model time run train model time",
        "Challenge_readability":5.0,
        "Challenge_reading_time":2.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to run more than one training models in the same time in azure ml?",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":1228.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":191159551887
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a Neural network model using mxnet package in R studio. I tested the model on local and it works as expected. I have deployed the same model as a webservice in AzureML using <code>publishwebservice()<\/code> function from R.<\/p>\n\n<p>When I try to predict the test data with the webservice using <code>consume()<\/code> function: <\/p>\n\n<pre><code>pred_cnn &lt;- consume(endpoint_cnn, testdf)\n<\/code><\/pre>\n\n<p>it always throws following error:<\/p>\n\n<blockquote>\n  <p>Error: AzureML returns error code: HTTP status code : 400 AzureML\n  error code  : LibraryExecutionError<\/p>\n  \n  <p>Module execution encountered an internal library error.<br>\n  The following\n  error occurred during evaluation of R script: R_tryEval: return error: \n  Error in UseMethod(\"predict\") :<br>\n     no applicable method for 'predict'\n  applied to an object of class \"MXFeedForwardModel\"<\/p>\n<\/blockquote>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1478264100187,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"creat neural network model mxnet packag studio deploi webservic publishwebservic function predict test data webservic consum function state predict appli object class mxfeedforwardmodel",
        "Challenge_last_edit_time":1478546093720,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40423227",
        "Challenge_link_count":0,
        "Challenge_original_content":"predict appli object class mxfeedforwardmodel creat neural network model mxnet packag studio test model local deploi model webservic publishwebservic function predict test data webservic consum function pred cnn consum endpoint cnn testdf throw return http statu libraryexecutionerror modul execut intern librari evalu tryeval return usemethod predict predict appli object class mxfeedforwardmodel",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"predict appli object class mxfeedforwardmodel creat neural network model mxnet packag studio test model local deploi model webservic function predict test data webservic function throw return http statu libraryexecutionerror modul execut intern librari evalu return usemethod predict appli object class mxfeedforwardmodel",
        "Challenge_readability":13.8,
        "Challenge_reading_time":12.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"No applicable method for 'predict' applied to an object of class \"MXFeedForwardModel\"",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":983.0,
        "Challenge_word_count":126,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":190983899813
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is it currently possible to install packages in 'Create R Model'? Currently this is a huge limitation of AzureML.<\/p>\n\n<p>I know it is possible to do it in 'Execute R Script' but in 'Execute R Script' you can't save the model.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1478771703003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal packag creat model limit save model execut packag instal",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40524551",
        "Challenge_link_count":0,
        "Challenge_original_content":"instal packag creat model instal packag creat model huge limit execut execut save model",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"instal packag creat model instal packag creat model huge limit execut execut save model",
        "Challenge_readability":6.7,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Install packages in Create R Model - AzureML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":269.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":190476296997
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/wSYTs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wSYTs.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am trying to install a package in azure ML studio using the command below.<\/p>\n\n<pre><code>install.packages(\"src\/DMwR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nlibrary(DMwR, lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>\n\n<p>DMwR.zip was upload as a dataset in azure. The error I get is below.<\/p>\n\n<pre><code>Error 0063: The following error occurred during evaluation of R script:\n---------- Start of error message from R ----------\nzip file 'src\/DMwR.zip' not found\n<\/code><\/pre>\n\n<p>How can I resolve this?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1479298809787,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"studio load instal packag upload packag dataset receiv messag state zip file resolut",
        "Challenge_last_edit_time":1479309068183,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40632047",
        "Challenge_link_count":2,
        "Challenge_original_content":"studio load instal packag instal packag studio instal packag src dmwr zip lib repo null verbos librari dmwr lib loc verbos dmwr zip upload dataset evalu start messag zip file src dmwr zip",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"studio load instal packag instal packag studio upload dataset",
        "Challenge_readability":8.1,
        "Challenge_reading_time":9.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Studio cannot load a installed package in R",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":630.0,
        "Challenge_word_count":85,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":189949190213
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know what I can either upload my data files to the azure ml (as new datasets) or I can use Blobs (and read data within ML experiment). I wonder if particularly one of them is recommended when training machine learning models and creating prediction-related ML solutions.<\/p>\n\n<p>My goal of using Azure is to cluster users based on a various of features. I have a large dataset (~ 50GB). I wonder if you have any recommendations.<\/p>\n\n<p>I appreciate any help!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1479636141783,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"decid upload data file blob run algorithm big data larg dataset option choos cluster base featur",
        "Challenge_last_edit_time":1479636636943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40702754",
        "Challenge_link_count":0,
        "Challenge_original_content":"blob run algorithm big data upload data file dataset blob read data train model creat predict relat goal cluster base featur larg dataset",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"blob run algorithm big data upload data file blob train model creat goal cluster base featur larg dataset",
        "Challenge_readability":8.5,
        "Challenge_reading_time":6.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it good or necessary to use Blobs when running machine learning algorithms with big data",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":135.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":189611858217
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to automatically set up Azure machine learning endpoints that don't necessarily have the same amount of variables. I am able to programmatically add new endpoints that are trained on different data as long as they have the same column and variable names (headers).<\/p>\n\n<p>When I try to create a new endpoint using a different column count it works. But when I try to call it it gives me errors.<\/p>\n\n<p>I set up an experiment where the default endpoint is accepting two parameters 'x' and 'y'. Then I trained it on a dataset using three columns 'x1', 'x2' and 'y'. The 'Train Model' module in the training experiment is picking out column 1.<\/p>\n\n<p>Calling the endpoint that was trained using three variables with three input columns:<\/p>\n\n<pre><code>{\n\"error\": {\n    \"code\": \"LibraryExecutionError\",\n    \"message\": \"Module execution encountered an internal library error.\",\n    \"details\": [\n        {\n            \"code\": \"TableSchemaColumnCountMismatch\",\n            \"target\": \" (AFx Library)\",\n            \"message\": \"data: The table column count (3) must match the schema column count (2).\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>Calling the endpoint that was trained using three variables with ony two input columns:<\/p>\n\n<pre><code>{\n\"error\": {\n    \"code\": \"LibraryExecutionError\",\n    \"message\": \"Module execution encountered an internal library error.\",\n    \"details\": [\n        {\n            \"code\": \"ScoredFeaturesMustMatchTrainingFeatures\",\n            \"target\": \"Score Model (AFx Library)\",\n            \"message\": \"table: The data set being scored must contain all features used during training, missing feature(s): 'x2'.\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>}<\/p>\n\n<p>It seems to be remembering the setup of the default endpoint and expects all other endpoints to conform to it's metadata. Is there any way around this?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1480587217760,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set endpoint column count programmat add endpoint train data column variabl creat endpoint column count call default endpoint accept paramet tri endpoint train variabl input column tabl column count match schema column count similarli tri endpoint train variabl input column data set score featur train miss featur default endpoint rememb setup endpoint conform metadata",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/40907847",
        "Challenge_link_count":0,
        "Challenge_original_content":"endpoint column count automat set endpoint necessarili variabl programmat add endpoint train data column variabl header creat endpoint column count set default endpoint accept paramet train dataset column train model modul train pick column call endpoint train variabl input column libraryexecutionerror messag modul execut intern librari tableschemacolumncountmismatch target afx librari messag data tabl column count match schema column count call endpoint train variabl oni input column libraryexecutionerror messag modul execut intern librari scoredfeaturesmustmatchtrainingfeatur target score model afx librari messag tabl data set score featur train miss featur rememb setup default endpoint endpoint conform metadata",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"endpoint column count automat set endpoint necessarili variabl programmat add endpoint train data column variabl creat endpoint column count set default endpoint accept paramet train dataset column train model modul train pick column call endpoint train variabl input column call endpoint train variabl oni input column rememb setup default endpoint endpoint conform metadata",
        "Challenge_readability":10.2,
        "Challenge_reading_time":22.26,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning endpoints with different column count",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":975.0,
        "Challenge_word_count":249,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":188660782240
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I get the following error when trying to retrieve the data from Azure Machine Learning<\/p>\n\n<pre><code>Error: LibraryExecutionError\nTarget: Score Model (AFx Library)\nMessage: table: The data set being scored must contain all features used during training, missing feature(s): 'NA'.\n<\/code><\/pre>\n\n<p>If I include NA within the values that get sent to Azure I get the following message <\/p>\n\n<pre><code>Parsing of input vector failed. Verify the input vector has the correct number of columns and data types\n<\/code><\/pre>\n\n<p>Has anyone got any idea's on how to fix this issue?<\/p>\n\n<p>James<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1482245841440,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"libraryexecutionerror retriev data messag state data set score featur train miss featur valu sent receiv messag state input vector pars idea",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41245211",
        "Challenge_link_count":0,
        "Challenge_original_content":"libraryexecutionerror retriev data libraryexecutionerror target score model afx librari messag tabl data set score featur train miss featur valu sent messag pars input vector verifi input vector column data type idea jame",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"libraryexecutionerror retriev data valu sent messag idea jame",
        "Challenge_readability":12.2,
        "Challenge_reading_time":7.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML LibraryExecutionError",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":187.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":187002158560
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>Is it currently possible to create a custom R clustering model, where you can define your own clustering model? Because AzureML does not let you connect Customer R Model with Train Clustering Model.<\/p>\n\n<p>This is a critical limitation of AzureML when it comes to clustering. <\/p>\n\n<p>Note: I know that you can create it in Execute R Script, but I want to be able to save the model so when new test data is inputted, I would assign it to the respective clusters.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1482854228483,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"limit creat cluster model connect train cluster model creat execut save model assign test data respect cluster",
        "Challenge_last_edit_time":1483523008932,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41348255",
        "Challenge_link_count":0,
        "Challenge_original_content":"cluster model creat cluster model defin cluster model connect model train cluster model critic limit come cluster note creat execut save model test data input assign respect cluster",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"cluster model creat cluster model defin cluster model connect model train cluster model critic limit come cluster note creat execut save model test data input assign respect cluster",
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML - Custom R clustering model",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":105.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":186393771517
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm new to Azure ML and I'm trying to implement a python script in Azure ML. I'm trying to deploy the web services, but I'm getting only a string as output.<\/p>\n\n<p>When I run the python script alone, I'm getting the result, but when implemented in Web service, i'm only getting a statement saying \"Execution ok\"  . Please let me know, how to go about it. <\/p>\n\n<pre><code>The output returns a Json format.\n<\/code><\/pre>\n\n<p>Following is my output from Python script:<\/p>\n\n<pre><code>User Patterns\n[{\"Jane\": [{\"Thermostat\": 20, \"Days\": [1, 2], \"Hour\": 6, \"Minute\": 43}], \n\"John\": [{\"Thermostat\": 18, \"Days\": [1, 2], \"Hour\": 0, \"Minute\": 15}], \n\"Jen\": [{\"Thermostat\": 22, \"Days\": [1, 2], \"Hour\": 10, \"Minute\": 1}]}]\n\nMissed Patterns\n[{\"Jane\": [], \"John\": [], \"Jen\": []}]\nPatternsssssssssss [{\"Jane\": [{\"Thermostat\": 20, \"Days\": [1, 2], \"Hour\": 6, \n\"Minute\": 43}], \"John\": [{\"Thermostat\": 18, \"Days\": [1, 2], \"Hour\": 0, \n\"Minute\": 15}], \"Jen\": [{\"Thermostat\": 22, \"Days\": [1, 2], \"Hour\": 10, \n\"Minute\": 1}]}]\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1484032309870,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"desir output web servic implement return output json format web servic return statement sai execut guidanc",
        "Challenge_last_edit_time":1484042346568,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41563122",
        "Challenge_link_count":0,
        "Challenge_original_content":"output web servic implement deploi web servic output run implement web servic statement sai execut output return json format output pattern jane thermostat dai hour minut john thermostat dai hour minut jen thermostat dai hour minut miss pattern jane john jen patternss jane thermostat dai hour minut john thermostat dai hour minut jen thermostat dai hour minut",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"output web servic implement deploi web servic output run implement web servic statement sai execut output",
        "Challenge_readability":8.8,
        "Challenge_reading_time":13.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to get output from a web service from a python script in Azure ML",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":392.0,
        "Challenge_word_count":163,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":185215690130
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have the below Python script. And I'm trying to return the new Dataframe created. Unfortunately, I'm getting \"NotImplementedError:\"<\/p>\n\n<p>Please let me know, how to return a Dataframe in Azure ML from a python script<\/p>\n\n<p>Code:<\/p>\n\n<pre><code>def azureml_main(df) :\n\n    therm_patterns,therm_missed_patterns = thermostat_phase(df)\n    th_pat = json.loads(therm_patterns)\n    missed_th_pat = json.loads(therm_missed_patterns)\n\n    light_patterns, light_missed_patterns = light_phase(df)\n    lt_pat = json.loads(light_patterns)\n    missed_lt_pat = json.loads(light_missed_patterns)\n\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['John'])):\n            th_pat[0]['John'].append(lt_pat[0]['John'][i])\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['Jane'])):\n            th_pat[0]['Jane'].append(lt_pat[0]['Jane'][i])\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['Jen'])):\n            th_pat[0]['Jen'].append(lt_pat[0]['Jen'][i])\n\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['John'])):\n            missed_th_pat[0]['John'].append(missed_lt_pat[0]['John'][i])\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['Jane'])):\n            missed_th_pat[0]['Jane'].append(missed_lt_pat[0]['Jane'][i])\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['Jen'])):\n            missed_th_pat[0]['Jen'].append(missed_lt_pat[0]['Jen'][i])\n\n    output = json.dumps(th_pat)\n\n    df = pd.DataFrame.from_dict({k: v[0] for k, v in json.loads(output) \n    [0].items()}, 'index' ).rename_axis('User').reset_index()\n\n\n    return df         \n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>  Error 0085: The following error occurred during script evaluation, please  \n  view the output log for more information:\n\n   ---------- Start of error message from Python interpreter ----------\n   Caught exception while executing function: Traceback (most recent call  \n   last):\n\n       File \"C:\\server\\invokepy.py\", line 211, in batch\n       xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\n\n       File \"C:\\server\\XDRReader\\xdrutils.py\", line 54, in DataFrameToRFile\n       xdrwriter.write_attribute_list(attributes)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 155, in   \n       write_attribute_list\n       self.write_object(value)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 215, in write_object\n       write_func(flags, converted, missingIndices)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 185, in write_objects\n       self.write_object(value)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 200, in write_object\n       raise NotImplementedError('Python Bridge conversion table not    \n       implemented for type [{0}]'.format(value.getType()))\n\n       NotImplementedError: Python Bridge conversion table not implemented  \n       for type [&lt;type 'list'&gt;]\n\n       Process returned with non-zero exit code 1\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1484124401607,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"return datafram creat notimplementederror messag bridg convers tabl implement type list",
        "Challenge_last_edit_time":1484584933627,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41586275",
        "Challenge_link_count":0,
        "Challenge_original_content":"return datafram return datafram creat unfortun notimplementederror return datafram therm pattern therm miss pattern thermostat phase pat json load therm pattern miss pat json load therm miss pattern light pattern light miss pattern light phase pat json load light pattern miss pat json load light miss pattern rang len pat rang len pat john pat john append pat john rang len pat rang len pat jane pat jane append pat jane rang len pat rang len pat jen pat jen append pat jen rang len miss pat rang len miss pat john miss pat john append miss pat john rang len miss pat rang len miss pat jane miss pat jane append miss pat jane rang len miss pat rang len miss pat jen miss pat jen append miss pat jen output json dump pat datafram dict json load output item index renam axi reset index return evalu output log start messag interpret caught except execut function traceback file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil file server xdrreader xdrutil line dataframetorfil xdrwriter write attribut list attribut file server xdrreader xdrwriter line write attribut list write object valu file server xdrreader xdrwriter line write object write func flag convert missingindic file server xdrreader xdrwriter line write object write object valu file server xdrreader xdrwriter line write object rais notimplementederror bridg convers tabl implement type format valu gettyp notimplementederror bridg convers tabl implement type process return exit",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"return datafram return datafram creat unfortun notimplementederror return datafram",
        "Challenge_readability":11.6,
        "Challenge_reading_time":36.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"How to return a Dataframe in Azure ML from a python script",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1101.0,
        "Challenge_word_count":254,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":185123598393
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created the the Azure Machine Learning sample \"<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/R-Model-Train-Score-2\" rel=\"nofollow noreferrer\">R Model Train &amp; Score<\/a>\" from the gallery and followed the tutorial.  However, when I setup, deploy the web service ( as [New] Preview) and test, I get the error:<\/p>\n\n<blockquote>\n  <p>Score Model (RPackage) : Given path to R installation not found on machine or R executable not at this location<\/p>\n<\/blockquote>\n\n<p>The classic deployment works fine.  Any ideas on how to get the [New] Preview deployment example to run as a web service?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1485327001460,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set deploi sampl model train score web servic preview deploy messag score model rpackag path instal execut locat test classic deploy advic preview deploy",
        "Challenge_last_edit_time":1485327448236,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/41845186",
        "Challenge_link_count":1,
        "Challenge_original_content":"model train score web servic creat sampl model train score galleri tutori setup deploi web servic preview test score model rpackag path instal execut locat classic deploy idea preview deploy run web servic",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"model train score web servic creat sampl model train score galleri tutori setup deploi web servic test score model path instal execut locat classic deploy idea preview deploy run web servic",
        "Challenge_readability":11.5,
        "Challenge_reading_time":8.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML R Model Train & Score Web Service",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":131.0,
        "Challenge_word_count":90,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":183920998540
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>In AzureML it is not possible to connect trained Clustering Model with web service output. <\/p>\n\n<p>Why does AzureML only allow ILearnerDotNet to be connected with Web Service Output and not IClusterDotNet? <\/p>\n\n<p>This is a serious bug which halts clustering models from deploying them as a web service.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1486111021187,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"train cluster model connect web servic output allow ilearnerdotnet connect web servic output iclusterdotnet prevent cluster model deploi web servic",
        "Challenge_last_edit_time":1486111807267,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42019977",
        "Challenge_link_count":0,
        "Challenge_original_content":"train cluster model connect web servic output connect train cluster model web servic output allow ilearnerdotnet connect web servic output iclusterdotnet halt cluster model deploi web servic",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"train cluster model connect web servic output connect train cluster model web servic output allow ilearnerdotnet connect web servic output iclusterdotnet halt cluster model deploi web servic",
        "Challenge_readability":9.1,
        "Challenge_reading_time":4.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"AzureML - Trained clustering Model cannot be connected with Web Service Output",
        "Challenge_topic":"Kubernetes Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":192.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":183136978813
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created an experiment to help categorise a description, this all works fine. However it does not tell me the weightings. When on the studio website I click test on the experiment and call the service I get back a JSON blob including lots of useful data such as the column names and weightings. When I actually use the web service from my app using C# the returned json does not include this information?<\/p>\n\n<p>Any reason for this?<\/p>\n\n<p>thanks\nAndy<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486123820837,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"webservic return column weight call app receiv test studio websit",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42024001",
        "Challenge_link_count":0,
        "Challenge_original_content":"webservic column return creat categoris descript weight studio websit click test servic json blob data column weight web servic app return json reason andi",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"webservic column return creat categoris descript weight studio websit click test servic json blob data column weight web servic app return json reason andi",
        "Challenge_readability":7.0,
        "Challenge_reading_time":6.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML webservice columns returned no as expected",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":94.0,
        "Challenge_word_count":88,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":183124179163
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is it possible to import images from your Azure storage account from within a Python script module as opposed to using the Import Images module that Azure ML Studio provides. Ideally I would like to use <code>cv2.imread()<\/code>. I only want to read in grayscale data but the Import Images module reads in RGB. \nCan I use the <code>BlockBlobService<\/code> library as if I were calling it from an external Python script?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1486438042043,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"import imag storag account modul studio imread read grayscal data import imag modul read rgb blockblobservic librari modul",
        "Challenge_last_edit_time":1486447273667,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42081202",
        "Challenge_link_count":0,
        "Challenge_original_content":"import imag studio import imag storag account modul oppos import imag modul studio ideal imread read grayscal data import imag modul read rgb blockblobservic librari call extern",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"import imag studio import imag storag account modul oppos import imag modul studio ideal read grayscal data import imag modul read rgb librari call extern",
        "Challenge_readability":8.0,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Importing images Azure Machine Learning Studio",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":812.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":182809957957
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Has anyone successfully used the entire opencv library with Azure ML Studio and the Python Module? I know to use a python module that is not included in the ananconda version it must be uploaded in as a zip and from there you can use the entire library. Could someone explain to me exactly what to upload as a zip and then how to access specific functions of the opencv library once uploaded.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1486532576980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"entir opencv librari studio modul guidanc upload file zip access function librari",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42105549",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio opencv successfulli entir opencv librari studio modul modul ananconda version upload zip entir librari explain exactli upload zip access function opencv librari upload",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"studio opencv successfulli entir opencv librari studio modul modul ananconda version upload zip entir librari explain exactli upload zip access function opencv librari upload",
        "Challenge_readability":8.5,
        "Challenge_reading_time":5.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio and OpenCV",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2112.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":182715423020
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I'm building a R model in Azure machine learning with a zipped xgboost package attached to the 'execute R script'. <\/p>\n\n<p>Azure machine learning uses R 3.2.2.<\/p>\n\n<p>The model returns with an error saying \"This is R 3.2.2, package 'xgboost' needs >= 3.3.0\".<\/p>\n\n<p>Is there any reason it insists on >= 3.3.0. If not, can I get it down to run with R 3.2.2?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1486989987357,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"build model zip packag attach execut model return sai packag reason version downgrad run",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42204411",
        "Challenge_link_count":0,
        "Challenge_original_content":"build model zip packag attach execut model return sai packag reason insist run",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"build model zip packag attach execut model return sai packag reason insist run",
        "Challenge_readability":3.4,
        "Challenge_reading_time":4.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Why require only R >= 3.3.0?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":106.0,
        "Challenge_word_count":66,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":182258012643
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When I try to run my experiment I find that I am stuck in queued. <\/p>\n\n<p>Yes I am on a free account but I am not running two projects at the same time. <\/p>\n\n<p>I have tried deleting all my projects and creating a new one but this also does not get passed the queued stage.<\/p>\n\n<p>Thank you for the help<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487030139750,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"stuck queu stage run time tri delet creat",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42215613",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio stuck queu run stuck queu free account run time tri delet creat pass queu stage",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"studio stuck queu run stuck queu free account run time tri delet creat pass queu stage",
        "Challenge_readability":5.1,
        "Challenge_reading_time":4.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Machine Learning Studio - Experiment stuck in queued",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1030.0,
        "Challenge_word_count":68,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":182217860250
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have an experiment created in <strong>Azure ML<\/strong>, where I have a <strong>dataset<\/strong> which consists of following columns:<\/p>\n\n<pre><code>Temperature, Timestamp, Kw_system, Powerscout etc. \n<\/code><\/pre>\n\n<p>I need to input a range of timestamp say, <strong>from Oct 1st to Oct 2nd<\/strong> <strong>and all the rows between this needs to be displayed<\/strong> on which I would do anomaly detection. Please let me know how to go about it<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1487596190653,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"input rang timestamp displai row anomali detect dataset column temperatur timestamp powerscout guidanc accomplish task",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42345470",
        "Challenge_link_count":0,
        "Challenge_original_content":"web servic input paramet creat dataset consist column temperatur timestamp powerscout input rang timestamp oct oct row displai anomali detect",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic input paramet creat dataset consist column input rang timestamp oct oct row displai anomali detect",
        "Challenge_readability":8.9,
        "Challenge_reading_time":6.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML web service input parameters",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":682.0,
        "Challenge_word_count":70,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":181651809347
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>During a test project on Azure Machine Learning Studio I have some questions based on my understandings.\nIn my project (in R) I have used Binary Logistic Regression, but in AML I found two Logistic regression Two-Class and MultiClass. So in that case I have used two-class Logistic regression. Am I Right in this case?<\/p>\n<p>In another case during running glm() in R tool it perform Logistic regression and after summary(loreg Eqn) it provides the each variable's co-efficient &amp; estimates.<\/p>\n<p>From R I have the following output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vjcU2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vjcU2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From AML after right-clicking Train Model and visualize:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YRgEA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRgEA.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The weight in the above Picture is the estimates, am I right (Dataset is diff)?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488195203623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"class logist binari logist regress studio interpret output glm function compar weight estim aml",
        "Challenge_last_edit_time":1656336502183,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42484746",
        "Challenge_link_count":4,
        "Challenge_original_content":"class logist binari logist regress test studio base binari logist regress aml logist regress class multiclass class logist regress run glm perform logist regress summari loreg eqn variabl effici estim output aml click train model visual weight pictur estim dataset diff",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"binari logist regress test studio base binari logist regress aml logist regress multiclass logist regress run glm perform logist regress summari variabl estim output aml train model visual weight pictur estim",
        "Challenge_readability":10.4,
        "Challenge_reading_time":13.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Two-Class-Logistic VS Binary Logistic Regression",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":650.0,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":181052796377
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to schedule my AzureML experiment by Azure Data Factory (ADF).  After passing my job from azureML pipeline, I face with this issue \"Batch execution failed with HTTP status code: BadGateway. The response from the Machine Learning service at endpoint <a href=\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/26c276f8420a4c30aae39b0c27845134\/services\/7b122129b2c041f9b4399d7e4b16e927\/jobs\/job_id\/start\" rel=\"nofollow noreferrer\">https:\/\/ussouthcentral.services.azureml.net\/workspaces\/26c276f8420a4c30aae39b0c27845134\/services\/7b122129b2c041f9b4399d7e4b16e927\/jobs\/job_id\/start<\/a> was 'Internal error occurred.\"<\/p>\n\n<p>Do you have any idea for solving this issue.\nThanks,<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488701373533,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"schedul data factori receiv messag state batch execut http statu badgatewai respons servic endpoint intern assist",
        "Challenge_last_edit_time":1488702037912,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42606010",
        "Challenge_link_count":2,
        "Challenge_original_content":"batch execut http statu badgatewai respons servic endpoint schedul data factori adf pass job pipelin batch execut http statu badgatewai respons servic endpoint http ussouthcentr servic net workspac cfacaaebc servic bbcfbdebe job job start intern idea",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"batch execut http statu badgatewai respons servic endpoint schedul data factori pass job pipelin batch execut http statu badgatewai respons servic endpoint intern idea",
        "Challenge_readability":10.5,
        "Challenge_reading_time":10.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"\"Batch execution failed with HTTP status code: BadGateway. The response from the Machine Learning service at endpoint",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":379.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":180546626467
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Do you know how back up machine learning models in Azure Machine Learning Studio in case of idle time when subscription is not bought. I would preferably back those models in Azure DB\/DWH on other accounts\/instances of Azure. Is it actually possible to copy models' flow to another locations or share it with other users?\nI would appreciate the answer.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1488979073483,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"model studio subscript activ copi model flow locat share model dwh account instanc",
        "Challenge_last_edit_time":1488979331840,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42672425",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio model studio idl time subscript bought prefer model dwh account instanc copi model flow locat share",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"studio model studio idl time subscript bought prefer model copi model flow locat share",
        "Challenge_readability":7.2,
        "Challenge_reading_time":4.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Studio back up",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":768.0,
        "Challenge_word_count":65,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":180268926517
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have AzureML scripts that take about 1-5 days to complete. All input data comes from Azure SQL and is stored to Azure SQL. The input data is below 10GB.<\/p>\n\n<p><strong>What are preferred methods to schedule such scripts inside Azure?<\/strong><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1489422637617,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"schedul dai complet input data come sql store sql prefer schedul insid",
        "Challenge_last_edit_time":1489424839323,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42768756",
        "Challenge_link_count":0,
        "Challenge_original_content":"schedul dai complet dai complet input data come sql store sql input data prefer schedul insid",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"schedul dai complet dai complet input data come sql store sql input data prefer schedul insid",
        "Challenge_readability":3.7,
        "Challenge_reading_time":3.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure: how to schedule AzureML scripts taking 1-5 days to complete?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":140.0,
        "Challenge_word_count":50,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":179825362383
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm newbie to azure machine learning and I'm trying to build a model that rates groups of items.<\/p>\n\n<p>My data is a file with a list of items with features (small list - less than 80 items) and I need to make groups (of diferent sizes - groups of 2, 3, 4,... 10 items, for all the possible combinations) so that the model rate those groups (rates from 1 to 10). I also have some group rates to train the model.<\/p>\n\n<p>I don't know how to transform the items into groups. <\/p>\n\n<p>Another thing is, I'm not sure which model is the best. From all I gather, I think that a multiclass classification is the most suitable for this problem. Is it?<\/p>\n\n<p>Thank you in advance and sorry for any grammar error in my text. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1490109456700,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"transform list item group predict group rate item group size rang item combin model suit believ multiclass classif",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42931155",
        "Challenge_link_count":0,
        "Challenge_original_content":"transform list item group predict group rate newbi build model rate group item data file list item featur small list item group difer size group item combin model rate group rate group rate train model transform item group model gather multiclass classif suitabl advanc sorri grammar text",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"transform list item group predict group rate newbi build model rate group item data file list item featur group model rate group group rate train model transform item group model gather multiclass classif suitabl advanc sorri grammar text",
        "Challenge_readability":6.3,
        "Challenge_reading_time":9.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How do I transform a list of items into groups to predict group ratings in azure machine learning?",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":151,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":179138543300
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Rscript:<\/p>\n\n<p>install.packages(\"src\/Rpackages\/Rcpp.zip\", lib = \".\", repos = NULL, verbose = TRUE)<br>\nlibrary(Rcpp, lib.loc=\".\", verbose=TRUE) \ninstall.packages(\"src\/Rpackages\/ggplot2_2.2.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)<br>\nlibrary(ggplot2, lib.loc=\".\", verbose=TRUE) <\/p>\n\n<p>Error :<\/p>\n\n<p>requestId = 2719717d8d5b4a479547886c19b0bcb4 errorComponent=Module. taskStatusCode=400. {\"Exception\":{\"ErrorId\":\"FailedToEvaluateRScript\",\"ErrorCode\":\"0063\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0063: The following error occurred during evaluation of R script:\\r\\n---------- Start of error message from R ----------\\r\\npackage or namespace load failed for 'ggplot2'\\r\\n\\r\\n\\r\\npackage or namespace load failed for 'ggplot2'\\r\\n----------- End of error message from R -----------\"}}Error: Error 0063: The following error occurred during evaluation of R script:---------- Start of error message from R ----------package or namespace load failed for 'ggplot2'package or namespace load failed for 'ggplot2'----------- End of error message from R ----------- Process exited with error code -2<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JDbHN.png\" rel=\"nofollow noreferrer\">azuremlstudio snapshot<\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1490225641677,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"load ggplot librari scriptbundl zip studio execut featur messag packag namespac load ggplot",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/42964609",
        "Challenge_link_count":1,
        "Challenge_original_content":"studio execut load ggplot librari scriptbundl zip rscript instal packag src rpackag rcpp zip lib repo null verbos librari rcpp lib loc verbos instal packag src rpackag ggplot zip lib repo null verbos librari ggplot lib loc verbos requestid ddbacbbcb errorcompon modul taskstatuscod except errorid failedtoevaluaterscript errorcod exceptiontyp moduleexcept messag evalu start messag npackag namespac load ggplot npackag namespac load ggplot end messag evalu start messag packag namespac load ggplot packag namespac load ggplot end messag process exit studio snapshot",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"studio execut load ggplot librari rscript lib repo null verbos librari lib repo null verbos librari requestid bcb errorcompon modul taskstatuscod evalu start messag namespac load ggplot packag namespac load end messag process exit studio snapshot",
        "Challenge_readability":17.3,
        "Challenge_reading_time":17.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"azure machine learning studio \"execute R script\" is unable to load ggplot2 library from scriptbundle.zip",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":272.0,
        "Challenge_word_count":124,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":179022358323
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a saved dataset (zip file) in Azure ML studio. Inside the zip file is a .pickle file. I am now using (Python 3.5) Jupyter in Azure's notebook service. <\/p>\n\n<p>I would like to open and load the .pickle file in my Jupyter notebook from the saved zip file in the Azure ML Studio. Any ideas on how to do that? My code is as follows (with error):<\/p>\n\n<pre><code>from azureml import Workspace\nfrom six.moves import cPickle as pick\nfrom six.moves import range\n\nws = Workspace(workspace_id = '...', authorization_token='...')\n\nwith ws.datasets['xxx.zip'].open() as zf:\n    with open(zf, 'rb') as p:\n        pload = pick.load(p)\n        train_dataset = pload['train_dataset']\n        del pload\nprint(train_dataset.shape)\n<\/code><\/pre>\n\n<p>---> 14      with open(zf, 'rb') as p:<\/p>\n\n<p>TypeError: invalid file: requests.packages.urllib3.response.HTTPResponse object at 0x7fe739589ef0<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1490853507160,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"open load pickl file save zip file studio jupyt notebook typeerror file",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43109655",
        "Challenge_link_count":0,
        "Challenge_original_content":"unzip open pickl file jupyt notebook save dataset zip file studio insid zip file pickl file jupyt notebook servic open load pickl file jupyt notebook save zip file studio idea import workspac move import cpickl pick move import rang workspac workspac author token dataset zip open open pload pick load train dataset pload train dataset del pload print train dataset shape open typeerror file request packag urllib respons httprespons object xfeef",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"unzip open pickl file jupyt notebook save dataset studio insid zip file pickl file jupyt notebook servic open load pickl file jupyt notebook save zip file studio idea open typeerror file object",
        "Challenge_readability":4.2,
        "Challenge_reading_time":11.46,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to unzip and open pickle file in Azure Jupyter Notebook",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":2491.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":178394492840
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In Azure Data Factory, I\u2019m trying to call an Azure Machine Learning model by a Data Factory Pipeline. I want to use a Azure SQL table as input and another Azure SQL table for the output.\nFirst I deployed a Machine Learning (classic) web service. Then I created an Azure Data Factory Pipeline, using a LinkedService (type= \u2018AzureML\u2019, using Request URI and API key of the ML-webservice) and a input and output dataset (\u2018AzureSqlTable\u2019 type).<\/p>\n\n<p>Deploying and Provisioning is succeeded. The pipeline starts as scheduled, but keeps \u2018Running\u2019 without any result. The pipeline activity is not being shown in the Monitor&amp;Manage: Activity Windows.<\/p>\n\n<p>On different sites and tutorials, I only find JSON-scripts using the activity type \u2018AzureMLBatchExecution\u2019 with BLOB in- and outputs. I want to use AzureSQL in- and output but I can\u2019t get this working.<\/p>\n\n<p>Can someone provide a sample JSON-script or tell me what\u2019s possibly wrong with the code below?<\/p>\n\n<p>Thanks!<\/p>\n\n<pre><code>{\n    \"name\": \"Predictive_ML_Pipeline\",\n    \"properties\": {\n        \"description\": \"use MyAzureML model\",\n        \"activities\": [\n            {\n                \"type\": \"AzureMLBatchExecution\",\n                \"typeProperties\": {},\n                \"inputs\": [\n                    {\n                        \"name\": \"AzureSQLDataset_ML_Input\"\n                    }\n                ],\n                \"outputs\": [\n                    {\n                        \"name\": \"AzureSQLDataset_ML_Output\"\n                    }\n                ],\n                \"policy\": {\n                    \"timeout\": \"02:00:00\",\n                    \"concurrency\": 3,\n                    \"executionPriorityOrder\": \"NewestFirst\",\n                    \"retry\": 1\n                },\n                \"scheduler\": {\n                    \"frequency\": \"Week\",\n                    \"interval\": 1\n                },\n                \"name\": \"My_ML_Activity\",\n                \"description\": \"prediction analysis on ML batch input\",\n                \"linkedServiceName\": \"AzureMLLinkedService\"\n            }\n        ],\n        \"start\": \"2017-04-04T09:00:00Z\",\n        \"end\": \"2017-04-04T18:00:00Z\",\n        \"isPaused\": false,\n        \"hubName\": \"myml_hub\",\n        \"pipelineMode\": \"Scheduled\"\n    }\n}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1491296905000,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"call model data factori pipelin sql tabl input output deploy provis pipelin activ keep run shown monitor activ window sampl json tutori azuresql output activ type batchexecut",
        "Challenge_last_edit_time":1491305530088,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43202802",
        "Challenge_link_count":0,
        "Challenge_original_content":"data factori azuresql output pipelin activ type batchexecut data factori model data factori pipelin sql tabl input sql tabl output deploi classic web servic creat data factori pipelin linkedservic type request uri api kei webservic input output dataset azuresqlt type deploi provis succeed pipelin start schedul keep run pipelin activ shown monitor activ window site tutori json activ type batchexecut blob output azuresql output sampl json what predict pipelin properti descript model activ type batchexecut typeproperti input azuresqldataset input output azuresqldataset output polici timeout concurr executionpriorityord newestfirst retri schedul frequenc week interv activ descript predict analysi batch input linkedservicenam linkedservic start end ispaus hubnam myml hub pipelinemod schedul",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"data factori azuresql output pipelin activ type batchexecut data factori model data factori pipelin sql tabl input sql tabl output deploi web servic creat data factori pipelin linkedservic input output dataset deploi provis succeed pipelin start schedul keep run pipelin activ shown monitor activ window site tutori activ type batchexecut blob output azuresql output sampl what",
        "Challenge_readability":12.7,
        "Challenge_reading_time":22.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Data Factory: AzureSQL in- and output for pipeline activity type AzureMLBatchExecution",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":550.0,
        "Challenge_word_count":217,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":177951095000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use \"Azure ML\" execute R script module. I have created a zip 4 packages I required and trying to install the libraries. Its giving me error for 1 package \"som\", rest all working fine. When I have tried same \"som\" package in my local machine, it is working fine.<\/p>\n\n<p>I have downloaded all the libraries from CRAN and are the latest one. <\/p>\n\n<p>Any clues where it can go wrong?<\/p>\n\n<p>Here us my R script code<\/p>\n\n<pre><code>install.packages(\"src\/plyr_1.8.4.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RODBC_1.3-14.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/sqldf_0.4-10.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/som_0.3-5.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(RODBC)\nlibrary(plyr)\nlibrary(sqldf)\nlibrary(som)\n<\/code><\/pre>\n\n<p>Here is error during execution<\/p>\n\n<pre><code>Error \nError 0063: The following error occurred during evaluation of R script:\n\n---------- Start of error message from R ----------\n\nthere is no package called 'som'\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1491882492790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal som packag execut modul download librari cran successfulli instal packag som packag messag packag call som",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43336415",
        "Challenge_link_count":0,
        "Challenge_original_content":"packag call som execut modul creat zip packag instal librari packag som rest tri som packag local download librari cran latest clue instal packag src plyr zip lib repo null verbos instal packag src rodbc zip lib repo null verbos instal packag src sqldf zip lib repo null verbos instal packag src som zip lib repo null verbos librari rodbc librari plyr librari sqldf librari som execut evalu start messag packag call som",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"packag call som execut modul creat zip packag instal librari packag som rest tri som packag local download librari cran latest clue execut",
        "Challenge_readability":8.0,
        "Challenge_reading_time":13.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML - there is no package called 'som'",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":295.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":177365507210
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to use the same ML-realizations as used in Microsoft Azure Machine Learning Studio but on my computer without their visual interface?<\/p>\n\n<p>I love Azure ML studio when I play with my \"trial\" models, but it would be great if I could store the sequence of the commands as a code on my machine. <\/p>\n\n<p>I mean whether there is some reference in C# or library in Python that enables to do that? Or is there an API for that? <\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1492879884060,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"realiz studio visual interfac librari enabl api",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/43561934",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio visual studio realiz studio visual interfac love studio plai trial model great store sequenc librari enabl api",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"studio visual studio studio visual interfac love studio plai trial model great store sequenc librari enabl api",
        "Challenge_readability":6.9,
        "Challenge_reading_time":6.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Microsoft Azure Machine Learning Studio from Visual Studio?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":774.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":176368115940
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I followed this guide to retrain a model (Guide: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-retrain-a-classic-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-retrain-a-classic-web-service<\/a>)<\/p>\n\n<p>Still there are some questions left. So before retraining a model, do I have to upload the new dataset into my  blob container storage ? If yes how do I do that via http ?<\/p>\n\n<p>Maybe is it possible to send the new dataset via the PATCH-call in the http body?<\/p>\n\n<p>Thanks in Advance!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1494934026127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"retrain model classic web servic upload dataset blob storag http send dataset patch http bodi",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44000208",
        "Challenge_link_count":2,
        "Challenge_original_content":"retrain model classic web servic guid retrain model guid http doc com retrain classic web servic left retrain model upload dataset blob storag http mayb send dataset patch http bodi advanc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"retrain model guid retrain model left retrain model upload dataset blob storag http mayb send dataset http bodi advanc",
        "Challenge_readability":9.3,
        "Challenge_reading_time":8.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML - How to retrain a model (classic web service)",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":537.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":174313973873
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have a model using MLPClassifier from scikitlearn. I pkl, zip it and uploaded to Azure ML. The process works fine with no errors when I run it, but once I call it from a WebService I get the following error. I also added the scikit learn files to the model so I can use MLPClassifier.<\/p>\n\n<pre><code>$ \"C:\\pyhome\\lib\\pickle.py\", line 1384, in find_class __import__(module, level=0) ImportError: No module named 'sklearn.neural_network.multilayer_perceptron' $\n<\/code><\/pre>\n\n<p>\u00a0\nThis is my python code in Azure ML<\/p>\n\n<pre><code>$ import sys\nsys.path.insert(0, \".\\\\Script Bundle\")\u00a0\u00a0\u00a0 \nimport os\nos.environ['PATH'] = os.path.dirname(\".\\\\Script Bundle\\\\DLLs\\\\\")+ ';' + os.environ['PATH']\nimport pandas as pd\nimport sklearn as sk\nfrom sklearn.externals import joblib\n#from sklearn.neural_network import MLPClassifier\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\u00a0\u00a0\u00a0 \n\u00a0\u00a0\u00a0 #print (\"sklearn version :\", sk.__version__)\n\u00a0\u00a0\u00a0 model = joblib.load('.\/Script Bundle\/RNNmodel.pkl')\n\u00a0\u00a0\u00a0 y_train = model.predict(dataframe1)\n\u00a0\u00a0\u00a0 dataframe1 = pd.DataFrame(y_train)\n\u00a0\u00a0\u00a0 return dataframe1,\n$\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1494968133857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"call mlpclassifi model webservic messag modul sklearn neural network multilay perceptron upload model scikit file model import librari load model predict output",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44011421",
        "Challenge_link_count":0,
        "Challenge_original_content":"mlpclassifi webservic model mlpclassifi scikitlearn pkl zip upload process run webservic scikit file model mlpclassifi pyhom lib pickl line class import modul level importerror modul sklearn neural network multilay perceptron import sy sy path insert bundl import environ path path dirnam bundl dll environ path import panda import sklearn sklearn extern import joblib sklearn neural network import mlpclassifi datafram datafram print sklearn version version model joblib load bundl rnnmodel pkl train model predict datafram datafram datafram train return datafram",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"mlpclassifi webservic model mlpclassifi scikitlearn pkl zip upload process run webservic scikit file model mlpclassifi",
        "Challenge_readability":8.7,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"MLPClassifier in Azure ML webservice call",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":161.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":174279866143
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've tried to upload the xgboost python library to Azure ML, however it claim that my library is not a Win32 application.\nI've made sure to install the 32 bit version of the package and i'm running conda 32 bit as well.\nI downloaded the library from:\n<a href=\"http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#xgboost\" rel=\"nofollow noreferrer\">http:\/\/www.lfd.uci.edu\/~gohlke\/pythonlibs\/#xgboost<\/a>\nand chose the 32 bit python 3.5 version.\nPython installation as below.\n<img src=\"https:\/\/i.stack.imgur.com\/NkKRk.jpg\" alt=\"python installation\"><\/p>\n\n<p>This is the error I get returned azure ml error\n<img src=\"https:\/\/i.stack.imgur.com\/V9M1f.jpg\" alt=\"azure ml error\"><\/p>\n\n<p>Here is my installation of anaconda conda installation\n<img src=\"https:\/\/i.stack.imgur.com\/MsLyg.jpg\" alt=\"conda installation\"><\/p>\n\n<p>Can anyone see where I went wrong? <\/p>\n\n<p>Best Regards<\/p>\n\n<p>EDIT:\nYes I followed the document and uploaded a zip file containing the wheel file. When I run the following it works just fine:\n\"import pip\" and   \"pip.main(['install', '.\/Script Bundle\/xgboost-0.6-cp35-cp35m-win32.whl'])\"\nBut when I add \"import xgboost\" I get this error.<a href=\"https:\/\/i.stack.imgur.com\/coYnH.jpg\" rel=\"nofollow noreferrer\">Import error<\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1496231820517,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"upload librari messag librari win instal bit version packag run conda bit instruct document upload zip file wheel file import import",
        "Challenge_last_edit_time":1496308899830,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44284183",
        "Challenge_link_count":6,
        "Challenge_original_content":"upload win nprocess return exit tri upload librari claim librari win instal bit version packag run conda bit download librari http lfd uci edu gohlk pythonlib chose bit version instal return instal anaconda conda instal went edit document upload zip file wheel file run import pip pip instal bundl cpm win whl add import import",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"upload win return exit tri upload librari claim librari win instal bit version packag run conda bit download librari chose bit version instal return instal anaconda conda instal went edit document upload zip file wheel file run import pip add import",
        "Challenge_readability":9.1,
        "Challenge_reading_time":17.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Uploading xgboost to azure machine learning: %1 is not a valid Win32 application\\r\\nProcess returned with non-zero exit code 1",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":650.0,
        "Challenge_word_count":166,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":173016179483
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>suppose I'm using the following configuration with the Tune Model Hyperparameters module with a Boosted Decision Tree Regression:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ysnWu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ysnWu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does this configuration let me tune the hyperparameters so that I'll have the best Coefficient of Determination, and at the same time it guarantees the lowest cross-validation average error?\nIf so, does anyone knows what the Tune Model Hyperparameters module do using this configuration in more details?<\/p>\n\n<p>Thank you.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1496510680007,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"clarif tune model hyperparamet modul simultan tune hyperparamet achiev coeffici determin lowest cross averag request modul configur",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44346696",
        "Challenge_link_count":2,
        "Challenge_original_content":"tune model hyperparamet modul tune cross time suppos configur tune model hyperparamet modul boost decis tree regress configur tune hyperparamet coeffici determin time lowest cross averag tune model hyperparamet modul configur",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"tune model hyperparamet modul tune cross time suppos configur tune model hyperparamet modul boost decis tree regress configur tune hyperparamet coeffici determin time lowest averag tune model hyperparamet modul configur",
        "Challenge_readability":13.7,
        "Challenge_reading_time":9.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Tune Model Hyperparameters Module: tune and cross validate at the same time",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":916.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":172737319993
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to access azure blob storage from within an Azure ML experiment. I completely followed the answer from \n<a href=\"https:\/\/stackoverflow.com\/questions\/35246826\/access-azure-blog-storage-from-within-an-azure-ml-experiment\">Access Azure blog storage from within an Azure ML experiment<\/a>\nhowever, there was some error.<\/p>\n\n<p>My code is<\/p>\n\n<pre><code>from azure.storage.blob import BlobService\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n   dataframe1.to_csv(\"output.csv\", index=True)\n   account_name = \"acount_name\"\n   account_key=\"rJfqPEFcbgpS...SKZrBs5J2eOq0IJYrc2Vg==\"\n   CONTAINER_NAME = \"CONTAINER_NAME\"\n\n   blob_service = BlobService(account_name, account_key, protocol='http')\n   blob_service.put_block_blob_from_path(CONTAINER_NAME,\"output\",\"output.csv\")\n<\/code><\/pre>\n\n<p>The error log is blow <\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\nFile \"C:\\pyhome\\lib\\site-\npackages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen\nbody=body, headers=headers)\nFile \"C:\\pyhome\\lib\\site-\npackages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in \n_make_request\nconn.request(method, url, **httplib_request_kw)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request\nself._send_request(method, url, body, headers)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request\nself.endheaders(body)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders\nself._send_output(message_body)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output\nself.send(msg)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send\nself.sock.sendall(data)\nsocket.timeout: timed out\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 376, in send\ntimeout=timeout\nFile \"C:\\pyhome\\lib\\site-\npackages\\requests\\packages\\urllib3\\connectionpool.py\", line 609, in urlopen\n_stacktrace=sys.exc_info()[2])\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", \nline 247, in increment\nraise six.reraise(type(error), error, _stacktrace)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", \nline 309, in reraise\nraise value.with_traceback(tb)\nFile \"C:\\pyhome\\lib\\site-\npackages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen\nbody=body, headers=headers)\nFile \"C:\\pyhome\\lib\\site-\npackages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in \n_make_request\nconn.request(method, url, **httplib_request_kw)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request\nself._send_request(method, url, body, headers)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request\nself.endheaders(body)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders\nself._send_output(message_body)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output\nself.send(msg)\nFile \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send\nself.sock.sendall(data)\nrequests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', \ntimeout('timed out',))\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"C:\\server\\invokepy.py\", line 199, in batch\nodfs = mod.azureml_main(*idfs)\nFile \"C:\\temp\\a24f27b62ef74991b262bafe5e685164.py\", line 39, in azureml_main\nblob_service.put_block_blob_from_path(CONTAINER_NAME,\"ppp\",\"output.csv\")\nFile \"c:\\temp\\script bundle\\azure\\storage\\blob\\blobservice.py\", line 975, in \nput_block_blob_from_path\nretry_wait)\nFile \"c:\\temp\\script bundle\\azure\\storage\\blob\\blobservice.py\", line 1072, in \nput_block_blob_from_file\nx_ms_lease_id)\nFile \"c:\\temp\\script bundle\\azure\\storage\\blob\\blobservice.py\", line 883, in \nput_blob\nself._perform_request(request)\nFile \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 171, in \n_perform_request\nresp = self._filter(request)\nFile \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 160, in \n_perform_request_worker\nreturn self._httpclient.perform_request(request)\nFile \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 181, in \nperform_request\nself.send_request_body(connection, request.body)\nFile \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 143, in \nsend_request_body\nconnection.send(request_body)\nFile \"c:\\temp\\script bundle\\azure\\storage\\_http\\requestsclient.py\", line 81, \nin send\nself.response = self.session.request(self.method, self.uri, data=request_body, \nheaders=self.headers, timeout=self.timeout)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 468, in request\nresp = self.send(prep, **send_kwargs)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send\nr = adapter.send(request, **kwargs)\nFile \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 426, in send\nraise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', timeout('timed \nout',))\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>Any help or thought is appreciated!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1497596585243,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"access blob storag log timeout execut",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44582867",
        "Challenge_link_count":1,
        "Challenge_original_content":"time access blob storag access blob storag complet access blog storag storag blob import blobservic datafram datafram datafram csv output csv index account acount account kei rjfqpefcbgp skzrbsjeoqijyrcvg blob servic blobservic account account kei protocol http blob servic block blob path output output csv log blow critic evalu output log start messag interpret caught except execut function traceback file pyhom lib site packag request packag urllib connectionpool line urlopen bodi bodi header header file pyhom lib site packag request packag urllib connectionpool line request conn request url httplib request file pyhom lib http client line request send request url bodi header file pyhom lib http client line send request endhead bodi file pyhom lib http client line endhead send output messag bodi file pyhom lib http client line send output send msg file pyhom lib http client line send sock sendal data socket timeout time except except traceback file pyhom lib site packag request adapt line send timeout timeout file pyhom lib site packag request packag urllib connectionpool line urlopen stacktrac sy exc file pyhom lib site packag request packag urllib util retri line increment rais rerais type stacktrac file pyhom lib site packag request packag urllib packag line rerais rais valu traceback file pyhom lib site packag request packag urllib connectionpool line urlopen bodi bodi header header file pyhom lib site packag request packag urllib connectionpool line request conn request url httplib request file pyhom lib http client line request send request url bodi header file pyhom lib http client line send request endhead bodi file pyhom lib http client line endhead send output messag bodi file pyhom lib http client line send output send msg file pyhom lib http client line send sock sendal data request packag urllib except protocolerror connect abort timeout time except except traceback file server invokepi line batch odf mod idf file temp afbefbbafe line blob servic block blob path output csv file temp bundl storag blob blobservic line block blob path retri wait file temp bundl storag blob blobservic line block blob file leas file temp bundl storag blob blobservic line blob perform request request file temp bundl storag storagecli line perform request resp filter request file temp bundl storag storagecli line perform request worker return httpclient perform request request file temp bundl storag http httpclient line perform request send request bodi connect request bodi file temp bundl storag http httpclient line send request bodi connect send request bodi file temp bundl storag http requestscli line send respons session request uri data request bodi header header timeout timeout file pyhom lib site packag request session line request resp send prep send kwarg file pyhom lib site packag request session line send adapt send request kwarg file pyhom lib site packag request adapt line send rais connectionerror err request request request except connectionerror connect abort timeout time process return exit",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"time access blob storag access blob storag complet access blog storag log blow",
        "Challenge_readability":15.1,
        "Challenge_reading_time":71.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":69,
        "Challenge_solved_time":null,
        "Challenge_title":"Time out: Access Azure blob storage from within an Azure ML experiment",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1660.0,
        "Challenge_word_count":430,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":171651414757
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>What is the easy way to access (read and write) files in blob storage in R scripts in Azure Machine Learning?<\/p>\n\n<p>I can access files in blob storage in python scripts using azure modules, but there seems no easy way to access by R scripts.<\/p>\n\n<p>I tried to import Azure SMR as a zip file in the R script, but the importing all dependencies is very tough work,<\/p>\n\n<p><a href=\"https:\/\/github.com\/Microsoft\/AzureSMR\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Microsoft\/AzureSMR<\/a><\/p>\n\n<p>Any suggestion and help is appreciated.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1497866812267,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"access file blob storag import depend",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44627664",
        "Challenge_link_count":2,
        "Challenge_original_content":"access file blob storag access read write file blob storag access file blob storag modul access tri import smr zip file import depend tough http github com azuresmr",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"access file blob storag access file blob storag access file blob storag modul access tri import smr zip file import depend tough",
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Access files in blob storage in R scripts in Azure Machine Learning?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3803.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":171381187733
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using a Python 3.4 Jupyter notebook to load a dataset in Azure ML which is stored in the cloud as a dataset in the Azure ML project environment. But using the default template created by Azure ML, I can't load the data due to a mixed datatypes error. <\/p>\n\n<pre><code>from azureml import Workspace\nimport pandas as pd\n\nws = Workspace()\nds = ws.datasets['rossmann-train.csv']\ndf = ds.to_dataframe()\n<\/code><\/pre>\n\n<blockquote>\n  <p>\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/kernel\/<strong>main<\/strong>.py:6: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.<\/p>\n<\/blockquote>\n\n<p>In my local environment I just import the dataset as follows: <\/p>\n\n<pre><code>df = pd.read_csv('train.csv',low_memory=False)\n<\/code><\/pre>\n\n<p>But I'm not sure how to do this in azure using the <code>ds<\/code> object. <\/p>\n\n<pre><code>df = pd.read_csv(ds)\n<\/code><\/pre>\n\n<p>and<\/p>\n\n<pre><code>pd.DataFrame.from_csv(ds)\n<\/code><\/pre>\n\n<p>raise the error:<\/p>\n\n<blockquote>\n  <p>OSError: Expected file path name or file-like object, got  type<\/p>\n<\/blockquote>\n\n<p>*edit: more info on the <code>ds<\/code> object: <\/p>\n\n<pre><code>In  [1]: type(ds)\nOut [1]: azureml.SourceDataset\nIn  [2]: print (ds)\nOut [2]: rossmann-train.csv\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1498645839977,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"load dataset jupyt notebook mix datatyp default templat creat load data import dataset object",
        "Challenge_last_edit_time":1498661288456,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44800005",
        "Challenge_link_count":0,
        "Challenge_original_content":"load csv set paramet jupyt notebook jupyt notebook load dataset store cloud dataset environ default templat creat load data mix datatyp import workspac import panda workspac dataset rossmann train csv datafram home nbuser anaconda lib site packag ipython kernel dtypewarn column mix type specifi dtype option import set low memori local environ import dataset read csv train csv low memori object read csv datafram csv rais oserror file path file object type edit object type sourcedataset print rossmann train csv",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"load csv set paramet jupyt notebook jupyt notebook load dataset store cloud dataset environ default templat creat load data mix datatyp dtypewarn column mix type specifi dtype option import set local environ import dataset object rais oserror file path object type edit object",
        "Challenge_readability":7.6,
        "Challenge_reading_time":17.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"load csv and set parameters in jupyter notebook on Azure ML",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1515.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":170602160023
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>the last day or two Azure ML studio performance is terrible.<\/p>\n\n<p>It can take up to 10 mins to save a simple experiment and another 10 mins to run it.<\/p>\n\n<p>These are simple tutorial experiments, nothing massive, using at maximum 18mb of data.<\/p>\n\n<p>When i finally get the experiment to run and try to view the evaluation results, ML Studio spins for 5 mins before giving the error \"Error producing the visualization of the output \"<\/p>\n\n<p>Note this error also occasionally occurs when i am just trying to view the list of saved experiments.<\/p>\n\n<p>Im in the process of completing the Microsoft data science professional course and this is completely blocking me from making any progress.<\/p>\n\n<p>Any info on what might be wrong would be appreciated. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1499184624330,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"experienc terribl perform studio minut save run maximum data evalu studio spin minut messag list save block progress data scienc profession cours",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/44910388",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio terribl perform dai studio perform terribl save run tutori massiv maximum data final run evalu studio spin produc visual output note list save process complet data scienc profession cours complet block progress",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"studio terribl perform dai studio perform terribl save run tutori massiv maximum data final run evalu studio spin produc visual output note list save process complet data scienc profession cours complet block progress",
        "Challenge_readability":10.6,
        "Challenge_reading_time":9.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"azure ML studio terrible performance",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":478.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":170063375670
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Can i Pass Dataset as a parameter to AzureML experiment in Streaming Analytics Job? Right now im passing parameters like this ,  <\/p>\n\n<pre><code>   SELECT test (var1,var2,var3,var4,var5) as Result\n   FROM [Input-eventhub]\n<\/code><\/pre>\n\n<p>So instead of that can i pass dataset instead of this like,\n      SELECT test (datset) as Result\n       FROM [Input-eventhub]Azurestre<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1499844731840,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pass dataset paramet stream analyt job pass individu paramet",
        "Challenge_last_edit_time":1499928470636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45051184",
        "Challenge_link_count":0,
        "Challenge_original_content":"pass dataset paramet stream analyt job pass dataset paramet stream analyt job pass paramet select test var var var var var input eventhub pass dataset select test datset input eventhub azurestr",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"pass dataset paramet stream analyt job pass dataset paramet stream analyt job pass paramet pass dataset select test azurestr",
        "Challenge_readability":12.7,
        "Challenge_reading_time":5.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Can i Pass Dataset as a parameter to AzureML experiment in Streaming Analytics Job?",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":47.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":169403268160
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>See attached, is it because I am on Chrome\/Firefox? I am currently on a Macbook, so I can't really test this out on Edge browser. Why does Microsoft build something that won't work for other browsers?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/v744g.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v744g.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":7,
        "Challenge_created_time":1500141920653,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"studio chrome firefox macbook test edg browser build browser",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45121271",
        "Challenge_link_count":2,
        "Challenge_original_content":"studio chrome firefox attach chrome firefox macbook test edg browser build browser",
        "Challenge_participation_count":7,
        "Challenge_preprocessed_content":"studio attach macbook test edg browser build browser",
        "Challenge_readability":8.5,
        "Challenge_reading_time":5.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Does Azure ML Studio not working with Chrome\/Firefox?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":194.0,
        "Challenge_word_count":52,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":169106079347
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I use the <strong>Import<\/strong> module from <strong>Azure Machine Learning (Azure ML)<\/strong> to get data from <strong>DocumentDB<\/strong>.<\/p>\n<p>The import works fine.<\/p>\n<hr \/>\n<p>In the <strong>DocumentDB<\/strong> <strong>documents<\/strong> are <code>DateTime<\/code> with <code>milliseconds<\/code>, like:<\/p>\n<blockquote>\n<p>&quot;CurrentTime&quot;: &quot;2017-07-17T20:18:55.757316&quot;<\/p>\n<\/blockquote>\n<p>and in <strong>Azure ML<\/strong> it is recognized as <code>DateTime-Feature<\/code> and it is shown like this:<\/p>\n<blockquote>\n<p>2017-07-17T20:18:55<\/p>\n<\/blockquote>\n<p>The problem is, that the <code>milliseconds<\/code> are missing.<\/p>\n<p>I have tried to get the <code>milliseconds<\/code> with the <strong>Edit Metadata<\/strong> module, but it doesn't work for me. Additionally, I have tried to use <strong>R<\/strong> to convert the <code>CurrentTime<\/code> to <strong>numeric<\/strong>, like this:<\/p>\n<pre><code>time.milliseconds = as.numeric(dataset1[['CurrentTime']]);\nprint(time.milliseconds,digits=15)\n<\/code><\/pre>\n<p>But the <code>milliseconds<\/code> are still missing.<\/p>\n<hr \/>\n<p>How can I get the <strong>whole<\/strong> <code>DateTime<\/code> with <code>milliseconds<\/code>?<\/p>\n<hr \/>\n<p><strong>UPDATE:<\/strong><\/p>\n<p>The complete <strong>R<\/strong> code:<\/p>\n<pre><code>options(&quot;digits.secs&quot;=6)\n\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\ntime.milliseconds = as.numeric(dataset1[['CurrentTime']]);\ndataset &lt;- cbind(dataset1, time.milliseconds)\n\nmaml.mapOutputPort(&quot;dataset&quot;);\n<\/code><\/pre>\n<p>The result:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/YGQOB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YGQOB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<hr \/>\n<p>My suggestion is, that the <strong>Import<\/strong> module with <strong>DocumentDB<\/strong> doesn't support <code>milliseconds<\/code>!?<\/p>\n<hr \/>\n<p>For now, I upload my <code>CurrentTime<\/code> as <code>milliseconds<\/code> from epoch to the <strong>DocumentDB<\/strong>. I hope there is a better way..<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":5,
        "Challenge_created_time":1500380215003,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"import data documentdb datetim documentdb millisecond import millisecond miss tri edit metadata modul millisecond datetim millisecond import modul documentdb millisecond workaround upload currenttim millisecond epoch documentdb",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45166568",
        "Challenge_link_count":2,
        "Challenge_original_content":"import documentdb datetim millisecond import modul data documentdb import documentdb document datetim millisecond currenttim recogn datetim featur shown millisecond miss tri millisecond edit metadata modul tri convert currenttim numer time millisecond numer dataset currenttim print time millisecond digit millisecond miss datetim millisecond updat complet option digit sec dataset maml mapinputport class data frame time millisecond numer dataset currenttim dataset cbind dataset time millisecond maml mapoutputport dataset import modul documentdb millisecond upload currenttim millisecond epoch documentdb hope",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"import documentdb datetim millisecond import modul data documentdb import documentdb document currenttim recogn shown miss tri edit metadata modul tri convert numer miss updat complet import modul documentdb upload epoch documentdb hope",
        "Challenge_readability":12.1,
        "Challenge_reading_time":28.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Import from DocumentDB to Azure Machine Learning - DateTime with milliseconds",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":48.0,
        "Challenge_word_count":188,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":168867784997
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>how can I import Basemap (from mpl_toolkits.basemap) in Azure ML (in section Notebooks)? \nIs there a general way to import libraries in Azure ML? \n(current version is shown as Python 3.4.5 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:47:47) IPython: 5.1.0)<\/p>\n\n<p>pip installs the GEOS package but there are missing dependencies (and I could export the GEOS_DIR)<\/p>\n\n<blockquote>\n  <p><i> Please install the corresponding packages using your\n      systems software management system (e.g. for Debian Linux do:\n      'apt-get install libgeos-3.3.3 libgeos-c1 libgeos-dev' and\/or\n      set the environment variable GEOS_DIR to point to the location\n      where geos is installed (for example, if geos_c.h\n      is in \/usr\/local\/include, and libgeos_c is in \/usr\/local\/lib,\n      set GEOS_DIR to \/usr\/local), or edit the setup.py script\n      manually and set the variable GEOS_dir (right after the line\n      that says \"set GEOS_dir manually here\". <\/i><\/p>\n<\/blockquote>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1500984799823,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"import basemap notebook section gener import librari miss depend instal geo packag instal packag softwar set environ variabl geo dir locat geo instal",
        "Challenge_last_edit_time":1501064717696,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45302957",
        "Challenge_link_count":0,
        "Challenge_original_content":"import basemap mpl section notebook import basemap mpl toolkit basemap section notebook gener import librari version shown anaconda bit default jul ipython pip instal geo packag miss depend export geo dir instal packag system softwar debian linux apt instal libgeo libgeo libgeo dev set environ variabl geo dir locat geo instal geo usr local libgeo usr local lib set geo dir usr local edit setup manual set variabl geo dir line sai set geo dir manual",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"import basemap import basemap gener import librari version shown anaconda ipython pip instal geo packag miss depend instal packag system softwar edit manual set variabl line sai set manual",
        "Challenge_readability":8.4,
        "Challenge_reading_time":12.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to import Basemap (mpl) in Azure ML (section Notebooks)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":321.0,
        "Challenge_word_count":143,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":168263200177
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to add the column names to the input dataset using below R script.<\/p>\n\n<pre><code>dataset1 &lt;- maml.mapInputPort(1)#class: data.frame\n# Sample operation\ncols &lt;- c(\"age\",\n    \"workclass\",\n    \"fnlwgt\",\n    \"education\",\n    \"education-num\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"capital-gain\",\n    \"capital-loss\",\n    \"hours-per-week\",\n    \"native-country\",\n    \"income\")\n colnames(data.frame) &lt;- cols\n data.set = dataset1;\n maml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>But I am getting the error like below figure.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/m4vwp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/m4vwp.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Can you please tell me how to add the column names to the input dataset using R script in Machine Learning model?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1501158990353,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"add column input dataset model guidanc properli add column dataset",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45351020",
        "Challenge_link_count":2,
        "Challenge_original_content":"add column input dataset model add column input dataset dataset maml mapinputport class data frame sampl oper col ag workclass fnlwgt educ educ num marit statu occup relationship race sex capit gain capit loss hour week nativ countri incom colnam data frame col data set dataset maml mapoutputport data set figur add column input dataset model",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"add column input dataset model add column input dataset figur add column input dataset model",
        "Challenge_readability":10.8,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"how to add the column names to the input dataset using R script in Machine Learning model",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":224.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":168089009647
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to load a trained model from the trained models tab in Azure ML studio into another experiment. According to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/load-trained-model#load-the-model-into-a-new-experiment\" rel=\"nofollow noreferrer\">the docs<\/a> its possible to do as follows:<\/p>\n\n<blockquote>\n  <p>Add the Load Trained Model module to your experiment. For Data source,\n  indicate the location of the trained model, using one of the following\n  options:\n      Select Web URL via HTTP and then type the URL.\n      The URL should point to the experiment and the file representing the trained model. In Azure Machine Learning, trained models are by\n  default saved in the iLearner format.<\/p>\n<\/blockquote>\n\n<p>However, does anyone know what URL I would use for models saved in my workspace? Where is the file representing the model hosted?  <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1501166477883,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"load train model train model tab studio url model save workspac file repres model host",
        "Challenge_last_edit_time":1553111397787,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45354047",
        "Challenge_link_count":1,
        "Challenge_original_content":"load train model studio load train model train model tab studio accord doc add load train model modul data sourc locat train model option select web url http type url url file repres train model train model default save ilearn format url model save workspac file repres model host",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"load train model studio load train model train model tab studio accord doc add load train model modul data sourc locat train model option select web url http type url url file repres train model train model default save ilearn format url model save workspac file repres model host",
        "Challenge_readability":10.2,
        "Challenge_reading_time":11.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I load a trained model in Azure ML Studio?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":673.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":168081522117
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When using a LightGBM R package in Azure ML I get the following error:<\/p>\n\n<pre><code>[Error]         +++ NT HARD ERROR (0xc0000139) +++\n[Error]             Parameter 0: 0x4ad4bc8 [log2f]\n[Error]             Parameter 1: 0x4b5e2e8 [C:\\src\\lightgbm\\libs\\x64\\lib_lightgbm.dll]\n[Error]             Parameter 2: 0xffffffffc0000139\n[Error]         [FATAL] Exception: 0xc0000139 (!! HARD ERROR !!) {Params: 0x4ad4bc8, 0x4b5e2e8, 0xffffffffc0000139, 0x0}\n[Error]         [ERROR] A fatal error occurred in the running application. The application will be terminated. Code: 0xc0000139.\n<\/code><\/pre>\n\n<p>on <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/cc704588.aspx\" rel=\"nofollow noreferrer\">page<\/a> <code>0xc0000139<\/code> is described as \"Entry Point Not Found\". What does this error mean and how can I solve it?<\/p>\n\n<p>I used XGBoost in the same way in Azure ML and it worked. And it did not ask for external libraries (dlls). LightGBM on the contrary asks lots of libraries (dlls) and I think the problem is connected with the dlls, but this error does not indicate what is actually missing.<\/p>\n\n<p><strong>What I did:<\/strong> <br>Installed LightGBM R package  on a Virtual Machine with Windows Server 2016. For this I used:<\/p>\n\n<ul>\n<li>CMake<\/li>\n<li>C++ Development kit (installed almost all packages)<\/li>\n<li>RTools<\/li>\n<\/ul>\n\n<p>Included in lightgbm\\libs\\x64 are the following packages because I previously got error <code>0xc0000135<\/code> with the names of these libraries:<\/p>\n\n<ul>\n<li>msvcp140.dll<\/li>\n<li>vcomp140.dll<\/li>\n<li>vcruntime140.dll<\/li>\n<li>all api-ms-win-core-*.dll and all api-ms-win-crt-*.dll<\/li>\n<\/ul>\n\n<p>I tried to change the R version from Microsoft R Open 3.2.2 to CRAN R 3.1.0. It executes witout errors but does not execute code after library import.<\/p>\n\n<p>The full output of Azure ML R script:<\/p>\n\n<pre><code>Record Starts at UTC 08\/03\/2017 12:28:27:\n\nRun the job:\"\/dll \"LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR\" \/Output0 \"..\\..\\Result Dataset\\Result Dataset.dataset\" \/Output1 \"..\\..\\R Device\\R Device.dataset\"  \/dataset1 \"..\\..\\Dataset1\\Dataset1.csv\"    \/bundlePath \"..\\..\\Script Bundle\\Script Bundle.zip\"  \/rStreamReader \"script.R\"  \/rLibVersion \"Microsoft R Open 3.2.2\"  \/ContextFile \"..\\..\\_context\\ContextFile.txt\"\"\n[Start] Program::Main\n[Start]     DataLabModuleDescriptionParser::ParseModuleDescriptionString\n[Stop]     DataLabModuleDescriptionParser::ParseModuleDescriptionString. Duration = 00:00:00.0045866\n[Start]     DllModuleMethod::DllModuleMethod\n[Stop]     DllModuleMethod::DllModuleMethod. Duration = 00:00:00.0000221\n[Start]     DllModuleMethod::Execute\n[Start]         DataLabModuleBinder::BindModuleMethod\n[Verbose]             moduleMethodDescription LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS;RunRSNR\n[Verbose]             assemblyFullName LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca\n[Start]             DataLabModuleBinder::LoadModuleAssembly\n[Verbose]                 Loaded moduleAssembly LanguageWorker, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca\n[Stop]             DataLabModuleBinder::LoadModuleAssembly. Duration = 00:00:00.0093763\n[Verbose]             moduleTypeName Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS\n[Verbose]             moduleMethodName RunRSNR\n[Information]             Module FriendlyName : Execute R Script\n[Information]             Module Release Status : Release\n[Stop]         DataLabModuleBinder::BindModuleMethod. Duration = 00:00:00.0125213\n[Start]         ParameterArgumentBinder::InitializeParameterValues\n[Verbose]             parameterInfos count = 6\n[Verbose]             parameterInfos[0] name = dataset1 , type = Microsoft.Numerics.Data.Local.DataTable\n[Start]             DataTableCsvHandler::HandleArgumentString\n[Stop]             DataTableCsvHandler::HandleArgumentString. Duration = 00:00:00.2364734\n[Verbose]             parameterInfos[1] name = dataset2 , type = Microsoft.Numerics.Data.Local.DataTable\n[Verbose]             Set optional parameter dataset2 value to NULL\n[Verbose]             parameterInfos[2] name = bundlePath , type = System.String\n[Verbose]             parameterInfos[3] name = rStreamReader , type = System.IO.StreamReader\n[Verbose]             parameterInfos[4] name = seed , type = System.Nullable`1[System.Int32]\n[Verbose]             Set optional parameter seed value to NULL\n[Verbose]             parameterInfos[5] name = rLibVersion , type = Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS+ExecuteRScriptRVersion\n[Verbose]             Converted string 'Microsoft R Open 3.2.2' to enum of type Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS+ExecuteRScriptRVersion\n[Stop]         ParameterArgumentBinder::InitializeParameterValues. Duration = 00:00:00.3175225\n[Verbose]         Begin invoking method RunRSNR ... \n[ModuleOutput] Executing Against R 3.2.2.0\n[ModuleOutput] Executing Against R 3.2.2.0\n[Information]         Microsoft Drawbridge Console Host [Version 1.0.2108.0]\n[Error]         +++ NT HARD ERROR (0xc0000139) +++\n[Error]             Parameter 0: 0x4ad4bc8 [log2f]\n[Error]             Parameter 1: 0x4b5e2e8 [C:\\src\\lightgbm\\libs\\x64\\lib_lightgbm.dll]\n[Error]             Parameter 2: 0xffffffffc0000139\n[Error]         [FATAL] Exception: 0xc0000139 (!! HARD ERROR !!) {Params: 0x4ad4bc8, 0x4b5e2e8, 0xffffffffc0000139, 0x0}\n[Error]         [ERROR] A fatal error occurred in the running application. The application will be terminated. Code: 0xc0000139.\n[Information]         [1] 56000\n[Information]         The following files have been unzipped for sourcing in path=[\"src\"]:\n[Information]                            Name  Length                Date\n[Information]         1 data.table_1.10.4.zip 1487417 2017-07-07 16:48:00\n[Information]         2            lgb1.model   45142 2017-08-02 17:38:00\n[Information]         3            lgb2.model   83455 2017-08-02 17:38:00\n[Information]         4    lightgbm_2.0.4.zip 1350111 2017-08-03 14:26:00\n[Information]         5      magrittr_1.5.zip  152732 2017-07-07 15:34:00\n[Information]         6                R6.zip  317766 2017-08-03 10:33:00\n[Information]         Loading objects:\n[Information]           port1\n[Information]         [1] \"Loading variable port1...\"\n[Information]         package 'magrittr' successfully unpacked and MD5 sums checked\n[Information]         package 'R6' successfully unpacked and MD5 sums checked\n[Information]         package 'data.table' successfully unpacked and MD5 sums checked\n[Information]         package 'lightgbm' successfully unpacked and MD5 sums checked\n[Information]         data.table 1.10.4\n[Information]           The fastest way to learn (by data.table authors): https:\/\/www.datacamp.com\/courses\/data-analysis-the-data-table-way\n[Information]           Documentation: ?data.table, example(data.table) and browseVignettes(\"data.table\")\n[Information]           Release notes, videos and slides: http:\/\/r-datatable.com\n[Error]         Process returned with non-zero exit code -1073741511\n[Stop]     DllModuleMethod::Execute. Duration = 00:00:14.8676292\n[Critical]     Error: Error 1000: RPackage library exception: Attempting to obtain R output before invoking execution process\n[Critical]     {\"InputParameters\":{\"DataTable\":[{\"Rows\":50,\"Columns\":131,\"estimatedSize\":16928768,...........\"Errors\":\"Microsoft.Analytics.Exceptions.ErrorMapping+ModuleException: Error 1000: RPackage library exception: Attempting to obtain R output before invoking execution process ---&gt; Microsoft.Analytics.Modules.R.ErrorHandling.RInvalidOperationException: Attempting to obtain R output before invoking execution process\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.NewRWorker.GetProcessOutputs(Boolean scrub) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\TempWorkers\\\\NewRWorker.cs:line 459\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.ExecuteR(NewRWorker worker, DataTable dataset1, DataTable dataset2, IEnumerable`1 bundlePath, StreamReader rStreamReader, Nullable`1 seed) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 278\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS._RunImpl(NewRWorker worker, DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptExternalResource source, String url, ExecuteRScriptGitHubRepositoryType githubRepoType, SecureString accountToken) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\RModule.cs:line 207\\r\\n   at Microsoft.MetaAnalytics.LanguageWorker.LanguageWorkerClientRS.RunRSNR(DataTable dataset1, DataTable dataset2, String bundlePath, StreamReader rStreamReader, Nullable`1 seed, ExecuteRScriptRVersion rLibVersion) in d:\\\\_Bld\\\\8831\\\\7669\\\\Sources\\\\Product\\\\Source\\\\Modules\\\\LanguageWorker\\\\LanguageWorker.Dll\\\\EntryPoints\\\\REntryPoint.cs:line 105\\r\\n   --- End of inner exception stack trace ---\",\"Warnings\":[],\"Duration\":\"00:00:14.8605990\"}\nModule finished after a runtime of 00:00:15.3186157 with exit code -2\nModule failed due to negative exit code of -2\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1501765744790,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"entri lightgbm packag messag miss relat extern librari dll tri version open cran execut librari import instal lightgbm packag virtual window server packag lightgbm lib previou output",
        "Challenge_last_edit_time":1501769547343,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45485184",
        "Challenge_link_count":3,
        "Challenge_original_content":"entri lightgbm packag lightgbm packag paramet xadbc logf paramet xbee src lightgbm lib lib lightgbm dll paramet xffc fatal except param xadbc xbee xffc fatal run termin page entri extern librari dll lightgbm contrari librari dll connect dll miss instal lightgbm packag virtual window server cmake kit instal packag rtool lightgbm lib packag previous librari msvcp dll vcomp dll vcruntim dll api win core dll api win crt dll tri version open cran execut witout execut librari import output record start utc run job dll languagework version cultur neutral publickeytoken cefca metaanalyt languagework languageworkerclientr runrsnr output dataset dataset dataset output devic devic dataset dataset dataset dataset csv bundlepath bundl bundl zip rstreamread rlibvers open contextfil context contextfil txt start program start datalabmoduledescriptionpars parsemoduledescriptionstr stop datalabmoduledescriptionpars parsemoduledescriptionstr durat start dllmodulemethod dllmodulemethod stop dllmodulemethod dllmodulemethod durat start dllmodulemethod execut start datalabmodulebind bindmodulemethod verbos modulemethoddescript languagework version cultur neutral publickeytoken cefca metaanalyt languagework languageworkerclientr runrsnr verbos assemblyfullnam languagework version cultur neutral publickeytoken cefca start datalabmodulebind loadmoduleassembl verbos load moduleassembl languagework version cultur neutral publickeytoken cefca stop datalabmodulebind loadmoduleassembl durat verbos moduletypenam metaanalyt languagework languageworkerclientr verbos modulemethodnam runrsnr modul friendlynam execut modul releas statu releas stop datalabmodulebind bindmodulemethod durat start parameterargumentbind initializeparametervalu verbos parameterinfo count verbos parameterinfo dataset type numer data local datat start datatablecsvhandl handleargumentstr stop datatablecsvhandl handleargumentstr durat verbos parameterinfo dataset type numer data local datat verbos set option paramet dataset valu null verbos parameterinfo bundlepath type verbos parameterinfo rstreamread type streamread verbos parameterinfo seed type nullabl verbos set option paramet seed valu null verbos parameterinfo rlibvers type metaanalyt languagework languageworkerclientr executerscriptrvers verbos convert open enum type metaanalyt languagework languageworkerclientr executerscriptrvers stop parameterargumentbind initializeparametervalu durat verbos begin invok runrsnr moduleoutput execut moduleoutput execut drawbridg consol host version paramet xadbc logf paramet xbee src lightgbm lib lib lightgbm dll paramet xffc fatal except param xadbc xbee xffc fatal run termin file unzip sourc path src length date data tabl zip lgb model lgb model lightgbm zip magrittr zip zip load object port load variabl port packag magrittr successfulli unpack sum packag successfulli unpack sum packag data tabl successfulli unpack sum packag lightgbm successfulli unpack sum data tabl fastest data tabl author http datacamp com cours data analysi data tabl document data tabl data tabl browsevignett data tabl releas note video slide http datat com process return exit stop dllmodulemethod execut durat critic rpackag librari except output invok execut process critic inputparamet datat row column estimateds analyt except errormap moduleexcept rpackag librari except output invok execut process analyt modul errorhandl rinvalidoperationexcept output invok execut process metaanalyt languagework newrwork getprocessoutput boolean scrub bld sourc sourc modul languagework languagework dll tempwork newrwork line metaanalyt languagework languageworkerclientr execut newrwork worker datat dataset datat dataset ienumer bundlepath streamread rstreamread nullabl seed bld sourc sourc modul languagework languagework dll entrypoint rmodul line metaanalyt languagework languageworkerclientr runimpl newrwork worker datat dataset datat dataset bundlepath streamread rstreamread nullabl seed executerscriptexternalresourc sourc url executerscriptgithubrepositorytyp githubrepotyp securestr accounttoken bld sourc sourc modul languagework languagework dll entrypoint rmodul line metaanalyt languagework languageworkerclientr runrsnr datat dataset datat dataset bundlepath streamread rstreamread nullabl seed executerscriptrvers rlibvers bld sourc sourc modul languagework languagework dll entrypoint rentrypoint line end except stack trace warn durat modul finish runtim exit modul neg exit",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"entri lightgbm packag lightgbm packag page entri extern librari lightgbm contrari librari connect dll miss instal lightgbm packag virtual window server cmake kit rtool packag previous librari tri version open cran execut witout execut librari import output",
        "Challenge_readability":17.6,
        "Challenge_reading_time":117.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":80,
        "Challenge_solved_time":null,
        "Challenge_title":"\"Entry Point Not Found\" Error LightGBM R package in Azure",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":245.0,
        "Challenge_word_count":761,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":167482255210
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to change the knowledge base of my QnA maker service in Microsoft QnA maker site. Is it possible to upload a file to this service from my code?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1502734142850,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"upload file qna maker servic qna maker site",
        "Challenge_last_edit_time":1502922984352,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45680457",
        "Challenge_link_count":0,
        "Challenge_original_content":"upload file qna maker visual studio knowledg base qna maker servic qna maker site upload file servic",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"upload file qna maker visual studio knowledg base qna maker servic qna maker site upload file servic",
        "Challenge_readability":6.9,
        "Challenge_reading_time":2.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there any way to upload a file to Microsoft QnA Maker KB from visual studio?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1223.0,
        "Challenge_word_count":46,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":166513857150
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>My simple experiment reads from an Azure Storage Table, Selects a few columns and writes to another Azure Storage Table. This experiment runs fine on the Workspace (Let's call it workspace1).<\/p>\n\n<p>Now I need to move this experiment as is to another workspace(Call it WorkSpace2) using Powershell and need to be able to run the experiment. \nI am currently using this Library - <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\">https:\/\/github.com\/hning86\/azuremlps<\/a> <\/p>\n\n<p>Problem :<\/p>\n\n<p>When I Copy the experiment using 'Copy-AmlExperiment' from WorkSpace 1 to WorkSpace 2, the experiment and all it's properties get copied except the Azure Table Account Key. \nNow, this experiment runs fine if I manually enter the account Key for the Import\/Export Modules on studio.azureml.net<\/p>\n\n<p>But I am unable to perform this via powershell. If I Export(Export-AmlExperimentGraph) the copied experiment from WorkSpace2 as a JSON and insert the AccountKey into the JSON file and Import(Import-AmlExperiment) it into WorkSpace 2. The experiment fails to run. <\/p>\n\n<p>On PowerShell I get an \"Internal Server Error : 500\".<\/p>\n\n<p>While running on studio.azureml.net, I get the notification as \"Your experiment cannot be run because it has been updated in another session. Please re-open this experiment to see the latest version.\"<\/p>\n\n<p>Is there anyway to move an experiment with external dependencies to another workspace and run it?<\/p>\n\n<p>Edit : I think the problem is something to do with how the experiment handles the AccountKey. When I enter it manually, it's converted into a JSON array comprising of RecordKey and IndexInRecord. But when I upload the JSON experiment with the accountKey, it continues to remain the same and does not get resolved into RecordKey and IndexInRecord.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1503400023133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run studio copi workspac powershel properti copi tabl account kei manual enter import export modul tri insert accountkei json file import workspac run intern server powershel notif studio net relat accountkei",
        "Challenge_last_edit_time":1503400670630,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45815960",
        "Challenge_link_count":2,
        "Challenge_original_content":"run studio copi workspac read storag tabl select column write storag tabl run workspac workspac workspac workspac powershel run librari http github com hning copi copi amlexperi workspac workspac properti copi tabl account kei run manual enter account kei import export modul studio net perform powershel export export amlexperimentgraph copi workspac json insert accountkei json file import import amlexperi workspac run powershel intern server run studio net notif run updat session open latest version extern depend workspac run edit accountkei enter manual convert json arrai compris recordkei indexinrecord upload json accountkei remain recordkei indexinrecord",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"run studio copi workspac read storag tabl select column write storag tabl run workspac workspac powershel run librari copi workspac workspac properti copi tabl account kei run manual enter account kei modul perform powershel copi workspac json insert accountkei json file workspac run powershel intern server run notif run updat session latest extern depend workspac run edit accountkei enter manual convert json arrai compris recordkei indexinrecord upload json accountkei remain recordkei indexinrecord",
        "Challenge_readability":10.2,
        "Challenge_reading_time":23.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to run experiment on Azure ML Studio after copying from different workspace",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":613.0,
        "Challenge_word_count":279,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":165847976867
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have recently setup an azure machine learning experiment to retrain, update and execute on a daily basis using azure data factory following the example documents <\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-update-resource-activity\" rel=\"nofollow noreferrer\">Update ML using ADF<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/data-factory-azure-ml-batch-execution-activity\" rel=\"nofollow noreferrer\">Predictive Pipeline using ADF<\/a><\/li>\n<\/ul>\n\n<p>and my pipeline is setup similar to below<\/p>\n\n<pre><code>{\n  \"name\": \"RetrainAndExecutePipeline\",\n  \"properties\": {\n    \"activities\": [{\n      \"type\": \"AzureMLBatchExecution\",\n      \"typeProperties\": {\n        \"webServiceOutputs\": {\n          \"Output-TrainedModel\": \"TrainedModel\"\n        },\n        \"webServiceInputs\": {},\n        \"globalParameters\": {}\n      },\n      \"outputs\": [{\n          \"name\": \"TrainedModel\"\n        }\n      ],\n      \"policy\": {\n        \"timeout\": \"01:00:00\",\n        \"concurrency\": 1,\n        \"executionPriorityOrder\": \"NewestFirst\",\n        \"retry\": 3\n      },\n      \"scheduler\": {\n        \"frequency\": \"Day\",\n        \"interval\": 1,\n        \"offset\": \"22:00:00\",\n        \"style\": \"StartOfInterval\"\n      },\n      \"name\": \"Retrain ML Model\",\n      \"linkedServiceName\": \"TrainingService\"\n    }],\n    \"start\": \"2017-08-20T22:00:00Z\",\n    \"end\": \"9999-09-09T00:00:00Z\",\n    \"isPaused\": false,\n    \"hubName\": \"autdatafactoryml_hub\",\n    \"pipelineMode\": \"Scheduled\"\n  }\n}\n<\/code><\/pre>\n\n<p>and the TrainedModel dataset below<\/p>\n\n<pre><code>{\n  \"name\": \"TrainedModel\",\n  \"properties\": {\n      \"published\": false,\n      \"type\": \"AzureBlob\",\n      \"linkedServiceName\": \"AzureStorageLinkedService\",\n      \"typeProperties\": {\n          \"fileName\": \"trainedModel.ilearner\",\n          \"folderPath\": \"trainingoutput\",\n          \"format\": {\n              \"type\": \"TextFormat\"\n          }\n      },\n      \"availability\": {\n          \"frequency\": \"Day\",\n          \"interval\": 1,\n          \"offset\": \"22:00:00\",\n          \"style\": \"StartOfInterval\"\n      }\n  }\n}\n<\/code><\/pre>\n\n<p>I have noticed that after a training is completed, the outputs that i get into the azure blob storage from the web service output connected to the \"Train Model\" node are the ilearner file and two randomly named files with no extensions even though I haven't specified them. \none xml formated file with contents<\/p>\n\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;RuntimeInfo&gt;\n  &lt;Language&gt;DotNet&lt;\/Language&gt;\n  &lt;Version&gt;4.5.0&lt;\/Version&gt;\n&lt;\/RuntimeInfo&gt;\n<\/code><\/pre>\n\n<p>and the other with the information that you can see when you visualize the output within the azure ML experiment formatted as json as below<\/p>\n\n<pre><code>{\n  \"visualizationType\": \"learner\",\n  \"learner\": {\n    \"name\": \"LogisticRegressionClassifier\",\n    \"isTrained\": true,\n    \"settings\": {\n      \"records\": [\n        ...\n      ],\n      \"features\": [\n        {\n          \"name\": \"Setting\",\n          \"index\": 0,\n          \"elementType\": \"System.String\",\n          \"featureType\": \"String Feature\"\n        },\n        {\n          \"name\": \"Value\",\n          \"index\": 1,\n          \"elementType\": \"System.String\",\n          \"featureType\": \"String Feature\"\n        }\n      ],\n      \"name\": null,\n      \"numberOfRows\": 8,\n      \"numberOfColumns\": 2\n    },\n    \"weights\": {\n      \"records\": [\n        ...\n      ],\n      \"features\": [\n        {\n          \"name\": \"Feature\",\n          \"index\": 0,\n          \"elementType\": \"System.String\",\n          \"featureType\": \"String Feature\"\n        },\n        {\n          \"name\": \"Weight\",\n          \"index\": 1,\n          \"elementType\": \"System.Double\",\n          \"featureType\": \"Numeric Feature\"\n        }\n      ],\n      \"name\": null,\n      \"numberOfRows\": 92,\n      \"numberOfColumns\": 2\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>This json file is the one that I am interested in as I presume this is the data that shows the co-efficient values and I am wanting to track how individual co-efficient values change as I update the training model, and I have not been able to find a way to capture this output.<\/p>\n\n<p>My question is, is there a way to capture multiple outputs from a single Web service output in an azure ML experiment using azure data factory?\nOr is there a completely different way for me to resolve this?<\/p>\n\n<p>I appreciate everyones' feedback and thank you in advance<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1503524681740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"captur multipl output singl web servic output data factori complet train receiv ilearn file randomli file extens desir json output captur output track individu coeffici valu updat train model",
        "Challenge_last_edit_time":1504056306347,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45849766",
        "Challenge_link_count":2,
        "Challenge_original_content":"web servic train output setup retrain updat execut daili basi data factori document updat adf predict pipelin adf pipelin setup retrainandexecutepipelin properti activ type batchexecut typeproperti webserviceoutput output trainedmodel trainedmodel webserviceinput globalparamet output trainedmodel polici timeout concurr executionpriorityord newestfirst retri schedul frequenc dai interv offset style startofinterv retrain model linkedservicenam trainingservic start end ispaus hubnam autdatafactoryml hub pipelinemod schedul trainedmodel dataset trainedmodel properti publish type azureblob linkedservicenam azurestoragelinkedservic typeproperti filenam trainedmodel ilearn folderpath trainingoutput format type textformat frequenc dai interv offset style startofinterv notic train complet output blob storag web servic output connect train model node ilearn file randomli file extens haven specifi xml format file dotnet visual output format json visualizationtyp learner learner logisticregressionclassifi istrain set record featur set index elementtyp featuretyp featur valu index elementtyp featuretyp featur null numberofrow numberofcolumn weight record featur featur index elementtyp featuretyp featur weight index elementtyp doubl featuretyp numer featur null numberofrow numberofcolumn json file presum data effici valu track individu effici valu updat train model captur output captur multipl output singl web servic output data factori complet everyon feedback advanc",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"web servic train output setup retrain updat execut daili basi data factori document updat adf predict pipelin adf pipelin setup trainedmodel dataset notic train complet output blob storag web servic output connect train model node ilearn file randomli file extens haven specifi xml format file visual output format json json file presum data valu track individu valu updat train model captur output captur multipl output singl web servic output data factori complet everyon feedback advanc",
        "Challenge_readability":17.0,
        "Challenge_reading_time":48.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"azure machine learning web services training output",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":654.0,
        "Challenge_word_count":392,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":165723318260
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>Azure ML Import Data from Azure SQL DB connection has special character <code>]<\/code> in password.<\/p>\n\n<p>In connection wizard it is successfully connected.<\/p>\n\n<p>But when run the experiment, it gives error with no error message.<\/p>\n\n<p>After long analysis this issue is identified.<\/p>\n\n<p>Anyone knows how to escape character for Import Data functionality in Azure ML?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1504090375203,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"import data connect password charact connect wizard run messag identifi charact password escap charact import data function",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/45958534",
        "Challenge_link_count":0,
        "Challenge_original_content":"import data connect charact password import data sql connect charact password connect wizard successfulli connect run messag analysi identifi escap charact import data function",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"import data connect charact password import data sql connect charact password connect wizard successfulli connect run messag analysi identifi escap charact import data function",
        "Challenge_readability":9.7,
        "Challenge_reading_time":5.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Import Data connection has special character in password",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":165157624797
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a ML WebService on portal.azure.com. I wish to create multiple endpoints programmitically for this webservice using Powershell. \nHowever all the cmdlets available (Add-AmlWebServiceEndpoint) involve using the Old or Classic WebServices.<\/p>\n\n<p>Is there anyway to achieve this for New Azure ML WebServices<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1505905355093,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat multipl endpoint webservic powershel cmdlet classic webservic",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46320315",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat multipl endpoint powershel webservic creat webservic portal com wish creat multipl endpoint programmit webservic powershel cmdlet add amlwebserviceendpoint involv classic webservic achiev webservic",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"creat multipl endpoint powershel webservic creat webservic wish creat multipl endpoint programmit webservic powershel cmdlet involv classic webservic achiev webservic",
        "Challenge_readability":11.0,
        "Challenge_reading_time":5.17,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to create multiple endpoints through powershell for New AzureML WebServices",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":110.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":163342644907
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><a href=\"https:\/\/docs.microsoft.com\/de-de\/azure\/machine-learning\/studio\/custom-r-modules\" rel=\"nofollow noreferrer\">This Microsoft Azure documentation<\/a> shows how to author custom modules for the Azure Machine Learning Studio. There is a paragraph about returning multiple outputs from your module. Yet following the instructions I can only see data in the visualization of the first output port while the others remain empty.<\/p>\n\n<p>This is a follow-up question to <a href=\"https:\/\/stackoverflow.com\/questions\/46340959\/multiple-inputs-outputs-from-execute-r-script\">this one<\/a>. I accepted the answer there because I misinterpreted the result of the custom module I wrote - it is possible for some output ports to be empty and I hastily assumed the output to be correct. However, running the same code in RStudio does indeed generate data that should have been returned in ML Studio as well. Also, printing the data works.  <\/p>\n\n<p><strong>Minimal example:<\/strong><\/p>\n\n<p>The source files contained in the module's ZIP file:<\/p>\n\n<p>test.R<\/p>\n\n<pre><code>foo &lt;- function() {\n    require(data.table)\n    out1 &lt;- data.table(mtcars)\n    out2 &lt;- data.table(cars)\n\n    print(\"out1:\")\n    print(head(out1))\n    print(\"out2:\")\n    print(head(out2))\n\n    return(list(out1, out2))\n}\n<\/code><\/pre>\n\n<p>test.xml<\/p>\n\n<pre><code>&lt;Module name=\"Multiple outputs\"&gt;\n  &lt;Owner&gt;...&lt;\/Owner&gt;\n  &lt;Language name=\"R\" sourceFile=\"test.R\" entryPoint=\"foo\"\/&gt; \n    &lt;Ports&gt;\n      &lt;Output id=\"out_1\" name=\"out1\" type=\"DataTable\"&gt;\n        &lt;Description&gt;...&lt;\/Description&gt;\n      &lt;\/Output&gt;\n      &lt;Output id=\"out_2\" name=\"out2\" type=\"DataTable\"&gt;\n        &lt;Description&gt;...&lt;\/Description&gt;\n      &lt;\/Output&gt;   \n    &lt;\/Ports&gt;\n&lt;\/Module&gt;\n<\/code><\/pre>\n\n<p>Which yields this module that runs successfully:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tyKFa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tyKFa.png\" alt=\"Module in Azure\"><\/a><\/p>\n\n<p>The visualizations of the output however look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/QSJ98.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QSJ98.png\" alt=\"Correct visualization of output 1\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wie4o.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wie4o.png\" alt=\"Empty visualization of output 2\"><\/a><\/p>\n\n<p>Whereas the output log looks good:<\/p>\n\n<pre><code>[ModuleOutput] [1] \"out1:\"\n[ModuleOutput] \n[ModuleOutput]     mpg cyl disp  hp drat    wt  qsec vs am gear carb\n[ModuleOutput] \n[ModuleOutput] 1: 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n[ModuleOutput] \n[ModuleOutput] 2: 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n[ModuleOutput] \n[ModuleOutput] 3: 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n[ModuleOutput] \n[ModuleOutput] 4: 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n[ModuleOutput] \n[ModuleOutput] 5: 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n[ModuleOutput] \n[ModuleOutput] 6: 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n[ModuleOutput] \n[ModuleOutput] [1] \"out2:\"\n[ModuleOutput] \n[ModuleOutput]    speed dist\n[ModuleOutput] \n[ModuleOutput] 1:     4    2\n[ModuleOutput] \n[ModuleOutput] 2:     4   10\n[ModuleOutput] \n[ModuleOutput] 3:     7    4\n[ModuleOutput] \n[ModuleOutput] 4:     7   22\n[ModuleOutput] \n[ModuleOutput] 5:     8   16\n[ModuleOutput] \n[ModuleOutput] 6:     9   10\n<\/code><\/pre>\n\n<p>I think I followed the instructions from the documentation correctly.\nHas someone encountered this problem before? Are there any known solutions?<\/p>\n\n<p>Any help would be much appreciated!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506416888300,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"return multipl output modul studio instruct data visual output port remain gener data rstudio print data output port studio",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46422216",
        "Challenge_link_count":8,
        "Challenge_original_content":"multipl output modul document author modul studio paragraph return multipl output modul instruct data visual output port remain accept misinterpret modul wrote output port hastili output run rstudio gener data return studio print data minim sourc file modul zip file test foo yield modul run successfulli visual output output log moduleoutput moduleoutput moduleoutput mpg cyl disp drat qsec gear carb moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput speed dist moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput moduleoutput instruct document",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"multipl output modul document author modul studio paragraph return multipl output modul instruct data visual output port remain accept misinterpret modul wrote output port hastili output run rstudio gener data return studio print data minim sourc file modul zip file yield modul run successfulli visual output output log instruct document",
        "Challenge_readability":10.8,
        "Challenge_reading_time":45.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":39,
        "Challenge_solved_time":null,
        "Challenge_title":"Multiple outputs from custom module",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":256.0,
        "Challenge_word_count":397,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":162831111700
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Trying to set up the workbench after the MS Ignite presentation. I have a free account tied to my ancient hotmail but it says<\/p>\n\n<p>\"No experimentation account found in your Azure subscriptions\"<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1506450408683,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set workbench open messag experiment account subscript",
        "Challenge_last_edit_time":1511310570432,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46433396",
        "Challenge_link_count":0,
        "Challenge_original_content":"open workbench set workbench ignit present free account ti ancient hotmail sai experiment account subscript",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"open workbench set workbench ignit present free account ti ancient hotmail sai experiment account subscript",
        "Challenge_readability":9.3,
        "Challenge_reading_time":3.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot open Azure Machine Learning Workbench",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":485.0,
        "Challenge_word_count":37,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":162797591317
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I referred the below stack overflow query regarding installing additional R package in Azure ML. However I'am getting the error <\/p>\n\n<p>Trail 1 : Installing miniCRAN package for windows (<a href=\"https:\/\/cran.r-project.org\/web\/packages\/imputeTS\/index.html\" rel=\"nofollow noreferrer\">https:\/\/cran.r-project.org\/web\/packages\/imputeTS\/index.html<\/a>)<\/p>\n\n<p>Trail 2:  Installing ImputeTS package for windows (<a href=\"https:\/\/cran.r-project.org\/web\/packages\/miniCRAN\/index.html\" rel=\"nofollow noreferrer\">https:\/\/cran.r-project.org\/web\/packages\/miniCRAN\/index.html<\/a>)<\/p>\n\n<p><strong>I double zipped and tried as per the below stack overflow query question. But, still facing the same issue<\/strong><\/p>\n\n<p>R version i'm using : <code>CRAN 3.1.0<\/code><\/p>\n\n<p>I need to use the <code>package ImputeTS.<\/code><\/p>\n\n<p><strong>Stack overflow query link :<\/strong>\n<a href=\"https:\/\/stackoverflow.com\/questions\/27568624\/installing-additional-r-package-on-azure-ml\">Installing additional R Package on Azure ML<\/a><\/p>\n\n<p><strong>Error 1:<\/strong> <\/p>\n\n<pre><code>    Error 0063: The following error occurred during evaluation of R script:\n\n    ---------- Start of error message from R ----------\n\n    zip file 'src\/miniCRAN.zip' not found\n<\/code><\/pre>\n\n<p><strong>Error 2:<\/strong> <\/p>\n\n<pre><code>     Error 0063: The following error occurred during evaluation of R script:\n\n     ---------- Start of error message from R ----------\n\n     zip file 'src\/imputeTS.zip' not found\n<\/code><\/pre>\n\n<p><strong>R script :<\/strong><\/p>\n\n<pre><code>JCI_CO2  &lt;- maml.mapInputPort(1)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\n\n#install.packages(\"src\/imputeTS.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n#(success &lt;- library(\"imputeTS\", lib.loc = \".\", logical.return = TRUE, verbose = TRUE))\n\n #library(imputeTS)\n #library(imputeTS,lib.loc = \".\")\n\n\ninstall.packages(\"src\/miniCRAN.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n(success &lt;- library(\"miniCRAN\", lib.loc = \".\", logical.return = TRUE, verbose = TRUE))\n\nlibrary(miniCRAN)\nlibrary(miniCRAN,lib.loc = \".\")\n\nlibrary(imputeTS)\n\ndt2 &lt;- JCI_CO2 %&gt;%\n  mutate(Date.Time = mdy_hm(Date.Time)) %&gt;%\n  filter(Date.Time %in% seq(min(Date.Time), max(Date.Time), by = \"15 min\")) %&gt;%\n  complete(Date.Time = seq(min(Date.Time), max(Date.Time), by = \"15 min\")) %&gt;%\n  mutate(RA.CO2 = na.interpolation(RA.CO2)) %&gt;%\n  arrange(desc(Date.Time))\n\n\n  JCI_CO2 &lt;- data.frame(dt2)\n\n  maml.mapOutputPort(\"JCI_CO2\");\n<\/code><\/pre>\n\n<p><strong>Note :<\/strong> <em>All the rest of the packages in the code i.e dplyr, tidyr, lubridate are already part of the azure ml R package. <strong>Except ImputeTS which i am trying to install.<\/em><\/strong><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1506522360287,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal imputet packag tri state zip file packag version",
        "Challenge_last_edit_time":1516551303190,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46450439",
        "Challenge_link_count":5,
        "Challenge_original_content":"instal addit packag imputet packag stack overflow queri instal addit packag trail instal minicran packag window http cran org web packag imputet index html trail instal imputet packag window http cran org web packag minicran index html doubl zip tri stack overflow queri version cran packag imputet stack overflow queri link instal addit packag evalu start messag zip file src minicran zip evalu start messag zip file src imputet zip jci mutat date time mdy date time filter date time seq date time date time complet date time seq date time date time mutat interpol arrang desc date time jci data frame maml mapoutputport jci note rest packag dplyr tidyr lubrid packag imputet instal",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"instal addit packag stack overflow queri instal addit packag trail instal minicran packag window trail instal imputet packag window doubl zip tri stack overflow queri version stack overflow queri link instal addit packag note rest packag dplyr tidyr lubrid packag imputet instal",
        "Challenge_readability":12.7,
        "Challenge_reading_time":35.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"Installing additional R package (ImputeTS R Package) in Azure ML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":797.0,
        "Challenge_word_count":256,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":162725639713
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have scheduled the Azure ML Batch Job via Azure Data Factory to run daily at 12:00 AM UTC.<\/p>\n\n<p>Don't know what is the issue, but it is failing for every month's 3rd day, otherwise it runs perfectly.<\/p>\n\n<p>Anybody facing same issue?<\/p>\n\n<p><strong>For September<\/strong> <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/z2sBN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z2sBN.png\" alt=\"Error Log that display in Azure Data Factory for September\"><\/a><\/p>\n\n<p><strong>For October<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/skXoU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/skXoU.png\" alt=\"Error Log that display in Azure Data Factory for October\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mQ1Oy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mQ1Oy.png\" alt=\"Azure ML Batch Service Job Log\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1507015266990,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"schedul batch job data factori run month dai run perfectli dai log septemb octob assist",
        "Challenge_last_edit_time":1507036866960,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46539159",
        "Challenge_link_count":6,
        "Challenge_original_content":"batch job data factori schedul batch job data factori run daili utc month dai run perfectli anybodi septemb octob",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"batch job data factori schedul batch job data factori run daili utc month dai run perfectli anybodi septemb octob",
        "Challenge_readability":11.4,
        "Challenge_reading_time":12.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Call Azure ML Batch Job via Azure Data Factory",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":150.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":162232733010
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I tried to install the azure machine learning workbench from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/quickstart-installation\" rel=\"nofollow noreferrer\">here<\/a>. Once I double click on the downloaded MSI file, it shows the first screen about licensing terms. Once I click on Continue, it shows dependencies. When I click Install, it starts installation. It downloads Miniconda with Python 3.5.2. While trying to install asn1crypto 0.23.0, it suddenly stops and displays 'Installation fails'. I tried running the MSI file with log option but no error is reported in the log.<\/p>\n\n<p>Here are my machine details:\nWindows 10\nVersion 1709 (OS Build 17017.1000)<\/p>\n\n<p>How can I troubleshoot this?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1508820867387,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal workbench window instal process instal asncrypto log file report troubleshoot advic",
        "Challenge_last_edit_time":1511310720168,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/46902487",
        "Challenge_link_count":1,
        "Challenge_original_content":"instal workbench window tri instal workbench doubl click download msi file screen licens term click depend click instal start instal download miniconda instal asncrypto suddenli stop displai instal tri run msi file log option report log window version build troubleshoot",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"instal workbench window tri instal workbench doubl click download msi file screen licens term click depend click instal start instal download miniconda instal asn crypto suddenli stop displai instal tri run msi file log option report log window version troubleshoot",
        "Challenge_readability":6.8,
        "Challenge_reading_time":10.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Installation of Azure Machine Learning Workbench fails on Windows 10",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":497.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":160427132613
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>When trying to reference\/load a dsource or dprep file generated with a data source file from blob storage, I receive the error \"No files for given path(s)\".<\/p>\n\n<p>Tested with .py and .ipynb files.  Here's the code:<\/p>\n\n<pre><code># Use the Azure Machine Learning data source package\nfrom azureml.dataprep import datasource\n\ndf = datasource.load_datasource('POS.dsource') #Error generated here\n\n# Remove this line and add code that uses the DataFrame\ndf.head(10)\n<\/code><\/pre>\n\n<p>Please let me know what other information would be helpful. Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1509386950993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"load dsourc dprep file gener data sourc file blob storag messag state file path test ipynb file addit",
        "Challenge_last_edit_time":1511310496380,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47021644",
        "Challenge_link_count":0,
        "Challenge_original_content":"workbench file blob load dsourc dprep file gener data sourc file blob storag receiv file path test ipynb file data sourc packag dataprep import datasourc datasourc load datasourc po dsourc gener remov line add datafram head",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"workbench file blob dsourc dprep file gener data sourc file blob storag receiv file path test ipynb file",
        "Challenge_readability":7.9,
        "Challenge_reading_time":7.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Workbench File from Blob",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":324.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":159861049007
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have come across a requirement wherein the first part involves reading a blob file (i.e. .csv) present in Azure blob storage and splitting the file data into multiple files, based on the distinct combination of few columns. The second part of the requirement involves writing\/uploading the multiple files to Azure blob Storage at a separate destination folder. <\/p>\n\n<p>I am able to split the blob file into multiple files, but am not able to write\/upload the partitioned files on to azure blob storage. Is there any possibility to write the files to blob storage. Any help will be highly appreciated.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1509532865763,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"upload multipl file blob storag split blob file multipl file write upload partit file blob storag",
        "Challenge_last_edit_time":1653556441368,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47052996",
        "Challenge_link_count":0,
        "Challenge_original_content":"upload multipl file blob storag come involv read blob file csv present blob storag split file data multipl file base distinct combin column involv write upload multipl file blob storag separ destin folder split blob file multipl file write upload partit file blob storag write file blob storag highli",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"upload multipl file blob storag come involv read blob file present blob storag split file data multipl file base distinct combin column involv multipl file blob storag separ destin folder split blob file multipl file partit file blob storag write file blob storag highli",
        "Challenge_readability":8.4,
        "Challenge_reading_time":8.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"how to upload multiple files to azure blob storage using azure machine Learning",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1317.0,
        "Challenge_word_count":113,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":159715134237
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>i try to test ML Studio with some data stored in XML. However i tried a lot of things i.e. convert xml to csv, to JSON but didn't find a good way to use ML studio with this data. <\/p>\n\n<p>The problem is not the conversion itself, It's just a problem of the semi structuring. In my opinion there should be a solution like using a CSV file as an array of data or like this.<\/p>\n\n<p>What would be the best way to use XML in ML studio? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1509971338090,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"xml data studio convert xml csv json suitabl data semi structur natur advic xml studio",
        "Challenge_last_edit_time":1510039828172,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47137077",
        "Challenge_link_count":0,
        "Challenge_original_content":"xml studio test studio data store xml tri convert xml csv json studio data convers semi structur csv file arrai data xml studio",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"xml studio test studio data store xml tri convert xml csv json studio data convers semi structur csv file arrai data xml studio",
        "Challenge_readability":4.9,
        "Challenge_reading_time":5.41,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"XML to Azure ML Studio",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":490.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":159276661910
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to deploy a new webservice in Azure ML, When i click on Deploy web service [new] preview. I get the following error:<\/p>\n\n<p>Web Service deployment failed. This account does not have sufficient access to the Azure subscription that contains the Workspace. In order to deploy a Web Service to Azure, the same account must be invited to the Workspace and be given access to the Azure subscription that contains the Workspace.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/V4NMD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/V4NMD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am an account owner of this azure subscription and i was able to deploy last week with no issues.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1509994612463,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi web servic messag account suffici access subscript workspac account owner subscript deploi week",
        "Challenge_last_edit_time":1509995432963,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47143839",
        "Challenge_link_count":2,
        "Challenge_original_content":"web servic deploy deploi webservic click deploi web servic preview web servic deploy account suffici access subscript workspac order deploi web servic account invit workspac access subscript workspac account owner subscript deploi week",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic deploy deploi webservic click deploi web servic preview web servic deploy account suffici access subscript workspac order deploi web servic account invit workspac access subscript workspac account owner subscript deploi week",
        "Challenge_readability":8.6,
        "Challenge_reading_time":9.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Web Service deployment failed in Azure ML",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":406.0,
        "Challenge_word_count":109,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":159253387537
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I want to use Azure ML Web Service for a non machine learning task with Python. The goal is the following:<\/p>\n\n<p>I have a Pandas DF like this:<\/p>\n\n<pre><code>   Id   Value\n0  111  0.1\n1  222  7.3\n2  333  3.1\n3  444  5.0\n<\/code><\/pre>\n\n<p>I can query this DF successfully (what is the value of a certain row by Id?):<\/p>\n\n<pre><code>float(df.loc[pot['Id'] == 222, 'Value'])\n<\/code><\/pre>\n\n<p>Now, I want to deploy a function in Azure ML Web Service with this functionality where a function uses an uploaded data set as fix lookup table. I constructed the function which gets an Id number as argument, looks for the value in the pre-uploade dataset and gives it back as a float:<\/p>\n\n<pre><code>from azureml import services\nimport pandas as pd\n\n@services.publish(workspace_id, workspace_token)\n@services.types(id=int)\n@services.returns(float)\ndef my_func(id):\n    my_df = ws.datasets[\"uploaded_df.csv\"].to_dataframe()\n    return float(my_df.loc[cent['Id'] == id, 'Value'])\n<\/code><\/pre>\n\n<p>I can deploy it on Azure Web Services but when I try to run a test query It gets stuck (no way even to peep into the details). What is the problem here?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1510650824677,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi function web servic queri pre upload panda data frame function design argument valu data frame return tri run test queri function stuck",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47281734",
        "Challenge_link_count":0,
        "Challenge_original_content":"web servic queri panda data frame web servic task goal panda valu queri successfulli valu row loc pot valu deploi function web servic function function upload data set lookup tabl construct function argument valu pre upload dataset import servic import panda servic publish workspac workspac token servic type servic return func dataset upload csv datafram return loc cent valu deploi web servic run test queri stuck peep",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"web servic queri panda data frame web servic task goal panda queri successfulli deploi function web servic function function upload data set lookup tabl construct function argument valu dataset deploi web servic run test queri stuck",
        "Challenge_readability":7.1,
        "Challenge_reading_time":14.7,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Web Service + Python for Querying Pandas Data Frame",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":428.0,
        "Challenge_word_count":180,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":158597175323
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have stored workflows. They're trees with decision points.  Basically every point in the data is a command issued. Adding it all up is a workflow to build something from the commands.<\/p>\n\n<p>I'm trying to use azure ml to take a partially completed workflow from a user and match it against these stored workflows.  <\/p>\n\n<p>Adding to the difficulty is that I'm never sure when the user has started or stopped a workflow so it's always a Time preference match that is never perfect.<\/p>\n\n<p>Despite days of searching I can't find any information on canned algorithms that do this type of pattern matching.<\/p>\n\n<p>Can someone suggest where I might find some information on taking a data series (not numeric) and match that to a tree graph of similar values in real time?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1510664982930,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"match partial complet workflow gener dataset store workflow tree decis start stop workflow achiev perfect time prefer match can algorithm perform type pattern match",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47286717",
        "Challenge_link_count":0,
        "Challenge_original_content":"fuzzi match gener dataset store dataset store workflow tree decis data workflow build partial complet workflow match store workflow start stop workflow time prefer match perfect dai search can algorithm type pattern match data seri numer match tree graph valu time",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"fuzzi match gener dataset store dataset store workflow tree decis data workflow build partial complet workflow match store workflow start stop workflow time prefer match perfect dai search can algorithm type pattern match data seri match tree graph valu time",
        "Challenge_readability":8.2,
        "Challenge_reading_time":10.19,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Fuzzy matching of user generated dataset to stored datasets",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":65.0,
        "Challenge_word_count":140,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":158583017070
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am using Azure Machine Learning to clustering data.<\/p>\n\n<p>The input data is from an Azure SQL Database, and it works fine.\nAt the end of everything I want to write the output to a table in the same Azure SQL Database, but I get this error:<\/p>\n\n<pre><code>Error: Error 1000: AFx Library library exception: \nSql encountered an error: Login failed for user\n<\/code><\/pre>\n\n<p>Anyone any idea?\nThank you very much!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1510839240587,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"write output sql databas",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47331047",
        "Challenge_link_count":0,
        "Challenge_original_content":"write output sql databas cluster data input data sql databas end write output tabl sql databas afx librari librari except sql login idea",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"write output sql databas cluster data input data sql databas end write output tabl sql databas idea",
        "Challenge_readability":7.0,
        "Challenge_reading_time":5.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure Machine Learning Write output to Azure SQL Database",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1068.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":158408759413
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have run an Azure machine learning experiment in Studio and currently am exporting the scored results to Azure SQL.\nI want to include in the export data, the \"run timestamp\" and the model configuration as extra columns.\nThis is usefull because i can then compare the results from different runs against each other in my Azure SQL database.<\/p>\n\n<p>Anyone know how to add this data to the scored data set ?<\/p>\n\n<p>Thanks in advance,<\/p>\n\n<p>Oliver<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1511949912030,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run timestamp model configur extra column score import compar run sql databas assist add data score data set",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47550020",
        "Challenge_link_count":0,
        "Challenge_original_content":"add model paramet run timestamp score run studio export score sql export data run timestamp model configur extra column useful compar run sql databas add data score data set advanc oliv",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"add model paramet run timestamp score run studio export score sql export data run timestamp model configur extra column useful compar run sql databas add data score data set advanc oliv",
        "Challenge_readability":7.6,
        "Challenge_reading_time":6.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Add model parameters and Run timestamp to scored results",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":44.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":157298087970
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>So I've been using Azure ML Studio for a while and now Microsoft have come up with the new tool called Azure ML Workbench.  The workbench seems to be pretty low level and it seems that the majority of functions need to be hand coded in Python.<\/p>\n\n<p>So if I have an experiment in Azure ML Studio using some of the drag and drop Training methods such as Matchbox and Boosted Decision Trees.  How can I convert these to run in Azure ML Workbench ?    <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1512274371540,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"modul studio workbench workbench low level hand drag drop train studio guidanc convert run workbench",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47615177",
        "Challenge_link_count":0,
        "Challenge_original_content":"modul studio workbench studio come call workbench workbench pretti low level function hand studio drag drop train matchbox boost decis tree convert run workbench",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"modul studio workbench studio come call workbench workbench pretti low level function hand studio drag drop train matchbox boost decis tree convert run workbench",
        "Challenge_readability":7.5,
        "Challenge_reading_time":6.39,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I use machine learning modules from Azure ML Studio in Azure ML Workbench",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":192.0,
        "Challenge_word_count":99,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":156973628460
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>The storage account for this workspace has been deleted. which I have been using for my Machine Learning Studio. What should I do as when I try to save my experiment it shows that no workspace is found.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Hsb99.png\" rel=\"nofollow noreferrer\">See the Image for reference which showing storage account has been deleted<\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1512317083010,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"storag account studio workspac delet save workspac",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47620266",
        "Challenge_link_count":1,
        "Challenge_original_content":"storag account workspac delet storag account workspac delet studio save workspac imag storag account delet",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"storag account workspac delet storag account workspac delet studio save workspac imag storag account delet",
        "Challenge_readability":6.1,
        "Challenge_reading_time":5.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"The storage account for this workspace has been deleted",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":397.0,
        "Challenge_word_count":61,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":156930916990
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Deployment error to ACS - BadRequestFormat. How do I get past this? This is my nth attempt to make the tutorial work end to end <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-1\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-1<\/a>.<\/p>\n\n<p>az ml env setup -n gopenv --location westcentralus -c<\/p>\n\n<p>Subscription set to Visual Studio Premium with MSDN<\/p>\n\n<p>Continue with this subscription (Y\/n)? y<\/p>\n\n<p>Resource group gopenvrg already exists, skipping creation.<\/p>\n\n<p>creating service principal.........done<\/p>\n\n<p>Created a service principal: %s 96f6dd9e-c9d6-4856-9f8b-5426c7a757ea<\/p>\n\n<p>waiting for AAD role to propagate.done<\/p>\n\n<p>Provisioning compute resources...<\/p>\n\n<p>BadRequestFormat: The request format was invalid. Details: Updating clusters with cluster type Local is not supported<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1512317838663,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploy ac badrequestformat tutori messag updat cluster cluster type local tutori multipl time assist",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47620404",
        "Challenge_link_count":2,
        "Challenge_original_content":"webservic deploy deploy ac badrequestformat past nth tutori end end http doc com preview tutori classifi iri env setup gopenv locat westcentralu subscript set visual studio premium msdn subscript resourc group gopenvrg skip creation creat servic princip creat servic princip fdde caea wait aad role propag provis comput resourc badrequestformat request format updat cluster cluster type local",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"webservic deploy deploy ac badrequestformat past nth tutori end end env setup gopenv westcentralu subscript set visual studio premium msdn subscript resourc group gopenvrg skip creation creat servic creat servic princip wait aad role provis comput badrequestformat request format updat cluster cluster type local",
        "Challenge_readability":13.5,
        "Challenge_reading_time":13.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure machine learning webservice deployment failed",
        "Challenge_topic":"Kubernetes Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":149.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":156930161337
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a R file and I want to run the same in azureML studio. \nAfter running the codes in Rstudio I zip the r file and import it into Azure studio's datasets.I pull the dataset and Execute R script module to the experiment and attach script bundle port to the zip file. It asks for a src path which I am not sure of. When I run, it says CONNECTION NOT FOUND. <\/p>\n\n<p>What should be done to find the connection?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1512542937367,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"upload run file studio import zip file dataset src path receiv connect messag guidanc",
        "Challenge_last_edit_time":1512547937872,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47668450",
        "Challenge_link_count":0,
        "Challenge_original_content":"upload file studio run file run studio run rstudio zip file import studio dataset pull dataset execut modul attach bundl port zip file src path run sai connect connect",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"upload file studio run file run studio run rstudio zip file import studio pull dataset execut modul attach bundl port zip file src path run sai connect connect",
        "Challenge_readability":4.3,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to upload .r file into azure ml studio and run it?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":288.0,
        "Challenge_word_count":93,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":156705062633
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I am getting error when i call get_execution_role() from sagemaker in python.\nI have attached the error for the same.\n<a href=\"https:\/\/i.stack.imgur.com\/z5NGd.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/z5NGd.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I have added the SagemakerFullAccess Policy to role and user both.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1512722610010,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary":"call execut role messag attach fullaccess polici role persist",
        "Challenge_last_edit_time":1513148569783,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47710558",
        "Challenge_link_count":2,
        "Challenge_original_content":"ident role execut role attach fullaccess polici role",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"ident role attach fullaccess polici role",
        "Challenge_readability":9.4,
        "Challenge_reading_time":5.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":22.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"The current AWS identity is not a role for sagemaker?",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":13622.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":156525389990
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run any <code>az ml<\/code> function and am running into this error:<\/p>\n\n<pre><code>$ az ml -h\n\naz: error: argument _command_package: invalid choice: ml\n<\/code><\/pre>\n\n<p>I do not see any suggestion so far that gets ml to be a supported function for azure-cli. Looking if anyone can help.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1512870414807,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run function mac messag state argument packag function cli",
        "Challenge_last_edit_time":1536291432208,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47735082",
        "Challenge_link_count":0,
        "Challenge_original_content":"line mac run function run argument packag choic function cli",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"line mac run function run function",
        "Challenge_readability":8.5,
        "Challenge_reading_time":4.35,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure command line 'az ml' on Mac not working",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1311.0,
        "Challenge_word_count":58,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":156377585193
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Ive followed this this to link my comet.ml project to GitHub - <a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/blob\/master\/github-pullrequest\/README.md\" rel=\"nofollow noreferrer\">link<\/a><\/p>\n\n<p>and had some models already trained (using keras)in my project<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/L44Bi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/L44Bi.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Ive linked my GitHub account and when creating a pull request I get error <\/p>\n\n<p><strong>Cant create pull request<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zlkYy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zlkYy.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>please advise<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513514834923,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"configur creat pull request github instruct link github train model kera creat pull request receiv messag state creat pull request",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47855236",
        "Challenge_link_count":5,
        "Challenge_original_content":"configur creat pull request github iv link github link model train kera iv link github account creat pull request creat pull request",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"configur creat pull request github iv link github link model train iv link github account creat pull request creat pull request",
        "Challenge_readability":11.2,
        "Challenge_reading_time":11.09,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How to configure comet (comet.ml) to create pull requests on GitHub?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":73.0,
        "Challenge_word_count":74,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Comet",
        "Challenge_open_time":155733165077
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm using Azure Machine Learning Studio which supports version 17.0 of scikit-learn. I would like to use newer one. You can do it as described <a href=\"https:\/\/stackoverflow.com\/questions\/44593469\/how-can-certain-python-libraries-be-imported-in-azure-mllike-the-line-import-hu\">here<\/a>. But this causes name aliasing so I would have to change the name of the library which file can be found <a href=\"https:\/\/pypi.python.org\/pypi\/scikit-learn\/0.19.1\" rel=\"nofollow noreferrer\">here<\/a>. I don't know which parts of files I'm supposed to change so I can import scikit-learn as something else than <code>import sklearn<\/code> <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":7,
        "Challenge_created_time":1513767640810,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"librari file order newer version scikit studio involv alias file order import scikit import sklearn",
        "Challenge_last_edit_time":1513770548856,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47904508",
        "Challenge_link_count":2,
        "Challenge_original_content":"librari file studio version scikit newer alias librari file file suppos import scikit import sklearn",
        "Challenge_participation_count":8,
        "Challenge_preprocessed_content":"librari file studio version newer alias librari file file suppos import",
        "Challenge_readability":7.4,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Change the name of a Python library within its file",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":155480359190
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have the following code in an Execute R Module.<\/p>\n\n<hr>\n\n<pre><code># Input\ndata1 &lt;- maml.mapInputPort(1) # Qualitative with 8 variables\n\ninstall.packages(\"src\/graphics.zip\", lib.loc = \".\", repos = NULL, verbose = \nTRUE)\ninstall.packages(\"src\/grDevices.zip\", lib.loc = \".\", repos = NULL, verbose = \nTRUE)\ninstall.packages(\"src\/stats.zip\", lib.loc = \".\", repos = NULL, verbose = \nTRUE)\ninstall.packages(\"src\/utils.zip\", lib.loc = \".\", repos = NULL, verbose = \nTRUE)\ninstall.packages(\"src\/MASS.zip\", lib.loc = \".\", repos = NULL, verbose = \nTRUE)\nsuccess &lt;- library(\"MASS\", lib.loc = \".\", logical.return = TRUE, verbose = \nTRUE)\nlibrary(MASS)\n\nmca &lt;- mca(data1, nf = 10)\n\nmca1 &lt;- data.frame(mca$rs)\n\n# Output\nmaml.mapOutputPort(\"mca1\");\n<\/code><\/pre>\n\n<hr>\n\n<p>When I execute I am getting the following error:<\/p>\n\n<p>RPackage library exception: Attempting to obtain R output before invoking execution process. (Error 1000)<\/p>\n\n<p>But it is working fine in RStudio.<\/p>\n\n<p>I also have a node that does the same process and it works without errors. I have executed it several times, sometimes it has worked for me and then it has returned error.<\/p>\n\n<p>Please let me know what the issue is.<\/p>\n\n<p>With regards,<\/p>\n\n<p>Celia<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513782215720,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"relat rpackag librari except execut execut modul involv instal load packag perform multipl analysi output messag output invok execut process note rstudio return execut modul",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47908925",
        "Challenge_link_count":0,
        "Challenge_original_content":"rpackag librari except execut modul input data maml mapinputport qualit variabl instal packag src graphic zip lib loc repo null verbos instal packag src grdevic zip lib loc repo null verbos instal packag src stat zip lib loc repo null verbos instal packag src util zip lib loc repo null verbos instal packag src mass zip lib loc repo null verbos librari mass lib loc logic return verbos librari mass mca mca data mca data frame mca output maml mapoutputport mca execut rpackag librari except output invok execut process rstudio node process execut time return celia",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"rpackag librari except execut modul execut rpackag librari except output invok execut process rstudio node process execut time return celia",
        "Challenge_readability":9.1,
        "Challenge_reading_time":16.06,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"RPackage library exception (error 1000)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":149,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":155465784280
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I'm trying to access the Tensorboard for the <code>tensorflow_resnet_cifar10_with_tensorboard<\/code> example, but not sure what the url should be, the help text gives 2 options:<\/p>\n\n<blockquote>\n  <p>You can access TensorBoard locally at <a href=\"http:\/\/localhost:6006\" rel=\"noreferrer\">http:\/\/localhost:6006<\/a> or using\n  your SageMaker notebook instance proxy\/6006\/(TensorBoard will not work\n  if forget to put the slash, '\/', in end of the url). If TensorBoard\n  started on a different port, adjust these URLs to match.<\/p>\n<\/blockquote>\n\n<p>When it says access locally, does that mean the local container Sagemaker creates in AWS? If so, how do I get there?<\/p>\n\n<p>Or if I use <code>run_tensorboard_locally=False<\/code>, what should the proxy url be? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1513800473547,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary":"access tensorboard tensorflow resnet cifar tensorboard url clarif access local proxi access local creat",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47913649",
        "Challenge_link_count":2,
        "Challenge_original_content":"url tensorboard access tensorboard tensorflow resnet cifar tensorboard url text option access tensorboard local http localhost notebook instanc proxi tensorboard forget slash end url tensorboard start port adjust url match sai access local local creat run tensorboard local proxi url",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"url tensorboard access tensorboard url text option access tensorboard local notebook instanc forget slash end url tensorboard start port adjust url match sai access local local creat proxi url",
        "Challenge_readability":10.3,
        "Challenge_reading_time":10.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":11.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the SageMaker url for Tensorboard?",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":6006.0,
        "Challenge_word_count":108,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":155447526453
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>When I use data factory to update Azure ML models like the document said (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/v1\/data-factory-azure-ml-update-resource-activity\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/v1\/data-factory-azure-ml-update-resource-activity<\/a>), <\/p>\n\n<p>I faced one problem: <\/p>\n\n<pre><code>The blob reference: test\/model.ilearner has an invalid or missing file extension. Supported file extensions for this output type are: \".csv, .tsv, .arff\".'. \n<\/code><\/pre>\n\n<p>I have searched the problem and found this solution:<\/p>\n\n<blockquote>\n  <p><a href=\"https:\/\/disqus.com\/home\/discussion\/thewindowsazureproductsite\/data_factory_create_predictive_pipelines_using_data_factory_and_machine_learning_microsoft_azure\/\" rel=\"nofollow noreferrer\">https:\/\/disqus.com\/home\/discussion\/thewindowsazureproductsite\/data_factory_create_predictive_pipelines_using_data_factory_and_machine_learning_microsoft_azure\/<\/a> . <\/p>\n<\/blockquote>\n\n<p>But my linked service for the outputs of training service pipeline and update service pipeline are already different. <\/p>\n\n<p>How can I solve this problem?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1513841933500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"data factori updat model messag miss file extens blob search link servic output train servic pipelin updat servic pipelin",
        "Challenge_last_edit_time":1555513484207,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/47920098",
        "Challenge_link_count":4,
        "Challenge_original_content":"data factori updat model data factori updat model document said http doc com data factori data factori updat resourc activ blob test model ilearn miss file extens file extens output type csv tsv arff search http disqu com home thewindowsazureproductsit data factori creat predict pipelin data factori link servic output train servic pipelin updat servic pipelin",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"data factori updat model data factori updat model document said search link servic output train servic pipelin updat servic pipelin",
        "Challenge_readability":18.8,
        "Challenge_reading_time":16.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Use azure data factory Updating Azure Machine Learning models",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":262.0,
        "Challenge_word_count":96,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":155406066500
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created an instance of the usaspending.gov database in my AWS RDS. A description of this database can be found here: <a href=\"https:\/\/aws.amazon.com\/public-datasets\/usaspending\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/public-datasets\/usaspending\/<\/a><\/p>\n\n<p>The data are available as a PostgreSQL snapshot, and I would like to access the database using Python's sqlalchemy package within a Jupyter notebook within Amazon SageMaker.<\/p>\n\n<p>I tried to set up my database connection with the code below, but I'm getting a Connection timed out error. I'm pretty new to AWS and Sagemaker, so maybe I messed up my sqlalchemy engine? I think my VPC security settings are OK (it looks like they accept inbound and outbound requests).<\/p>\n\n<p>Any ideas what I could be missing?<\/p>\n\n<p>engine = create_engine(\u2018postgresql:\/\/root:password@[my endpoint]\/[DB instance]<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/v9nn5.png\" alt=\"connection timed out\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/W2G7R.png\" alt=\"VPC inbound settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JZ6AI.png\" alt=\"VPC outbound settings\"><\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1514495321563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"connect time access rd instanc usaspend gov databas sqlalchemi packag jupyt notebook vpc secur set",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48014413",
        "Challenge_link_count":8,
        "Challenge_original_content":"connect time sqlalchemi access usaspend data creat instanc usaspend gov databas rd descript databas http com public dataset usaspend data postgresql snapshot access databas sqlalchemi packag jupyt notebook tri set databas connect connect time pretti mayb mess sqlalchemi engin vpc secur set accept inbound outbound request idea miss engin creat engin postgresql root password endpoint instanc",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"connect time sqlalchemi access usaspend data creat instanc databas rd descript databas data postgresql snapshot access databas sqlalchemi packag jupyt notebook tri set databas connect connect time pretti mayb mess sqlalchemi engin vpc secur set idea miss engin instanc",
        "Challenge_readability":10.2,
        "Challenge_reading_time":18.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Connection timed out - using sqlalchemy to access AWS usaspending data",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1528.0,
        "Challenge_word_count":148,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":154752678437
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><code>QuotedPremium<\/code> column is a string feature so I need to convert it to numeric value in order to use algorithm. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/QUgm0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QUgm0.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So, for that I am using Edit Metadata module, where I specify data type to be converted is <code>Floating Point<\/code>. <\/p>\n\n<p>After I run it - I got an error:<\/p>\n\n<pre><code>Could not convert type System.String to type System.Double, inner exception message: Input string was not in a correct format.\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vOEku.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vOEku.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What am I missing here?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1515029209813,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"convert featur numer valu studio edit metadata modul specifi data type convert state input format",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48087484",
        "Challenge_link_count":4,
        "Challenge_original_content":"convert featur numer studio quotedpremium column featur convert numer valu order algorithm edit metadata modul specifi data type convert run convert type type doubl except messag input format miss",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"convert featur numer studio column featur convert numer valu order algorithm edit metadata modul specifi data type convert run miss",
        "Challenge_readability":10.5,
        "Challenge_reading_time":11.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Error converting string feature to numeric in Azure ML studio",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2072.0,
        "Challenge_word_count":102,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":154218790187
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I need to operationalize a whole R data structure (object oriented) and its methods. Below an example to understand:<\/p>\n\n<pre><code>library(data.table)\n\nmyClass = setClass(\"myClass\", contains = \"data.table\")\n\nsource('.\/DB.r')\nsource('.\/operators.R')\n<\/code><\/pre>\n\n<p>My structure inherits from data.table, is populated with data from the DB and has some methods overloaded, and also custom. This works fine in R-SQL Server<\/p>\n\n<p>My problem now is to publish it as a service. As far as what i've seen in <a href=\"https:\/\/docs.microsoft.com\/en-us\/machine-learning-server\/operationalize\/how-to-deploy-web-service-publish-manage-in-r#standard-workflow-examples\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/machine-learning-server\/operationalize\/how-to-deploy-web-service-publish-manage-in-r#standard-workflow-examples<\/a> all functions have to be in the same file (other scripts cannot contain functions; they don't get published). For example, below it is mandatory that <strong>ans<\/strong> is assigned with the sum result so that it is returned as the service output <code>api &lt;- publishService( ..., inputs = list(hp = \"numeric\", wt = \"numeric\"), outputs = list(ans = \"numeric\"), ... )<\/code><\/p>\n\n<pre><code># separate script loaded in main file\nt2 &lt;- function(a, b) { sum(a, b) }\nans &lt;- t2(hp, wt)\n<\/code><\/pre>\n\n<p>but it can't be part of a function. The result will be null if it is.<\/p>\n\n<p>So my question is: can I upload the files to the server and load them in a session and create an instance of <code>myClass<\/code> and build the service functions on top of that? This would be to have an object in memory with all its methods and invoke them via wrapper service fuctions, so that they work in a REST way.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1515077017740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"operation data structur servic structur inherit data tabl popul data overload tri publish servic function file structur upload file server load session creat instanc structur build servic function",
        "Challenge_last_edit_time":1515077841180,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48097646",
        "Challenge_link_count":2,
        "Challenge_original_content":"operation modul file web server server operation data structur object orient librari data tabl myclass setclass myclass data tabl sourc sourc oper structur inherit data tabl popul data overload sql server publish servic http doc com server operation deploi web servic publish standard workflow function file function publish mandatori an assign sum return servic output api publishservic input list numer numer output list an numer separ load file function sum an function null upload file server load session creat instanc myclass build servic function object memori invok wrapper servic fuction rest",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"operation modul file web server operation data structur structur inherit popul data overload server publish servic function file mandatori an assign sum return servic output function null upload file server load session creat instanc build servic function object memori invok wrapper servic fuction rest",
        "Challenge_readability":11.4,
        "Challenge_reading_time":23.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"How to operationalize module with several files in R web server (Machine learning server)?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":62.0,
        "Challenge_word_count":235,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":154170982260
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm trying to invoke the endpoint from the <code>tensorflow_iris_dnn_classifier_using_estimators<\/code> provided in the sample notesbooks. In the sample, it invokes the endpoint using the same python object generated in the deployment process. In a large system, I need to know how to invoke this endpoint without this object and possibly in different languages. This is what I've tried:<\/p>\n\n<pre><code>import struct\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\n\nquery = [6.4, 3.2, 4.5, 1.5]\nbuf = struct.pack('%sf' % len(query), *query)\n\nresponse = client.invoke_endpoint(\n    EndpointName='sagemaker-tensorflow-py2-cpu-2018-01-16-18-22-54-458',\n    Body=buf\n)\n<\/code><\/pre>\n\n<p>What are I doing wrong? I get the following error from cloudwatch:<\/p>\n\n<pre><code>[2018-01-16 19:51:21,091] ERROR in serving: 'utf8' codec can't decode byte 0xcd in position 0: invalid continuation byte\n2018-01-16 19:51:21,091 ERROR - model server - 'utf8' codec can't decode byte 0xcd in position 0: invalid continuation byte\n10.32.0.2 - - [16\/Jan\/2018:19:51:21 +0000] \"POST \/invocations HTTP\/1.1\" 500 0 \"-\" \"AHC\/2.0\"\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516132400243,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok iri endpoint sampl notebook sdk invok endpoint object gener deploy process languag relat utf codec invok endpoint",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48288843",
        "Challenge_link_count":0,
        "Challenge_original_content":"invok iri endpoint sampl notebook sdk invok endpoint tensorflow iri dnn classifi estim sampl notesbook sampl invok endpoint object gener deploy process larg invok endpoint object languag tri import struct import boto client boto client runtim queri buf struct pack len queri queri respons client invok endpoint endpointnam tensorflow cpu bodi buf cloudwatch serv utf codec decod byte xcd posit byte model server utf codec decod byte xcd posit byte jan invoc http ahc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"invok iri endpoint sdk invok endpoint sampl notesbook sampl invok endpoint object gener deploy process larg invok endpoint object languag tri cloudwatch",
        "Challenge_readability":9.2,
        "Challenge_reading_time":15.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"How to invoke the Iris endpoint in the sample-notebooks for Amazon SageMaker using SDK",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":764.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":153115599757
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I did some experimentation with aws sagemaker, and the download time of large data sets from S3 is very problematic, especially when the model is still in development, and you want some kind of initial feedback relatively fast<\/p>\n\n<p>Is there some kind of local storage or other way to speed things up?<\/p>\n\n<p><strong>EDIT<\/strong>\nI refer to the batch training service, that allows you to submit a job as a docker container.<\/p>\n\n<p>While this service is intended for already validated jobs that typically run for a long time (which makes the download time less significant) there's still a need for quick feedback<\/p>\n\n<ol>\n<li><p>There's no other way to do the \"integration\" testing of your job with the sagemaker infrastructure (configuration files, data files, etc.)<\/p><\/li>\n<li><p>When experimenting with different variations to the model, it's important to be able to get initial feedback relatively fast<\/p><\/li>\n<\/ol>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1516273838943,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"download time larg data set problemat model initi feedback rel fast speed process local storag highlight quick feedback variat model import integr test infrastructur",
        "Challenge_last_edit_time":1578086688792,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48319893",
        "Challenge_link_count":0,
        "Challenge_original_content":"persist local storag model train experiment download time larg data set problemat model initi feedback rel fast local storag speed edit batch train servic allow submit job docker servic intend job typic run time download time signific quick feedback integr test job infrastructur configur file data file variat model import initi feedback rel fast",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"persist local storag model train experiment download time larg data set problemat model initi feedback rel fast local storag speed edit batch train servic allow submit job docker servic intend job typic run time quick feedback integr test job infrastructur variat model import initi feedback rel fast",
        "Challenge_readability":15.4,
        "Challenge_reading_time":12.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":6.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there some kind of persistent local storage in aws sagemaker model training?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":5129.0,
        "Challenge_word_count":155,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":152974161057
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a training model with k-means and ALG algorithm in sparkR language.\nI wanted to deploy the model throw AWS Sagemaker service.\nI have ran some inbuilt examples, which uses conda_python3 language, but how it is possible with SparkR .?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516596093847,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi train model creat alg algorithm sparkr languag servic tri run inbuilt conda languag sparkr",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48374786",
        "Challenge_link_count":0,
        "Challenge_original_content":"deploi model train sparkr servic creat train model alg algorithm sparkr languag deploi model throw servic ran inbuilt conda languag sparkr",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"deploi model train sparkr servic creat train model alg algorithm sparkr languag deploi model throw servic ran inbuilt languag sparkr",
        "Challenge_readability":7.1,
        "Challenge_reading_time":3.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to deploy model trained with SparkR in sagemaker service?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":378.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":152651906153
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained and deployed a model in AWS Sagemaker, Now I am trying to invoke the endpoint with client as c# .NET.\nIn the below code, it seems, I am getting errors because of invalid value of Body parameter.<\/p>\n\n<pre><code>AmazonSageMakerRuntimeClient aawsClient = new AmazonSageMakerRuntimeClient();\nAmazon.SageMakerRuntime.Model.InvokeEndpointRequest request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest();\nrequest.EndpointName = \"sagemaker-mxnet-py2-cpu-2018-01-23-07-04-11\";\nrequest.Accept = \"text\/csv\";            \nrequest.ContentType = \"text\/csv\";\n\/\/request.Body = compressedMemStream;\nAmazon.SageMakerRuntime.Model.InvokeEndpointResponse resposnse = aawsClient.InvokeEndpoint(request);\n<\/code><\/pre>\n\n<p>I have tried by passing a MemoryStream which written with a '.gz' file or with '.jpeg' file. By executing InvokeEndPoint(), Getting error as: \"<strong>unable to evaluate payload provided<\/strong>\" <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1516700568607,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok api net train deploi model invok endpoint client net valu bodi paramet tri pass memorystream written file jpeg file messag state payload evalu",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48398509",
        "Challenge_link_count":0,
        "Challenge_original_content":"invok api net train deploi model invok endpoint client net valu bodi paramet amazonruntimecli aawsclient amazonruntimecli runtim model invokeendpointrequest request runtim model invokeendpointrequest request endpointnam mxnet cpu request accept text csv request contenttyp text csv request bodi compressedmemstream runtim model invokeendpointrespons resposns aawsclient invokeendpoint request tri pass memorystream written file jpeg file execut invokeendpoint evalu payload",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"invok api net train deploi model invok endpoint client net valu bodi paramet tri pass memorystream written file file execut invokeendpoint evalu payload",
        "Challenge_readability":14.5,
        "Challenge_reading_time":12.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Invoke AWS Sagemaker API with c# .NET?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":743.0,
        "Challenge_word_count":97,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":152547431393
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>Hi I'm new to Azure ML but really am interested in what it has to offer.<\/p>\n\n<p>I have built a model using R code and Azure ML, deployed it as a web service and finally using the web app template I have converted it into an app service. However when I test it within the app it gives me the following error,<\/p>\n\n<blockquote>\n  <p>\"<em>{\"error\":{\"code\":\"LibraryExecutionError\",\"message\":\"Module execution\n  encountered an internal library\n  error.\",\"details\":[{\"code\":\"FailedToEvaluateRScript\",\"target\":\"Execute\n  R Script Piped (RPackage)\",\"message\":\"The following error occurred\n  during evaluation of R script: R_tryEval: return error: Error in\n  size(x[, pos]) : \\n error in evaluating the argument 'x' in selecting\n  a method for function 'size': Error in validObject(x, complete = TRUE)\n  : \\n invalid class \\\"itemMatrix\\\" object: item labels not\n  unique\\n\"}]}}<\/em> \"<\/p>\n<\/blockquote>\n\n<p>Which I interpret as saying that it cannot read the R script. I find this confusing as when I test it as a web service it works perfect. Can you please adviseme what I might be doing wrong?<\/p>\n\n<p>Many thanks. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1516811177980,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"test model web app templat messag modul execut intern librari evalu class itemmatrix object uniqu item label model perfectli test web servic",
        "Challenge_last_edit_time":1516830201807,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48427215",
        "Challenge_link_count":0,
        "Challenge_original_content":"web app templat built model deploi web servic final web app templat convert app servic test app libraryexecutionerror messag modul execut intern librari failedtoevaluaterscript target execut pipe rpackag messag evalu tryeval return size po evalu argument select function size validobject complet class itemmatrix object item label uniqu interpret sai read test web servic perfect advisem",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"web app templat built model deploi web servic final web app templat convert app servic test app libraryexecutionerror messag modul execut intern librari pipe messag evalu return size evalu argument select function size validobject class object item label interpret sai read test web servic perfect advisem",
        "Challenge_readability":11.3,
        "Challenge_reading_time":14.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Web App Template Error",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":113.0,
        "Challenge_word_count":165,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":152436822020
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have created a \"Blank Jupyter Notebook\" project in Azure ML Workbench. When I try to run the Sample notebook found in the project, I get this error message:<\/p>\n\n<pre><code>ERROR:root:Line magic function `%azureml` not found.\n<\/code><\/pre>\n\n<p>What is missing?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517289481150,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"messag state line magic function run sampl notebook blank jupyt notebook workbench assist identifi miss",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48514346",
        "Challenge_link_count":0,
        "Challenge_original_content":"root line magic function creat blank jupyt notebook workbench run sampl notebook messag root line magic function miss",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"root line magic function creat blank jupyt notebook workbench run sampl notebook messag miss",
        "Challenge_readability":6.4,
        "Challenge_reading_time":4.07,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"ERROR:root:Line magic function `%azureml` not found?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":259.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":151958518850
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have created an applicataion in c#, where I need to put some data on S3 bucket, and to Invoke AWS sagemaker APIs.\nSince the same Amazon.RegionEndPoint class exists in both the references, it is giving below error.<\/p>\n\n<blockquote>\n  <p>The type 'RegionEndpoint' exists in both 'AWSSDK.Core,\n  Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604' and\n  'AWSSDK, Version=2.3.55.2<\/p>\n<\/blockquote>\n\n<p>Basically I am trying to upload files on AWS S3, following code I have used.<\/p>\n\n<pre><code>AmazonS3Client s3Client = new AmazonS3Client(_AWS_ACCESS_KEY_ID, _AWS_SECRETE_ACCESS_KEY, Amazon.RegionEndpoint.USEast2);\n PutObjectRequest request = new PutObjectRequest\n  {\n    BucketName = _BucketName,\n    Key = i_sDestFileName,\n    FilePath = i_sSourceFilePath,\n    ContentType = \"text\/plain\"\n  };\n  s3Client.PutObject(request);\n<\/code><\/pre>\n\n<p>It is working fine on a single application, but when I integrated code with Sagemaker API invokation, the conflict occurs for Amazon.RegionEndpoint.USEast2.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1517316966157,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"conflict regionendpoint class upload file invok api class awssdk core awssdk singl conflict integr api invoc",
        "Challenge_last_edit_time":1517391033467,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48522013",
        "Challenge_link_count":0,
        "Challenge_original_content":"conflict regionendpoint creat applicataion data bucket invok api regionendpoint class type regionendpoint awssdk core version cultur neutral publickeytoken cfe awssdk version upload file amazonscli sclient amazonscli access kei secret access kei regionendpoint useast putobjectrequest request putobjectrequest bucketnam bucketnam kei sdestfilenam filepath ssourcefilepath contenttyp text plain sclient putobject request singl integr api invok conflict regionendpoint useast",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"conflict creat applicataion data bucket invok api class type regionendpoint cultur neutral publickeytoken awssdk upload file singl integr api invok conflict",
        "Challenge_readability":13.3,
        "Challenge_reading_time":13.69,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting Conflict error for Amazon.RegionEndpoint with Sagemaker",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2527.0,
        "Challenge_word_count":116,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":151931033843
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am executing a Python-Tensorflow script on Amazon Sagemaker.  I need to checkpoint my model to the S3 instance I am using, but I can't find out how to do this without using the Sagemake Tensorflow version.<\/p>\n\n<p>How does one checkpoint to an S3 instance without using the Sagemaker TF version?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1517328237857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"checkpoint tensorflow model instanc tensorflow version",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48525733",
        "Challenge_link_count":0,
        "Challenge_original_content":"tensorflow checkpoint execut tensorflow checkpoint model instanc sagemak tensorflow version checkpoint instanc version",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"tensorflow checkpoint execut checkpoint model instanc sagemak tensorflow version checkpoint instanc version",
        "Challenge_readability":7.4,
        "Challenge_reading_time":4.08,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"TensorFlow Checkpoints to S3",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":925.0,
        "Challenge_word_count":54,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":151919762143
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a model which was trained locally, then transfered to AWS ECS. I would like to deploy it to Sagemaker.<\/p>\n\n<p>Currently, I do:<\/p>\n\n<pre><code>from sagemaker.estimator import Estimator\nmodel = Estimator(image,\n                  role, 1, 'ml.c4.2xlarge',\n                  output_path=\"s3:\/\/{}\/output\".format(sess.default_bucket()),\n                  sagemaker_session=sess)\n<\/code><\/pre>\n\n<p>But when I call<\/p>\n\n<pre><code>from sagemaker.predictor import csv_serializer\npredictor = agent.deploy(1, 'ml.t2.medium', serializer=csv_serializer)\n<\/code><\/pre>\n\n<p>I get:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-5-0ca9477e4acb&gt; in &lt;module&gt;()\n      1 from sagemaker.predictor import csv_serializer\n----&gt; 2 predictor = model.deploy(1, 'ml.t2.medium', serializer=csv_serializer)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, endpoint_name, **kwargs)\n    177         \"\"\"\n    178         if not self.latest_training_job:\n--&gt; 179             raise RuntimeError('Estimator has not been fit yet.')\n    180         endpoint_name = endpoint_name or self.latest_training_job.name\n    181         self.deploy_instance_type = instance_type\n\nRuntimeError: Estimator has not been fit yet.\n<\/code><\/pre>\n\n<p>But it has been fit... just not on Sagemaker. How can I overcome this problem?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1517387923450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi local train model ec runtim deploi model model fit overcom",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48537811",
        "Challenge_link_count":0,
        "Challenge_original_content":"deploi model ec model train local transfer ec deploi estim import estim model estim imag role xlarg output path output format sess default bucket session sess predictor import csv serial predictor agent deploi medium serial csv serial runtimeerror traceback predictor import csv serial predictor model deploi medium serial csv serial anaconda env lib site packag estim deploi initi instanc count instanc type endpoint kwarg latest train job rais runtimeerror estim fit endpoint endpoint latest train job deploi instanc type instanc type runtimeerror estim fit fit overcom",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"deploi model ec model train local transfer ec deploi overcom",
        "Challenge_readability":13.3,
        "Challenge_reading_time":18.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I deploy an ML model from ECS to Sagemaker?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":792.0,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":151860076550
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have a stream analytics job in azure is integrated with Machine Learning(ML). \nI want store the data to a table in storage. My query is:<\/p>\n\n<pre><code>WITH machinelearning\n     AS (SELECT \n             [IoTHub].[ConnectionDeviceId] AS [DeviceId]\n           , [ventEnqueuedUtcTime]\n           , [temperature]\n           , [humidity]\n           , [machinelearning]([temperature], [humidity]) AS [result]\n         FROM [MachineLearningInput])\n\n     SELECT \n         [System].Timestamp time\n       , [DeviceId]\n       , [EventEnqueuedUtcTime]\n       , CAST([result].[temperature] AS FLOAT) AS [temperature]\n       , CAST([result].[humidity] AS FLOAT) AS [humidity]\n       , CAST([result].[Scored Probabilities] AS FLOAT) AS 'probabalities of rain'\n     INTO \n         [MachineLearningOutput]\n     FROM [machinelearning];\n<\/code><\/pre>\n\n<p>Without the function <strong>machinelearning(temperature, humidity)<\/strong> this query works fine.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1517392239177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"stream analyt job integr queri function save data tabl storag queri function",
        "Challenge_last_edit_time":1517395056143,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48539113",
        "Challenge_link_count":0,
        "Challenge_original_content":"stream analyt queri function save data tabl storag stream analyt job integr store data tabl storag queri machinelearn select iothub connectiondeviceid deviceid ventenqueuedutctim temperatur humid machinelearn temperatur humid machinelearninginput select timestamp time deviceid eventenqueuedutctim cast temperatur temperatur cast humid humid cast score probabl probab rain machinelearningoutput machinelearn function machinelearn temperatur humid queri",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"stream analyt queri function save data tabl storag stream analyt job integr store data tabl storag queri function machinelearn queri",
        "Challenge_readability":15.5,
        "Challenge_reading_time":11.47,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure stream analytics query with ML function is not saving data to table storage",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":124.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":151855760823
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>My sagemaker endpoint has a \/ping and according to AWS Cloudwatch it gets pinged about every 5 seconds:<\/p>\n\n<pre><code>10.32.0.1 - - [01\/Feb\/2018:08:08:35 +0000] \"GET \/ping HTTP\/1.1\" 200 1 \"-\" \"AHC\/2.0\"\n<\/code><\/pre>\n\n<p>However, I don't see what would happen if this ping would fail. Where can I configure the health check?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1517472690400,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"health endpoint endpoint ping ping ping guidanc configur health",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48558057",
        "Challenge_link_count":0,
        "Challenge_original_content":"add health endpoint endpoint ping accord cloudwatch ping feb ping http ahc ping configur health",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"add health endpoint endpoint ping accord cloudwatch ping ping configur health",
        "Challenge_readability":4.6,
        "Challenge_reading_time":4.77,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I add a health check to a Sagemaker Endpoint?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3461.0,
        "Challenge_word_count":57,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":151775309600
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Thanks by advance for your help to solve this issue.\nI trained a model on Sagemaker. This is a TensorFlow estimator taking images as input, computing high-level features (ie bottlenecks) with InceptionV3, then using a dense layer to predict new classes.<\/p>\n\n<p>It kinda works: I can train it, serve it, and predict new images ONE AFTER ANOTHER.<\/p>\n\n<p>Now I'd like to predict a whole batch of images at once, in one unique HTTP call \/ predict() call. How?<\/p>\n\n<p>Here is how I do:<\/p>\n\n<pre><code>from IPython.display import Image\nimport numpy as np\nfrom keras.preprocessing import image\n\nestimator = TensorFlow(entry_point=..., ...)\nestimator.fit(train_data_location)\npredictor = estimator.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n\nimage_list = [\n    'e9bfa679-31bb-464e-9d9f-3bdb0ef9c121.jpeg',  # 131\n    'b27880e1-6de8-43cf-a684-bb02aef1e44b.jpeg',  # 170\n]\ndirectory = '\/path\/to\/dir\/'\nimages = np.empty((len(image_list), 299, 299, 3), dtype=np.float32)\n# for filename in image_list:\nfor i,filename in enumerate(image_list):\n    path = os.path.join(directory, filename)\n    Image(path)\n    img = image.load_img(path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    images[i] = x\n\nprint(images.shape)\n# to_send = images[-1]  # works for a unique image\nto_send = images  # doesn't work for a batch of images\n# some other attempts that did not work\n# to_send = images.tolist()\n# to_send = [images[0].tolist(), images[1].tolist()]\nprint(np.shape(to_send))\n\npredict_response = predictor.predict(to_send)\nprint('The model predicted the following classes: \\n{}'.format(\n    predict_response['outputs']['classes']['int64Val']))\n<\/code><\/pre>\n\n<p>This fires the following results:<\/p>\n\n<blockquote>\n  <p>(2, 299, 299, 3)<\/p>\n  \n  <p>(2, 299, 299, 3)  # Notice here the shape of what I send. So why does it complain about the shape [1,2,299,299,3] in the logs below ??<\/p>\n  \n  <p>ModelError: An error occurred\n  (ModelError) when calling the InvokeEndpoint operation: Received\n  server error (500) from model with message \"\". See\n  <a href=\"https:\/\/eu-west-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-py2-cpu-2018-02-05-16-48-38-496\" rel=\"nofollow noreferrer\">https:\/\/eu-west-1.console.aws.amazon.com\/cloudwatch\/home?region=eu-west-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-tensorflow-py2-cpu-2018-02-05-16-48-38-496<\/a>\n  in account 047562184710 for more information<\/p>\n<\/blockquote>\n\n<p>So here are the logs from AWS:<\/p>\n\n<pre><code># [2018-02-06 09:29:20,937] ERROR in serving: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input must be 4-dimensional[1,2,299,299,3]\n# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](ExpandDims, ResizeBilinear\/size)]]\")\n# Traceback (most recent call last):\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/container_support\/serving.py\", line 161, in _invoke\n# self.transformer.transform(content, input_content_type, requested_output_content_type)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/serve.py\", line 255, in transform\n# return self.transform_fn(data, content_type, accepts), accepts\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/serve.py\", line 180, in f\n# prediction = self.predict_fn(input)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/serve.py\", line 195, in predict_fn\n# return self.proxy_client.request(data)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/proxy_client.py\", line 51, in request\n# return request_fn(data)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/proxy_client.py\", line 79, in predict\n# result = stub.Predict(request, self.request_timeout)\n# File \"\/usr\/local\/lib\/python2.7\/dist-packages\/grpc\/beta\/_client_adaptations.py\", line 310, in __call__\n# self._request_serializer, self._response_deserializer)\n# File \"\/usr\/local\/lib\/python2.7\/dist-packages\/grpc\/beta\/_client_adaptations.py\", line 196, in _blocking_unary_unary\n# raise _abortion_error(rpc_error_call)\n# AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input must be 4-dimensional[1,2,299,299,3]\n# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](ExpandDims, ResizeBilinear\/size)]]\")\n# 2018-02-06 09:29:20,937 ERROR - model server - AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input must be 4-dimensional[1,2,299,299,3]\n# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](ExpandDims, ResizeBilinear\/size)]]\")\n# Traceback (most recent call last):\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/container_support\/serving.py\", line 161, in _invoke\n# self.transformer.transform(content, input_content_type, requested_output_content_type)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/serve.py\", line 255, in transform\n# return self.transform_fn(data, content_type, accepts), accepts\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/serve.py\", line 180, in f\n# prediction = self.predict_fn(input)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/serve.py\", line 195, in predict_fn\n# return self.proxy_client.request(data)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/proxy_client.py\", line 51, in request\n# return request_fn(data)\n# File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/tf_container\/proxy_client.py\", line 79, in predict\n# result = stub.Predict(request, self.request_timeout)\n# File \"\/usr\/local\/lib\/python2.7\/dist-packages\/grpc\/beta\/_client_adaptations.py\", line 310, in __call__\n# self._request_serializer, self._response_deserializer)\n# File \"\/usr\/local\/lib\/python2.7\/dist-packages\/grpc\/beta\/_client_adaptations.py\", line 196, in _blocking_unary_unary\n# raise _abortion_error(rpc_error_call)\n# AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input must be 4-dimensional[1,2,299,299,3]\n# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](ExpandDims, ResizeBilinear\/size)]]\")\n# [2018-02-06 09:29:20,956] ERROR in serving: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input must be 4-dimensional[1,2,299,299,3]\n# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](ExpandDims, ResizeBilinear\/size)]]\")\n# 2018-02-06 09:29:20,956 ERROR - model server - AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input must be 4-dimensional[1,2,299,299,3]\n# #011 [[Node: ResizeBilinear = ResizeBilinear[T=DT_FLOAT, _output_shapes=[[1,299,299,3]], align_corners=false, _device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](ExpandDims, ResizeBilinear\/size)]]\")\n# 10.32.0.1 - - [06\/Feb\/2018:09:29:20 +0000] \"POST \/invocations HTTP\/1.1\" 500 0 \"-\" \"AHC\/2.0\"\n<\/code><\/pre>\n\n<p>BTW, I experience the same problem if I load the images with PIL instead of Keras:<\/p>\n\n<pre><code>image = Image.open(path)\nimage_array = np.array(image)\n<\/code><\/pre>\n\n<p>And here is the code on server side:<\/p>\n\n<pre><code>def serving_input_fn(params):\n\"\"\" See https:\/\/www.tensorflow.org\/programmers_guide\/\nsaved_model#using_savedmodel_with_estimators\nand\nSee https:\/\/github.com\/aws\/sagemaker-python-sdk#creating-a-serving_input_fn\nand https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/\ntf-training-inference-code-template.html\n\"\"\"\n# Download InceptionV3 if need be, in order to \n# compute high level features (called bottleneck here),\n# which are then fed into the model\nmodel_dir = '.\/pretrained_model\/'\nmaybe_download_and_extract(params['data_url'],\n                           dest_directory=model_dir)\nmodel_path = os.path.join(model_dir, params['model_file_name'])\nwith tf.gfile.FastGFile(model_path, 'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    bottleneck_tensor, resized_input_tensor, input_tensor = (\n        tf.import_graph_def(\n            graph_def,\n            name='',\n            input_map=None,\n            return_elements=[\n                params['bottleneck_tensor_name'],\n                params['resized_input_tensor_name'],\n                'DecodeJpeg:0',\n            ]))\nreturn tf.estimator.export.ServingInputReceiver(bottleneck_tensor, {\n    INPUT_TENSOR_NAME: input_tensor\n})\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1517912382210,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"train model tensorflow estim inceptionv predict class imag predict imag predict batch imag uniqu http predict tri send batch imag model messag state input dimension tri load imag pil kera persist log abortionerror statuscod argument",
        "Challenge_last_edit_time":1525630189592,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48640599",
        "Challenge_link_count":5,
        "Challenge_original_content":"predict batch imag model advanc train model tensorflow estim imag input comput high level featur bottleneck inceptionv dens layer predict class kinda train serv predict imag predict batch imag uniqu http predict ipython displai import imag import numpi kera preprocess import imag estim tensorflow entri estim fit train data locat predictor estim deploi initi instanc count instanc type xlarg imag list ebfa bdbefc jpeg bbaefeb jpeg directori path dir imag len imag list dtype filenam imag list filenam enumer imag list path path directori filenam imag path img imag load img path target size imag img arrai img imag print imag shape send imag uniqu imag send imag batch imag send imag tolist send imag tolist imag tolist print shape send predict respons predictor predict send print model predict class format predict respons output class intval fire notic shape send complain shape log modelerror modelerror call invokeendpoint oper receiv server model messag http consol com cloudwatch home region logeventview group endpoint tensorflow cpu account log serv abortionerror statuscod argument input dimension node resizebilinear resizebilinear output shape align corner devic job localhost replica task devic cpu expanddim resizebilinear size traceback file opt lib site packag serv line invok transform transform input type request output type file opt lib site packag serv line transform return transform data type accept accept file opt lib site packag serv line predict predict input file opt lib site packag serv line predict return proxi client request data file opt lib site packag proxi client line request return request data file opt lib site packag proxi client line predict stub predict request request timeout file usr local lib dist packag grpc beta client adapt line request serial respons deseri file usr local lib dist packag grpc beta client adapt line block unari unari rais abort rpc abortionerror abortionerror statuscod argument input dimension node resizebilinear resizebilinear output shape align corner devic job localhost replica task devic cpu expanddim resizebilinear size model server abortionerror statuscod argument input dimension node resizebilinear resizebilinear output shape align corner devic job localhost replica task devic cpu expanddim resizebilinear size traceback file opt lib site packag serv line invok transform transform input type request output type file opt lib site packag serv line transform return transform data type accept accept file opt lib site packag serv line predict predict input file opt lib site packag serv line predict return proxi client request data file opt lib site packag proxi client line request return request data file opt lib site packag proxi client line predict stub predict request request timeout file usr local lib dist packag grpc beta client adapt line request serial respons deseri file usr local lib dist packag grpc beta client adapt line block unari unari rais abort rpc abortionerror abortionerror statuscod argument input dimension node resizebilinear resizebilinear output shape align corner devic job localhost replica task devic cpu expanddim resizebilinear size serv abortionerror statuscod argument input dimension node resizebilinear resizebilinear output shape align corner devic job localhost replica task devic cpu expanddim resizebilinear size model server abortionerror statuscod argument input dimension node resizebilinear resizebilinear output shape align corner devic job localhost replica task devic cpu expanddim resizebilinear size feb invoc http ahc btw load imag pil kera imag imag open path imag arrai arrai imag server serv input param http tensorflow org programm guid save model savedmodel estim http github com sdk creat serv input http doc com latest train infer templat html download inceptionv order comput high level featur call bottleneck fed model model dir pretrain model mayb download extract param data url dest directori model dir model path path model dir param model file gfile fastgfil model path graph graphdef graph parsefromstr read bottleneck tensor resiz input tensor input tensor import graph graph input map return element param bottleneck tensor param resiz input tensor decodejpeg return estim export servinginputreceiv bottleneck tensor input tensor input tensor",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"predict batch imag model advanc train model tensorflow estim imag input comput featur inceptionv dens layer predict class kinda train serv predict imag predict batch imag uniqu http predict fire notic shape send complain shape log modelerror call invokeendpoint oper receiv server model messag account log btw load imag pil kera server",
        "Challenge_readability":19.4,
        "Challenge_reading_time":114.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":76,
        "Challenge_solved_time":null,
        "Challenge_title":"Predict batch of images with a SageMaker model",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":2088.0,
        "Challenge_word_count":606,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":151335617790
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Using AZ ML workbench for a class project (required tool) I coded the desired logic below in an exploration notebook but cannot find a way to include this into a Data-prep Transform Data flow.<\/p>\n\n<p><code>all_columns = df.columns\nsum_columns = [col_name for col_name in all_columns if col_name not in ['NPI', 'Gender', 'State', 'Credentials', 'Specialty']]\nsum_op_columns = list(set(sum_columns) &amp; set(df_op['Drug Name'].values))<\/code><\/p>\n\n<p>The logic is using the column names from one data source df_op (opioid drugs) to choose which subset of columns to include from another data source df (all drugs). When adding a py script\/expression Transform Data Flow I'm only seeing the ability to reference the single df. Alternatives?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1518293450697,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"logic multipl data sourc data prep transform data flow workbench logic involv column data sourc choos subset column data sourc singl data sourc express transform data flow",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48725112",
        "Challenge_link_count":0,
        "Challenge_original_content":"workbench multipl data sourc data prep transform dataflow express workbench class desir logic explor notebook data prep transform data flow column column sum column col col column col npi gender state credenti specialti sum column list set sum column set drug valu logic column data sourc opioid drug choos subset column data sourc drug express transform data flow see abil singl",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"workbench multipl data sourc data prep transform dataflow express workbench class desir logic explor notebook transform data flow logic column data sourc choos subset column data sourc transform data flow see abil singl",
        "Challenge_readability":10.9,
        "Challenge_reading_time":10.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Can AZ ML workbench reference multiple data sources from Data Prep Transform Dataflow expression",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":71.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":150954549303
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I\u2019m trying to set up a web service from a Python notebook in azureml and I want it to return a dictionary of {string: float}. The floats serialize just fine, but the strings do not.<\/p>\n\n<p>This is the function I'm using:<\/p>\n\n<pre><code>def demoservice(N, Vy, My):\n    X = scaler.fit_transform([N, Vy, My])\n    res = clf.predict(X)\n    a = [ {'diam': x[0], 'radius': x[1], 'thickness': x[2]} for x in res]\n    return a\n<\/code><\/pre>\n\n<p>the call:<\/p>\n\n<pre><code>demoservice(\"140\", \"100\", \"0\")\n<\/code><\/pre>\n\n<p>correctly returns:<\/p>\n\n<pre><code>[{'diam': 16.0, 'radius': 2.0, 'thickness': 5.0}]\n<\/code><\/pre>\n\n<p>but the web call returns this json response:<\/p>\n\n<pre><code>{\"Results\":{\"output1\":{\"type\":\"table\",\"value\":{\"Values\":[[\"{\\\"type\\\": \\\"list\\\", \\\"value\\\": [{\\\"type\\\": \\\"dict\\\", \\\"value\\\": [[{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"ZGlhbQ==\\\"}, {\\\"type\\\": \\\"float\\\", \\\"value\\\": \\\"16.0\\\"}], [{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"cmFkaXVz\\\"}, {\\\"type\\\": \\\"float\\\", \\\"value\\\": \\\"2.0\\\"}], [{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"dGhpY2tuZXNz\\\"}, {\\\"type\\\": \\\"float\\\", \\\"value\\\": \\\"5.0\\\"}]]}]}\"]]}},\"output2\":{\"type\":\"table\",\"value\":{\"Values\":[[\"data:text\/plain,Execution OK\\r\\n\",null]]}}}}\n<\/code><\/pre>\n\n<p>As you can see, in the response the dictionary's keys are not being serialized correctly. For example:<\/p>\n\n<pre><code>{\\\"type\\\": \\\"bytes\\\", \\\"value\\\": \\\"cmFkaXVz\\\"}\n<\/code><\/pre>\n\n<p>here I have <code>cmFkaXVz<\/code> instead of a readable value.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1518375728430,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"set web servic notebook return dictionari serial web return json respons kei serial",
        "Challenge_last_edit_time":1518457477892,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48735257",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio web servic serial set web servic notebook return dictionari serial function demoservic scaler fit transform re clf predict diam radiu thick re return demoservic return diam radiu thick web return json respons output type tabl valu valu type list valu type dict valu type byte valu zglhbq type valu type byte valu cmfkaxvz type valu type byte valu dghpytuzxnz type valu output type tabl valu valu data text plain execut null respons dictionari kei serial type byte valu cmfkaxvz cmfkaxvz readabl valu",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"studio web servic serial set web servic notebook return dictionari serial function return web return json respons respons dictionari kei serial readabl valu",
        "Challenge_readability":9.2,
        "Challenge_reading_time":19.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML studio web service serialization",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":87.0,
        "Challenge_word_count":156,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":150872271570
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have proven quantum mechanics while trying to deploy and use an AWS SageMaker Models. Notice the errors seem to indicate that the model\/endpoint both do and don't exist.<\/p>\n\n<h3>Training my model<\/h3>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=10, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 400, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs)\n<\/code><\/pre>\n\n<p>runs fine.<\/p>\n\n<h3>Deploying the endpoint<\/h3>\n\n<pre><code>predictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>gets the error,<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-west-2:01234567890:model\/sagemaker-mxnet-py2-cpu-2018-02-13-17-18-59-047\".\n<\/code><\/pre>\n\n<h3>Running the endpoint,<\/h3>\n\n<pre><code>predictor.predict(np.arange(100))\n<\/code><\/pre>\n\n<p>gives,<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint sagemaker-mxnet-py2-cpu-2018-02-13-17-18-59-047 of account 01234567890 not found.\n<\/code><\/pre>\n\n<h3>Deleting the endpoint like,<\/h3>\n\n<pre><code>sagemaker.Session().delete_endpoint(predictor.endpoint)\n<\/code><\/pre>\n\n<p>gives,<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-west-2:01234567890:endpoint\/sagemaker-mxnet-py2-cpu-2018-02-13-17-18-59-047\".\n<\/code><\/pre>\n\n<p>How do I fix this?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1518549873797,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi model model endpoint creat endpoint model run endpoint endpoint delet endpoint",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48774210",
        "Challenge_link_count":0,
        "Challenge_original_content":"model pop vacuum proven quantum mechan deploi model notic model endpoint train model mxnet lstm trainer role role train instanc count train instanc type xlarg hyperparamet batch size epoch rate momentum log interv fit input run deploi endpoint predictor deploi initi instanc count instanc type xlarg clienterror validationexcept call createmodel oper creat model arn model mxnet cpu run endpoint predictor predict arang validationerror validationerror call invokeendpoint oper endpoint mxnet cpu account delet endpoint session delet endpoint predictor endpoint clienterror validationexcept call deleteendpoint oper endpoint arn endpoint mxnet cpu",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"model pop vacuum proven quantum mechan deploi model notic train model run deploi endpoint run endpoint delet endpoint",
        "Challenge_readability":14.9,
        "Challenge_reading_time":22.28,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker models popping in and out of the vacuum",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":561.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":150698126203
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I had created a free Azure machine learning workspace. \nCreated an experiment while following a tutorial.\nSaved the experiment multiple times through the process.\nNext day when i came back to the workspace, i no longer see the saved experiment.<\/p>\n\n<p>Is this expected? Is it a limitation of a free workspace?\ni could not see it mentioned anywhere.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1518763641740,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat free workspac save multipl time miss return workspac dai limit free workspac",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48821417",
        "Challenge_link_count":0,
        "Challenge_original_content":"save miss free workspac creat free workspac creat tutori save multipl time process dai came workspac longer save limit free workspac",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"save miss free workspac creat free workspac creat tutori save multipl time process dai came workspac longer save limit free workspac",
        "Challenge_readability":5.4,
        "Challenge_reading_time":5.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Saved Experiment missing after 24h in Azure ML free workspace",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":128.0,
        "Challenge_word_count":67,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":150484358260
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When running the command:<\/p>\n\n<pre><code>az ml service create realtime -f score.py --model-file model.hdf5 -s schema.json -n modelapp -r python --collect-model-data false -c aml_config\\conda_dependencies.yml\n<\/code><\/pre>\n\n<p>The image and service both seem to be created. But after a while, this error comes up:<\/p>\n\n<pre><code>{\n\"Azure-cli-ml Version\": \"0.1.0a27.post3\",\n\"Error\": {\n    \"Error Message\": \"No response from health endpoint after multiple deploy attempts. Setting status to failed.\"\n},\n\"Response Code\": 500,\n\"Response Content\": {\n    \"CreatedTime\": \"2018-02-16T14:40:56.358161Z\",\n    \"EndTime\": \"2018-02-16T14:51:23.79374Z\",\n    \"Error\": {\n        \"Code\": \"DeploymentFailed\",\n        \"Message\": \"No response from health endpoint after multiple deploy attempts. Setting status to failed.\",\n        \"StatusCode\": 500\n    },\n    \"OperationType\": \"Service\",\n    \"State\": \"Failed\"\n}\n}\n<\/code><\/pre>\n\n<p>When I run the <code>az ml service logs realtime<\/code> command, I see many messages saying <\/p>\n\n<pre><code>2018-02-16T14:59:44.964990Z, INFO, 00000000-0000-0000-0000-000000000000, , 127.0.0.1 - - [16\/Feb\/2018:14:59:44 +0000] \"GET \/ HTTP\/1.0\" 200 7 \"-\" \"Go-http-client\/1.1\"\n<\/code><\/pre>\n\n<p>I am using Azure ML Workbench version 0.1.1712.18263. This process has worked fine about a month ago when I first deployed an endpoint to this model management account. In Azure, I see the service with a status:Failed and no URL, but the primary and secondary keys are populated.<\/p>\n\n<p>I tried reinstalling the software just in case it was an older version, but it did not help. What else can I do to make the endpoint active?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1518793470993,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi servic workbench imag servic creat respons health endpoint multipl deploi set statu tri reinstal softwar version servic statu url primari secondari kei popul advic endpoint activ",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48829525",
        "Challenge_link_count":0,
        "Challenge_original_content":"workbench deploi servic run run servic creat realtim score model file model hdf schema json modelapp collect model data aml config conda depend yml imag servic creat come cli version messag respons health endpoint multipl deploi set statu respons respons createdtim endtim deploymentfail messag respons health endpoint multipl deploi set statu statuscod operationtyp servic state run servic log realtim messag sai feb http http client workbench version process month ago deploi endpoint model account servic statu url primari secondari kei popul tri reinstal softwar older version endpoint activ",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"workbench deploi servic run run imag servic creat come run messag sai workbench version process month ago deploi endpoint model account servic statu url primari secondari kei popul tri reinstal softwar older version endpoint activ",
        "Challenge_readability":7.0,
        "Challenge_reading_time":21.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Workbench not deploying service even though its running?",
        "Challenge_topic":"Kubernetes Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":267.0,
        "Challenge_word_count":207,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":150454529007
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to use pyodbc to import a dataframe in Azure ML Workbench. This works in local runs, but not for docker. It fails when trying to establish a connection to the SQL Server, because the driver is not present.<\/p>\n\n<pre><code>cnxn = pyodbc.connect('DRIVER='{ODBC Driver 13 for SQL Server}';PORT=1433;SERVER='+server+';PORT=1443;DATABASE='+database+';UID='+username+';PWD='+ password)\n<\/code><\/pre>\n\n<p>Error Message:<\/p>\n\n<blockquote>\n  <p>pyodbc.Error: ('01000', \"[01000] [unixODBC][Driver Manager]Can't open\n  lib 'ODBC Driver 13 for SQL Server' : file not found (0)\n  (SQLDriverConnect)\")<\/p>\n<\/blockquote>\n\n<p>When searching for a solution i found that i could put these lines in the docker file<\/p>\n\n<blockquote>\n  <p>ADD odbcinst.ini \/etc\/odbcinst.ini<\/p>\n  \n  <p>RUN apt-get update<\/p>\n  \n  <p>RUN apt-get install -y tdsodbc unixodbc-dev<\/p>\n  \n  <p>RUN apt install unixodbc-bin -y<\/p>\n  \n  <p>RUN apt-get clean -y<\/p>\n<\/blockquote>\n\n<p>However I'm new to docker, and cannot figure out where to put these lines in the ML Workbench. It seems the docker file is generated through <strong>docker.compute<\/strong> and <strong>conda_dependencies.yml<\/strong>, but nothing similar to the lines above can be found in either of those or anywere else in the solution.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519131149430,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pyodbc import datafram workbench driver present figur line docker file",
        "Challenge_last_edit_time":1519131650636,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48885715",
        "Challenge_link_count":0,
        "Challenge_original_content":"pyodbc workbench pyodbc import datafram workbench local run docker establish connect sql server driver present cnxn pyodbc connect driver odbc driver sql server port server server port databas databas uid usernam pwd password messag pyodbc unixodbc driver open lib odbc driver sql server file sqldriverconnect search line docker file add odbcinst ini odbcinst ini run apt updat run apt instal tdsodbc unixodbc dev run apt instal unixodbc bin run apt clean docker figur line workbench docker file gener docker comput conda depend yml line anywer",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"pyodbc workbench pyodbc import datafram workbench local run docker establish connect sql server driver present messag search line docker file add run updat run instal tdsodbc run apt instal run clean docker figur line workbench docker file gener line anywer",
        "Challenge_readability":10.5,
        "Challenge_reading_time":16.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How do you use pyodbc in Azure Machine Learning Workbench",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":942.0,
        "Challenge_word_count":169,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":150116850570
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I am doing a simple operations in Azure ML Studio using Python script.<\/p>\n\n<pre><code>import numpy as np\n\ndt_mean = np.mean(dt.iloc[:,0].values)\n<\/code><\/pre>\n\n<p>but it throws error<\/p>\n\n<pre><code>[Critical]     Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:\n---------- Start of error message from Python interpreter ----------\nCaught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 211, in batch\n    xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\n  File \"C:\\server\\XDRReader\\xdrutils.py\", line 51, in DataFrameToRFile\n    attributes = XDRBridge.DataFrameToRObject(dataframe)\n  File \"C:\\server\\XDRReader\\xdrbridge.py\", line 40, in DataFrameToRObject\n    if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):\nTypeError: object of type 'numpy.float64' has no len()\nProcess returned with non-zero exit code 1\n<\/code><\/pre>\n\n<p>This one works perfectly fine in Spyder. But it's not working in Azure ML Python script. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1519216830073,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"perform oper studio messag object type numpi len spyder",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48906201",
        "Challenge_link_count":0,
        "Challenge_original_content":"typeerror object type numpi len oper studio import numpi iloc valu throw critic evalu output log start messag interpret caught except execut function traceback file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil file server xdrreader xdrutil line dataframetorfil attribut xdrbridg dataframetorobject datafram file server xdrreader xdrbridg line dataframetorobject len datafram type datafram datafram typeerror object type numpi len process return exit perfectli spyder",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"typeerror object type len oper studio throw perfectli spyder",
        "Challenge_readability":8.4,
        "Challenge_reading_time":14.66,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"TypeError: object of type 'numpy.float64' has no len() when finding mean",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1037.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":150031169927
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm using the new Azure ML Workbench and set of model management services doing a regular Python (not Pyspark) project.  I have a conda_dependencies.yml file which looks like this:<\/p>\n\n<pre><code>name: project_environment\nchannels: \n  - conda-forge\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML Workbench only supports 3.5.2.\n  - python=3.5.2\n  - scikit-learn  \n  - xgboost\n<\/code><\/pre>\n\n<p>We deploying to my Azure cluster it never seems to install xgboost and thus when deploying the webservice I always get this error<\/p>\n\n<pre><code>File \"\/var\/azureml-app\/score.py\", line 31, in init\n    import xgboost\nImportError: No module named 'xgboost'\n<\/code><\/pre>\n\n<p>At the point where its calling my score.py to load my saved xgboost model.\nCan someone explain to me how to get xgboost installed for this? The error occurs whether I create the environment step by step or via the single command as in this example: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3<\/a><\/p>\n\n<p>This are the commands leading up to the error (the cluster has already been provisioned and set):<\/p>\n\n<pre><code>az ml manifest create --manifest-name oapmodelv1manifest -f score.py -r python -i &lt;modelid&gt; -s schema.json\naz ml image create -n oapv1image --manifest-id &lt;manifestid&gt; -c aml_config\\conda_dependencies.yml\naz ml service create realtime --image-id &lt;imageid&gt; -n oapmlapp --collect-model-data true --debug\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1519592070557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal cluster webservic conda depend yml file list depend modul instal importerror deploi webservic guidanc instal relev",
        "Challenge_last_edit_time":1519595205443,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/48978360",
        "Challenge_link_count":2,
        "Challenge_original_content":"instal cluster webservic workbench set model servic regular pyspark conda depend yml file environ channel conda forg depend interpret version workbench scikit deploi cluster instal deploi webservic file var app score line init import importerror modul call score load save model explain instal creat environ step step singl http doc com preview tutori classifi iri cluster provis set manifest creat manifest oapmodelvmanifest score schema json imag creat oapvimag manifest aml config conda depend yml servic creat realtim imag oapmlapp collect model data debug",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"instal cluster webservic workbench set model servic regular file deploi cluster instal deploi webservic call load save model explain instal creat environ step step singl",
        "Challenge_readability":12.0,
        "Challenge_reading_time":21.43,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"How to install xgboost on Azure ML cluster webservice?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":926.0,
        "Challenge_word_count":198,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":149655929443
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When trying to access SageMakerRuntime -> invokeEndPoint method with Javascript and im getting this error: <\/p>\n\n<ul>\n<li>MissingAuthenticationTokenException<\/li>\n<\/ul>\n\n<p>The strange thing is when i access a random SageMaker method my code works.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519733032240,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"missingauthenticationtokenexcept access runtim invokeendpoint javascript access random",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49008265",
        "Challenge_link_count":0,
        "Challenge_original_content":"runtim missingauthenticationtokenexcept access runtim invokeendpoint javascript missingauthenticationtokenexcept access random",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"runtim missingauthenticationtokenexcept access runtim invokeendpoint javascript missingauthenticationtokenexcept access random",
        "Challenge_readability":23.2,
        "Challenge_reading_time":4.03,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMakerRuntime: MissingAuthenticationTokenException",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":297.0,
        "Challenge_word_count":33,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":149514967760
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to run my own algorithm container in amazon sagemaker,at the time of deployment time ,I am getting error like below.<\/p>\n\n<pre><code>predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)\n\nValueError: Error hosting endpoint decision-trees-sample-2018-03-01-09-59-06-832: Failed Reason:  The primary container for production variant AllTraffic did not pass the ping health check.\n<\/code><\/pre>\n\n<p>then I run same line of code this time i am getting  below error.<\/p>\n\n<pre><code> predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)\n\nClientError: An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:69759707XXxXX:endpoint\/decision-trees-sample-2018-03-01-09-59-06-832\".\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1519899961097,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"run algorithm relat primari variant alltraff pass ping health relat endpoint",
        "Challenge_last_edit_time":1527821379003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49047576",
        "Challenge_link_count":0,
        "Challenge_original_content":"sagemak throw build algorithm execut time run algorithm time deploy time predictor tree deploi xlarg serial csv serial valueerror host endpoint decis tree sampl reason primari variant alltraff pass ping health run line time predictor tree deploi xlarg serial csv serial clienterror validationexcept call createendpoint oper creat endpoint arn endpoint decis tree sampl",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"sagemak throw build algorithm execut time run algorithm time deploy time run line time",
        "Challenge_readability":14.5,
        "Challenge_reading_time":11.93,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMake throwing error Building your own algorithm container execution time?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2116.0,
        "Challenge_word_count":95,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":149348038903
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I have a endpoint up and running in AWS Sagemaker. However, I'm not sure how to send data to this endpoint and get back a prediction. <\/p>\n\n<p>The documentation is also not clear on this. Any help would be appreciated.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1520758617133,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"link web app endpoint set send data receiv predict document clear assist",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49218305",
        "Challenge_link_count":0,
        "Challenge_original_content":"link web app endpoint run send data endpoint predict document clear",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"link web app endpoint run send data endpoint predict document clear",
        "Challenge_readability":5.9,
        "Challenge_reading_time":3.22,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How to link aws sagemaker with a web app?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2097.0,
        "Challenge_word_count":48,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":148489382867
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>After creating ALS model object,using pyspark. <\/p>\n\n<p>Sample code example:<\/p>\n\n<pre><code>from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\n\nlines = spark.read.text(\"data\/mllib\/als\/sample_movielens_ratings.txt\").rdd\nparts = lines.map(lambda row: row.value.split(\"::\"))\nratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n                                     rating=float(p[2]), timestamp=long(p[3])))\nratings = spark.createDataFrame(ratingsRDD)\n(rating_data, test) = ratings.randomSplit([0.8, 0.2])\n\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n          coldStartStrategy=\"drop\")\n\n    als_model = als_spec.fit(rating_data)\n<\/code><\/pre>\n\n<p>here i am just creating ALS model and making cloudepickel.\nif we are using fit then also need to do transform?<\/p>\n\n<p>I am trying pickel the my als_model object using the below code :<\/p>\n\n<pre><code>with open(os.path.join(model_path, 'als-als-model.pkl'), 'w') as out:\n                cloudpickle.dump(als_model, out)\n<\/code><\/pre>\n\n<p>I am getting error like below:<\/p>\n\n<pre><code>  File \"\/usr\/local\/spark\/python\/lib\/py4j-0.10.6-src.zip\/py4j\/protocol.py\", line 324, in get_return_value\n    format(target_id, \".\", name, value))\nPy4JError: An error occurred while calling o224.__getnewargs__. Trace:\npy4j.Py4JException: Method __getnewargs__([]) does not exist\n#011at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n#011at \n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-124-8c94f4ee0de9&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 tree.fit(data_location)\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name)\n    152         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    153         if wait:\n--&gt; 154             self.latest_training_job.wait(logs=logs)\n    155         else:\n    156             raise NotImplemented('Asynchronous fit not available')\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":8,
        "Challenge_created_time":1521115609450,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"pickl al model object cloudpickl messag getnewarg transform fit al model object",
        "Challenge_last_edit_time":1521198520328,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49299079",
        "Challenge_link_count":0,
        "Challenge_original_content":"cloudpickl dump pyspark alsmodel object pyj pyjexcept getnewarg creat al model object pyspark sampl pyspark evalu import regressionevalu pyspark import al pyspark sql import row line spark read text data mllib al sampl movielen rate txt rdd line map lambda row row valu split ratingsrdd map lambda row userid movieid rate timestamp rate spark createdatafram ratingsrdd rate data test rate randomsplit build model al train data note set cold start strategi drop nan evalu metric al al maxit regparam usercol userid itemcol movieid ratingcol rate coldstartstrategi drop al model al spec fit rate data creat al model cloudepickel fit transform pickel al model object open path model path al al model pkl cloudpickl dump al model file usr local spark lib pyj src zip pyj protocol line return valu format target valu pyjerror call getnewarg trace pyj pyjexcept getnewarg pyj reflect reflectionengin getmethod reflectionengin java valueerror traceback tree fit data locat anaconda env mxnet lib site packag estim fit input wait log job latest train job trainingjob start input wait latest train job wait log log rais notimpl asynchron fit",
        "Challenge_participation_count":8,
        "Challenge_preprocessed_content":"creat al model object pyspark sampl creat al model cloudepickel fit transform pickel object",
        "Challenge_readability":14.6,
        "Challenge_reading_time":30.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"Cloudpickle.dump(pyspark_Alsmodel_object),getting error py4j.Py4JException: Method __getnewargs__([]) does not exist?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":148132390550
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I am trying to use python SDk with tensorflow for test classification on sagemaker. I am able to modify this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_abalone_age_predictor_using_keras\/abalone.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_abalone_age_predictor_using_keras\/abalone.py<\/a> and run it but when I  change the arch to include embeddings layer, I get the error<\/p>\n\n<blockquote>\n  <p>\"Fetch argument  cannot be interpreted as a Tensor. (Tensor Tensor(\"first-layer\/embeddings:0\", shape=(*, <em>), dtype=float32_ref) is not an element of this graph.\"<\/em><\/p>\n<\/blockquote>\n\n<p>When I run it as a standalone model, it runs perfectly. \nhere is arch for standalone model<\/p>\n\n<pre><code>model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                        EMBEDDING_DIM,\n                        weights=[embedding_matrix],\n                        input_length=MAX_SEQUENCE_LENGTH,\n                        trainable=False))\n\nmodel.add(Conv1D(64, kernel_size=10, padding='same', activation='relu'))\nmodel.add(Conv1D(64, kernel_size=15, padding='same', activation='selu'))\nmodel.add(Conv1D(128, kernel_size=15, padding='same', activation='relu'))\nmodel.add(Conv1D(64, kernel_size=25, padding='same', activation='softmax'))\nmodel.add(Conv1D(128, kernel_size=15, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\n<\/code><\/pre>\n\n<p>Here is my model_fn for sagemaker:<\/p>\n\n<pre><code>embedding = tf.keras.layers.Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False, name='first-layer')(features[INPUT_TENSOR_NAME])\n\nfirst = tf.keras.layers.Conv1D(64, kernel_size=10, padding='same', activation='relu')(embedding)\nsecond = tf.keras.layers.Conv1D(64, kernel_size=15, padding='same', activation='relu')(first)\n third = tf.keras.layers.Conv1D(128, kernel_size=15, padding='same', activation='relu')(second)\n fourth = tf.keras.layers.Conv1D(64, kernel_size=25, padding='same', activation='softmax')(third)\n fifth = tf.keras.layers.Conv1D(128, kernel_size=15, padding='same', activation='relu')(fourth)\nsixth = tf.keras.layers.BatchNormalization()(fifth)\n\noutput = tf.keras.layers.Flatten()(sixth)\noutput_layer = tf.keras.layers.Dense(2, activation='softmax'))(output)\n<\/code><\/pre>\n\n<p>There no issue with input dimension or value, if I replace this arch with a simple arch of dense layer only, code works perfectly. <\/p>\n\n<p>I have already tried solution on \n<a href=\"https:\/\/stackoverflow.com\/questions\/44219094\/tensorflow-the-tensor-is-not-the-element-of-this-graph\">TensorFlow: The tensor is not the element of this graph<\/a> but I get a new error <\/p>\n\n<blockquote>\n  <p>Input graph and Layer graph are not the same: Tensor(\"random_shuffle_queue_DequeueMany:1\", shape=(128, 200), dtype=float32, device=\/device:CPU:0) is not from the passed-in graph.*<\/p>\n<\/blockquote>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1521133708143,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"embed layer tensorflow model test classif messag state fetch argument interpret tensor tri stack overflow model run perfectli standalon model",
        "Challenge_last_edit_time":1521138021168,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49305362",
        "Challenge_link_count":3,
        "Challenge_original_content":"embed layer produc interpret tensor sdk tensorflow test classif modifi http github com awslab blob master sdk tensorflow abalon ag predictor kera abalon run arch embed layer fetch argument interpret tensor tensor tensor layer embed shape dtype ref element graph run standalon model run perfectli arch standalon model model sequenti model add embed len index embed dim weight embed matrix input length sequenc length trainabl model add convd kernel size pad activ relu model add convd kernel size pad activ selu model add convd kernel size pad activ relu model add convd kernel size pad activ softmax model add convd kernel size pad activ relu model add batchnorm model add flatten model add dens activ softmax model embed kera layer embed len index embed dim weight embed matrix input length sequenc length trainabl layer featur input tensor kera layer convd kernel size pad activ relu embed kera layer convd kernel size pad activ relu kera layer convd kernel size pad activ relu fourth kera layer convd kernel size pad activ softmax fifth kera layer convd kernel size pad activ relu fourth sixth kera layer batchnorm fifth output kera layer flatten sixth output layer kera layer dens activ softmax output input dimens valu replac arch arch dens layer perfectli tri tensorflow tensor element graph input graph layer graph tensor random shuffl queue dequeuemani shape dtype devic devic cpu pass graph",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"embed layer produc interpret tensor sdk tensorflow test classif modifi run arch embed layer fetch argument interpret tensor element run standalon model run perfectli arch standalon model input dimens valu replac arch arch dens layer perfectli tri tensorflow tensor element graph input graph layer graph shape dtype",
        "Challenge_readability":18.7,
        "Challenge_reading_time":40.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":26,
        "Challenge_solved_time":null,
        "Challenge_title":"embeddings layer produce cannot be interpreted as a Tensor in sagemaker?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":406.0,
        "Challenge_word_count":241,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":148114291857
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am able to host the models developed in <em>SageMaker<\/em> by using the deploy functionality. Currently, I see that the different models that I have developed needs to deployed on different ML compute instances.<\/p>\n\n<p>Is there a way to deploy all models on the same instance, using separate instances seems to be very expensive option. If it is possible to deploy multiple models on the same instance, will that create different endpoints for the models?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1521700601540,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi multipl model comput instanc order save cost creat endpoint model",
        "Challenge_last_edit_time":1603678525603,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49422065",
        "Challenge_link_count":0,
        "Challenge_original_content":"host multipl model comput instanc host model deploi function model deploi comput instanc deploi model instanc separ instanc expens option deploi multipl model instanc creat endpoint model",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"host multipl model host model deploi function model deploi comput instanc deploi model instanc separ instanc expens option deploi multipl model instanc creat endpoint model",
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.74,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker hosting multiple models on the same machine (ML compute instance)",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2487.0,
        "Challenge_word_count":86,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":147547398460
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am running the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">k-means example in SageMaker<\/a>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker import KMeans\n\ndata_location = 's3:\/\/{}\/kmeans_highlevel_example\/data'.format(bucket)\noutput_location = 's3:\/\/{}\/kmeans_example\/output'.format(bucket)\n\nkmeans = KMeans(role=role, \n                train_instance_count=2,\n                train_instance_type='ml.c4.8xlarge',\n                output_path=output_location,\n                k=10, \n                data_location=data_location)\n<\/code><\/pre>\n<p>When I run this line, it appears access denied error.<\/p>\n<pre><code>%%time\n\nkmeans.fit(kmeans.record_set(train_set[0]))\n<\/code><\/pre>\n<p>The error returns:<\/p>\n<blockquote>\n<p>ClientError: An error occurred (AccessDenied) when calling the\nPutObject operation: Access Denied<\/p>\n<\/blockquote>\n<p>I also read other questions, but their answers do not solve my problem.<br \/>\nWould you please look at my case?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522012018310,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"access run call putobject oper unsuccess",
        "Challenge_last_edit_time":1618306977208,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49480931",
        "Challenge_link_count":1,
        "Challenge_original_content":"access run import kmean data locat kmean highlevel data format bucket output locat kmean output format bucket kmean kmean role role train instanc count train instanc type xlarg output path output locat data locat data locat run line access time kmean fit kmean record set train set return clienterror accessdeni call putobject oper access read",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"access run run line access return clienterror call putobject oper access read",
        "Challenge_readability":17.5,
        "Challenge_reading_time":13.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"SageMaker example access denied",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":5183.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":147235981690
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a sagemaker tensorflow model using a custom estimator, similar to the abalone.py sagemaker tensorflow example, using build_raw_serving_input_receiver_fn in the serving_input_fn:<\/p>\n\n<pre><code>def serving_input_fn(params):\n    tensor = tf.placeholder(tf.float32, shape=[1, NUM_FEATURES])\n    return build_raw_serving_input_receiver_fn({INPUT_TENSOR_NAME: tensor})()\n<\/code><\/pre>\n\n<p>Predictions are being request from java-script using json:<\/p>\n\n<pre><code>  response = @client.invoke_endpoint(\n    endpoint_name: @name,\n    content_type: \"application\/json\",\n    accept: \"application\/json\",\n    body: values.to_json\n    )\n<\/code><\/pre>\n\n<p>Everything fine so far. Now I want to add some feature engineering (scaling transformations on the features using a scaler derived from the training data). Following the pattern of the answer for <a href=\"https:\/\/stackoverflow.com\/questions\/46474658\/data-normalization-with-tensorflow-tf-transform\">Data Normalization with tensorflow tf-transform\n<\/a> I've now got serving_input_fn like this:<\/p>\n\n<pre><code>def serving_input_fn(params):\n    feature_placeholders = {\n        'f1': tf.placeholder(tf.float32, [None]),\n        'f2': tf.placeholder(tf.float32, [None]),\n        'f3': tf.placeholder(tf.float32, [None]),\n    }\n    features = {\n        key: tf.expand_dims(tensor, -1)\n        for key, tensor in feature_placeholders.items()\n    }\n    return tf.estimator.export.ServingInputReceiver(add_engineering(features), feature_placeholders)\n<\/code><\/pre>\n\n<p>From saved_model_cli show --dir . --all I can see the input signature has changed:<\/p>\n\n<pre><code>signature_def['serving_default']:\nThe given SavedModel SignatureDef contains the following input(s):\ninputs['f1'] tensor_info:\n    dtype: DT_FLOAT\n    shape: (-1)\n    name: Placeholder_1:0\ninputs['f2'] tensor_info:\n    dtype: DT_FLOAT\n    shape: (-1)\n    name: Placeholder_2:0\ninputs['f3'] tensor_info:\n    dtype: DT_FLOAT\n    shape: (-1)\n    name: Placeholder:0\n<\/code><\/pre>\n\n<p>How do I prepare features for prediction from this new model? In python I've been unsuccessfully trying things like<\/p>\n\n<pre><code>requests = [{'f1':[0.1], 'f2':[0.1], 'f3':[0.2]}]\npredictor.predict(requests)\n<\/code><\/pre>\n\n<p>also need to send prediction requests from java-script.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522067498880,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"prepar featur predict tensorflow model estim featur engin input signatur unsuccess prepar featur predict model send predict request javascript",
        "Challenge_last_edit_time":1522076363032,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49491562",
        "Challenge_link_count":1,
        "Challenge_original_content":"invok endpoint signatur featur prep tensorflow model estim abalon tensorflow build raw serv input receiv serv input serv input param tensor placehold shape num featur return build raw serv input receiv input tensor tensor predict request java json respons client invok endpoint endpoint type json accept json bodi valu json add featur engin scale transform featur scaler deriv train data pattern data normal tensorflow transform serv input serv input param featur placehold placehold placehold placehold featur kei expand dim tensor kei tensor featur placehold item return estim export servinginputreceiv add engin featur featur placehold save model cli dir input signatur signatur serv default savedmodel signaturedef input input tensor dtype shape placehold input tensor dtype shape placehold input tensor dtype shape placehold prepar featur predict model unsuccessfulli request predictor predict request send predict request java",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"featur prep tensorflow model estim tensorflow predict request json add featur engin pattern data normal tensorflow input signatur prepar featur predict model unsuccessfulli send predict request",
        "Challenge_readability":14.8,
        "Challenge_reading_time":29.14,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"sagemaker invoke_endpoint signature_def feature prep",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":780.0,
        "Challenge_word_count":206,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":147180501120
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to host custom docker container through sagemaker. \nIm using nginx, gunicorn, flask setup. \nI'm able to invoke(ping) the endpoint for my application.\ninput to my service is 'application\/json' format and expected output from the service is json. <\/p>\n\n<p>When i call the service i get following output in the client : <\/p>\n\n<pre><code>'&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2Final\/\/EN\"&gt;\\n&lt;title&gt;Redirecting...&lt;\/title&gt;\\n&lt;h1&gt;Redirecting...&lt;\/h1&gt;\\n&lt;p&gt;You should be redirected automatically to target URL: &lt;a href=\"http:\/\/boaucpph.aws.local:8080\/invocations\/\"&gt;http:\/\/boaucpph.aws.local:8080\/invocations\/&lt;\/a&gt;.  If not click the link.'\n<\/code><\/pre>\n\n<p>and my endpoint logs tell : <\/p>\n\n<pre><code>10.32.0.2 - - [28\/Mar\/2018:20:50:41 +0000] \"POST \/invocations HTTP\/1.1\" 301 293 \"-\" \"AHC\/2.0\"\n<\/code><\/pre>\n\n<p>my nginx.conf <\/p>\n\n<pre><code>worker_processes 1;\ndaemon off; # Prevent forking\n\npid \/tmp\/nginx.pid;\nerror_log \/var\/log\/nginx\/error.log;\n\nevents {\n  # defaults\n}\n\n http {\n  include \/etc\/nginx\/mime.types;\n  default_type application\/octet-stream;\n  access_log \/var\/log\/nginx\/access.log combined;\n\n  upstream gunicorn {\n    server unix:\/tmp\/gunicorn.sock;\n  }\n\n  server {\n    listen 8080 deferred;\n    client_max_body_size 100M;\n\n    keepalive_timeout 5;\n\n    location ~ ^\/(ping|invocations) {\n      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n      proxy_set_header Host $http_host;\n      proxy_redirect off;\n      proxy_pass http:\/\/gunicorn;\n    }\n\n    location \/ {\n      return 404 \"{}\";\n    }\n  }\n}\n<\/code><\/pre>\n\n<p>Has anyone faced similar problem ? any suggestions on this would be of great help. <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522275336927,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"host docker endpoint client receiv http nginx conf file",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49545051",
        "Challenge_link_count":2,
        "Challenge_original_content":"respons endpoint http host docker nginx gunicorn flask setup invok ping endpoint input servic json format output servic json servic output client nredirect nredirect nyou redirect automat target url http boaucpph local invoc click link endpoint log mar invoc http ahc nginx conf worker process daemon prevent fork pid tmp nginx pid log var log nginx log event default http nginx mime type default type octet stream access log var log nginx access log combin upstream gunicorn server unix tmp gunicorn sock server listen defer client bodi size keepal timeout locat ping invoc proxi set header forward proxi add forward proxi set header host http host proxi redirect proxi pass http gunicorn locat return great",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"respons endpoint http host docker nginx gunicorn flask setup invok endpoint input servic format output servic json servic output client endpoint log great",
        "Challenge_readability":12.4,
        "Challenge_reading_time":21.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagemaker : No response back from the endpoint \"HTTP 301 293\"",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":166,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":146972663073
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I would like to save a Spark DataFrame from AWS SageMaker to S3. In Notebook, I ran<\/p>\n\n<p><code>myDF.write.mode('overwrite').parquet(\"s3a:\/\/my-bucket\/dir\/dir2\/\")<\/code><\/p>\n\n<p>I get<\/p>\n\n<blockquote>\n  <p>Py4JJavaError: An error occurred while calling o326.parquet. :\n  java.lang.RuntimeException: java.lang.ClassNotFoundException: Class\n  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at\n  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at\n  org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at\n  org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)   at\n  org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)     at\n  org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)    at\n  org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:394)\n    at\n  org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n    at\n  org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n    at\n  org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n    at\n  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n    at\n  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n    at\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n    at\n  org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n    at\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n    at\n  org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n    at\n  org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n    at\n  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n    at\n  org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n    at\n  org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at\n  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at\n  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)     at\n  py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at\n  py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at\n  py4j.Gateway.invoke(Gateway.java:280)     at\n  py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)   at\n  py4j.GatewayConnection.run(GatewayConnection.java:214)    at\n  java.lang.Thread.run(Thread.java:745) Caused by:\n  java.lang.ClassNotFoundException: Class\n  org.apache.hadoop.fs.s3native.NativeS3FileSystem not found    at\n  org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n    at\n  org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)<\/p>\n<\/blockquote>\n\n<p>How should I do it correctly in Notebook? Many thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522434506127,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"save spark datafram messag class org apach hadoop snativ nativesfilesystem guidanc save datafram notebook",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49579526",
        "Challenge_link_count":0,
        "Challenge_original_content":"save parquet save spark datafram notebook ran mydf write mode overwrit parquet bucket dir dir pyjjavaerror call parquet java lang runtimeexcept java lang classnotfoundexcept class org apach hadoop snativ nativesfilesystem org apach hadoop conf configur getclass configur java org apach hadoop filesystem getfilesystemclass filesystem java org apach hadoop filesystem createfilesystem filesystem java org apach hadoop filesystem access filesystem java org apach hadoop filesystem cach getintern filesystem java org apach hadoop filesystem cach filesystem java org apach hadoop filesystem filesystem java org apach hadoop path getfilesystem path java org apach spark sql execut datasourc datasourc writeinfileformat datasourc scala org apach spark sql execut datasourc datasourc write datasourc scala org apach spark sql execut datasourc saveintodatasourcecommand run saveintodatasourcecommand scala org apach spark sql execut executedcommandexec sideeffectresult lzycomput scala org apach spark sql execut executedcommandexec sideeffectresult scala org apach spark sql execut executedcommandexec doexecut scala org apach spark sql execut sparkplan anonfun execut appli sparkplan scala org apach spark sql execut sparkplan anonfun execut appli sparkplan scala org apach spark sql execut sparkplan anonfun executequeri appli sparkplan scala org apach spark rdd rddoperationscop withscop rddoperationscop scala org apach spark sql execut sparkplan executequeri sparkplan scala org apach spark sql execut sparkplan execut sparkplan scala org apach spark sql execut queryexecut tordd lzycomput queryexecut scala org apach spark sql execut queryexecut tordd queryexecut scala org apach spark sql dataframewrit runcommand dataframewrit scala org apach spark sql dataframewrit save dataframewrit scala org apach spark sql dataframewrit save dataframewrit scala org apach spark sql dataframewrit parquet dataframewrit scala sun reflect nativemethodaccessorimpl invok nativ sun reflect nativemethodaccessorimpl invok nativemethodaccessorimpl java sun reflect delegatingmethodaccessorimpl invok delegatingmethodaccessorimpl java java lang reflect invok java pyj reflect methodinvok invok methodinvok java pyj reflect reflectionengin invok reflectionengin java pyj gatewai invok gatewai java pyj abstractcommand invokemethod abstractcommand java pyj callcommand execut callcommand java pyj gatewayconnect run gatewayconnect java java lang thread run thread java java lang classnotfoundexcept class org apach hadoop snativ nativesfilesystem org apach hadoop conf configur getclassbynam configur java org apach hadoop conf configur getclass configur java notebook",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"save parquet save spark datafram notebook ran jjavaerror call class class notebook",
        "Challenge_readability":49.3,
        "Challenge_reading_time":50.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":47,
        "Challenge_solved_time":null,
        "Challenge_title":"How to save parquet in S3 from AWS SageMaker?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1986.0,
        "Challenge_word_count":139,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":146813493873
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am calling a Sagemaker endpoint using java Sagemaker SDK. The data that I am sending needs little cleaning before the model can use it for prediction. How can I do that in Sagemaker.<\/p>\n\n<p>I have a pre-processing function in the Jupyter notebook instance which is cleaning the training data before passing that data to train the model. Now I want to know if I can use that function while calling the endpoint or is that function already being used?\nI can show my code if anyone wants?<\/p>\n\n<p><strong>EDIT 1<\/strong>\nBasically, in the pre-processing, I am doing label encoding. Here is my function for preprocessing<\/p>\n\n<pre><code>def preprocess_data(data):\n print(\"entering preprocess fn\")\n # convert document id &amp; type to labels\n le1 = preprocessing.LabelEncoder()\n le1.fit(data[\"documentId\"])\n data[\"documentId\"]=le1.transform(data[\"documentId\"])\n le2 = preprocessing.LabelEncoder()\n le2.fit(data[\"documentType\"])\n data[\"documentType\"]=le2.transform(data[\"documentType\"])\n print(\"exiting preprocess fn\")\n return data,le1,le2\n<\/code><\/pre>\n\n<p>Here the 'data' is a pandas dataframe.<\/p>\n\n<p>Now I want to use these le1,le2 at the time of calling endpoint. I want to do this preprocessing in sagemaker itself not in my java code.  <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522442406957,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"preprocess input data predict pre process function jupyt notebook instanc call endpoint function label encod pre process java",
        "Challenge_last_edit_time":1522914368472,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49581156",
        "Challenge_link_count":0,
        "Challenge_original_content":"preprocess input data predict call endpoint java sdk data send littl clean model predict pre process function jupyt notebook instanc clean train data pass data train model function call endpoint function edit pre process label encod function preprocess preprocess data data print enter preprocess convert document type label preprocess labelencod fit data documentid data documentid transform data documentid preprocess labelencod fit data documenttyp data documenttyp transform data documenttyp print exit preprocess return data data panda datafram time call endpoint preprocess java",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"preprocess input data predict call endpoint java sdk data send littl clean model predict function jupyt notebook instanc clean train data pass data train model function call endpoint function edit label encod function preprocess data panda datafram time call endpoint preprocess java",
        "Challenge_readability":10.9,
        "Challenge_reading_time":16.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":8.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"how can I preprocess input data before making predictions in sagemaker?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":4215.0,
        "Challenge_word_count":176,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":146805593043
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. How do i revert back to an older version of data files using dvc cli?<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522591456249,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"revert older version data file cli pipelin proof concept",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/get-older-version-of-data-files\/18",
        "Challenge_link_count":0,
        "Challenge_original_content":"older version data file pipelin poc add track data file revert older version data file cli",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"older version data file pipelin poc add track data file revert older version data file cli",
        "Challenge_readability":4.4,
        "Challenge_reading_time":2.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Get older version of data files",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":2532.0,
        "Challenge_word_count":47,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":146656543751
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have trained a model using Amazon SageMaker and I need to convert the model into a Tensorflow model. Is there any way it can be achieved? <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1522856239380,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"convert model train tensorflow model guidanc achiev",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49655133",
        "Challenge_link_count":0,
        "Challenge_original_content":"convert model tensorflow model train model convert model tensorflow model achiev",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"convert model tensorflow model train model convert model tensorflow model achiev",
        "Challenge_readability":7.1,
        "Challenge_reading_time":2.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to convert a Amazon SageMaker model to a Tensorflow model",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":255.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":146391760620
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have trained and saved a model using Amazon SageMaker which saves the model in the format of <code>model.tar.gz<\/code> which when untarred, has a file <code>model_algo-1<\/code> which is a serialized Apache MXNet object. To load the model in memory I need to deserialize the model. I tried doing so as follows:<\/p>\n\n<p><code>import mxnet as mx\nprint(mx.ndarray.load('model_algo-1'))<\/code><\/p>\n\n<p>Reference taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html<\/a><\/p>\n\n<p>However, doing this yields me the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"\/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/ndarray\/utils.py\", line \n175, in load\nctypes.byref(names)))\nFile \"\/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/base.py\", line 146, in \ncheck_call\nraise MXNetError(py_str(_LIB.MXGetLastError()))\nmxnet.base.MXNetError: [19:06:25] src\/ndarray\/ndarray.cc:1112: Check failed: \nheader == kMXAPINDArrayListMagic Invalid NDArray file format\n\nStack trace returned 10 entries:\n[bt] (0) \/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/libmxnet.so(+0x192112) \n[0x7fe432bfa112]\n[bt] (1) \/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/libmxnet.so(+0x192738) \n[0x7fe432bfa738]\n[bt] (2) \/usr\/local\/lib\/python3.4\/site-\npackages\/mxnet\/libmxnet.so(+0x24a5c44) [0x7fe434f0dc44]\n[bt] (3) \/usr\/local\/lib\/python3.4\/site-\npackages\/mxnet\/libmxnet.so(MXNDArrayLoad+0x248) [0x7fe434d19ad8]\n[bt] (4) \/usr\/lib64\/libffi.so.6(ffi_call_unix64+0x4c) [0x7fe48c5bbcec]\n[bt] (5) \/usr\/lib64\/libffi.so.6(ffi_call+0x1f5) [0x7fe48c5bb615]\n[bt] (6) \/usr\/lib64\/python3.4\/lib-dynload\/_ctypes.cpython-\n34m.so(_ctypes_callproc+0x2fb) [0x7fe48c7ce18b]\n[bt] (7) \/usr\/lib64\/python3.4\/lib-dynload\/_ctypes.cpython-34m.so(+0xa4cf) \n[0x7fe48c7c84cf]\n[bt] (8) \/usr\/lib64\/libpython3.4m.so.1.0(PyObject_Call+0x8c) \n[0x7fe4942fcb5c]\n[bt] (9) \/usr\/lib64\/libpython3.4m.so.1.0(PyEval_EvalFrameEx+0x36c5) \n[0x7fe4943ac915]\n<\/code><\/pre>\n\n<p>Could someone suggest how this can be resolved?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1522869667753,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"deseri apach mxnet object save format model tar messag ndarrai file format",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49658834",
        "Challenge_link_count":2,
        "Challenge_original_content":"deseri apach mxnet object train save model save model format model tar untar file model algo serial apach mxnet object load model memori deseri model tri import mxnet print ndarrai load model algo taken http doc com latest cdf train html yield traceback file line file usr local lib site packag mxnet ndarrai util line load ctype byref file usr local lib site packag mxnet base line rais mxneterror str lib mxgetlasterror mxnet base mxneterror src ndarrai ndarrai header kmxapindarraylistmag ndarrai file format stack trace return entri usr local lib site packag mxnet libmxnet xfebfa usr local lib site packag mxnet libmxnet xfebfa usr local lib site packag mxnet libmxnet xac xfefdc usr local lib site packag mxnet libmxnet mxndarrayload xfedad usr lib libffi ffi unix xfecbbcec usr lib libffi ffi xfecbb usr lib lib dynload ctype cpython ctype callproc xfb xfecceb usr lib lib dynload ctype cpython xacf xfecccf usr lib libpython pyobject xfefcbc usr lib libpython pyeval evalframeex xfeac",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"deseri apach mxnet object train save model save model format untar file serial apach mxnet object load model memori deseri model tri taken yield",
        "Challenge_readability":14.4,
        "Challenge_reading_time":29.32,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Error while deserializing the Apache MXNet object",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":971.0,
        "Challenge_word_count":171,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":146378332247
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>I'm trying to load spacy into SageMaker. I run the following in Jupyter notebook instance<\/p>\n\n<pre><code>!pip install spacy\n<\/code><\/pre>\n\n<p>I end up getting this error<\/p>\n\n<pre><code>  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n  error: command 'gcc' failed with exit status 1\n<\/code><\/pre>\n\n<p>and this as well<\/p>\n\n<pre><code>gcc: error: murmurhash\/mrmr.cpp: No such file or directory\nerror: command 'gcc' failed with exit status 1\n<\/code><\/pre>\n\n<p>How can I resolve this issue withing Sagemaker?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1522911042367,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"instal spaci relat ccplu murmurhash mrmr cpp instal",
        "Challenge_last_edit_time":1531211662796,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49665836",
        "Challenge_link_count":0,
        "Challenge_original_content":"instal spaci load spaci run jupyt notebook instanc pip instal spaci end gcc exec ccplu execvp file directori gcc exit statu gcc murmurhash mrmr cpp file directori gcc exit statu with",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"instal spaci load spaci run jupyt notebook instanc end with",
        "Challenge_readability":10.9,
        "Challenge_reading_time":7.18,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to install spacy on AWS Sagemaker",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":4798.0,
        "Challenge_word_count":82,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":146336957633
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hi,<\/p>\n<p>Are you thinking of supporting data versioning of databases.<br>\nTracking changes of transformations from raw data to cleaned data?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1522998440046,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"inquir data version databas track transform raw data clean data",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/data-versioning-of-databases\/20",
        "Challenge_link_count":0,
        "Challenge_original_content":"data version databas data version databas track transform raw data clean data",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"data version databas data version databas track transform raw data clean data",
        "Challenge_readability":8.8,
        "Challenge_reading_time":2.48,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Data versioning of databases",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":543.0,
        "Challenge_word_count":24,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":146249559954
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>What are the scenarios where Apache Spark ML on HdInsight is preferred over Azure ML studio?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1523434263913,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"apach spark hdinsight studio",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49769813",
        "Challenge_link_count":0,
        "Challenge_original_content":"spark hdinsight studio apach spark hdinsight prefer studio",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"spark hdinsight studio apach spark hdinsight prefer studio",
        "Challenge_readability":5.6,
        "Challenge_reading_time":1.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Spark ML on HdInsight vs Azure machine learning studio?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1537.0,
        "Challenge_word_count":24,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":145813736087
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm trying to train a custom tensorflow model, using AWS SageMaker. Thus, in the <code>model_fn<\/code> method, that I should provide, I want to be able to read an external file. I've uploaded the file to S3 and try to read like below:<\/p>\n\n<pre><code>BUCKET_PATH = 's3:\/\/&lt;bucket_name&gt;\/data\/&lt;prefix&gt;\/'\n\ndef model_fn(features, labels, mode, params):\n    # Load vocabulary\n    vocab_path = os.path.join(BUCKET_PATH, 'vocab.pkl')\n    with open(vocab_path, 'rb') as f:\n        vocab = pickle.load(f)\n    n_vocab = len(vocab)\n    ...\n<\/code><\/pre>\n\n<p>I get an <code>IOError: [Errno 2] No such file or directory<\/code><\/p>\n\n<p>How can I read this file during training?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1523441575947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"train tensorflow model read extern file model upload ioerror state file directori guidanc read file train",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49772344",
        "Challenge_link_count":0,
        "Challenge_original_content":"read file train train tensorflow model model read extern file upload file read bucket path data model featur label mode param load vocabulari vocab path path bucket path vocab pkl open vocab path vocab pickl load vocab len vocab ioerror errno file directori read file train",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"read file train train tensorflow model read extern file upload file read read file train",
        "Challenge_readability":9.6,
        "Challenge_reading_time":8.89,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to read a file during training in AWS SageMaker?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":752.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":145806424053
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have deployed a TensorFlow model on AWS SageMaker, and I want to be able to invoke it using a csv file as the body of the call. The documentation says about creating a <code>serving_input_function<\/code> like the one below: <\/p>\n\n<pre><code>def serving_input_fn(hyperparameters):\n  # Logic to the following:\n  # 1. Defines placeholders that TensorFlow serving will feed with inference requests\n  # 2. Preprocess input data\n  # 3. Returns a tf.estimator.export.ServingInputReceiver or tf.estimator.export.TensorServingInputReceiver,\n  # which packages the placeholders and the resulting feature Tensors together.\n<\/code><\/pre>\n\n<p>In step 2, where it says preprocess input data, how do I get a handle on input data to process them?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1523450778890,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok tensorflow model deploi csv file bodi preprocess input data serv input function input data",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49775557",
        "Challenge_link_count":0,
        "Challenge_original_content":"invok model train tensorflow csv file bodi deploi tensorflow model invok csv file bodi document sai creat serv input function serv input hyperparamet logic defin placehold tensorflow serv feed infer request preprocess input data return estim export servinginputreceiv estim export tensorservinginputreceiv packag placehold featur tensor step sai preprocess input data input data process",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"invok model train tensorflow csv file bodi deploi tensorflow model invok csv file bodi document sai creat step sai preprocess input data input data process",
        "Challenge_readability":10.8,
        "Challenge_reading_time":10.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I invoke a SageMaker model, trained with TensorFlow, using a csv file in the body of the call?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1249.0,
        "Challenge_word_count":118,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":145797221110
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I am trying to create a web-service via Azure Model Management and am struggling.<\/p>\n\n<p>I've followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/tutorial-classifying-iris-part-3\" rel=\"nofollow noreferrer\">instructions<\/a> and have managed to operationalize locally in a Docker container.  My 'score.py' file includes a query to a SQL database using pyodbc.  This functions perfectly when I test this on my local environment using the ML Workbench, however once this has been deployed in a Docker container I come across this error:<\/p>\n\n<pre><code>'Response Content': b'(\\'01000\\', \"[01000] [unixODBC][Driver Manager]Can\\'t open lib \\'ODBC Driver 13 for SQL Server\\' : file not found (0) (SQLDriverConnect)\")'\n<\/code><\/pre>\n\n<p>I have included pyodbc in my conda_dependencies.yml.  <\/p>\n\n<p>Has anyone got any suggestions? Is there any further dependencies that I need to include?  <\/p>\n\n<p>Azure seem to have recently added the ability to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/preview\/model-management-custom-container\" rel=\"nofollow noreferrer\">customize container images<\/a> using what they call a 'Docker Steps file'.  I have practically no experience in Docker, but after reading <a href=\"https:\/\/stackoverflow.com\/questions\/46405777\/connect-docker-python-to-sql-server-with-pyodbc\">this question<\/a> i tried including a 'Docker Steps file' containing this:<\/p>\n\n<pre><code>ADD odbcinst.ini \/etc\/odbcinst.ini\nRUN apt-get update\nRUN apt-get install -y tdsodbc unixodbc-dev\nRUN apt install unixodbc-bin -y\nRUN apt-get clean -y\n<\/code><\/pre>\n\n<p>However i understand 'ADD' commands are not possible in this type of file, so this seems to have made no difference.<\/p>\n\n<p>Hopefully this this all makes sense! Any advice would be very much appreciated! I hope I'm not the only one stumbling my way through Azure ML!<\/p>\n\n<p>EDIT:<\/p>\n\n<p>I'm still stuck, but making progress...<\/p>\n\n<p>I accessed the root of the container using:<\/p>\n\n<pre><code>docker exec -ti -u root container_name bash\n<\/code><\/pre>\n\n<p>From here I ran 'odbcinst -j`, resulting in:<\/p>\n\n<pre><code>unixODBC 2.3.6\nDRIVERS............: \/etc\/odbcinst.ini\nSYSTEM DATA SOURCES: \/etc\/odbc.ini\nFILE DATA SOURCES..: \/etc\/ODBCDataSources\nUSER DATA SOURCES..: \/root\/.odbc.ini\nSQLULEN Size.......: 8\nSQLLEN Size........: 8\nSQLSETPOSIROW Size.: 8\n<\/code><\/pre>\n\n<p>I couldn't seem to actually locate <code>odbc.ini<\/code> so i followed <a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/connect\/odbc\/linux-mac\/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017#microsoft-odbc-driver-131-for-sql-server\" rel=\"nofollow noreferrer\">these instructions<\/a> for installing 'ODBC Driver 13' for Ubuntu 16.04.  Now when I run the service i get a different error:<\/p>\n\n<pre><code>{'Error': MlCliError({'Error': 'Error occurred while attempting to score service myapp.', 'Response Code': 502, 'Response Content': b'&lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;\/title&gt;&lt;\/head&gt;\\r\\n&lt;body bgcolor=\"white\"&gt;\\r\\n&lt;center&gt;&lt;h1&gt;502 Bad Gateway&lt;\/h1&gt;&lt;\/center&gt;\\r\\n&lt;hr&gt;&lt;center&gt;nginx\/1.10.3 (Ubuntu)&lt;\/center&gt;\\r\\n&lt;\/body&gt;\\r\\n&lt;\/html&gt;\\r\\n', 'Response Headers': {'Content-Length': '182', 'Content-Type': 'text\/html', 'Date': 'Wed, 18 Apr 2018 14:06:30 GMT', 'Server': 'nginx\/1.10.3 (Ubuntu)', 'Connection': 'keep-alive'}},), 'Azure-cli-ml Version': '0.1.0b2'}\n<\/code><\/pre>\n\n<p>I have also tried altering my score.py file to return: <code>pyodbc.drivers()<\/code> this results in a blank '[]'<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":10,
        "Challenge_created_time":1523548986393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat web servic model score file queri sql databas pyodbc function perfectli test local workbench deploi docker pyodbc conda depend yml tri imag docker step file access root instal odbc driver ubuntu",
        "Challenge_last_edit_time":1524064366532,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49801180",
        "Challenge_link_count":4,
        "Challenge_original_content":"pyodbc web servic model creat web servic model instruct operation local docker score file queri sql databas pyodbc function perfectli test local environ workbench deploi docker come respons unixodbc driver open lib odbc driver sql server file sqldriverconnect pyodbc conda depend yml depend abil imag docker step file practic docker read tri docker step file add odbcinst ini odbcinst ini run apt updat run apt instal tdsodbc unixodbc dev run apt instal unixodbc bin run apt clean add type file hopefulli sens advic hope stumbl edit stuck progress access root docker exec root bash ran odbcinst unixodbc driver odbcinst ini data sourc odbc ini file data sourc odbcdatasourc data sourc root odbc ini sqlulen size sqllen size sqlsetposirow size locat odbc ini instruct instal odbc driver ubuntu run servic mlclierror score servic myapp respons respons gatewai gatewai nnginx ubuntu respons header length type text html date wed apr gmt server nginx ubuntu connect aliv cli version tri alter score file return pyodbc driver blank",
        "Challenge_participation_count":10,
        "Challenge_preprocessed_content":"pyodbc model creat model instruct operation local docker file queri sql databas pyodbc function perfectli test local environ workbench deploi docker come pyodbc depend abil imag docker step file practic docker read tri docker step file add type file hopefulli sens advic hope stumbl edit stuck access root ran odbcinst locat instruct instal odbc driver ubuntu run servic tri alter file return blank",
        "Challenge_readability":11.3,
        "Challenge_reading_time":47.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":34,
        "Challenge_solved_time":null,
        "Challenge_title":"pyodbc not working in web-service container, Azure Model Management",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":597.0,
        "Challenge_word_count":391,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":145699013607
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a Sagemaker instance running for a while now. I didn't change anything in between, but now I can't see new logs on Cloudwatch anymore. The old logs are still there, but no new ones since 2 days.<\/p>\n\n<p>The Sagemaker instance is still running. It's just not logging anymore. And as the code didn't change and I don't have anything time-dependant in there, I'm pretty sure I hit a limit. But I don't know which one:<\/p>\n\n<ul>\n<li>The Log group has only one log stream<\/li>\n<li>The single log stream has a size of 175MB.<\/li>\n<\/ul>\n\n<p>I found <a href=\"http:\/\/docs.amazonaws.cn\/en_us\/AmazonCloudWatch\/latest\/logs\/cloudwatch_limits_cwl.html\" rel=\"noreferrer\">CloudWatch Logs Limits<\/a> and <a href=\"http:\/\/docs.amazonaws.cn\/en_us\/AmazonCloudWatch\/latest\/events\/cloudwatch_limits_cwe.html\" rel=\"noreferrer\">CloudWatch Events Limits<\/a>, but that didn't help me.<\/p>\n\n<p>What could be the problem? How can I investigate it?<\/p>\n\n<p>According to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/logging-cloudwatch.html\" rel=\"noreferrer\">AWS docs<\/a> this should not happen. The general AWS support did not help.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":13,
        "Challenge_created_time":1524222364687,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"instanc log data cloudwatch instanc run hit limit cloudwatch log event limit accord document gener",
        "Challenge_last_edit_time":1524750998996,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/49940262",
        "Challenge_link_count":3,
        "Challenge_original_content":"cloudwatch stop log instanc run log cloudwatch anymor log on dai instanc run log anymor time depend pretti hit limit log group log stream singl log stream size cloudwatch log limit cloudwatch event limit accord doc gener",
        "Challenge_participation_count":15,
        "Challenge_preprocessed_content":"cloudwatch stop log instanc run log cloudwatch anymor log on dai instanc run log anymor pretti hit limit log group log stream singl log stream size cloudwatch log limit cloudwatch event limit accord doc gener",
        "Challenge_readability":8.0,
        "Challenge_reading_time":14.98,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":13.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Why did Cloudwatch stop logging Sagemaker?",
        "Challenge_topic":"Metrics Logging",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":2500.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":145025635313
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a the following challenge with SageMaker:<\/p>\n\n<ul>\n<li>I've downloaded one of the tutorial notebooks (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_abalone_age_predictor_using_keras\/tensorflow_abalone_age_predictor_using_keras.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_abalone_age_predictor_using_keras\/tensorflow_abalone_age_predictor_using_keras.ipynb<\/a>)<\/li>\n<li><p>I ran the training locally (successfully) with the modifying the following line:<\/p>\n\n<pre><code>abalone_estimator = TensorFlow(entry_point='abalone.py',\n                       role=role,\n                       training_steps= 100,\n                       evaluation_steps= 100,                                 \n                       hyperparameters={'learning_rate': 0.001},\n                       train_instance_count=1,\n                       **train_instance_type='local'**)\n\nabalone_estimator.fit(inputs)\n<\/code><\/pre><\/li>\n<li><p>I then wanted to deploy my model to AWS with the following line but it seems the SDK deploys it locally (it doesn't fail, I just see it running on my machine)<\/p>\n\n<p><code>abalone_predictor = abalone_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')<\/code><\/p><\/li>\n<\/ul>\n\n<p>Any tips on how to either fix it so it gets deployed to AWS or alternatively re-load my training model and deploy it to AWS from scratch? <\/p>\n\n<p>Many thanks,\nStefan<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524758821607,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"successfulli train model local deploi sdk deploi local tip deploy reload train model deploi scratch",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50047551",
        "Challenge_link_count":2,
        "Challenge_original_content":"train local deploi download tutori notebook http github com awslab blob master sdk tensorflow abalon ag predictor kera tensorflow abalon ag predictor kera ipynb ran train local successfulli modifi line abalon estim tensorflow entri abalon role role train step evalu step hyperparamet rate train instanc count train instanc type local abalon estim fit input deploi model line sdk deploi local run abalon predictor abalon estim deploi initi instanc count instanc type xlarg tip deploi load train model deploi scratch stefan",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"train local deploi download tutori notebook ran train local modifi line deploi model line sdk deploi local tip deploi train model deploi scratch stefan",
        "Challenge_readability":19.1,
        "Challenge_reading_time":18.99,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker - training locally but deploying to AWS?",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":797.0,
        "Challenge_word_count":119,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":144489178393
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>import boto3\nimport io\nimport pandas as pd\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    s3 = boto3.client('s3',\n    aws_access_key_id='REMOVED',\n    aws_secret_access_key='REMOVED')\n    obj = s3.get_object(Bucket='bucket', Key='data.csv000')\n    df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n    return df,\n<\/code><\/pre>\n\n<p>I'm tring to read data from S3 using the <code>Execute Python<\/code> module. I have downloaded the boto3 package and converted it to a zip. I have then uploaded and connected that .zip to the third input option of the module. When I run this code, I recieve an error stating botocore is not installed. Has anyone been able to read directly from S3 into Azure ML studio? I've tried using the R script module which also fails, so now I'm trying python.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1524879380623,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"read data download upload boto packag receiv state botocor instal tri modul",
        "Challenge_last_edit_time":1524880175620,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50072018",
        "Challenge_link_count":0,
        "Challenge_original_content":"read data import boto import import panda entri function input argument param panda datafram param panda datafram datafram datafram boto client access kei remov secret access kei remov obj object bucket bucket kei data csv read csv bytesio obj bodi read return tring read data execut modul download boto packag convert zip upload connect zip input option modul run reciev state botocor instal read directli studio tri modul",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"read data tring read data modul download boto packag convert zip upload connect zip input option modul run reciev state botocor instal read directli studio tri modul",
        "Challenge_readability":7.3,
        "Challenge_reading_time":12.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to read data from S3 using python in Azure ML",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":391.0,
        "Challenge_word_count":134,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":144368619377
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have an Azure Machine Learning experiment with thousands of labels.  When I run the web service I get a table with two rows (label name and label probability) but thousands of columns.  I would like to select just the 5 labels with the highest probability (and preferably sorted descending).  Anyone now an Azure ML Studio component or R script that can do this please? Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1525013713367,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"select label highest probabl tabl thousand label gener studio compon accomplish task",
        "Challenge_last_edit_time":1526050254003,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50087905",
        "Challenge_link_count":0,
        "Challenge_original_content":"choos label highest score web servic thousand label run web servic tabl row label label probabl thousand column select label highest probabl prefer sort descend studio compon",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"choos label highest score web servic thousand label run web servic tabl row thousand column select label highest probabl studio compon",
        "Challenge_readability":8.2,
        "Challenge_reading_time":5.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML R script to choose labels with highest score in web service",
        "Challenge_topic":"Data Labeling",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":53.0,
        "Challenge_word_count":78,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":144234286633
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm newbie of aws sagemaker. I'm trying to create notebook instance in aws console but the response is failed. Details error is: \"Unable to create encrypted EBS volume. Check if the KMS key exists. Invalid arn (Service: AWSKMS; Status Code: 400; Error Code: NotFoundException;\". Can you help me fix this? Thanks a lot!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1525486106503,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat notebook instanc messag state encrypt eb volum creat km kei assist",
        "Challenge_last_edit_time":1525530943043,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50185044",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat notebook instanc newbi creat notebook instanc consol respons creat encrypt eb volum km kei arn servic awskm statu notfoundexcept",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"creat notebook instanc newbi creat notebook instanc consol respons creat encrypt eb volum km kei arn servic awskm statu notfoundexcept",
        "Challenge_readability":5.0,
        "Challenge_reading_time":4.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't create notebook instance in amazon sagemaker",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":997.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":143761893497
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created training job in sagemaker with my own training and inference code using MXNet framework. I am able to train the model successfully and created endpoint as well. But while inferring the model, I am getting the following error:<\/p>\n\n<p><strong><em>\u2018ClientError: An error occurred (413) when calling the InvokeEndpoint operation: HTTP content length exceeded 5246976 bytes.\u2019<\/em><\/strong><\/p>\n\n<p>What I understood from my research is the error is due to the size of the image. The image shape is (480, 512, 3). I trained the model with images of same shape (480, 512, 3).<\/p>\n\n<p>When I resized the image to (240, 256), the error was gone. But producing another error 'shape inconsistent in convolution' as I the trained the model with images of size (480, 512).<\/p>\n\n<p>I didn\u2019t understand why I am getting this error while inferring.\nCan't we use images of larger size to infer the model?\nAny suggestions will be helpful<\/p>\n\n<p>Thanks, Harathi<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1525811131837,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"infer endpoint creat train infer mxnet framework imag larger size infer model",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50241771",
        "Challenge_link_count":0,
        "Challenge_original_content":"infer endpoint creat train job train infer mxnet framework train model successfulli creat endpoint infer model clienterror call invokeendpoint oper http length exceed byte understood research size imag imag shape train model imag shape resiz imag gone produc shape inconsist convolut train model imag size didnt infer imag larger size infer model harathi",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"infer endpoint creat train job train infer mxnet framework train model successfulli creat endpoint infer model clienterror call invokeendpoint oper http length exceed byte understood research size imag imag shape train model imag shape resiz imag gone produc shape inconsist convolut train model imag size didnt infer imag larger size infer model harathi",
        "Challenge_readability":8.4,
        "Challenge_reading_time":12.44,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting error while infering sagemaker endpoint",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1317.0,
        "Challenge_word_count":158,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":143436868163
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I\u2019m developing a Machine Learning group with a colleagues. The local area network infrastructure will be:<\/p>\n<ul>\n<li>Git server (one machine)<\/li>\n<li>Computing-data server (another machine)<\/li>\n<li>Local machines for users<\/li>\n<\/ul>\n<p>The way of working will be:<\/p>\n<ul>\n<li>Each user will have their own git-repos in their local machines (with they authentication keys in their local machines).<\/li>\n<li>Data and scripts have to be transferred to the computing server (data can be temporally in the server, only while is needed by the ML scripts)<\/li>\n<li>Users launch the ML operations from their local machines via SSH in the computing server<\/li>\n<\/ul>\n<p>We are performing some tests, and from now, we are doing synchronization operations in the DVC chain: first ,code and data are synched between the local machine and the computing server via SSH. After, the training is launch via SSH. Next, the outputs are transfered back from server to local machines, so that DVC can track the changes, and son on\u2026<\/p>\n<p>Questions and issues (among others):<\/p>\n<ol>\n<li>The path for the *.dvc file can be specified? (one path for each model)<\/li>\n<li>Can DVC be in the computing server while users have their git-repos in the local machines?<\/li>\n<\/ol>\n<p>Thank you very much for your effort, we really appreciate your work.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1525936928519,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"group colleagu local area network infrastructur git server comput data server separ git repo local data transfer comput server launch oper local ssh comput server perform synchron oper chain specifi path file comput server git repo local",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-and-a-lan-infrastructure-where-git-repos-are-not-in-the-computing-server\/24",
        "Challenge_link_count":0,
        "Challenge_original_content":"lan infrastructur git repo comput server group colleagu local area network infrastructur git server comput data server local git repo local authent kei local data transfer comput server data tempor server launch oper local ssh comput server perform test synchron oper chain data synch local comput server ssh train launch ssh output transfer server local track son path file specifi path model comput server git repo local effort",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"lan infrastructur git repo comput server group colleagu local area network infrastructur git server server local local data transfer comput server launch oper local ssh comput server perform test synchron oper chain data synch local comput server ssh train launch ssh output transfer server local track son path file specifi comput server local effort",
        "Challenge_readability":11.7,
        "Challenge_reading_time":17.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Does DVC and a LAN infrastructure where Git repos are not in the computing server?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":893.0,
        "Challenge_word_count":222,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":143311071481
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>As a part of <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/\" rel=\"nofollow noreferrer\">Azure Machine Learning<\/a> process, I need to <code>continually<\/code> migrate data from on-premises SQL Db to Azure SQL Db using <code>Data Management Gateway<\/code>.<\/p>\n\n<p>This Azure official article describes how to: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/team-data-science-process\/move-sql-azure-adf\" rel=\"nofollow noreferrer\">Move data from an on-premises SQL server to SQL Azure with Azure Data Factory<\/a>. But the details are a bit confusing to me. If someone to briefly describe the process, how would you do that. What are 2-3 <code>main<\/code> steps one needs to perform on <code>on-premises<\/code> and 2-3 steps on <code>Azure Cloud<\/code>? No details are needed. <strong>Note<\/strong>: The solution has to involve using <code>Data Management Gateway<\/code><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526403046690,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"migrat data premis sql databas sql databas data gatewai process offici articl brief descript process request step premis cloud involv data gatewai",
        "Challenge_last_edit_time":1526409433763,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50355494",
        "Challenge_link_count":2,
        "Challenge_original_content":"migrat data premis sql sql process migrat data premis sql sql data gatewai offici articl data premis sql server sql data factori bit briefli process step perform premis step cloud note involv data gatewai",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"migrat data sql sql process migrat data sql sql offici articl data sql server sql data factori bit briefli process step perform step note involv",
        "Challenge_readability":10.2,
        "Challenge_reading_time":12.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"How to continually migrate data from on-premises SQL Db to Azure SQL Db",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":103.0,
        "Challenge_word_count":117,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":142844953310
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"I am trying to launch a job using the low level api in boto3 sagemaker client. After calling sagemaker.create_training_job(**params) I try to get a waiter. This code is directly from the documentation for creating a training job (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html)\nI get this error:\n\nTraceback (most recent call last):\r\n  File \"traindeploy.py\", line 97, in create_training_job\r\n    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 53, in wait\r\n    Waiter.wait(self, **kwargs)\r\n  File \"\/path\/to\/lib\/Python\/3.6\/lib\/python\/site-packages\/botocore\/waiter.py\", line 323, in wait\r\n    last_response=response,\r\nbotocore.exceptions.WaiterError: Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal failure state\n\n\nThese are my job params:\n\n{\r\n  \"AlgorithmSpecification\": {\r\n    \"TrainingImage\": \"<image-url-from-ecr>\",\r\n    \"TrainingInputMode\": \"File\"\r\n  },\r\n  \"RoleArn\": \"<role-arn>\",\r\n  \"OutputDataConfig\": {\r\n    \"S3OutputPath\": \"s3:\/\/path-to-bucket\/some-folder-output\/\"\r\n  },\r\n  \"ResourceConfig\": {\r\n    \"InstanceCount\": 2,\r\n    \"InstanceType\": \"ml.c4.8xlarge\",\r\n    \"VolumeSizeInGB\": 50\r\n  },\r\n  \"TrainingJobName\": \"some-jobname\",\r\n  \"HyperParameters\": {},\r\n  \"StoppingCondition\": {\r\n    \"MaxRuntimeInSeconds\": 3600\r\n  },\r\n  \"InputDataConfig\": [\r\n    {\r\n      \"ChannelName\": \"train\",\r\n      \"DataSource\": {\r\n        \"S3DataSource\": {\r\n          \"S3DataType\": \"S3Prefix\",\r\n          \"S3Uri\": \"s3:\/\/path-to-bucket\/some-folder-input\/\",\r\n          \"S3DataDistributionType\": \"FullyReplicated\"\r\n        }\r\n      },\r\n      \"CompressionType\": \"None\",\r\n      \"RecordWrapperType\": \"None\"\r\n    }\r\n  ]\r\n}\n\n\nCan someone please advise what is causing this and how will I get a waiter on a training job?",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526422450000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"launch job low level api boto client call creat train job param tri waiter termin state job paramet advic waiter train job",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/repost.aws\/questions\/QUeYBDZTVwQlWovkq2eE1CkQ\/waiter-training-job-completed-or-stopped-failed-waiter-encountered-a-terminal",
        "Challenge_link_count":1,
        "Challenge_original_content":"waiter trainingjobcompletedorstop waiter termin launch job low level api boto client call creat train job param waiter directli document creat train job http doc com latest train model creat train job html traceback file traindeploi line creat train job waiter train job complet stop wait trainingjobnam job file path lib lib site packag botocor waiter line wait waiter wait kwarg file path lib lib site packag botocor waiter line wait respons respons botocor except waitererror waiter trainingjobcompletedorstop waiter termin state job param algorithmspecif trainingimag traininginputmod file rolearn outputdataconfig soutputpath path bucket folder output resourceconfig instancecount instancetyp xlarg volumesizeingb trainingjobnam jobnam hyperparamet stoppingcondit maxruntimeinsecond inputdataconfig channelnam train datasourc sdatasourc sdatatyp sprefix suri path bucket folder input sdatadistributiontyp fullyrepl compressiontyp recordwrappertyp waiter train job",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"waiter trainingjobcompletedorstop waiter termin launch job low level api boto client call waiter directli document creat train job traceback file line file line wait kwarg file line wait waiter trainingjobcompletedorstop waiter termin state job param algorithmspecif rolearn outputdataconfig resourceconfig trainingjobnam hyperparamet stoppingcondit inputdataconfig waiter train job",
        "Challenge_readability":17.1,
        "Challenge_reading_time":23.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":401.0,
        "Challenge_word_count":147,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":142825550000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We have developers working with data shared on a local network, and I\u2019d like to understand whether\/how dvc could integrate with this pipeline. I think I\u2019m asking whether its possible (or even makes sense) to have a single, shared cache \u2013 kinda like the dvc cloud workflow you describe but without push\/pull. The code just reads\/writes data outside the git repo.<\/p>\n<p>Another reason I\u2019d like to leave data outside the repo is because many projects have the same (large) dataset as a dependency.<\/p>\n<p><s>Is the answer hardlinks? Worried 'cause there\u2019s already a lot of linking going on \u2026<\/s> (Duh. Not across filesystems!)<\/p>\n<p>To be clear, this looks like an awesome tool that I\u2019d like to adapt to if possible.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526441315809,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"integr pipelin share network storag singl share cach push pull leav data outsid repo feasibl larg dataset depend hardlink realiz filesystem adapt workflow",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/can-and-how-dvc-work-with-shared-network-storage\/26",
        "Challenge_link_count":0,
        "Challenge_original_content":"share network storag data share local network integr pipelin sens singl share cach kinda cloud workflow push pull read write data outsid git repo reason leav data outsid repo larg dataset depend hardlink worri there link duh filesystem clear adapt",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"share network storag data share local network integr pipelin singl share cach kinda cloud workflow data outsid git repo reason leav data outsid repo dataset depend hardlink worri there link clear adapt",
        "Challenge_readability":6.4,
        "Challenge_reading_time":9.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Can and how DVC work with shared network storage?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1063.0,
        "Challenge_word_count":126,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":142806684191
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>Hello!<br>\nI am currently evaluating tools for keeping track of data in our workflow, and I am looking at how DVC works. First off, thanks for developing it, Dmitry, as it is a very nice tool.<\/p>\n<p>I have read the tutorial on your blog and there are a couple of question I have, though, which I suspect are quite critical.<\/p>\n<p>First: the use of hard links is a great idea when it comes to speed, but what happens if one changes the data file? I tried and the <code>dvc status<\/code> reports \u201ccorrupted cache data\u201d, as the new hash is different from the saved one. Apparently, there is no way of going back, is dvc assuming that data are immutable? Because it might be in some use cases, but not in general. Also, mistakes might happen.<\/p>\n<p>Second, the run functionality is extremely useful, but it appears to be as much fragile. Apparently, dvc is not tied to git when it comes to tracking which files are used for a specific run: I have to specify the dependency to source code using the -d flag to make runs reproducible. What happens if I do not specify a file that is actually being used? What happens if that file changes and a new run is executed? Apparently, dvc does not check for differences and no message or warning is raised. repro can be forced, and results change, but no warning is raised even in this case.<\/p>\n<p>So, I am interested into knowing if I am using the tool in an inappropriate way: am I missing something in DVC philosophy?<\/p>\n<p>Thanks!<br>\n~Alessandro<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526466665911,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"evalu track data workflow critic data immut allow data file run function fragil ti git run reproduc file specifi warn clarif inappropri miss philosophi",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-and-code-dependencies\/28",
        "Challenge_link_count":0,
        "Challenge_original_content":"track data depend evalu keep track data workflow dmitri nice read tutori blog coupl critic link great idea come speed data file tri statu report corrupt cach data hash save appar data immut gener run function extrem fragil appar ti git come track file run specifi depend sourc flag run reproduc specifi file file run execut appar messag warn rais repro forc warn rais inappropri miss philosophi alessandro",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"track data depend evalu keep track data workflow dmitri nice read tutori blog coupl critic link great idea come speed data file tri report corrupt cach data hash save appar data immut gener run function extrem fragil appar ti git come track file run specifi depend sourc flag run reproduc specifi file file run execut appar messag warn rais repro forc warn rais inappropri miss philosophi alessandro",
        "Challenge_readability":7.6,
        "Challenge_reading_time":18.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Tracking data and code dependencies",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":1491.0,
        "Challenge_word_count":272,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":142781334089
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I know how to download a project from GitHub as explained <a href=\"https:\/\/stackoverflow.com\/a\/6466993\/1232087\">here<\/a>. Working on <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">Microsoft Azure Bot Service<\/a>. And need to download a <code>package<\/code> that is for <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-debug-emulator?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">Microsoft Bot Framework Emulator<\/a>. The link for the download is given in the in the section <code>Download the Bot Framework Emulator<\/code> of <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/bot-service-debug-emulator?view=azure-bot-service-3.0\" rel=\"nofollow noreferrer\">this<\/a> Microsoft article. That link takes me to <a href=\"https:\/\/github.com\/Microsoft\/BotFramework-Emulator\/releases\" rel=\"nofollow noreferrer\">this GitHub site<\/a>. But I'm not clear on how to download the package from there.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526493976403,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"download packag github bot framework emul clear download github site articl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50377423",
        "Challenge_link_count":5,
        "Challenge_original_content":"download packag github download github explain bot servic download packag bot framework emul link download section download bot framework emul articl link github site clear download packag",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"download packag github download github explain bot servic download bot framework emul link download section articl link github site clear download packag",
        "Challenge_readability":14.8,
        "Challenge_reading_time":13.94,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"How to download a package from GitHub",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":525.0,
        "Challenge_word_count":91,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":142754023597
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":14,
        "Challenge_body":"<p>Hi<\/p>\n<p>I was wondering how to setup a shared cache directory, if possible?<\/p>\n<p>Thanks<br>\nMatthias<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526900412793,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"set share cach directori",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-directory\/31",
        "Challenge_link_count":0,
        "Challenge_original_content":"share cach directori setup share cach directori matthia",
        "Challenge_participation_count":14,
        "Challenge_preprocessed_content":"share cach directori setup share cach directori matthia",
        "Challenge_readability":7.9,
        "Challenge_reading_time":1.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Shared cache directory",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2202.0,
        "Challenge_word_count":17,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":142347587207
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>We currently use a single repository for data science related projects (mainly due to limitations of GitHub private repositories). When cloning that repository, all Git LFS files are downloaded, which takes quite a while. Is there a way that using DVC would allow us to defer loading data from some of our data-science sub-projects until we are ready to work on the notebooks?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1526909062879,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"time download git lf file clone repositori data scienc limit github privat repositori defer load data sub readi notebook",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/defer-loading-data-when-cloning-repository\/33",
        "Challenge_link_count":0,
        "Challenge_original_content":"defer load data clone repositori singl repositori data scienc relat mainli limit github privat repositori clone repositori git lf file download allow defer load data data scienc sub readi notebook",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"defer load data clone repositori singl repositori data scienc relat clone repositori git lf file download allow defer load data readi notebook",
        "Challenge_readability":9.9,
        "Challenge_reading_time":5.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Defer loading data when cloning repository?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":529.0,
        "Challenge_word_count":68,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":142338937121
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>When executing the command below: <\/p>\n\n<p><code>az ml model register -m &lt;pkl name&gt;.pkl -n &lt;model name&gt; -d \"dummy model\" --debug --verbose<\/code><\/p>\n\n<p>I get the error stating that the URL cannot be connected to. The verbose message does not show any error before the one below. I can confirm that the model management account and environments have been set. <\/p>\n\n<p>I am using the Visual Studio subscription to test out some functionality. Any help is appreciated!<\/p>\n\n<pre><code>{\n    \"Azure-cli-ml Version\": \"0.1.0a27.post3\",\n    \"Error\": \"Error connecting to https:\/\/australiaeast.modelmanagement.azureml.net\/api\/subscriptions\/ad19a4a2-ed65-4574-aec3-e247c4d96efd\/resourceGroups\/rcity-rg-bi-001-azureml-3797f\/accounts\/rcity-bi-mlexpmgmt-002\/models.\"\n}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527037222223,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi model model messag state url connect verbos messag model account environ set visual studio subscript test function",
        "Challenge_last_edit_time":1527064442136,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50478395",
        "Challenge_link_count":1,
        "Challenge_original_content":"deploi model modelmanag execut model regist pkl dummi model debug verbos state url connect verbos messag model account environ set visual studio subscript test function cli version connect http australiaeast modelmanag net api subscript adaa aec ecdefd resourcegroup rciti account rciti mlexpmgmt model",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"deploi model modelmanag execut state url connect verbos messag model account environ set visual studio subscript test function",
        "Challenge_readability":12.0,
        "Challenge_reading_time":10.86,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Error when deploying Azure ML model to modelmanagement",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":129.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":142210777777
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have downloaded ONNX model form CustomVision.ai and now I want to import into tensorflow and I am follwing \"<a href=\"https:\/\/github.com\/onnx\/tutorials\/blob\/master\/tutorials\/OnnxTensorflowImport.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/onnx\/tutorials\/blob\/master\/tutorials\/OnnxTensorflowImport.ipynb<\/a>\" for guidance.<\/p>\n\n<p>I have installed all the prerequisites as discussed in the above link. I am facing an error while executing \"tf_rep = prepare(model)\"---ValidationError: BatchNormalization.scale in initializer but not in graph input<\/p>\n\n<pre><code>import onnx\nfrom onnx_tf.backend import prepare\nmodel = onnx.load('C:\\\\Pankaj\\\\XYZ\\\\abc.onnx')\ntf_rep = prepare(model)\n<\/code><\/pre>\n\n<p>Thank you for your help and time.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527058196637,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"import onnx model customvis tensorflow guidanc github link messag state validationerror batchnorm scale initi graph input instal prerequisit link",
        "Challenge_last_edit_time":1527058971816,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50481601",
        "Challenge_link_count":2,
        "Challenge_original_content":"import onnx model tensorflow validationerror batchnorm scale initi graph input download onnx model customvis import tensorflow follw http github com onnx tutori blob master tutori onnxtensorflowimport ipynb guidanc instal prerequisit link execut rep prepar model validationerror batchnorm scale initi graph input import onnx onnx backend import prepar model onnx load pankaj xyz abc onnx rep prepar model time",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"import onnx model initi graph input download onnx model import tensorflow follw guidanc instal prerequisit link execut initi graph input time",
        "Challenge_readability":18.1,
        "Challenge_reading_time":11.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Import ONNX model to tensorflow-ValidationError: BatchNormalization.scale in initializer but not in graph input",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":434.0,
        "Challenge_word_count":84,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":142189803363
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Amazon Sagemaker to run a xgboost model in order to bet the best hyperparameter combination. I have to use the sagemaker implementation and not the notebook alternative to test if it runs faster than a gridsearch.\nMy problem is how can I make this work in a loop. Any Ideas? \nMy understanding is that I have to code numerous jobs with different combinations.\nI tried this as a test:<\/p>\n\n<pre><code>for i in range (1,3):\n    for j in range (13,15):\n        job_name = 'regression' + '-'+str(i) +\"-\"+str(j)+\"-\" +strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n\n        job_name_params = copy.deepcopy(parameters_xgboost)\n        job_name_params['TrainingJobName'] = job_name\n        job_name_params['OutputDataConfig']['S3OutputPath']= \".....\"\n        job_name_params['HyperParameters']['objective'] = \"reg:linear\"\n        job_name_params['HyperParameters']['silent'] = \"0\"\n        job_name_params['HyperParameters']['max_depth'] = str(i)\n        job_name_params['HyperParameters']['min_child_weight'] = str(j)\n        job_name_params['HyperParameters']['eta'] = \"0.01\"\n        job_name_params['HyperParameters']['num_round'] = \"1000\"\n        job_name_params['HyperParameters']['subsample'] = \"0.5\"\n        job_name_params['HyperParameters']['colsample_bytree'] = \"0.5\"\n\n        sm = boto3.Session().client('.....')\n\n\n        sm.create_training_job(**job_name_params)\n        sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n        status = sm.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n        print(\"Training job ended with status: \" + status)\n<\/code><\/pre>\n\n<p>parameters_xgboost is how Sagemaker reads basic info and list of hyper params.<\/p>\n\n<p>The good thing is that it works. The bad this is that this trains the models one at a time. I would like all of these combinations to run a the same time.\nHow can I do that?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527249785780,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run model hyperparamet combin implement notebook test run faster grid search loop numer job combin model train time run combin time",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50528814",
        "Challenge_link_count":0,
        "Challenge_original_content":"grid search run model order bet hyperparamet combin implement notebook test run faster gridsearch loop idea numer job combin tri test rang rang job regress str str strftime gmtime job param copi deepcopi paramet job param trainingjobnam job job param outputdataconfig soutputpath job param hyperparamet object reg linear job param hyperparamet silent job param hyperparamet depth str job param hyperparamet child weight str job param hyperparamet eta job param hyperparamet num round job param hyperparamet subsampl job param hyperparamet colsampl bytre boto session client creat train job job param waiter train job complet stop wait trainingjobnam job statu train job trainingjobnam job trainingjobstatu print train job end statu statu paramet read list hyper param train model time combin run time",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"grid search run model order bet hyperparamet combin implement notebook test run faster gridsearch loop idea numer job combin tri test read list hyper param train model time combin run time",
        "Challenge_readability":13.2,
        "Challenge_reading_time":23.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Xgboost Amazon Sagemaker grid search alternative",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":667.0,
        "Challenge_word_count":177,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":141998214220
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>i've created a clustering model on sagemaker and i'm invoking it via CLI with this command: \naws sagemaker-runtime invoke-endpoint --endpoint-name myendpoint --body  $mydata --content-type text\/csv output.json --region eu-west-1<\/p>\n\n<p>If my data starts with a negative number, i get an error\n\"usage: aws [options]   [ ...] [parameters]\nTo see help text, you can run:<\/p>\n\n<p>aws help\n  aws  help\n  aws   help\naws: error: argument --body: expected one argument\"<\/p>\n\n<p>While if it's a positive number, everything works. How can i escape the first minus of the data to make it work?\nThanks in advance<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1527534538363,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok cluster model cli data start neg receiv messag escap minu data",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50572300",
        "Challenge_link_count":0,
        "Challenge_original_content":"invok endpoint csv creat cluster model invok cli runtim invok endpoint endpoint myendpoint bodi mydata type text csv output json region data start neg usag option paramet text run argument bodi argument posit escap minu data advanc",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"csv creat cluster model invok cli runtim myendpoint mydata data start neg usag text run argument argument posit escap minu data advanc",
        "Challenge_readability":9.4,
        "Challenge_reading_time":7.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Aws Sagemaker invoke-endpoint call and csv",
        "Challenge_topic":"Kubernetes Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1488.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":141713461637
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I installed Azure ML workbench and attempted to create an env with<\/p>\n\n<p>az ml env setup -c -n  -g  --location <\/p>\n\n<p>but keep getting failure<\/p>\n\n<pre><code>        \"code\": \"Conflict\",\n        \"message\": \"SubDeployment: OperationId=08586736946377270318, ProvisioningState=Failed, StatusCode=Conflict, StatusMessage=Template output evaluation skipped: at least one resource deployment operation failed. Please list deployment operations for details.\n<\/code><\/pre>\n\n<p>How can I look into what this error is exactly, and how to get around it?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1527928551227,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set workbench app receiv messag conflict creat environ assist identifi",
        "Challenge_last_edit_time":1528028009390,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50655011",
        "Challenge_link_count":0,
        "Challenge_original_content":"workbench app setup instal workbench creat env env setup locat conflict messag subdeploy operationid provisioningst statuscod conflict statusmessag templat output evalu skip resourc deploy oper list deploy oper exactli",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"workbench app setup instal workbench creat env env setup exactli",
        "Challenge_readability":13.0,
        "Challenge_reading_time":7.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ml workbench demo app keep failing during setup",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":55.0,
        "Challenge_word_count":75,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":141319448773
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p><strong>Problem statement first:<\/strong> How does one properly setup tensorflow for running on a DSVM using a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/desktop-workbench\/experimentation-service-configuration#running-a-script-on-a-remote-docker\" rel=\"nofollow noreferrer\">remote Docker environment<\/a>? Can this be done in <code>aml_config\/*.runconfig<\/code>?<\/p>\n\n<p>I receive the following message and I would like to be able to utilize the increased speeds of the extended FMA operations.<\/p>\n\n<blockquote>\n  <p>tensorflow\/core\/platform\/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<\/p>\n<\/blockquote>\n\n<p><strong>Background:<\/strong> I utilize a local docker environment managed through Azure ML Workbench for initial testing and code validation so that I'm not running an expensive DSVM constantly. Once I assess that my code is to my liking, I then run it on a remote docker instance on an <a href=\"https:\/\/azuremarketplace.microsoft.com\/en-us\/marketplace\/apps\/microsoft-ads.linux-data-science-vm-ubuntu\" rel=\"nofollow noreferrer\">Azure DSVM<\/a>.<\/p>\n\n<p>I want a consistent conda environment across my compute environments, so this works out extremely well. However, I cannot figure out how to control the tensorflow build to optimize for the hardware at hand (i.e. my local docker on macOS vs. remote docker on Ubuntu DSVM)<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1528216512097,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set tensorflow run dsvm remot docker environ receiv messag cpu instruct tensorflow binari compil util local docker environ initi test run remot docker instanc dsvm figur control tensorflow build optim hardwar hand",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50704982",
        "Challenge_link_count":2,
        "Challenge_original_content":"tensorflow optim dsvm statement properli setup tensorflow run dsvm remot docker environ aml config runconfig receiv messag util increas speed extend fma oper tensorflow core platform cpu featur guard cpu instruct tensorflow binari compil avx fma background util local docker environ workbench initi test run expens dsvm constantli assess run remot docker instanc dsvm consist conda environ comput environ extrem figur control tensorflow build optim hardwar hand local docker maco remot docker ubuntu dsvm",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"tensorflow optim dsvm statement properli setup tensorflow run dsvm remot docker environ receiv messag util increas speed extend fma oper cpu instruct tensorflow binari compil avx fma background util local docker environ workbench initi test run expens dsvm constantli assess run remot docker instanc dsvm consist conda environ comput environ extrem figur control tensorflow build optim hardwar hand",
        "Challenge_readability":13.1,
        "Challenge_reading_time":18.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"TensorFlow Optimization in DSVM",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":79.0,
        "Challenge_word_count":170,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":141031487903
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using Azure Machine Learning Studio and what to add a running total on my dataset. This includes a date column, and I want to sum all the rows (for a group) on or before the row date.<\/p>\n\n<p>In SQL Server, I would use:<\/p>\n\n<pre><code>    SELECT [t1].*,\nSUM([t1].[Amount (Settlement CCY)) \nOVER (\n  PARTITION BY [t1].[Contract Ref], [t1].[LOBCode], [t1].[Superline], [t1].[Occupation], [t1].[TransType], [t1].[SettCCY]\n  ORDER BY     [t1].[Transaction Date] ASC\n  ROWS BETWEEN UNBOUNDED PRECEDING\n       AND     CURRENT ROW\n)\nFROM [t1]\nGROUP BY [t1].[contract ref], [t1].[Transaction date], [t1].[LOBCode], [t1].[Superline], [t1].[Occupation], [t1].[TransType], [t1].[SettCCY]\n<\/code><\/pre>\n\n<p>but Azure Machine learning uses SQLite where the Over \/ Partition clauses aren't implemented.<\/p>\n\n<p>I've tried an alternative in python\/pandas:<\/p>\n\n<pre><code>dataframe1 = dataframe1.assign(cumAMTscTD=dataframe1.groupby(['ContractRef', 'Basis', 'LOBCode', 'Superline', 'Occupation', 'TransType', 'SettCCY'])['AmtSettCCY'].transform('sum')).sort_values(['ContractRef','TransDate'])\n<\/code><\/pre>\n\n<p>but this sums up everything for the group, not just the those for the dates up toe current row. I assume therefore it doesn't cover the:<\/p>\n\n<pre><code>ROWS BETWEEN UNBOUNDED PRECEDING\n   AND     CURRENT ROW\n<\/code><\/pre>\n\n<p>How would I acheive this?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1528369446120,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"run dataset studio sum row group row date partit claus implement studio sqlite tri panda sum group date row achiev",
        "Challenge_last_edit_time":1528370042660,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50739607",
        "Challenge_link_count":0,
        "Challenge_original_content":"partit row equival panda studio add run dataset date column sum row group row date sql server select sum settlement ccy partit contract ref lobcod superlin occup transtyp settcci order transact date asc row unbound preced row group contract ref transact date lobcod superlin occup transtyp settcci sqlite partit claus aren implement tri panda datafram datafram assign cumamtsctd datafram groupbi contractref basi lobcod superlin occup transtyp settcci amtsettcci transform sum sort valu contractref transdat sum group date toe row cover row unbound preced row acheiv",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"partit row equival panda studio add run dataset date column sum row row date sql server sqlite partit claus aren implement tri sum group date toe row cover acheiv",
        "Challenge_readability":9.8,
        "Challenge_reading_time":17.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Partition by Rows equivalent in pandas (python",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":744.0,
        "Challenge_word_count":160,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":140878553880
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I have a bucket called <code>my_bucket<\/code> and a folder in it called <code>Images<\/code>. I am trying to read the files (images) inside the <code>Image<\/code> folder.<\/p>\n\n<pre><code>file = pd.read_csv(some_csv_file)\nX = file.values[:,0]\n\nrole = get_execution_role()\nbucket='my_bucket'\ndata_key = 'Images'\ndata_dir = 's3:\/\/{}\/{}'.format(bucket, data_key)\ns = '\/'\n\nfor img_name in X:\n    seq = (data_dir, img_name)\n    img_path = s.join(seq)\n    img = imread(img_path)\n<\/code><\/pre>\n\n<p>But it gives the following error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-20-a273242ed30e&gt; in &lt;module&gt;()\n     43     img_path = s.join(seq)\n     44     print(img_path)\n---&gt; 45     img = imread(img_path)\n     46     img = imresize(img, (32, 32))\n     47     img = img.astype('float32') # this will help us in later stage\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/numpy\/lib\/utils.py in newfunc(*args, **kwds)\n     99             \"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\n    100             warnings.warn(depdoc, DeprecationWarning, stacklevel=2)\n--&gt; 101             return func(*args, **kwds)\n    102 \n    103         newfunc = _set_function_name(newfunc, old_name)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/scipy\/misc\/pilutil.py in imread(name, flatten, mode)\n    162     \"\"\"\n    163 \n--&gt; 164     im = Image.open(name)\n    165     return fromimage(im, flatten=flatten, mode=mode)\n    166 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/PIL\/Image.py in open(fp, mode)\n   2541 \n   2542     if filename:\n-&gt; 2543         fp = builtins.open(filename, \"rb\")\n   2544         exclusive_fp = True\n   2545 \n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/my_bucket\/Images\/377.jpg'\n<\/code><\/pre>\n\n<p><code>377.jpg<\/code> is the first row in <code>X<\/code>. I checked manually in the S3 storage; this file is present there. So, why am I getting this error, and how to fix it? The only reason I can think of is, maybe the process of specifying the S3 path is wrong - but in the S3 documentation, the process to specify storage is given as <code>'s3:\/\/{}\/{}'.format(bucket, data_key)<\/code>. Moreover, in the last line of the error message, the filename is <code>s3:\/\/my_bucket\/Images\/377.jpg<\/code>, which is the path I navigate manually to locate the file in the bucket.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":3,
        "Challenge_created_time":1528452088657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"read file folder bucket messag file specifi path manual verifi file present specifi locat reason",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50758127",
        "Challenge_link_count":0,
        "Challenge_original_content":"file specifi path manual bucket call bucket folder call imag read file imag insid imag folder file read csv csv file file valu role execut role bucket bucket data kei imag data dir format bucket data kei img seq data dir img img path seq img imread img path filenotfounderror traceback img path seq print img path img imread img path img imres img img img astyp later stage anaconda env lib site packag numpi lib util newfunc arg kwd arrayrang deprec arang warn warn depdoc deprecationwarn stacklevel return func arg kwd newfunc set function newfunc anaconda env lib site packag scipi misc pilutil imread flatten mode imag open return fromimag flatten flatten mode mode anaconda env lib site packag pil imag open mode filenam builtin open filenam exclus filenotfounderror errno file directori bucket imag jpg jpg row manual storag file present reason mayb process specifi path document process specifi storag format bucket data kei line messag filenam bucket imag jpg path navig manual locat file bucket",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"file specifi path manual bucket call folder call read file insid folder row manual storag file present reason mayb process specifi path document process specifi storag line messag filenam path navig manual locat file bucket",
        "Challenge_readability":9.1,
        "Challenge_reading_time":30.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Why is the code not able to find the file specified in the AWS S3 path, when I can find it manually?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":3376.0,
        "Challenge_word_count":271,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":140795911343
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I made a web service from a python notebook and the output is:<\/p>\n\n<pre><code>{\"Results\":{\"output1\":{\"type\":\"table\",\"value\":{\"Values\":[[\"1\"]]}},\"output2\":{\"type\":\"table\",\"value\":{\"Values\":[[\"data:text\/plain,Execution OK\\r\\n\",null]]}}}}\n<\/code><\/pre>\n\n<p>But I just wanted the response to be the value in the key \"Values\" so that way I don't have to parse it on the client side. Is that possible?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1528731407633,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat web servic notebook studio output desir format respons valu kei valu avoid pars client advic",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50801404",
        "Challenge_link_count":0,
        "Challenge_original_content":"studio web servic output web servic notebook output output type tabl valu valu output type tabl valu valu data text plain execut null respons valu kei valu pars client",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"studio web servic output web servic notebook output respons valu kei valu pars client",
        "Challenge_readability":12.3,
        "Challenge_reading_time":5.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML studio web service output",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":88.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":140516592367
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have a custom container (derived from <code>nvidia\/cuda:9.0-runtime<\/code>) to run trainings on sagemaker. But on startup i'm getting the error <code>CUDA driver version is insufficient for CUDA runtime version at torch\/csrc\/cuda\/Module.cpp:32<\/code> which apparently wants to tell that my cuda version doesnt support the graphics driver (...how nice would it be to expose both version numbers along with the error message...), but i cannot figure out how to find out what display driver is mounted in the container. All i can find is that it says that sagemaker has nvidia-docker buildin. I tried to fire <code>nvidia-smi<\/code> before the error occures, but that command isnt known in the container. There is a mysterious sentence <\/p>\n\n<pre><code>\"If you plan to use GPU devices for model inferences (by specifying \nGPU-based ML compute instances in your CreateEndpointConfig request),\nmake sure that your containers are nvidia-docker compatible.\"\n<\/code><\/pre>\n\n<p>I'm pretty sure that this is the case, but there is no checkbox or whatever to toggle \"run this container with host GPU access\". Any ideas how i can proceed?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1528983599377,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"receiv messag mismatch cuda driver version cuda runtim version determin displai driver mount proce nvidia docker built advic run host gpu access",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50858907",
        "Challenge_link_count":0,
        "Challenge_original_content":"cuda nvidia driver mismatch deriv nvidia cuda runtim run train startup cuda driver version insuffici cuda runtim version torch csrc cuda modul cpp appar cuda version doesnt graphic driver nice expos version messag figur displai driver mount sai nvidia docker buildin tri nvidia smi isnt mysteri sentenc plan gpu devic model infer specifi gpu base comput instanc createendpointconfig request nvidia docker compat pretti checkbox toggl run host gpu access idea proce",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"cuda mismatch run train startup appar cuda version doesnt graphic driver figur displai driver mount sai buildin tri isnt mysteri sentenc pretti checkbox toggl run host gpu access idea proce",
        "Challenge_readability":10.3,
        "Challenge_reading_time":14.95,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"cuda \/ nvidia-driver mismatch on sagemaker with custom container",
        "Challenge_topic":"GPU Training",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1059.0,
        "Challenge_word_count":179,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":140264400623
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I am trying to invoke an Amazon Sagemaker Endpoint from a local python notebook. This is the code I am using.  <\/p>\n\n<pre><code>import boto3\n\naws_access_key_id = '...............'\naws_secret_access_key = '................'\ntkn = '..........'\nregion_name = '............'\n\namz = boto3.client('sagemaker-runtime',\n                   aws_access_key_id=aws_access_key_id,\n                   aws_secret_access_key=aws_secret_access_key,\n                   aws_session_token=tkn,\n                   region_name=region_name)\n\n\nresponse = amz.invoke_endpoint(\n    EndpointName='mymodel',\n    Body=b'bytes'\n)               \n<\/code><\/pre>\n\n<p>However, this doesn't work. Do I have to specify something else in <em>Body<\/em> ?   <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529050767617,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok endpoint local notebook specifi bodi paramet",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50871651",
        "Challenge_link_count":0,
        "Challenge_original_content":"notebook invok endpoint local invok endpoint local notebook import boto access kei secret access kei tkn region amz boto client runtim access kei access kei secret access kei secret access kei session token tkn region region respons amz invok endpoint endpointnam mymodel bodi byte specifi bodi",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"notebook invok endpoint local invok endpoint local notebook specifi bodi",
        "Challenge_readability":11.3,
        "Challenge_reading_time":8.81,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"Python Notebook Invoke Endpoint Sagemaker from Local",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":3972.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":140197232383
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi all,\n\u00a0\nI have pandas 0.22.0 installed in ubuntu16.04, it successfully run\u00a0python example\/tutorial\/train.py:\npython example\/tutorial\/train.py\u00a0\nElasticnet model (alpha=0.500000, l1_ratio=0.500000):\n\u00a0 RMSE: 0.82224284976\n\u00a0 MAE: 0.627876141016\n\u00a0 R2: 0.126787219728\n\n\nbut failed as below:\n\u00a0mlflow run example\/tutorial -P alpha=0.5 --no-conda\n\n=== Fetching project from example\/tutorial ===\n=== Work directory for this run: example\/tutorial ===\n=== Created directory \/tmp\/tmpigdg385u for downloading remote URIs passed to arguments of type 'path' ===\n=== Running command: python train.py 0.5 0.1 ===\nTraceback (most recent call last):\n\u00a0 File \"train.py\", line 9, in <module>\n\u00a0 \u00a0 import pandas as pd\nImportError: No module named pandas\n=== Run failed ===\n\n\n\n\ndouble checked that pandas installed:\n$ python\nPython 3.5.1+ (default, Mar 30 2016, 22:46:26)\u00a0\n[GCC 5.3.1 20160330] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pandas as pd\n>>>\u00a0\n\n\n\n\nAny advice or work around?\n\n\nThanks,\nForest",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529404855000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"run run absenc panda modul instal run train file advic workaround",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QnASq7ITAoI",
        "Challenge_link_count":0,
        "Challenge_original_content":"modul panda run panda instal ubuntu successfulli runpython tutori train tutori train elasticnet model alpha ratio rmse mae run tutori alpha conda fetch tutori directori run tutori creat directori tmp tmpigdgu download remot uri pass argument type path run train traceback file train line import panda importerror modul panda run doubl panda instal default mar gcc linux type copyright credit licens import panda advic forest",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"modul panda run panda instal successfulli runpython elasticnet model rmse mae run fetch directori run creat directori download remot uri pass argument type path run traceback file line import panda importerror modul panda run doubl panda instal gcc linux type copyright credit licens import panda advic forest",
        "Challenge_readability":7.7,
        "Challenge_reading_time":13.12,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"No module named pandas when \" mlflow run\"",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":57.0,
        "Challenge_word_count":131,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139843145000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I understand the Sagemaker currently does not support Python 3 with Tensorflow (according to this <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/19\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/19<\/a>)<\/p>\n\n<p>But is it possible to create your own docker container with Python 3 and Tensorflow as is explained here? \n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529409983087,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"tensorflow explor creat docker tensorflow explain github",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50928040",
        "Challenge_link_count":4,
        "Challenge_original_content":"tensorflow tensorflow accord http github com sdk creat docker tensorflow explain http github com awslab blob master advanc function scikit scikit ipynb",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"tensorflow tensorflow creat docker tensorflow explain",
        "Challenge_readability":25.8,
        "Challenge_reading_time":9.9,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Python 3 with Tensorflow on Sagemaker",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":956.0,
        "Challenge_word_count":47,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":139838016913
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hi all,\n\nWe're planning to schedule an in-person MLflow meetup at Databricks in\nthe next few weeks to give people an overview of the project and get\ninput on the roadmap. If you'd like to attend, can you tell us your\ndate preferences at https:\/\/goo.gl\/forms\/8ZEnF9DXEVBWJlKU2? We'll also\nrecord the content presented for people not in the Bay Area.\n\nThis initial meetup will cover:\n\n* Introductory concepts and MLflow tutorial\n* Deploying MLflow\n* Project roadmap\n* How to contribute to MLflow\n\nAll the best,\n\nMatei and the MLflow team",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529425986000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"schedul meetup week request attende date prefer record present bai area meetup cover introductori concept tutori deploi roadmap contribut",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/dlTQmJNTnqI",
        "Challenge_link_count":1,
        "Challenge_original_content":"schedul meetup week plan schedul meetup week peopl overview input roadmap attend date prefer http goo zenfdxevbwjlku record present peopl bai area initi meetup cover introductori concept tutori deploi roadmap contribut matei team",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"schedul meetup week plan schedul meetup week peopl overview input roadmap attend date prefer record present peopl bai area initi meetup cover introductori concept tutori deploi roadmap contribut matei team",
        "Challenge_readability":8.2,
        "Challenge_reading_time":7.11,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Scheduling an MLflow meetup in the next few weeks",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":83.0,
        "Challenge_word_count":93,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139822014000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I am looking to send CSV records coming from kinesis analytics to sagemaker endpoint and getting an inference through a lambda function and then passing it on to a firehose API to dump it into S3. But the data is not getting into sagemaker for some reason.<\/p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>'use strict';\nconsole.log('Loading function');\nvar AWS = require('aws-sdk');\nvar sagemakerruntime = new AWS.SageMakerRuntime({apiVersion: '2017-05-13'});\nvar firehose = new AWS.Firehose({apiVersion: '2015-08-04'});\nexports.handler = (event, context, callback) =&gt; {\n    let success = 0;\n    let failure = 0;\n    const output = event.records.map((record) =&gt; {\n        \/* Data is base64 encoded, so decode here *\/\n        const recordData = Buffer.from(record.data, 'base64');\n        try {\n            var params = {\n                Body: new Buffer('...') || recordData \/* Strings will be Base-64 encoded on your behalf *\/, \/* required *\/\n                EndpointName: 'String', \/* required *\/\n                Accept: 'text\/csv',\n                ContentType: 'text\/csv'\n            };\n            sagemakerruntime.invokeEndpoint(params, function(err, data) {\n                var result1;\n                if (err) console.log(err, err.stack); \/\/ an error occurred\n                else     console.log(data);           \/\/ successful response\n                result1=data;\n                var params = {\n                    DeliveryStreamName: 'String', \/* required *\/\n                    Record: { \/* required *\/\n                        Data: new Buffer('...') || result1 \/* Strings will be Base-64 encoded on your behalf *\/ \/* required *\/\n                    }\n                };\n                firehose.putRecord(params, function(err, data) {\n                    if (err) console.log(err, err.stack); \/\/ an error occurred\n                    else     console.log(data);           \/\/ successful response\n                });\n            });\n            success++;\n            return {\n                recordId: record.recordId,\n                result: 'Ok',\n            };\n        } catch (err) {\n            failure++;\n            return {\n                recordId: record.recordId,\n                result: 'DeliveryFailed',\n            };\n        }\n    });\n    console.log(`Successful delivered records ${success}, Failed delivered records ${failure}.`);\n    callback(null, {\n        records: output,\n    });\n};<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1529495256510,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"send csv record kinesi analyt endpoint lambda function pass firehos api dump data reason",
        "Challenge_last_edit_time":1529495349167,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50947519",
        "Challenge_link_count":0,
        "Challenge_original_content":"send csv record lambda function send csv record come kinesi analyt endpoint infer lambda function pass firehos api dump data reason strict consol log load function var sdk var runtim runtim apivers var firehos firehos apivers export handler event context callback const output event record map record data base encod decod const recorddata buffer record data base var param bodi buffer recorddata base encod behalf endpointnam accept text csv contenttyp text csv runtim invokeendpoint param function err data var err consol log err err stack consol log data respons data var param deliverystreamnam record data buffer base encod behalf firehos putrecord param function err data err consol log err err stack consol log data respons return recordid record recordid catch err return recordid record recordid deliveryfail consol log deliv record deliv record callback null record output",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"send csv record lambda function send csv record come kinesi analyt endpoint infer lambda function pass firehos api dump data reason",
        "Challenge_readability":10.6,
        "Challenge_reading_time":26.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Sending CSV records to Amazon Sagemaker through lambda function",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":351.0,
        "Challenge_word_count":212,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":139752743490
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I'm trying to find all dates in one column that fall between the dates in 2 other columns, but I'm still getting results from outside of the limits. I suspect it may be an issue with strftime, and I should be using strptime instead, but I can't seem to get it to work.<\/p>\n\n<p>The data I have is something like this:<\/p>\n\n<pre><code>Prod_Date                 Date                      Lead_Date\n08\/02\/1985 12:00:00 AM    08\/02\/1970 12:00:00 AM    08\/02\/1988 12:00:00 AM\n08\/02\/1986 12:00:00 AM    08\/02\/1971 12:00:00 AM    08\/02\/2018 12:00:00 AM\n08\/02\/1987 12:00:00 AM    08\/02\/1972 12:00:00 AM    08\/02\/1986 12:00:00 AM\n08\/02\/1988 12:00:00 AM    08\/02\/1973 12:00:00 AM    08\/02\/2018 12:00:00 AM\n<\/code><\/pre>\n\n<p>I want to limit the dataframe to Prod_Date values that fall between the Date and Lead_Date columns. I don't get any errors with my code, but I'll get a lot of erroneous values that are either before the Date value, or after the Lead_Date value.<\/p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\n<div class=\"snippet-code\">\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code># imports up here can be used to \nimport pandas as pd\nimport datetime\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    # Convert strings to datetime\n    dataframe1['Date'] = dataframe1.Date.apply(\n        lambda x: pd.to_datetime(x).strftime('%d\/%m\/%Y %I:%M:%S %p'))\n\n    dataframe1['Prod_Date'] = dataframe1.Prod_Date.apply(\n        lambda x: pd.to_datetime(x).strftime('%d\/%m\/%Y %I:%M:%S %p'))\n \n    dataframe1['Lead_Date'] = dataframe1.Lead_Date.apply(\n        lambda x: pd.to_datetime(x).strftime('%d\/%m\/%Y %I:%M:%S %p'))\n        \n    # Keeping only the rows for production that are contained between the date and lead_date:\n    dataframe1 = dataframe1[(dataframe1['Date'] &lt; dataframe1['Prod_Date']) &amp; (dataframe1['Prod_Date'] &lt; dataframe1['Lead_Date'])]<\/code><\/pre>\n<\/div>\n<\/div>\n<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1529515001767,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"date column fall date column outsid limit strftime sampl data frame",
        "Challenge_last_edit_time":1529516729936,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50953736",
        "Challenge_link_count":0,
        "Challenge_original_content":"date date column date column fall date column outsid limit strftime strptime data prod date date date limit datafram prod date valu fall date date column erron valu date valu date valu import import panda import datetim entri function input argument param panda datafram param panda datafram datafram datafram convert datetim datafram date datafram date appli lambda datetim strftime datafram prod date datafram prod date appli lambda datetim strftime datafram date datafram date appli lambda datetim strftime keep row date date datafram datafram datafram date datafram prod date datafram prod date datafram date",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"date date column date column fall date column outsid limit strftime strptime data limit datafram valu fall date column erron valu date valu valu",
        "Challenge_readability":9.1,
        "Challenge_reading_time":26.37,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"Finding a date between two date columns in Azure Python",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":60.0,
        "Challenge_word_count":252,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":139732998233
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to understand how to convert azure ml <code>String Feature<\/code> data type into float using python script. my data set is contain \"HH:MM\" data time format. It recognized as <code>String Feature<\/code> like the following img:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gy4A7.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gy4A7.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aB4P6.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aB4P6.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I want to convert it into float type which will divide the timestamp by  84600 ( 24 hour) so <code>17:30<\/code> will be converted into <code>0,729166666666667<\/code>, so I write python script to convert that. This is my script:<\/p>\n\n<pre><code>import pandas as pd\nimport numpy as np \n\ndef timeToFloat(x):\n    frt = [3600,60]\n    data = str(x)\n    result = float(sum([a*b for a,b in zip(frt, map(int,data.split(':')))]))\/86400\n    return result if isNotZero(x) else 0.0\n\ndef isNotZero(x):\n    return (x is \"0\")\n\ndef azureml_main(dataframe1 = None):\n\n    df = pd.DataFrame(dataframe1)\n    df[\"Departure Time\"] = pd.to_numeric(df[\"Departure Time\"]).apply(timeToFloat)\n\n    print(df[\"Departure Time\"])\n\n    return df,\n<\/code><\/pre>\n\n<p>When I run the script it was failed. Then I try to check whether it is <code>str<\/code> or not, but it returns <code>None<\/code>.<\/p>\n\n<p>can we treat <code>String Feature<\/code> as <code>String<\/code>? or how should I covert this data correctly?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529548568263,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"convert featur data type data set time data format recogn featur convert type divid timestamp hour execut convert data",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50959561",
        "Challenge_link_count":4,
        "Challenge_original_content":"convert featur datafram convert featur data type data set data time format recogn featur img convert type divid timestamp hour convert write convert import panda import numpi timetofloat frt data str sum zip frt map data split return isnotzero isnotzero return datafram datafram datafram departur time numer departur time appli timetofloat print departur time return run str return treat featur covert data",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"convert datafram convert data type data set data time format recogn img convert type divid timestamp convert write convert run return treat covert data",
        "Challenge_readability":7.9,
        "Challenge_reading_time":20.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Convert `String Feature` DataFrame into Float in Azure ML Using Python Script",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":299.0,
        "Challenge_word_count":187,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":139699431737
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm storing midi files in an S3 bucket and am trying to download them into the SageMake jupyter notebook.  I am using this code<\/p>\n\n<pre><code>import os\nimport boto3  # Python library for Amazon API \nimport botocore\nfrom botocore.exceptions import ClientError\ndef download_from_s3(url):\n    \"\"\"ex: url = s3:\/\/sagemakerbucketname\/data\/validation.tfrecords\"\"\"\n    url_parts = url.split(\"\/\")  # =&gt; ['s3:', '', 'sagemakerbucketname', 'data', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource('s3')\n            print('Downloading {} to {}'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == \"404\":\n                print('The object {} does not exist in bucket {}'.format(\n                    key, bucket_name))\n            else:\n                raise\n<\/code><\/pre>\n\n<p>however I am getting An error occurred (403) when calling the HeadObject operation: Forbidden<\/p>\n\n<p>Here are the permissions attached for the S3:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529593887460,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"download midi file bucket jupyt notebook messag state call headobject oper forbidden permiss attach bucket allow action",
        "Challenge_last_edit_time":1530796978847,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50971948",
        "Challenge_link_count":0,
        "Challenge_original_content":"download file store midi file bucket download sagemak jupyt notebook import import boto librari api import botocor botocor except import clienterror download url url bucketnam data tfrecord url url split bucketnam data bucket url kei path url filenam url path filenam creat client boto resourc print download format url filenam bucket bucket download file kei filenam botocor except clienterror respons print object bucket format kei bucket rais call headobject oper forbidden permiss attach version statement allow action getobject putobject deleteobject listbucket resourc arn",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"download file store midi file bucket download sagemak jupyt notebook call headobject oper forbidden permiss attach",
        "Challenge_readability":11.5,
        "Challenge_reading_time":16.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":6.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Amazon SageMaker Downloading Files from S3",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":8331.0,
        "Challenge_word_count":133,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":139654112540
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"Hey Guys,\n\n\nFirst of all, great job with this effort. It's certainly something a lot of people are waiting for (or have tried to create themselves).\n\n\nI was wondering about the scalability of the tracking server. I see in the code there is an abstraction of for the tracking Store, which is currently a FileStore if I'm correct. What are the plans to support other stores for this (ElasticSearch, Kafka, S3, ...?)\n\n\nCheers,\nD.",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529642287000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"inquir scalabl track server track store elasticsearch kafka",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/TQe7ATr8Wqw",
        "Challenge_link_count":0,
        "Challenge_original_content":"track server scalabl gui great job effort certainli peopl wait tri creat scalabl track server abstract track store filestor plan store elasticsearch kafka cheer",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"track server scalabl gui great job effort certainli peopl wait scalabl track server abstract track store filestor plan store cheer",
        "Challenge_readability":7.1,
        "Challenge_reading_time":5.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"MLFlow Tracking server scalability",
        "Challenge_topic":"Model Registry",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":107.0,
        "Challenge_word_count":76,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139605713000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi guys, me again ;)\n\n\nAre you guys planning on integrating Atlas for keeping hold of which version of a model is deployed for which business case?\n\n\nCheers,\nD.",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529644466000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"inquir integr atla track deploi version model busi",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/YRUxFOeiP4U",
        "Challenge_link_count":0,
        "Challenge_original_content":"atla integr gui gui plan integr atla keep hold version model deploi busi cheer",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"atla integr gui gui plan integr atla keep hold version model deploi busi cheer",
        "Challenge_readability":13.4,
        "Challenge_reading_time":2.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":1,
        "Challenge_solved_time":null,
        "Challenge_title":"Atlas Integration",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":39.0,
        "Challenge_word_count":29,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139603534000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I had been wondering if it were possible to apply \"data preparation\" (.dprep) files to incoming data in the score.py, similar to how Pipeline objects may be applied.  This would be very useful for model deployment. To find out, I asked this question on the MSDN forums and received a <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/640e93c3-0b88-4668-9869-91da85f7f1e3\/purpose-of-dprep-files-azure-machine-learning-workbench?forum=MachineLearning\" rel=\"nofollow noreferrer\">response<\/a> confirming it were possible, but little explanation about how to actually do it.  The response was:<\/p>\n\n<blockquote>\n  <p>in your score.py file, you can invoke the dprep package from Python\n  SDK to apply the same transformation to the incoming scoring data.\n  make sure you bundle your .dprep file in the image you are building.<\/p>\n<\/blockquote>\n\n<p>So my questions are:<\/p>\n\n<ul>\n<li><p>What function do I apply to invoke this dprep package?<\/p>\n\n<ul>\n<li>Is it: <code>run_on_data(user_config, package_path, dataflow_idx=0, secrets=None, spark=None)<\/code> ?<\/li>\n<\/ul><\/li>\n<li><p>How do I bundle it into the image when creating a web-service from the CLI?<\/p>\n\n<ul>\n<li>Is there a switch to <code>-f<\/code> for score files?<\/li>\n<\/ul><\/li>\n<\/ul>\n\n<p>I have scanned through the entire <a href=\"https:\/\/opdhsblobprod01.blob.core.windows.net\/contents\/4a6d75bb3af747de838e6ccc97c5d978\/fc2d1c1c13c843cb9c6ca45d2038633b?sv=2015-04-05&amp;sr=b&amp;sig=gDlMGB31l0tV%2FBrLS6R1B0qeKhuGoKppykGocKHslSo%3D&amp;st=2018-06-22T10%3A29%3A14Z&amp;se=2018-06-23T10%3A39%3A14Z&amp;sp=r\" rel=\"nofollow noreferrer\">documentation<\/a> and <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/tree\/master\/articles\/machine-learning\/desktop-workbench\" rel=\"nofollow noreferrer\">Workbench Repo<\/a> but cannot seem to find any examples.<\/p>\n\n<p>Any suggestions would be much appreciated!<\/p>\n\n<p>Thanks!<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Scenario:<\/p>\n\n<ol>\n<li><p>I import my data from a live database and let's say this data set has 10 columns.<\/p><\/li>\n<li><p>I then feature engineer this (.dsource) data set using the Workbench resulting in a .dprep file which may have 13 columns.<\/p><\/li>\n<li><p>This .dprep data set is then imported as a pandas DataFrame and used to train and test my model.<\/p><\/li>\n<li><p>Now I have a model ready for deployment.<\/p><\/li>\n<li><p>This model is deployed via Model Management to a Container Service and will be fed data from a live database which once again will be of the original format (10 columns).  <\/p><\/li>\n<li><p>Obviously this model has been trained on the transformed data (13 columns) and will not be able to make a prediction on the 10 column data set.<\/p><\/li>\n<\/ol>\n\n<p>What function may I use in the 'score.py' file to apply the same transformation I created in workbench?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1529664352543,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"appli data prepar dprep file incom data score workbench model deploy receiv respons littl explan function invok dprep packag bundl imag creat web servic cli train model transform data fed data live databas origin format function score file appli transform creat workbench",
        "Challenge_last_edit_time":1529670794452,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/50986233",
        "Challenge_link_count":3,
        "Challenge_original_content":"appli dprep packag incom data score workbench appli data prepar dprep file incom data score pipelin object appli model deploy msdn forum receiv respons littl explan respons score file invok dprep packag sdk appli transform incom score data bundl dprep file imag build function appli invok dprep packag run data config packag path dataflow idx secret spark bundl imag creat web servic cli switch score file scan entir document workbench repo edit import data live databas data set column featur engin dsourc data set workbench dprep file column dprep data set import panda datafram train test model model readi deploy model deploi model servic fed data live databas origin format column obvious model train transform data column predict column data set function score file appli transform creat workbench",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"appli dprep packag incom data workbench appli data prepar file incom data pipelin object appli model deploy msdn forum receiv respons littl explan respons file invok dprep packag sdk appli transform incom score data bundl dprep file imag build function appli invok dprep packag bundl imag creat cli switch score file scan entir document workbench repo edit import data live databas data set column featur engin data set workbench dprep file column dprep data set import panda datafram train test model model readi deploy model deploi model servic fed data live databas origin format obvious model train transform data predict column data set function file appli transform creat workbench",
        "Challenge_readability":9.0,
        "Challenge_reading_time":36.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":30,
        "Challenge_solved_time":null,
        "Challenge_title":"How to apply a dprep package to incoming data in score.py Azure Workbench",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":312.0,
        "Challenge_word_count":348,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":139583647457
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I'm new to AWS and trying to build a model (from the web console) by referring to their <a href=\"https:\/\/youtu.be\/1kJf0Lvzj8A?t=36m53s\" rel=\"noreferrer\">demo<\/a>. However, when I try to create the model, it gives me the below error.<\/p>\n\n<blockquote>\n  <p>Could not access model data at\n  <code>https:\/\/s3.console.aws.amazon.com\/s3\/buckets\/<\/code><em>bucket_name<\/em><code>\/models\/<\/code><em>model_name<\/em><code>-v0.1.hdf5.<\/code>\n  Please ensure that the role\n  \"arn:aws:iam::<em>id<\/em>:role\/service-role\/AmazonSageMaker-ExecutionRole-<em>xxx<\/em>\"\n  exists and that its trust relationship policy allows the action\n  \"sts:AssumeRole\" for the service principal \"sagemaker.amazonaws.com\".\n  Also ensure that the role has \"s3:GetObject\" permissions and that the\n  object is located in <strong>eu-west-1<\/strong>.<\/p>\n<\/blockquote>\n\n<p>I checked the IAM Role and it has <code>AmazonSageMakerFullAccess<\/code> and <code>AmazonS3FullAccess<\/code> policies attached. And also, the trust relationship is also specified for the role (as below).<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"sagemaker.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>I'm specifying the ECR and the S3 path correctly, but I can't figure out what is happening. Can someone help me to fix this?<\/p>\n\n<p>Sorry if I couldn't provide more info, but I will give any other information if required.<\/p>\n\n<p><strong>UPDATE:<\/strong><\/p>\n\n<p>Below are the IAM policies.<\/p>\n\n<p><strong>AmazonS3FullAccess<\/strong><\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:*\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p><strong>AmazonSageMaker-ExecutionPolicy-xxx<\/strong> <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket_name&gt;\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p><strong>AmazonSageMakerFullAccess<\/strong> <\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:BatchGetImage\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"cloudwatch:PutMetricData\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:DeleteAlarms\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeSecurityGroups\",\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":4,
        "Challenge_created_time":1530021332657,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat model web consol messag model data access iam role permiss iam role polici attach trust relationship specifi iam polici amazonsfullaccess executionpolici amazonfullaccess",
        "Challenge_last_edit_time":1530026234243,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51044657",
        "Challenge_link_count":2,
        "Challenge_original_content":"validationerror creat model build model web consol creat model access model data http consol com bucket bucket model model hdf role arn iam role servic role executionrol trust relationship polici allow action st assumerol servic princip amazonaw com role getobject permiss object locat iam role amazonfullaccess amazonsfullaccess polici attach trust relationship specifi role version statement allow princip servic amazonaw com action st assumerol specifi ecr path figur sorri updat iam polici amazonsfullaccess version statement allow action resourc executionpolici version statement action listbucket allow resourc arn action getobject putobject deleteobject allow resourc arn amazonfullaccess version statement allow action resourc allow action ecr getauthorizationtoken ecr getdownloadurlforlay ecr batchgetimag ecr batchchecklayeravail cloudwatch putmetricdata cloudwatch putmetricalarm cloudwatch describealarm cloudwatch deletealarm createnetworkinterfac createnetworkinterfacepermiss deletenetworkinterfac deletenetworkinterfacepermiss describenetworkinterfac describevpc describedhcpopt describesubnet describesecuritygroup autosc deletescalingpolici autosc deletescheduledact autosc deregisterscalabletarget autosc describescalabletarget autosc describescalingact autosc describescalingpolici autosc describescheduledact autosc putscalingpolici autosc putscheduledact autosc registerscalabletarget log createloggroup log createlogstream log describelogstream log getlogev log putlogev resourc allow action getobject putobject deleteobject resourc arn arn arn allow action createbucket getbucketloc listbucket listallmybucket resourc allow action getobject resourc condit stringequalsignorecas existingobjecttag action iam createservicelinl allow resourc arn iam role servic role autosc amazonaw com awsserviceroleforapplicationautosc endpoint condit stringlik iam awsservicenam autosc amazonaw com allow action iam passrol resourc condit stringequ iam passedtoservic amazonaw com",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"validationerror creat model build model creat model access model data role trust relationship polici allow action st assumerol servic princip role getobject permiss object locat iam role polici attach trust relationship specifi role specifi ecr path figur sorri updat iam polici fullaccess amazonfullaccess",
        "Challenge_readability":25.9,
        "Challenge_reading_time":59.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"ValidationError when creating a SageMaker Model",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":5821.0,
        "Challenge_word_count":297,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":139226667343
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello,\n\n\nI'll give MLflow a try within my team: that'll be a few data scientists trying out some use cases to evaluate MLflow in our environment.\n\n\n\nI couldn't find a specification document for sizing servers for MLflow installation. Is there some kind of guideline that could tell me amount of CPU, RAM, etc. for the server on which MLflow runs? It's of course not very important in this early stage, but nevertheless I think having such a guideline would be good.\n\n\nCheers,\nEmre",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530070766000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"document server size instal guidanc cpu ram server run",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/f0hQ6k4w8dA",
        "Challenge_link_count":0,
        "Challenge_original_content":"document server size team data scientist evalu environ document size server instal guidelin cpu ram server run cours import earli stage guidelin cheer emr",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"document server size team data scientist evalu environ document size server instal guidelin cpu ram server run cours import earli stage guidelin cheer emr",
        "Challenge_readability":8.0,
        "Challenge_reading_time":6.42,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Is there a specification document for server sizing?",
        "Challenge_topic":"Resource Quota",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":42.0,
        "Challenge_word_count":90,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139177234000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\n\nI have problem running MLflow web-based user interface. I gave the details in the following StackOverflow question:\n\n\n\u00a0\u00a0 https:\/\/stackoverflow.com\/questions\/51064366\/cant-run-mlflow-web-based-user-interface\n\n\nAny ideas?\n\n\n--\nEmre",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530092849000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"run web base interfac sought stackoverflow",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/BoUzYBMGml4",
        "Challenge_link_count":1,
        "Challenge_original_content":"run web base interfac run web base interfac gave stackoverflow http stackoverflow com run web base interfac idea emr",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"run interfac run interfac gave stackoverflow idea emr",
        "Challenge_readability":11.8,
        "Challenge_reading_time":3.55,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't run MLflow web-based user interface",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":34.0,
        "Challenge_word_count":27,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139155151000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>I understand this may not be fully related to dvc, but since the problem happens when following the tutorial at <a href=\"https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46\" rel=\"nofollow noopener\">https:\/\/blog.dataversioncontrol.com\/data-version-control-tutorial-9146715eda46<\/a>. So here is my question:<\/p>\n<p>When following tutorial at the step in executing<\/p>\n<pre><code>dvc run -d data\/Posts.tsv -d code\/split_train_test.py         -d code\/conf.py         -o data\/Posts-test.tsv -o data\/Posts-train.tsv         python code\/split_train_test.py 0.33 20180319\n<\/code><\/pre>\n<p>it throws error<\/p>\n<pre><code>from ._sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \\\nImportError: \/tmp\/_MEIUqCWxh\/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by \/usr\/lib\/python2.7\/dist-packages\/scipy\/sparse\/_sparsetools.x86_64-linux-gnu.so)\nFailed to run command: Stage 'Posts-test.tsv.dvc' cmd python code\/split_train_test.py 0.33 20180319 failed\n<\/code><\/pre>\n<p>I am not familiar with python, nor data science, but was just trying to evaluate if dvc fits our internal requirement so we can decide if going with dvc or not.<\/p>\n<p>How can I fix this error? Otherwise any even simpler version that can basically just show dataset, model are versioned so we can see the differences, say, between version 0.0.1 and 0.0.2 and its diff, or that kind of things?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530112347491,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"tutori data version control execut familiar data scienc simpler version version dataset model",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/documentation-tutorial-problem\/40",
        "Challenge_link_count":2,
        "Challenge_original_content":"document tutori fulli relat tutori http blog dataversioncontrol com data version control tutori eda tutori step execut run data tsv split train test conf data test tsv data train tsv split train test throw sparsetool import csr tocsc csr tobsr csr count block importerror tmp meiuqcwxh libstdc version glibcxx usr lib dist packag scipi spars sparsetool linux gnu run stage test tsv cmd split train test familiar data scienc evalu fit intern decid simpler version dataset model version version diff",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"document tutori fulli relat tutori tutori step execut throw familiar data scienc evalu fit intern decid simpler version dataset model version version diff",
        "Challenge_readability":10.0,
        "Challenge_reading_time":18.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"Documentation: tutorial problem?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":686.0,
        "Challenge_word_count":159,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":139135652509
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>By following quickstart and tutorial at <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html<\/a>, and <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/tutorial.html\" rel=\"noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/tutorial.html<\/a>, the execution of train.py works fine. <\/p>\n\n<pre><code>Elasticnet model (alpha=0.500000, l1_ratio=0.500000):\n  RMSE: 0.8222428497595403\n  MAE: 0.6278761410160693\n  R2: 0.12678721972772622\n<\/code><\/pre>\n\n<p>But when launching the ui <code>mlflow ui<\/code>, and accessing to the web page localhost:5000, the browser complains <\/p>\n\n<pre><code>Not Found\n\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n<\/code><\/pre>\n\n<p>What went wrong and how to fix this? <\/p>\n\n<p>Thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1530177397510,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"access successfulli execut train quickstart tutori launch access web page browser displai messag assist identifi",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51079061",
        "Challenge_link_count":4,
        "Challenge_original_content":"access quickstart tutori http org doc latest quickstart html http org doc latest tutori html execut train elasticnet model alpha ratio rmse mae launch access web page localhost browser complain request url server enter url manual spell went",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"access quickstart tutori execut launch access web page localhost browser complain went",
        "Challenge_readability":8.5,
        "Challenge_reading_time":11.58,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":5.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Unable to access to mlflow ui",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":7497.0,
        "Challenge_word_count":89,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":139070602490
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>This tool looks very promising. However, I am wondering about the functionality of the pipeline feature. From the documentation it seems that I can chain an input file to an output file and each logged operation will be appended to the repro argument when I commit. In other words it is not clear to me <strong>how to create a batch process<\/strong> without first defining each step separately.<\/p>\n<p>It is also not clear to me <strong>how to handle multiple input and output files.<\/strong><\/p>\n<p>I am very curious if this would be possible.<\/p>\n<p>Here is an example of an offline analysis that I am performing on human EEG data and behavioral data (using python and java):<\/p>\n<pre><code>sample size \u00d7 \"ASCII_\" + n + \".txt\" --&gt; Pandas --&gt; SPSS Syntax --&gt; Tables + Figures\n\n    --&gt; First Level Summary: Individual Participants (Diagnostics)\n   \/\n--&lt;\n   \\\n    --&gt; Second Level Summary: Group Level\n\n    + --&gt; Behavioral CSV\n\n\nEEG Raw Binary Data --&gt; EEG Manual Preprocessing --&gt; MATLAB Scripts --&gt; Figures\n\n    + --&gt; EEG CSV\n\n\n Behavioral CSV --\n                  \\\n                   &gt;--&gt; Correlations (SPSS Syntax)\n                  \/\n        EEG CSV --<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530180340930,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"creat batch process multipl input output file promis pipelin featur perform offlin analysi human eeg data data java chain input output file defin step separ multipl input output file",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/batch-pipeline-support-and-multi-i-o\/42",
        "Challenge_link_count":0,
        "Challenge_original_content":"batch pipelin multi promis function pipelin featur document chain input file output file log oper append repro argument commit clear creat batch process defin step separ clear multipl input output file offlin analysi perform human eeg data data java sampl size ascii txt panda spss syntax tabl figur level summari individu diagnost level summari group level csv eeg raw binari data eeg manual preprocess matlab figur eeg csv csv correl spss syntax eeg csv",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"batch pipelin multi promis function pipelin featur document chain input file output file log oper append repro argument commit clear creat batch process defin step separ clear multipl input output file offlin analysi perform human eeg data data",
        "Challenge_readability":10.8,
        "Challenge_reading_time":14.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Batch pipeline support and multi I\/O",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":810.0,
        "Challenge_word_count":174,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":139067659070
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a place listing all the properties that can be configured using the <code>dvc config<\/code> command? I couldn\u2019t find such info in the documentation, for instance.<\/p>\n<p>This would be very helpful for people starting out, to understand the possibilities of DVC.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530525747567,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"comprehens list properti configur config document believ list beginn",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/what-are-the-possible-values-for-dvc-config-command\/46",
        "Challenge_link_count":0,
        "Challenge_original_content":"valu config list properti configur config document instanc peopl start",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"valu list properti configur document instanc peopl start",
        "Challenge_readability":9.3,
        "Challenge_reading_time":4.16,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"What are the possible values for `dvc config` command?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":641.0,
        "Challenge_word_count":50,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":138722252433
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi everyone,\n\nAs a heads-up, we published MLflow 0.2.0 and 0.2.1 at the end of last week with a number of new features and fixes. This post details the biggest ones: https:\/\/databricks.com\/blog\/2018\/07\/03\/mlflow-0-2-released.html (namely a TensorFlow integration package for saving and serving TF models, as well as improvements to the server, including the ability to store data on S3). You can also see a more detailed change log at https:\/\/github.com\/databricks\/mlflow\/blob\/master\/CHANGELOG.rst.\n\nIt should be possible to upgrade to the new version using pip install --upgrade mlflow .\n\nMatei",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530622365000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"relat upgrad version releas featur tensorflow integr packag save serv model improv server abil store data upgrad pip instal upgrad",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/pWpD1HQOf8U",
        "Challenge_link_count":2,
        "Challenge_original_content":"releas head publish end week featur biggest on http com blog releas html tensorflow integr packag save serv model improv server abil store data log http github com blob master changelog rst upgrad version pip instal upgrad matei",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"releas publish end week featur biggest on log upgrad version pip instal matei",
        "Challenge_readability":8.1,
        "Challenge_reading_time":7.73,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow 0.2.1 release",
        "Challenge_topic":"TensorFlow Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":31.0,
        "Challenge_word_count":87,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":138625635000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hello,\n\n\nI wonder how MLflow compares to ModelDB (A system to manage machine learning models).\n\n\nIn ModelDB web page, I've seen Matei Zaharia as one of the contributors:\n\n\n\u00a0\u00a0\u00a0 https:\/\/mitdbg.github.io\/modeldb\/\n\n\nAny ideas?\n\n\n\n--\nEmre",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530686529000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"compar modeldb model notic matei zaharia contributor modeldb idea comparison system",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/NW5GxrD9NQ0",
        "Challenge_link_count":1,
        "Challenge_original_content":"compar modeldb model compar modeldb model modeldb web page matei zaharia contributor http mitdbg github modeldb idea emr",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"compar modeldb compar modeldb modeldb web page matei zaharia contributor idea emr",
        "Challenge_readability":7.6,
        "Challenge_reading_time":3.75,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How does MLflow compare to ModelDB (A system to manage machine learning models)?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":701.0,
        "Challenge_word_count":44,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":138561471000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Tensorflow's Estimator provides a method to get desired variable values after training\/testing using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#get_variable_value\" rel=\"nofollow noreferrer\">get_variable_value<\/a>.\nDoes there exist similar functionality in Sagemaker's Estimator, so that I am able to obtain weights after my model is trained.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530694571770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"estim weight model train tensorflow estim variabl valu function",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51169661",
        "Challenge_link_count":1,
        "Challenge_original_content":"weight train estim tensorflow estim desir variabl valu train test variabl valu function estim weight model train",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"weight train estim tensorflow estim desir variabl valu function estim weight model train",
        "Challenge_readability":17.0,
        "Challenge_reading_time":5.71,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Get weights after training from Sagemaker Estimator",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":487.0,
        "Challenge_word_count":44,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":138553428230
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I would like to put some of my data in different folders in aws, is there any way to link multiple projects\/ urls and then decide which one I wan to deposit it too ?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530804515195,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"link multipl cloud account order store data folder allow link multipl url choos deposit data",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/linking-multiple-cloud-accounts\/51",
        "Challenge_link_count":0,
        "Challenge_original_content":"link multipl cloud account data folder link multipl url decid wan deposit",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"link multipl cloud account data folder link multipl url decid wan deposit",
        "Challenge_readability":7.2,
        "Challenge_reading_time":2.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":2,
        "Challenge_solved_time":null,
        "Challenge_title":"Linking multiple cloud accounts",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":443.0,
        "Challenge_word_count":38,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":138443484805
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi Databricks,\n\n\nI was successfully able to push image to ecr -\n\n\n\u00a0mlflow.sagemaker.push_image_to_ecr(image='mlflow_sage')\n\n\nI was trying to deploy the model using -\n\n\nmlflow.sagemaker.deploy(app_name,\u00a0model_path,\u00a0execution_role_arn,\u00a0bucket,\u00a0run_id=None,\u00a0image='mlflow_sage',\u00a0region_name='us-west-2')\n\n\napp_name\u00a0\u2013 Name of the deployed app.\n\n\nI am not sure what is deployed app. Could help here.\n\n\nThanks,\nSunil",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530872605000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"successfulli push imag ecr push imag ecr deploi model deploi meant deploi app assist",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/M51fi08Drk4",
        "Challenge_link_count":0,
        "Challenge_original_content":"deploi successfulli push imag ecr push imag ecr imag sage deploi model deploi app model path execut role arn bucket run imag sage region app deploi app deploi app sunil",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"successfulli push imag ecr deploi model deploi app deploi app sunil",
        "Challenge_readability":9.2,
        "Challenge_reading_time":5.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"mlflow.sagemaker.deploy",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":45.0,
        "Challenge_word_count":46,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":138375395000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>We are using aws sagemaker that is using ecs container, Is there a way, we can setup environment variable (e.g. stage or prod) in container when calling sagemaker api using low level python sdk <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1530897378177,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"set environ variabl ec set environ variabl stage prod call api low level sdk",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51215092",
        "Challenge_link_count":0,
        "Challenge_original_content":"setup env variabl ec setup environ variabl stage prod call api low level sdk",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"setup env variabl ec setup environ variabl call api low level sdk",
        "Challenge_readability":9.1,
        "Challenge_reading_time":3.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":8.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Setup env variable in aws SageMaker container (bring your own container)",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":8657.0,
        "Challenge_word_count":45,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":138350621823
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there a way to dynamically train a model through a web application in AZURE? It does not seem they have any sort of API to do this.\nI know I can train a model then create a web service through AZURE. I can then call this model and either re train it or make predictions. But i want to train a model through a C# .NET application.<\/p>\n\n<p>I am trying to create a web application that allows users to up load data then have them be able to create a model to make predictions. I wanted to use AZURE on the back end for the statistical analysis rather then creating a class and web service in Python. <\/p>\n\n<p>I basically do not want to use the AZURE studio or command line but i would like to train it through an API or another service (that im hoping they offer!)<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531170139557,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"dynam train model web api train model creat web servic creat web allow upload data creat model predict studio line hope api servic train model",
        "Challenge_last_edit_time":1531217122870,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51253977",
        "Challenge_link_count":0,
        "Challenge_original_content":"train model dynam asp net mvc web dynam train model web sort api train model creat web servic model train predict train model net creat web allow load data creat model predict end statist analysi creat class web servic studio line train api servic hope",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"train model dynam mvc web dynam train model web sort api train model creat web servic model train predict train model net creat web allow load data creat model predict end statist analysi creat class web servic studio line train api servic",
        "Challenge_readability":7.2,
        "Challenge_reading_time":10.27,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure: Training a machine learning model dynamically through a ASP.NET MVC web application",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":282.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":138077860443
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I was trying to run this example: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_abalone_age_predictor_using_layers\/tensorflow_abalone_age_predictor_using_layers.ipynb\" rel=\"nofollow noreferrer\">tensorflow_abalone_age_predictor_using_layers<\/a>\n, in which <code>abalone_predictor.predict(tensor_proto)<\/code> is used to call the endpoint and make the prediction. I was trying to use the java API <code>AmazonSageMakerRuntime<\/code> to achieve the same effect, but I don't know how to specify the <code>body<\/code> and <code>contentType<\/code> for the <code>InvokeEndPointRequest<\/code>. The document is not in detailed abou the format of the request. Greatly appreciate any piece of help!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531409441673,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"java api invok endpoint predict replic sdk specifi bodi contenttyp invokeendpointrequest format request",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51309523",
        "Challenge_link_count":1,
        "Challenge_original_content":"java api invok endpoint run tensorflow abalon ag predictor layer abalon predictor predict tensor proto endpoint predict java api amazonruntim achiev specifi bodi contenttyp invokeendpointrequest document abou format request greatli piec",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"java api invok endpoint run endpoint predict java api achiev specifi document abou format request greatli piec",
        "Challenge_readability":16.0,
        "Challenge_reading_time":10.68,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":7,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use sagemaker java API to invoke a endpoint?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":3299.0,
        "Challenge_word_count":79,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137838558327
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In the SageMaker hyper parameter tuning jobs, you can use a RegEx expression to parse your logs and output a objective metric to the web console. Is it possible to do this during a normal training job?<\/p>\n\n<p>It would be great to have this feature so I don't need to look through all the logs to find the metric.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531516517530,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"object metric normal train job manual search log metric regex express output metric web consol hyper paramet tune job",
        "Challenge_last_edit_time":1531516701888,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51332898",
        "Challenge_link_count":0,
        "Challenge_original_content":"output object metric train job hyper paramet tune job regex express pars log output object metric web consol normal train job great featur log metric",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"output object metric train job hyper paramet tune job regex express pars log output object metric web consol normal train job great featur log metric",
        "Challenge_readability":7.9,
        "Challenge_reading_time":4.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to have SageMaker output Objective Metrics during a training job?",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":485.0,
        "Challenge_word_count":71,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137731482470
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Using the aws sagemaker cli tools it's possible to invoke endpoint that are hosted in sagemaker using a command like:<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint --body file:\/\/container\/local_test\/payload.json \\\n--endpoint-name $(DEPLOYMENT_NAME)-staging \\\n--content-type application\/json \\\n--accept application\/json \\\noutput.json\n<\/code><\/pre>\n\n<p>By default, this command goes to the <code>\/invocations<\/code> endpoint. Is it possible to go to a different endpoint? For example, if I implemented a <code>health-report<\/code> endpoint? It's definately possible to make one as in the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">BYOM example<\/a>. I'm just not sure how I'd access it.<\/p>\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531518140760,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"access endpoint default invoc endpoint cli access endpoint health report endpoint implement",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51333159",
        "Challenge_link_count":1,
        "Challenge_original_content":"hit endpoint host invoc cli invok endpoint host runtim invok endpoint bodi file local test payload json endpoint deploy stage type json accept json output json default goe invoc endpoint endpoint implement health report endpoint defin byom access",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"hit endpoint host invoc cli invok endpoint host default goe endpoint endpoint implement endpoint defin byom access",
        "Challenge_readability":16.7,
        "Challenge_reading_time":12.21,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to hit endpoints hosted in SageMaker besides \/invocations?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":530.0,
        "Challenge_word_count":92,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137729859240
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have deployed a random cut forest model endpoint on AWS sagemaker. I am trying to test the inference endpoint with POST man. I am successfully able to authenticate into the endpoint with access and secret key.\nCan someone confirm if the way I am sending the csv payload is correct ? It seems something is not working since whatever the third column value, I get the same score from the endpoint.\n'1530000000000,E39E4F5CFFA2CA4A84099D2415583C1C,<strong>433190.06640625<\/strong>'<\/p>\n\n<p>Pasting the curl for the POSTman generated code:<\/p>\n\n<p>curl --request POST \\\n  --url <a href=\"https:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/randomcutforest-2018-06-05-01-08-02-956\/invocations\" rel=\"nofollow noreferrer\">https:\/\/runtime.sagemaker.us-east-1.amazonaws.com\/endpoints\/randomcutforest-2018-06-05-01-08-02-956\/invocations<\/a> \\\n  --header 'authorization: AWS4-HMAC-SHA256 Credential=\/20180713\/us-east-1\/sagemaker\/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=d51371b2549e132c21a3402824b57258a74e6fa9f078d91a44bf54b0d110ea57' \\\n  --header 'cache-control: no-cache' \\\n  --header 'content-type: text\/csv' \\\n  --header 'host: runtime.sagemaker.us-east-1.amazonaws.com' \\\n  --header 'postman-token: cb7cdfa5-025b-e4f4-c033-a4fb685133c4' \\\n  --header 'x-amz-date: 20180713T190238Z' \\\n  --data '1530000000000,E39E4F5CFFA2CA4A84099D2415583C1C,433190.06640625'<\/p>\n\n<pre><code>{\n    \"scores\": [\n        {\n            \"score\": 7.6438561895\/\/ This value never changes\n        }\n    ]\n}\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531523229947,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi random cut forest model endpoint test infer endpoint postman send csv payload endpoint return score valu column",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51333824",
        "Challenge_link_count":2,
        "Challenge_original_content":"call curl infer endpoint csv deploi random cut forest model endpoint test infer endpoint man successfulli authent endpoint access secret kei send csv payload column valu score endpoint eefcffacaadcc past curl postman gener curl request url http runtim amazonaw com endpoint randomcutforest invoc header author hmac sha credenti request signedhead length type host amz date signatur dbecabaefafdabfbdea header cach control cach header type text csv header host runtim amazonaw com header postman token cbcdfa afbc header amz date data eefcffacaadcc score score valu",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"call curl infer endpoint csv deploi random cut forest model endpoint test infer endpoint man successfulli authent endpoint access secret kei send csv payload column valu score endpoint past curl postman gener curl author signatur host",
        "Challenge_readability":13.3,
        "Challenge_reading_time":20.6,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"sagemaker calling curl with inference endpoint with csv",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":684.0,
        "Challenge_word_count":127,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137724770053
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Call to <code>get_execution_role()<\/code> from notebook instance fails with the error message <code>NoSuchEntityException: An error occurred (NoSuchEntity) when calling the GetRole operation: The user with name &lt;name&gt; cannot be found.<\/code><\/p>\n\n<p>Stack trace:<\/p>\n\n<pre><code>NoSuchEntityExceptionTraceback (most recent call last)\n&lt;ipython-input-1-1e2d3f162cfe&gt; in &lt;module&gt;()\n      5 sagemaker_session = sagemaker.Session()\n      6 \n----&gt; 7 role = get_execution_role()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in get_execution_role(sagemaker_session)\n    871     if not sagemaker_session:\n    872         sagemaker_session = Session()\n--&gt; 873     arn = sagemaker_session.get_caller_identity_arn()\n    874 \n    875     if 'role' in arn:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in get_caller_identity_arn(self)\n    701         # Call IAM to get the role's path\n    702         role_name = role[role.rfind('\/') + 1:]\n--&gt; 703         role = self.boto_session.client('iam').get_role(RoleName=role_name)['Role']['Arn']\n    704 \n    705         return role\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/botocore\/client.pyc in _api_call(self, *args, **kwargs)\n    312                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n    313             # The \"self\" in this scope is referring to the BaseClient.\n--&gt; 314             return self._make_api_call(operation_name, kwargs)\n    315 \n    316         _api_call.__name__ = str(py_operation_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/botocore\/client.pyc in _make_api_call(self, operation_name, api_params)\n    610             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n    611             error_class = self.exceptions.from_code(error_code)\n--&gt; 612             raise error_class(parsed_response, operation_name)\n    613         else:\n    614             return parsed_response\n\nNoSuchEntityException: An error occurred (NoSuchEntity) when calling the GetRole operation: The user with name &lt;name&gt; cannot be found.\n<\/code><\/pre>\n\n<p>However using boto client directly to get info about the role succeeds. This works fine:<\/p>\n\n<pre><code>response = client.get_role(\n    RoleName='role-name',\n)['Role']['Arn']\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531584152177,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"messag nosuchentityexcept call getrol oper specifi call execut role notebook instanc boto client directli role",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51341099",
        "Challenge_link_count":0,
        "Challenge_original_content":"nosuchentityexcept nosuchent call getrol oper execut role notebook instanc messag nosuchentityexcept nosuchent call getrol oper stack trace nosuchentityexceptiontraceback session session role execut role home anaconda env tensorflow lib site packag session pyc execut role session session session session arn session caller ident arn role arn home anaconda env tensorflow lib site packag session pyc caller ident arn iam role path role role role rfind role boto session client iam role rolenam role role arn return role home anaconda env tensorflow lib site packag botocor client pyc api arg kwarg accept keyword argument oper scope basecli return api oper kwarg api str oper home anaconda env tensorflow lib site packag botocor client pyc api oper api param pars respons class except rais class pars respons oper return pars respons nosuchentityexcept nosuchent call getrol oper boto client directli role respons client role rolenam role role arn",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"nosuchentityexcept call getrol oper notebook instanc messag stack trace boto client directli role",
        "Challenge_readability":15.5,
        "Challenge_reading_time":30.64,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":20,
        "Challenge_solved_time":null,
        "Challenge_title":"NoSuchEntityException: An error occurred (NoSuchEntity) when calling the GetRole operation: The user with name <name> cannot be found",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":2538.0,
        "Challenge_word_count":200,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137663847823
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I try to read from tfrecords in S3 from a Sage Maker notebook instance following instructions here: <a href=\"https:\/\/www.tensorflow.org\/versions\/master\/deploy\/s3\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/versions\/master\/deploy\/s3<\/a><\/p>\n\n<pre><code>import tensorflow as tf\nimport os\nos.environ['AWS_ACCESS_KEY_ID'] = '&lt;my-key&gt;'\nos.environ['AWS_SECRET_ACCESS_KEY'] = '&lt;my-secret&gt;'\n\nfrom tensorflow.python.lib.io import file_io\nprint(file_io.stat('s3:\/\/&lt;my-bucket&gt;\/data\/DEMO-mnist\/train.tfrecords'))\n<\/code><\/pre>\n\n<p>The above code fails with the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nNotFoundError                             Traceback (most recent call last)\n&lt;ipython-input-7-770c0aef6d7b&gt; in &lt;module&gt;()\n      1 from tensorflow.python.lib.io import file_io\n----&gt; 2 print(file_io.stat('s3:\/\/&lt;my-bucket&gt;\/data\/DEMO-mnist\/train.tfrecords'))\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/lib\/io\/file_io.py in stat(filename)\n    551   with errors.raise_exception_on_not_ok_status() as status:\n    552     pywrap_tensorflow.Stat(compat.as_bytes(filename), file_statistics, status)\n--&gt; 553     return file_statistics\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\n    517             None, None,\n    518             compat.as_text(c_api.TF_Message(self.status.status)),\n--&gt; 519             c_api.TF_GetCode(self.status.status))\n    520     # Delete the underlying status object from memory otherwise it stays alive\n    521     # as there is a reference to status from this from the traceback due to\n\nNotFoundError: Object s3:\/\/&lt;my-bucket&gt;\/data\/DEMO-mnist\/train.tfrecords does not exist\n<\/code><\/pre>\n\n<p>However the same code works fine if I run from a regular EC2 instance without using SageMaker.<\/p>\n\n<p>IAM role used for the notebook instance has full S3 access.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1531585893973,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"read tfrecord notebook instanc access iam role instanc run regular instanc messag object access",
        "Challenge_last_edit_time":1533030868423,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51341336",
        "Challenge_link_count":2,
        "Challenge_original_content":"read tfrecord notebook instanc read tfrecord sage maker notebook instanc instruct http tensorflow org version master deploi import tensorflow import environ access kei environ secret access kei tensorflow lib import file print file stat data mnist train tfrecord notfounderror traceback tensorflow lib import file print file stat data mnist train tfrecord anaconda env tensorflow lib site packag tensorflow lib file stat filenam rais except statu statu pywrap tensorflow stat compat byte filenam file statist statu return file statist anaconda env tensorflow lib site packag tensorflow framework impl exit type arg valu arg traceback arg compat text api messag statu statu api getcod statu statu delet underli statu object memori stai aliv statu traceback notfounderror object data mnist train tfrecord run regular instanc iam role notebook instanc access",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"read tfrecord notebook instanc read tfrecord sage maker notebook instanc instruct run regular instanc iam role notebook instanc access",
        "Challenge_readability":15.3,
        "Challenge_reading_time":26.5,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Can't read from tfrecords in S3 from notebook instance",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":814.0,
        "Challenge_word_count":162,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137662106027
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Is there anyone who has successfully implemented a sample code for calling an inference endpoint using sagemaker client java sdk ? I am trying to call a endpoint with text\/csv payload.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1531767912323,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"implement sampl call infer endpoint client java sdk endpoint text csv payload",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51368583",
        "Challenge_link_count":0,
        "Challenge_original_content":"infer endpoint java successfulli implement sampl call infer endpoint client java sdk endpoint text csv payload",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"infer endpoint java successfulli implement sampl call infer endpoint client java sdk endpoint payload",
        "Challenge_readability":9.4,
        "Challenge_reading_time":3.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"How to call sagemaker inference endpoint in Java ?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":2925.0,
        "Challenge_word_count":38,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137480087677
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>For using the word2vec model , you need to have word2vec vectors file(which is a .bin file) installed. Is there a way to import this file from a url or have it stored somewhere in the azure machine learning studio (like the azure blob storage) ?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531815895597,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"wordvec model studio wordvec vector file instal bin file import file url store blob storag",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51376559",
        "Challenge_link_count":0,
        "Challenge_original_content":"wordvec model studio wordvec model wordvec vector file bin file instal import file url store studio blob storag",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"vec model studio vec model vec vector file instal import file url store studio",
        "Challenge_readability":5.1,
        "Challenge_reading_time":3.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use google word2vec model in azure machine learning studio?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":329.0,
        "Challenge_word_count":55,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":137432104403
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi folks,\n\n\nFew people have checked on easy model deployment method. And Matei also mentioned on Slide 9 in last MLflow meetup.\n\n\nWhat do you folks think of this \"one click\" solution to get your endpoint? See on site: http:\/\/dockai.com\u00a0\nYou can try wine quality example there.\u00a0\n\n\n\nBased on the feedback, we would like to extend and open it up. Feel free to reach out directly to me.\n\n\nThanks,\nHenry",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1531881520000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"model deploy serv feedback click model deploy link websit wine qualiti open extend open base feedback",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/sI7yw-eZxnE",
        "Challenge_link_count":1,
        "Challenge_original_content":"model deploy serv folk peopl model deploy matei slide meetup folk click endpoint site http dockai com wine qualiti base feedback extend open free reach directli henri",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"model folk peopl model deploy matei slide meetup folk click endpoint site wine qualiti base feedback extend open free reach directli henri",
        "Challenge_readability":3.3,
        "Challenge_reading_time":5.13,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow models deployment\/serving",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":56.0,
        "Challenge_word_count":72,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":137366480000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Is there a way to create sagemaker endpoint using AWS lambda ?<\/p>\n\n<p>The maximum timeout limit for lambda is 300 seconds while my existing model takes 5-6 mins to host ?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532027138877,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"creat endpoint lambda maximum timeout limit insuffici host model minut",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51430151",
        "Challenge_link_count":0,
        "Challenge_original_content":"creat endpoint delet lambda creat endpoint lambda maximum timeout limit lambda model host",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"creat endpoint delet lambda creat endpoint lambda maximum timeout limit lambda model host",
        "Challenge_readability":7.5,
        "Challenge_reading_time":3.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Create AWS sagemaker endpoint and delete the same using AWS lambda",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":1559.0,
        "Challenge_word_count":41,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137220861123
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using AWS sagemaker Jupiter notebook and getting following error:<\/p>\n\n<pre><code>&lt;ipython-input-42-14cd1ee49f9c&gt; in &lt;module&gt;()\n      1 import s3fs\n----&gt; 2 import fastparquet as fp\n      3 s3 = s3fs.S3FileSystem()\n      4 fs = s3fs.core.S3FileSystem()\n      5 \n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/fastparquet\/__init__.py in &lt;module&gt;()\n      6 \n      7 from .thrift_structures import parquet_thrift\n----&gt; 8 from .core import read_thrift\n      9 from .writer import write\n     10 from . import core, schema, converted_types, api\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/fastparquet\/core.py in &lt;module&gt;()\n     12 \n     13 from . import encoding\n---&gt; 14 from .compression import decompress_data\n     15 from .converted_types import convert, typemap\n     16 from .schema import _is_list_like, _is_map_like\n\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/fastparquet\/compression.py in &lt;module&gt;()\n     43     def snappy_decompress(data, uncompressed_size):\n     44         return snappy.decompress(data)\n---&gt; 45     compressions['SNAPPY'] = snappy.compress\n     46     decompressions['SNAPPY'] = snappy_decompress\n     47 except ImportError:\n\nAttributeError: module 'snappy' has no attribute 'compress'\n<\/code><\/pre>\n\n<p>I noticed that the attribute is named snappy.compress. Shouldn't it be snappy_compress with underscore?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532045120667,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"fastparquet modul jupit notebook relat snappi compress modul compress attribut attribut snappi compress snappi compress",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51433339",
        "Challenge_link_count":0,
        "Challenge_original_content":"fastparquet modul jupit notebook import sf import fastparquet sf sfilesystem sf core sfilesystem anaconda env mxnet lib site packag fastparquet init thrift structur import parquet thrift core import read thrift writer import write import core schema convert type api anaconda env mxnet lib site packag fastparquet core import encod compress import decompress data convert type import convert typemap schema import list map anaconda env mxnet lib site packag fastparquet compress snappi decompress data uncompress size return snappi decompress data compress snappi snappi compress decompress snappi snappi decompress importerror attributeerror modul snappi attribut compress notic attribut snappi compress snappi compress underscor",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"fastparquet modul jupit notebook notic attribut underscor",
        "Challenge_readability":12.0,
        "Challenge_reading_time":17.57,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":18,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it a bug in fastparquet module",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":1378.0,
        "Challenge_word_count":131,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":137202879333
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Memory error occurs in amazon sagemaker when preprocessing 2 gb of data which is stored in s3. No problem in loading the data. Dimension of data is 7 million rows and 64 columns. One hot encoding is also not possible. Doing so results in memory error.\nNotebook instance is ml.t2.medium. How to solve this issue?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1532342962513,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"memori preprocess data store data million row column hot encod memori notebook instanc medium",
        "Challenge_last_edit_time":1532439505556,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51477054",
        "Challenge_link_count":0,
        "Challenge_original_content":"memori memori preprocess data store load data dimens data million row column hot encod memori notebook instanc medium",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"memori memori preprocess data store load data dimens data million row column hot encod memori notebook instanc",
        "Challenge_readability":7.4,
        "Challenge_reading_time":4.3,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":13.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Memory error in amazon sagemaker",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":14396.0,
        "Challenge_word_count":59,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":136905037487
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>My question is as the title states really, does DVC actually require Git, or would Mercurial work just as well? Does DVC actually call Git to do anything, or does it just store information in normal files which the user commits.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532508768537,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"git mercuri call git perform action simpli store file commit",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-actually-require-git-or-would-mercurial-work-just-as-well\/55",
        "Challenge_link_count":0,
        "Challenge_original_content":"git mercuri titl state git mercuri git store normal file commit",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"git mercuri titl state git mercuri git store normal file commit",
        "Challenge_readability":10.0,
        "Challenge_reading_time":3.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"Does DVC actually require Git, or would Mercurial work just as well?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":947.0,
        "Challenge_word_count":52,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":136739231463
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>This question is based on the solution of an other question the user user4446237 asked: <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\">Updating pandas to version 0.19 in Azure ML Studio<\/a><\/p>\n\n<p>I have followed the steps provided in the answer and I also get the information at the end, that I have imported the new version of pandas (0.23.3) instead of pandas (0.18.0). However after retrieving the version of the packages the code runs into an error:<\/p>\n\n<pre><code>Caught exception while executing function: Traceback (most recent call last):\n  File \"C:\\server\\invokepy.py\", line 192, in batch\n    idfs = [parameter for infile in infiles\n  File \"C:\\server\\invokepy.py\", line 194, in &lt;listcomp&gt;\n    infile, is_buffer=False)]\n  File \"C:\\server\\XDRReader\\xdrutils.py\", line 47, in XDRToPyObjects\n    return XDRBridge.xdr_to_py_positional(attrList)\n  File \"C:\\server\\XDRReader\\xdrbridge.py\", line 216, in xdr_to_py_positional\n    retList.append(XDRBridge.xdrobject_to_dataframe(key, value))\n  File \"C:\\server\\XDRReader\\xdrbridge.py\", line 155, in xdrobject_to_dataframe\n    }, index=np.arange(len(columns[0].values())), copy=False)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\core\\frame.py\", line 223, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\core\\frame.py\", line 356, in _init_dict\n    columns = data_names = Index(keys)\n  File \"C:\\pyhome\\lib\\site-packages\\pandas\\indexes\\base.py\", line 129, in __new__\n    from .range import RangeIndex\nSystemError: Parent module 'pandas.indexes' not loaded, cannot perform relative import\n<\/code><\/pre>\n\n<p>The code I am using is pretty much the same from Jay Gong:<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Is there anything I can do about this problem or am I reaching the limitations of Azure ML Studio's Python Script Module.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1532615466370,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"updat panda version studio retriev version packag ran systemerror previou reach limit studio modul",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51541328",
        "Challenge_link_count":1,
        "Challenge_original_content":"updat panda studio base updat panda version studio step end import version panda panda retriev version packag run caught except execut function traceback file server invokepi line batch idf paramet infil infil file server invokepi line infil buffer file server xdrreader xdrutil line xdrtopyobject return xdrbridg xdr posit attrlist file server xdrreader xdrbridg line xdr posit retlist append xdrbridg xdrobject datafram kei valu file server xdrreader xdrbridg line xdrobject datafram index arang len column valu copi file pyhom lib site packag panda core frame line init mgr init dict data index column dtype dtype file pyhom lib site packag panda core frame line init dict column data index kei file pyhom lib site packag panda index base line rang import rangeindex systemerror parent modul panda index load perform rel import pretti jai gong import sy import panda print version del sy modul panda del sy modul numpi del sy modul pytz del sy modul del sy modul dateutil sy path insert bundl sy modul startswith panda startswith numpi startswith pytz startswith dateutil startswith del sy modul import panda print version entri function input argument param panda datafram param panda datafram datafram datafram reach limit studio modul",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"updat panda studio base updat panda version studio step end import version panda panda retriev version packag run pretti jai gong reach limit studio modul",
        "Challenge_readability":10.3,
        "Challenge_reading_time":33.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":31,
        "Challenge_solved_time":null,
        "Challenge_title":"Updating pandas in Azure ML Studio results in an Error",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":267.0,
        "Challenge_word_count":278,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":136632533630
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I\u2019m looking for a simple expression that puts a \u20181\u2019 in column E if \u2018SomeContent\u2019 is contained in column D.  I\u2019m doing this in Azure ML Workbench through their Add Column (script) function.  Here\u2019s some examples they give.<\/p>\n\n<pre><code>row.ColumnA + row.ColumnB is the same as row[\"ColumnA\"] + row[\"ColumnB\"] \n1 if row.ColumnA &lt; 4 else 2 \ndatetime.datetime.now() \nfloat(row.ColumnA) \/ float(row.ColumnB - 1) \n'Bad' if pd.isnull(row.ColumnA) else 'Good'\n<\/code><\/pre>\n\n<p>Any ideas on a 1 line script I could use for this?  Thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532635853827,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat line add column somecont present column workbench add column function express achiev",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51546796",
        "Challenge_link_count":0,
        "Challenge_original_content":"add column transform queri column express put column somecont column workbench add column function here row columna row columnb row columna row columnb row columna datetim datetim row columna row columnb isnul row columna idea line",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"add column transform queri column express put column somecont column workbench add column function here idea line",
        "Challenge_readability":6.4,
        "Challenge_reading_time":7.62,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"How to Add Column (script) transform that queries another column for content",
        "Challenge_topic":"Columnar Manipulation",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":32.0,
        "Challenge_word_count":87,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":136612146173
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am not able to find activation function for Regression Neural Network in Azure Machine Learning Studio. I am not able to identify what is the activation function taken for my NN. Followed this document also-<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/neural-network-regression\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/neural-network-regression<\/a><\/p>\n\n<p>Can someone suggest where to mention it\/ what is the default activation function used?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532931331770,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"activ function regress neural network studio identifi default activ function relev document assist locat activ function determin default",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51587956",
        "Challenge_link_count":2,
        "Challenge_original_content":"activ function regress neural net studio activ function regress neural network studio identifi activ function taken document http doc com studio modul neural network regress default activ function",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"activ function regress neural net studio activ function regress neural network studio identifi activ function taken document default activ function",
        "Challenge_readability":14.5,
        "Challenge_reading_time":8.4,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Activation function of Regression Neural Net in Azure ML Studio?",
        "Challenge_topic":"Hyperparameter Sweep",
        "Challenge_topic_macro":"Performance Management",
        "Challenge_view_count":352.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":136316668230
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>So, I have a program with multiple functions. Each of these functions has been packaged in zip files with the XML, and all have been successfully uploaded as custom modules to Azure. <\/p>\n\n<p>I'm trying to call these functions via a custom R script. Something like: <\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\n# Importing my custom module\nsource(\"src\/A.R\")\nsource(\"src\/B.R\")\nsource(\"src\/C.R\")\n\n#function A performs work and calls function B &amp; C while doing so. \nfor(i in 1:100){\n  a = A(data[i,])\n}\n\ndata.set &lt;- data.frame(a)\n# Select data.frame to be sent to the output Dataset port\n\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>However, this is failing. Completely. It doesn't even import any of the custom modules with the <\/p>\n\n<pre><code>source(\"src\/A.R\")\n<\/code><\/pre>\n\n<p>commands. <\/p>\n\n<p>How do I do this? <\/p>\n\n<p>Additionally, the custom modules call each other during the course of their processing. Azure Ml would support that behavior, right? <\/p>\n\n<p>(the project has at the end moment been ported to this platform, so I'm stuck with trying to ensure my code works on something it wasn't designed for.) <\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1532936459323,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"modul environ successfulli upload modul function modul import guidanc modul call process",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51589128",
        "Challenge_link_count":0,
        "Challenge_original_content":"modul env program multipl function function packag zip file xml successfulli upload modul function map base option input port variabl dataset maml mapinputport class data frame import modul sourc src sourc src sourc src function perform call function data data set data frame select data frame sent output dataset port maml mapoutputport data set complet import modul sourc src modul cours process end moment port platform stuck wasn design",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"modul program multipl function function packag zip file xml successfulli upload modul function complet import modul modul cours process end moment port platform stuck wasn design",
        "Challenge_readability":5.9,
        "Challenge_reading_time":15.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":19,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use custom R modules in Azure ML env.?",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":61.0,
        "Challenge_word_count":186,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":136311540677
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I've created a SageMaker model for a Seq2Seq neural network, and then started a SageMaker endpoint:<\/p>\n\n<pre><code>create_endpoint_config_response = sage.create_endpoint_config(\n    EndpointConfigName = endpoint_config_name,\n    ProductionVariants=[{\n        'InstanceType':'ml.m4.xlarge', \n        'InitialInstanceCount':1,\n        'ModelName':model_name,\n        'VariantName':'AllTraffic'}])\n\ncreate_endpoint_response = sage.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>\n\n<p>This standard endpoint does not support beam search. What is the best approach for creating a SageMaker endpoint that supports beam search?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1533006311500,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat model seqseq neural network start endpoint standard endpoint beam search advic creat endpoint beam search",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51604901",
        "Challenge_link_count":0,
        "Challenge_original_content":"implement beam search decod host endpoint creat model seqseq neural network start endpoint creat endpoint config respons sage creat endpoint config endpointconfignam endpoint config productionvari instancetyp xlarg initialinstancecount modelnam model variantnam alltraff creat endpoint respons sage creat endpoint endpointnam endpoint endpointconfignam endpoint config standard endpoint beam search creat endpoint beam search",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"implement beam search decod host endpoint creat model seq seq neural network start endpoint standard endpoint beam search creat endpoint beam search",
        "Challenge_readability":16.8,
        "Challenge_reading_time":9.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"How to implement a beam search decoder in an SageMaker hosting endpoint?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":138.0,
        "Challenge_word_count":63,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":136241688500
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>ClientError: Data download failed:PermanentRedirect (301): The bucket is in this region: us-west-1. Please use this region to retry the request<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533114628520,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"receiv clienterror messag state data download permanentredirect messag bucket region retri request region",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51629435",
        "Challenge_link_count":0,
        "Challenge_original_content":"sagermak clienterror data download permanentredirect clienterror data download permanentredirect bucket region region retri request",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"sagermak clienterror data download permanentredirect clienterror data download permanentredirect bucket region region retri request",
        "Challenge_readability":11.4,
        "Challenge_reading_time":2.91,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS Sagermaker ClientError: Data download failed:PermanentRedirect (301)",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":708.0,
        "Challenge_word_count":26,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":136133371480
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi mlflow-users,\n\nMLflow Release 0.4.0 is ready, released 2018-08-01. The release is available on\u00a0PyPI\u00a0and docs are\u00a0updated. Here are the release notes (also available on GitHub):\n\n\n\nBreaking changes:\n\n[Projects] Removed the\u00a0use_temp_cwd\u00a0argument to\u00a0mlflow.projects.run()\n(--new-dir\u00a0flag in the\u00a0mlflow run\u00a0CLI). Runs of local projects now use the local\u00a0 project directory as their working directory. Git projects are still fetched into temporary directories (#215,\u00a0@smurching)\n[Tracking] GCS artifact storage is now a pluggable dependency (no longer installed by default). To enable GCS support, install\u00a0google-cloud-storage\u00a0on both the client and tracking server via pip (#202,\u00a0@smurching).\n[Tracking] Clients running MLflow 0.4.0 and above require a server running MLflow 0.4.0\nor above, due to a fix that ensures clients no longer double-serialize JSON into strings when sending data to the server (#200,\u00a0@aarondav). However, the MLflow 0.4.0 server remains backwards-compatible with older clients (#216,\u00a0@aarondav)\n\nFeatures:\n\n[Examples] Add a more advanced tracking example: using MLflow with PyTorch and TensorBoard (#203)\n[Models] H2O model support (#170,\u00a0@ToonKBC)\n[Projects] Support for running projects in subdirectories of Git repos (#153,\u00a0@juntai-zheng)\n[SageMaker] Support for specifying a compute specification when deploying to SageMaker (#185,\u00a0@dbczumar)\n[Server] Added --static-prefix option to serve UI from a specified prefix to MLflow UI and server (#116,\u00a0@andrewmchen)\n[Tracking] Azure blob storage support for artifacts (#206,\u00a0@mateiz)\n[Tracking] Add support for Databricks-backed RestStore (#200,\u00a0@aarondav)\n[UI] Enable productionizing frontend by adding CSRF support (#199,\u00a0@aarondav)\n[UI] Update metric and parameter filters to let users control column order (#186,\u00a0@mateiz)\n\nBug fixes:\n\nFixed incompatible file structure returned by GCSArtifactRepository (#173,\u00a0@jakeret)\nFixed metric values going out of order on x axis (#204,\u00a0@mateiz)\nFixed occasional hanging behavior when using the projects.run API (#193,\u00a0@smurching)\nMiscellaneous bug and documentation fixes from\u00a0@aarondav,\u00a0@andrewmchen,\u00a0@arinto,\u00a0@jakeret,\u00a0@mateiz,\u00a0@smurching,\u00a0@stbof\n\n\n\nThanks,\nSid",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533151106000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"relat updat releas releas break remov temp cwd argument server run client run releas featur model blob storag artifact",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/JqCLKtnz69A",
        "Challenge_link_count":0,
        "Challenge_original_content":"releas releas readi releas releas onpypiand doc areupd releas note github break remov theus temp cwdargument run dirflag runcli run local local directori directori git fetch temporari directori smurch track gc artifact storag pluggabl depend longer instal default enabl gc installgoogl cloud storageon client track server pip smurch track client run server run client longer doubl serial json send data server aarondav server remain backward compat older client aarondav featur add advanc track pytorch tensorboard model model toonkbc run subdirectori git repo juntai zheng specifi comput deploi dbczumar server static prefix option serv specifi prefix server andrewmchen track blob storag artifact mateiz track add back reststor aarondav enabl production frontend csrf aarondav updat metric paramet filter control column order mateiz incompat file structur return gcsartifactrepositori jakeret metric valu order axi mateiz hang run api smurch miscellan document aarondav andrewmchen arinto jakeret mateiz smurch stbof sid",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"releas releas readi releas releas onpypiand doc areupd releas note break remov runcli run local local directori directori git fetch temporari directori track gc artifact storag pluggabl depend enabl gc client track server pip track client run server run client longer json send data server server remain older client featur add advanc track pytorch tensorboard model model run subdirectori git repo specifi comput deploi server option serv specifi prefix server track blob storag artifact track add reststor enabl production frontend csrf updat metric paramet filter control column order incompat file structur return gcsartifactrepositori metric valu order axi hang api miscellan document sid",
        "Challenge_readability":12.3,
        "Challenge_reading_time":27.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow Release 0.4.0",
        "Challenge_topic":"Artifact Tracking",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":40.0,
        "Challenge_word_count":303,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":136096894000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying out Deep AR fore<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/now-available-in-amazon-sagemaker-deepar-algorithm-for-more-accurate-time-series-forecasting\/\" rel=\"nofollow noreferrer\">Deep AR Forecasting<\/a>casting training algorithm. My training job keeps failing with the following error while parsing the jsonlines file:\nrow: 1) Failure reason\nClientError: Error when parsing json (source: \/opt\/ml\/input\/data\/train\/daily_call_vol_lines.json, row: 1)\nI am attaching the file (json) with json lines format I tried with\nAny help into why the parser fails on sagemaker side would help!\nPasting the file content:\n{\n    \"TimeStamp\": \"2017-07-01\",\n    \"Number of Calls\": 14\n}\n{\n    \"TimeStamp\": \"2017-07-02\",\n    \"Number of Calls\": 62\n}\n{\n    \"TimeStamp\": \"2017-07-03\",\n    \"Number of Calls\": 972\n}<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533158590927,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"file pars forecast train algorithm pars jsonlin file file messag json format file",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51642448",
        "Challenge_link_count":1,
        "Challenge_original_content":"forecaset file pars foredeep forecastingcast train algorithm train job keep pars jsonlin file row reason clienterror pars json sourc opt input data train daili vol line json row attach file json json line format tri parser past file timestamp call timestamp call timestamp call",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"forecaset file pars foredeep forecastingcast train algorithm train job keep pars jsonlin file row reason clienterror pars json attach file json line format tri parser past file timestamp call timestamp call timestamp call",
        "Challenge_readability":13.0,
        "Challenge_reading_time":10.96,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"sagemaker deep forecaseting file parsing error",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":671.0,
        "Challenge_word_count":94,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":136089409073
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am currently attempting to change the value of input as it goes through data process in Azure ML. However, I cannot find a clue about how to access to the input data with python.<\/p>\n\n<p>For example, if you were to use python, you can access to the column of data with<\/p>\n\n<pre><code>print(dataframe1[\"Hello World\"])\n<\/code><\/pre>\n\n<p>I tried to change the name of Web Service Input and tried to do it like how I did for other dataframe (e.g. sample)<\/p>\n\n<pre><code>print(dataframe[\"sample\"])\n<\/code><\/pre>\n\n<p>But it returns an error with no luck, and from what I read from an error, it's not compatible to dataframe:<\/p>\n\n<pre><code>object of type 'NoneType' has no len()\n<\/code><\/pre>\n\n<p>I tried to look up a solution with Nonetype error, but there is no good solution.\nThe whole error message:<\/p>\n\n<pre><code>requestId = 1f0f621f1d8841baa7862d5c05154942 errorComponent=Module. taskStatusCode=400. {\"Exception\":{\"ErrorId\":\"FailedToEvaluateScript\",\"ErrorCode\":\"0085\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0085: The following error occurred during script evaluation, please view the output log for more information:\\r\\n---------- Start of error message from Python interpreter ----------\\r\\nCaught exception while executing function: Traceback (most recent call last):\\r\\n File \\\"C:\\\\server\\\\invokepy.py\\\", line 211, in batch\\r\\n xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\\r\\n File \\\"C:\\\\server\\\\XDRReader\\\\xdrutils.py\\\", line 51, in DataFrameToRFile\\r\\n attributes = XDRBridge.DataFrameToRObject(dataframe)\\r\\n File \\\"C:\\\\server\\\\XDRReader\\\\xdrbridge.py\\\", line 40, in DataFrameToRObject\\r\\n if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):\\r\\nTypeError: object of type 'NoneType' has no len()\\r\\nProcess returned with non-zero exit code 1\\r\\n\\r\\n---------- End of error message from Python interpreter ----------\"}}Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:---------- Start of error message from Python interpreter ----------Caught exception while executing function: Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 211, in batch xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True) File \"C:\\server\\XDRReader\\xdrutils.py\", line 51, in DataFrameToRFile attributes = XDRBridge.DataFrameToRObject(dataframe) File \"C:\\server\\XDRReader\\xdrbridge.py\", line 40, in DataFrameToRObject if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):TypeError: object of type 'NoneType' has no len()Process returned with non-zero exit code 1---------- End of error message from Python interpreter ---------- Process exited with error code -2\n<\/code><\/pre>\n\n<p>I have also tried to <a href=\"https:\/\/i.stack.imgur.com\/DWZK6.png\" rel=\"nofollow noreferrer\">a way to pass python script in data<\/a>, but it is not able to make any change to Web Service Input value as I want it to be.<\/p>\n\n<p>I have tried to look on forums like msdn or SO, but it's been difficult to find any information about it. Please let me know if you need any more information if needed. I would greatly appreciate your help!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533163130393,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"access input data studio tri access column data tri pass data web servic input valu search forum",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51643168",
        "Challenge_link_count":1,
        "Challenge_original_content":"studio input valu goe data process valu input goe data process clue access input data access column data print datafram world tri web servic input tri datafram sampl print datafram sampl return luck read compat datafram object type nonetyp len tri nonetyp messag requestid fffdbaadc errorcompon modul taskstatuscod except errorid failedtoevaluatescript errorcod exceptiontyp moduleexcept messag evalu output log start messag interpret ncaught except execut function traceback file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil file server xdrreader xdrutil line dataframetorfil attribut xdrbridg dataframetorobject datafram file server xdrreader xdrbridg line dataframetorobject len datafram type datafram datafram ntypeerror object type nonetyp len nprocess return exit end messag interpret evalu output log start messag interpret caught except execut function traceback file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil file server xdrreader xdrutil line dataframetorfil attribut xdrbridg dataframetorobject datafram file server xdrreader xdrbridg line dataframetorobject len datafram type datafram datafram typeerror object type nonetyp len process return exit end messag interpret process exit tri pass data web servic input valu tri forum msdn greatli",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"studio input valu goe data process valu input goe data process clue access input data access column data tri web servic input tri datafram return luck read compat datafram tri nonetyp messag tri pass data web servic input valu tri forum msdn greatli",
        "Challenge_readability":11.3,
        "Challenge_reading_time":41.76,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":23,
        "Challenge_solved_time":null,
        "Challenge_title":"Azure ML Studio: How to change input value with Python before it goes through data process",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":309.0,
        "Challenge_word_count":400,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":136084869607
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Folder structure for my S3 bucket is:<\/p>\n\n<pre><code>Bucket\n    -&gt;training-set\n           -&gt;medium\n                 -&gt;    img1.jpeg\n                 -&gt;    img2.jpeg\n                 -&gt;    img3.PNG\n<\/code><\/pre>\n\n<p>My training-set.lst file looks like this:<\/p>\n\n<pre><code>1  \\t 1  \\t medium\/img1.jpeg\n2  \\t 1  \\t medium\/img2.jpeg\n3  \\t 1  \\t medium\/img3.PNG\n<\/code><\/pre>\n\n<p>I created this using excel sheet.<\/p>\n\n<p>Error:\nTraining failed with the following error: ClientError: Invalid lst file: training-set.lst<\/p>\n\n<pre><code>   \"InputDataConfig\": [\n        {\n          \"ChannelName\": \"train\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/training-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": 's3:\/\/{}\/test-set\/'.format(bucket)\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"train_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/training-set\/training-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        },\n        {\n          \"ChannelName\": \"validation_lst\",\n          \"CompressionType\": \"None\",\n          \"ContentType\": \"application\/x-image\",\n          \"DataSource\": {\n            \"S3DataSource\": {\n              \"S3DataDistributionType\": \"FullyReplicated\",\n              \"S3DataType\": \"S3Prefix\",\n              \"S3Uri\": \"s3:\/\/bucket\/test-set\/test-set.lst\"\n            }\n          },\n          \"RecordWrapperType\": \"None\"\n        }\n    ]\n<\/code><\/pre>\n\n<p>I am trying to use this in Amazon Sagemaker. But I'm unable to do that. Can someone please help?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1533291751550,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"lst file lst file creat excel sheet imag bucket messag file",
        "Challenge_last_edit_time":1533294102060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51670563",
        "Challenge_link_count":0,
        "Challenge_original_content":"lst file folder structur bucket bucket train set medium img jpeg img jpeg img png train set lst file medium img jpeg medium img jpeg medium img png creat excel sheet train clienterror lst file train set lst inputdataconfig channelnam train compressiontyp contenttyp imag datasourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri train set format bucket recordwrappertyp channelnam compressiontyp contenttyp imag datasourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri test set format bucket recordwrappertyp channelnam train lst compressiontyp contenttyp imag datasourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri bucket train set train set lst recordwrappertyp channelnam lst compressiontyp contenttyp imag datasourc sdatasourc sdatadistributiontyp fullyrepl sdatatyp sprefix suri bucket test set test set lst recordwrappertyp",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"lst file folder structur bucket file creat excel sheet train clienterror lst file",
        "Challenge_readability":14.9,
        "Challenge_reading_time":23.49,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Invalid .lst file in sagemaker",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":591.0,
        "Challenge_word_count":147,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":135956248450
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>I typically run scripts from my project\u2019s root directory, and use parameters to point to inputs and outputs as needed. Wrapping with <code>dvc run<\/code> generates a stage file in the root directory, which clutters the project.<\/p>\n<p>I tried using the <code>-f<\/code> option in <code>dvc run<\/code> to specify a different path for the stage file, but it ended up causing problems with the data paths. In particular it appears that DVC considers the dependencies and output paths to be relative to the stage file.<\/p>\n<p>My current solution is to make a <code>runs<\/code> directory from where I execute all <code>dvc run<\/code> commands. All the stage files end up in there. But this is a bit clunky and isn\u2019t the pattern I usually use.<\/p>\n<p>I\u2019m wondering if I\u2019m missing something here? Is there a way to store stage files to somewhere other than where the <code>dvc run<\/code> command was executed?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533304349308,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"gener stage file root directori run clutter tri option specifi path stage file data path creat run directori execut run store stage file directori",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-output-the-stage-file-in-a-different-directory-than-cwd\/58",
        "Challenge_link_count":0,
        "Challenge_original_content":"output stage file directori cwd typic run root directori paramet input output wrap run gener stage file root directori clutter tri option run specifi path stage file end data path depend output path rel stage file run directori execut run stage file end bit clunki isnt pattern miss store stage file run execut",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"output stage file directori cwd typic run root directori paramet input output wrap gener stage file root directori clutter tri option specifi path stage file end data path depend output path rel stage file directori execut stage file end bit clunki isnt pattern miss store stage file execut",
        "Challenge_readability":9.5,
        "Challenge_reading_time":12.1,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":10,
        "Challenge_solved_time":null,
        "Challenge_title":"Is it possible to output the stage file in a different directory than CWD?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1268.0,
        "Challenge_word_count":160,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":135943650692
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi mlflow-users,\n\nMLflow Release 0.4.1 is ready, released 2018-08-03. The release is available on\u00a0PyPI\u00a0and docs are\u00a0updated. Here are the release notes (also available\u00a0on GitHub):\n\n\n\nBreaking changes: None\n\nFeatures:\n\n[Projects] MLflow will use the conda installation directory given by the $MLFLOW_CONDA_HOME if specified (e.g. running conda commands by invoking \"$MLFLOW_CONDA_HOME\/bin\/conda\"), defaulting to running \"conda\" otherwise. (#231,\u00a0@smurching)\n[UI] Show GitHub links in the UI for projects run from http(s):\/\/ GitHub URLs (#235,\u00a0@smurching)\n\nBug fixes:\n\nFix GCSArtifactRepository issue when calling list_artifacts on a path containing nested directories (#233,\u00a0@jakeret)\nFix Spark model support when saving\/loading models to\/from distributed filesystems (#180,\u00a0@tomasatdatabricks)\nAdd missing mlflow.version import to sagemaker module (#229,\u00a0@dbczumar)\nValidate metric, parameter and run IDs in file store and Python client (#224,\u00a0@mateiz)\nValidate that the tracking URI is a remote URI for Databricks project runs (#234,\u00a0@smurching)\nFix bug where we'd fetch git projects at SSH URIs into a local directory with the same name as the URI, instead of into a temporary directory (#236,\u00a0@smurching)\n\n\n\n\n\nThanks,\nSid",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533324652000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"relat releas releas note featur break gcsartifactrepositori spark model metric paramet run id file store client fetch git ssh uri local directori uri temporari directori",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/2Diy6_Sflr0",
        "Challenge_link_count":0,
        "Challenge_original_content":"releas releas readi releas releas onpypiand doc areupd releas note availableon github break featur conda instal directori conda home specifi run conda invok conda home bin conda default run conda smurch github link run http github url smurch gcsartifactrepositori call list artifact path nest directori jakeret spark model save load model distribut filesystem tomasatdatabrick add miss version import modul dbczumar metric paramet run id file store client mateiz track uri remot uri run smurch fetch git ssh uri local directori uri temporari directori smurch sid",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"releas releas readi releas releas onpypiand doc areupd releas note break featur conda instal directori specifi default run conda github link run github url gcsartifactrepositori call path nest directori spark model model distribut filesystem add miss version import modul metric paramet run id file store client track uri remot uri run fetch git ssh uri local directori uri temporari directori sid",
        "Challenge_readability":11.8,
        "Challenge_reading_time":15.54,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow Release 0.4.1",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":28.0,
        "Challenge_word_count":175,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":135923348000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I have data projects where very large files change by only a few characters each new version. What disk usage expectations should users have?<\/p>\n<p>Also, there are often cases where we have many files in a directory that look very similar. Are there optimizations by which chunks of data within each file are hashed and de-duplicated?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533613591080,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"redund data version version larg file increas disk usag optim hash duplic chunk data file",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/redundant-data-across-version-and-within-versions\/60",
        "Challenge_link_count":0,
        "Challenge_original_content":"redund data version version data larg file charact version disk usag file directori optim chunk data file hash duplic",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"redund data version version data larg file charact version disk usag file directori optim chunk data file hash",
        "Challenge_readability":7.0,
        "Challenge_reading_time":5.05,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"Redundant Data across version and within versions",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":860.0,
        "Challenge_word_count":63,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":135634408920
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Hey there!<\/p>\n<p>I\u2019m trying to <code>dvc push<\/code> my data file to a private s3 bucket but am getting the error:<\/p>\n<p><code>Failed to push data to the cloud: The config profile (tdobbins) could not be found<\/code><\/p>\n<p>I pointed dvc to my .config file by doing:<\/p>\n<p><code>dvc remote modify myremote credentialpath aws\/.config<\/code><\/p>\n<p>and used:<\/p>\n<p><code>dvc remote modify myremote profile tdobbins<\/code><\/p>\n<p>Is there anything obvious that I\u2019m missing?<\/p>\n<p>Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533670100321,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"push data privat bucket push messag state config profil tdobbin config file modifi remot profil",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/adding-private-s3-buckets\/62",
        "Challenge_link_count":0,
        "Challenge_original_content":"privat bucket push data file privat bucket push data cloud config profil tdobbin config file remot modifi myremot credentialpath config remot modifi myremot profil tdobbin obviou miss",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"privat bucket data file privat bucket config file obviou miss",
        "Challenge_readability":11.2,
        "Challenge_reading_time":6.67,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":4,
        "Challenge_solved_time":null,
        "Challenge_title":"Adding private S3 buckets",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":969.0,
        "Challenge_word_count":69,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":135577899679
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"Hi mlflow-users,\n\nMLflow Release 0.4.2 is ready, released 2018-08-07. The release is available on\u00a0PyPI\u00a0and docs are\u00a0updated. Here are the release notes (also available\u00a0on GitHub):\n\n\n\nBreaking changes: None\n\nFeatures:\n\nMLflow experiments REST API and\u00a0mlflow experiments create\u00a0now support providing\u00a0--artifact-location\u00a0(#232,\u00a0@aarondav)\n[UI] Runs can now be sorted by columns, and added a Select All button (#227,\u00a0@ToonKBC)\nDatabricks File System (DBFS) artifactory support added (#226,\u00a0@andrewmchen)\ndatabricks-cli version upgraded to >= 0.8.0 to support new DatabricksConfigProvider interface (#257,\u00a0@aarondav)\n\nBug fixes:\n\nMLflow client sends REST API calls using snake_case instead of camelCase field names (#232,\u00a0@aarondav)\nMinor bug fixes (#243,\u00a0#242,\u00a0@aarondav;\u00a0#251,\u00a0@javierluraschi;\u00a0#245,\u00a0@smurching;\u00a0#252,\u00a0@mateiz)",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533678749000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"relat releas releas featur rest api file dbf artifactori break",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/NH0x9ch4MW0",
        "Challenge_link_count":0,
        "Challenge_original_content":"releas releas readi releas releas onpypiand doc areupd releas note availableon github break featur rest api createnow artifact locat aarondav run sort column select button toonkbc file dbf artifactori andrewmchen cli version upgrad databricksconfigprovid interfac aarondav client send rest api call snake camelcas field aarondav minor aarondav javierluraschi smurch mateiz",
        "Challenge_participation_count":0,
        "Challenge_preprocessed_content":"releas releas readi releas releas onpypiand doc areupd releas note break featur rest api createnow run sort column select button file artifactori version upgrad databricksconfigprovid interfac client send rest api call camelcas field minor",
        "Challenge_readability":9.5,
        "Challenge_reading_time":10.61,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow Release 0.4.2",
        "Challenge_topic":"Artifact Tracking",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":16.0,
        "Challenge_word_count":114,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":135569251000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"Hello. I am developing a system which uses predictions with stored ML model, like *.pkl from sklearn.\n\n\nI'd like to use mlflow as a prediction service server but still doubt about its scalability. AFAIU, mlflow is providing Rest API only with flask. Although it will work well in dev or test environments, but I can't sure that it can handle millions of requests per second, which is so common nowadays.\n\n\nIn short, I have following questions to the main developers:\nDo you have any plan to support other types of RPC call in the future? I found that you are using protobuf to generate requests\/responses. IMHO, it would be not so difficult to extend it to support gRPC on top of those definitions.\nIs there any plan to build a dedicated model service entity for scalability?\nThanks in advance. All kinds of opinions are welcome.\n\n\n\nBest,\nDongjin",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533808953000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"predict store model predict servic server scalabl million request plan type rpc futur plan build dedic model servic entiti scalabl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-JLdZ6r7IdY",
        "Challenge_link_count":0,
        "Challenge_original_content":"deploi model scale predict store model pkl sklearn predict servic server doubt scalabl afaiu rest api flask dev test environ million request common nowadai short plan type rpc futur protobuf gener request respons imho extend grpc definit plan build dedic model servic entiti scalabl advanc welcom dongjin",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"deploi model scale predict store model sklearn predict servic server doubt scalabl afaiu rest api flask dev test environ million request common nowadai short plan type rpc futur protobuf gener imho extend grpc definit plan build dedic model servic entiti scalabl advanc welcom dongjin",
        "Challenge_readability":7.0,
        "Challenge_reading_time":10.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":12,
        "Challenge_solved_time":null,
        "Challenge_title":"Deploying a model in scale",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":152.0,
        "Challenge_word_count":151,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":135439047000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I am trying to add a new ssh remote.<\/p>\n<p><code>dvc remote add base ssh:\/\/elena@server_name.company.lan:\/home\/elena\/test_dvc\/data<\/code><\/p>\n<p>and I get a <code>Initialization error: Config file error: Unsupported URL<\/code> when trying <code>dvc status\/pull\/push<\/code>.<\/p>\n<p>I tried adding<br>\n<code>dvc remote modify base credentialpath ~\/.ssh\/config<\/code><br>\nbut the error persists.<\/p>\n<p>Can anyone help with an example on how to fully configure an ssh remote for dvc?<\/p>\n<p>Later edit:<br>\nSolved: Typo in the remote url (the remote path was missing \u2018\/\u2019 )<\/p>\n<p>Thanks,<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533818634630,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"unsupport url add ssh remot perform statu pull push modifi credenti path persist request fulli configur ssh remot later typo remot url",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/getting-unsupported-url-when-adding-ssh-remote\/64",
        "Challenge_link_count":0,
        "Challenge_original_content":"unsupport url ssh remot add ssh remot remot add base ssh elena server compani lan home elena test data initi config file unsupport url statu pull push tri remot modifi base credentialpath ssh config persist fulli configur ssh remot later edit typo remot url remot path miss",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"unsupport url ssh remot add ssh remot tri persist fulli configur ssh remot later edit typo remot url",
        "Challenge_readability":10.7,
        "Challenge_reading_time":8.45,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"Getting \"Unsupported URL\" when adding SSH remote",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1379.0,
        "Challenge_word_count":80,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":135429365370
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I have trained a AWS SageMaker Model using the in-built Linear Learner algorithm. I can download the trained model artifacts (model.tar.gz) from S3.<\/p>\n\n<p>How can I deploy the model in an local environment which is independent of AWS, so I can make predictions inferences calls without internet access?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533869148300,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"deploi linear learner model local environ independ order predict infer call internet access train model built linear learner algorithm download train model artifact",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51778200",
        "Challenge_link_count":0,
        "Challenge_original_content":"deploi linear learner model local environ train model built linear learner algorithm download train model artifact model tar deploi model local environ independ predict infer call internet access",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"deploi linear learner model local environ train model linear learner algorithm download train model artifact deploi model local environ independ predict infer call internet access",
        "Challenge_readability":9.2,
        "Challenge_reading_time":4.79,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I deploy AWS SageMaker Linear Learner Model in a Local Environment",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":518.0,
        "Challenge_word_count":60,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":135378851700
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am using the aws sagemaker for logistic regression. For validating the model on test data, the following code is used <\/p>\n\n<pre><code>runtime= boto3.client('runtime.sagemaker')\n\npayload = np2csv(test_X)\nresponse = runtime.invoke_endpoint(EndpointName=linear_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\ntest_pred = np.array([r['score'] for r in result['predictions']])\n<\/code><\/pre>\n\n<p>The result contains the prediction values and the probability scores. \nI want to know how I can run a prediction model to predict the outcome based on two specific features. Eg. I have 30 features in the model and have trained model using those features. Now for my prediction, I want to know the outcome when feature1='x' and feature2='y'. But when I filter the data to those columns and pass that in the same code, I get the following error.<\/p>\n\n<pre><code>Customer Error: The feature dimension of the input: 4 does not match the feature dimension of the model: 30. Please fix the input and try again.\n<\/code><\/pre>\n\n<p>What is the equivalent of say glm.predict('feature1','feature2')in R in AWS Sagemaker implementation?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1533920085857,
        "Challenge_favorite_count":2.0,
        "Challenge_gpt_summary":"logist regress run predict model base featur filter data column pass state featur dimens input match featur dimens model guidanc predict base featur",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51790953",
        "Challenge_link_count":0,
        "Challenge_original_content":"logist regress logist regress model test data runtim boto client runtim payload npcsv test respons runtim invok endpoint endpointnam linear endpoint contenttyp text csv bodi payload json load respons bodi read decod test pred arrai score predict predict valu probabl score run predict model predict base featur featur model train model featur predict featur featur filter data column pass featur dimens input match featur dimens model input equival glm predict featur featur implement",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"logist regress logist regress model test data predict valu probabl score run predict model predict base featur featur model train model featur predict featur featur filter data column pass equival implement",
        "Challenge_readability":8.6,
        "Challenge_reading_time":15.29,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":15,
        "Challenge_solved_time":null,
        "Challenge_title":"Logistic regression in sagemaker",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":1557.0,
        "Challenge_word_count":161,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":135327914143
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I would like to know whether we could store run info for reproducibility and add output data to a storage that can be than pulled.<br>\nFor example, I have a raw dataset and a cleanup script<\/p>\n<pre><code class=\"lang-auto\">$ ls \nraw cleanup.py\n<\/code><\/pre>\n<p>I can run the cleanup<\/p>\n<pre><code class=\"lang-auto\">$ dvc run -d raw -o clean python cleanup.py raw clean\n$ cat clean.dvc\ncmd: python cleanup.py\ndeps:\n- md5: xxxx\n  path: raw\nmd5: yyyyy\nouts:\n- cache: true\n  md5: zzzz\n  path: clean\n<\/code><\/pre>\n<p>and I observe how clean folder is produced. However if I add clean folder with dvc in order to share it the information on how to produce clean folder is modifed<\/p>\n<pre><code class=\"lang-auto\">$ dvc add clean\n$ cat clean.dvc\nmd5: xxxx\nouts:\n- cache: true\n   md5: xxxxx\n<\/code><\/pre>\n<p>So, can we have both features : stored command on how dataset can be produced and is stored in the cache ?<\/p>\n<p>Thanks<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534263237672,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"store run reproduc output data storag pull successfulli run cleanup add clean folder order share produc clean folder modifi featur store dataset produc store cach",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/dvc-run-and-add-store-command-and-data\/68",
        "Challenge_link_count":0,
        "Challenge_original_content":"run add store data store run reproduc add output data storag pull raw dataset cleanup raw cleanup run cleanup run raw clean cleanup raw clean cat clean cmd cleanup dep path raw out cach path clean observ clean folder produc add clean folder order share produc clean folder modif add clean cat clean out cach featur store dataset produc store cach",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"run add store data store run reproduc add output data storag pull raw dataset cleanup run cleanup observ clean folder produc add clean folder order share produc clean folder modif featur store dataset produc store cach",
        "Challenge_readability":6.5,
        "Challenge_reading_time":11.88,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":9,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC run and add: store command and data",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":495.0,
        "Challenge_word_count":155,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":134984762328
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I am trying to use aws sagemaker with Windows using Docker :\nHere is the docker file :<\/p>\n\n<pre><code># Build an image that can do training and inference in SageMaker\n# This is a Python 2 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:16.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3.5 \\\n         nginx \\\n         libgcc-5-dev \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Here we get all python packages.\n# There's substantial overlap between scipy and numpy that we eliminate by\n# linking them together. Likewise, pip leaves the install caches populated which uses\n# a significant amount of space. These optimizations save a fair amount of space in the\n# image, which reduces start up time.\nRUN wget https:\/\/bootstrap.pypa.io\/3.3\/get-pip.py &amp;&amp; python3.5 get-pip.py &amp;&amp; \\\n    pip3 install numpy==1.14.3 scipy scikit-learn==0.19.1 xgboost==0.72.1 pandas==0.22.0 flask gevent gunicorn &amp;&amp; \\\n        (cd \/usr\/local\/lib\/python3.5\/dist-packages\/scipy\/.libs; rm *; ln ..\/..\/numpy\/.libs\/* .) &amp;&amp; \\\n        rm -rf \/root\/.cache\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=\"\/opt\/program:${PATH}\"\n\n# Set up the program in the image\nCOPY xgboost \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n\n<p>My question is should I, since I work under windows 7, change these path : ?<\/p>\n\n<p>Thank you<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534427312350,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"window docker path docker file window",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51878668",
        "Challenge_link_count":1,
        "Challenge_original_content":"window window docker docker file build imag train infer imag nginx gunicorn flask stack serv infer stabl ubuntu maintain run apt updat apt instal instal wget nginx libgcc dev certif var lib apt list packag substanti overlap scipi numpi elimin link likewis pip leav instal cach popul signific space optim save fair space imag reduc start time run wget http bootstrap pypa pip pip pip instal numpi scipi scikit panda flask gevent gunicorn usr local lib dist packag scipi lib numpi lib root cach set environ variabl pythonunbuff keep buffer standard output stream log deliv quickli pythondontwritebytecod keep write pyc file unnecessari updat path train serv program invok env pythonunbuff env pythondontwritebytecod env path opt program path set program imag copi opt program workdir opt program window path",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"window window docker docker file window path",
        "Challenge_readability":7.6,
        "Challenge_reading_time":23.52,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker with windows",
        "Challenge_topic":"Docker Deployment",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":359.0,
        "Challenge_word_count":244,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":134820687650
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>HI<br>\nCan we add local storage for DVC. like (i dont want to store it on s3 or gcp, need  to only point to local storage)<br>\nex :<\/p>\n<ol>\n<li>dvc add file:\\\\ xxx.xx.xx.x\\images\\annex\\dvc-storage<br>\nor<\/li>\n<li>dvc add X:\/annex\/dvc-storage\/data.xml ( local storage)<br>\nAfter trying above option. i am getting error.<br>\nInitialization error: Config file error: Unsupported URL.<br>\nPlease provide an appropriate solution or syntax<\/li>\n<\/ol>\n<p>Note : storage is tyron.   the storage location is mounted to window or on linux.<\/p>\n<p>With ref : <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-fit-in-a-local-area-network-infrastucture-where-git-repos-are-not-in-the-computing-server\/24\/3\" class=\"inline-onebox\">Does DVC fit in a Local Area Network infrastucture where git repos are not in the computing server?<\/a><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534489981766,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"local storag store tri local storag option messag state unsupport url storag locat mount window linux tyron syntax",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/dvc-support-for-the-local-storage\/71",
        "Challenge_link_count":1,
        "Challenge_original_content":"local storag add local storag store local storag add file imag annex storag add annex storag data xml local storag option initi config file unsupport url syntax note storag tyron storag locat mount window linux ref fit local area network infrastuctur git repo comput server",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"local storag add local storag add add option initi config file unsupport url syntax note storag tyron storag locat mount window linux ref fit local area network infrastuctur git repo comput server",
        "Challenge_readability":10.5,
        "Challenge_reading_time":10.97,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC support for the local storage",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2069.0,
        "Challenge_word_count":103,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":134758018234
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>Several people (<a href=\"https:\/\/stackoverflow.com\/questions\/41780141\/using-a-scala-udf-in-pyspark\">1<\/a>, <a href=\"https:\/\/medium.com\/wbaa\/using-scala-udfs-in-pyspark-b70033dd69b9\" rel=\"nofollow noreferrer\">2<\/a>, <a href=\"https:\/\/github.com\/amesar\/spark-python-scala-udf\" rel=\"nofollow noreferrer\">3<\/a>) have discussed using a Scala UDF in a PySpark application, usually for performance reasons.  I am interested in the opposite - using a python UDF in a Scala Spark project.<\/p>\n\n<p>I am particularly interested in building a model using sklearn (and <a href=\"https:\/\/databricks.com\/blog\/2018\/06\/05\/introducing-mlflow-an-open-source-machine-learning-platform.html\" rel=\"nofollow noreferrer\">MLFlow<\/a>) then efficiently applying that to records in a Spark streaming job.  I know I could also host the python model behind a REST API and <a href=\"https:\/\/stackoverflow.com\/questions\/41799578\/restapi-service-call-from-spark-streaming\">make calls to that API in the Spark streaming application<\/a> in <a href=\"https:\/\/spark.apache.org\/docs\/2.3.0\/api\/scala\/index.html#org.apache.spark.sql.Dataset@mapPartitions[U](f:org.apache.spark.api.java.function.MapPartitionsFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]\" rel=\"nofollow noreferrer\"><code>mapPartitions<\/code><\/a>, but managing concurrency for that task and setting up the API for hosted model isn't something I'm super excited about.<\/p>\n\n<p>Is this possible without too much custom development with something like Py4J? Is this just a bad idea?<\/p>\n\n\n\n<p>Thanks!<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":10,
        "Challenge_created_time":1534609811997,
        "Challenge_favorite_count":3.0,
        "Challenge_gpt_summary":"udf scala spark build model sklearn effici appli record spark stream job host model rest api concurr set api advic pyj idea",
        "Challenge_last_edit_time":1575115357760,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51910607",
        "Challenge_link_count":6,
        "Challenge_original_content":"pyspark udf scala spark peopl scala udf pyspark perform reason opposit udf scala spark build model sklearn effici appli record spark stream job host model rest api call api spark stream mappartit concurr task set api host model isn pyj idea",
        "Challenge_participation_count":11,
        "Challenge_preprocessed_content":"pyspark udf scala spark peopl scala udf pyspark perform reason opposit udf scala spark build model sklearn effici appli record spark stream job host model rest api call api spark stream concurr task set api host model isn idea",
        "Challenge_readability":15.9,
        "Challenge_reading_time":21.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":11.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"How to use a PySpark UDF in a Scala Spark project?",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":1100.0,
        "Challenge_word_count":146,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":134638188003
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to install \"ompr.roi\" package which requires R version >= 3.4.0.<\/p>\n\n<p>But Azure ML Studio supports R version till 3.2.2. Pls refer below screenshot,<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/bAoaW.jpg\" alt=\"Error in Execute R Script\"><\/p>\n\n<p>Is there any way I can use this library in Azure ML Studio.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1534748925187,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"instal ompr roi packag studio version studio version till librari studio",
        "Challenge_last_edit_time":1534749536030,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51925635",
        "Challenge_link_count":1,
        "Challenge_original_content":"version studio instal ompr roi packag version studio version till pl screenshot librari studio",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"version studio instal packag version studio version till pl screenshot librari studio",
        "Challenge_readability":5.4,
        "Challenge_reading_time":4.51,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Issue with R version in Azure ML Studio",
        "Challenge_topic":"Environment Configuration",
        "Challenge_topic_macro":"Deployment Management",
        "Challenge_view_count":173.0,
        "Challenge_word_count":51,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":134499074813
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>In our backend development process, we have two environments: testing and production. We develop our code, and then we push the code into the testing repository. Then on the release date, we push everything into production. <\/p>\n\n<p>Now that we are going to use ML studio, I'm struggling with setting up testing and production environments for my ML studio experiments.<\/p>\n\n<p>I created two identical experiments with independent APIs; one experiment for testing and the other experiment is used by the production. When it comes to moving the trained experiment from testing to production, I make all the changes I made in the testing environment to the production environment, which is a very time demanding process. <\/p>\n\n<p>Do you know any better solution so we can deploy and test our changes and then deploy the latest changes to the production? How people use ML studio in their CD\/CI process?<\/p>\n\n<p>The attached image shows the design that I have now. I'd appreciate if you can help me in improving this process. Maybe ML studio has some features to manage this scenario that I don't know.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/eBcuP.jpg\" alt=\"\"><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534811611857,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"set test environ studio creat ident independ api test move train test time consum deploi test deploi latest guidanc peopl studio process",
        "Challenge_last_edit_time":1534822500183,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51940106",
        "Challenge_link_count":1,
        "Challenge_original_content":"practic studio api backend process environ test push test repositori releas date push studio set test environ studio creat ident independ api test come move train test test environ environ time process deploi test deploi latest peopl studio process attach imag design improv process mayb studio featur",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"practic studio api backend process environ test push test repositori releas date push studio set test environ studio creat ident independ api test come move train test test environ environ time process deploi test deploi latest peopl studio process attach imag design improv process mayb studio featur",
        "Challenge_readability":8.1,
        "Challenge_reading_time":15.25,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":13,
        "Challenge_solved_time":null,
        "Challenge_title":"What is the best practice to develop CD\/CI when you use ML studio APIs?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":51.0,
        "Challenge_word_count":199,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":134436388143
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>Hi<\/p>\n<p>I was wondering how the remote mechanisms work.<br>\nAssuming I have two different DVC repos, that may use the same base dataset.<br>\nCan I use the same remote for both of them, so the dataset is not stored twice?<\/p>\n<p>This is strictly speaking not possible because of hashing collisions, is it?<\/p>\n<p>Regards<br>\nMatthias<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534853808824,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"inquir remot repositori base dataset avoid store dataset twice hash collis",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/same-remote-for-multiple-repos\/75",
        "Challenge_link_count":0,
        "Challenge_original_content":"remot multipl repo remot mechan repo base dataset remot dataset store twice strictli speak hash collis matthia",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"remot multipl repo remot mechan repo base dataset remot dataset store twice strictli speak hash collis matthia",
        "Challenge_readability":5.9,
        "Challenge_reading_time":4.63,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Same remote for multiple repos",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1356.0,
        "Challenge_word_count":58,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":134394191176
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":5,
        "Challenge_body":"<p>Hi,<\/p>\n<p>first of all, very nice tool, thank you for that!<\/p>\n<p>I am currently experimenting with dvc in a pytorch ML pipeline.<br>\nOne step of the pipeline is training and pickling a model.<\/p>\n<p>Usually training a model starts with random numbers, so if I do not set random seeds I will end up in a dfiferent model each training re-run, resulting in a new md5 hash for the model file each time.<br>\nEven if I set seeds, and the model ends up in the same values, I was not able (yet) with pytorch to pickle the model in a way that it exactly equals the previous run in terms of md5 hash.<\/p>\n<p>Do you recommend any best practices regarding such kind of indeterministic\/random output (for pytorch and\/or in general)?<br>\nI think that there might arise problems when sharing pipelines but not data, or when having different metrics output.<br>\nIs it a problem at all? Is dvc capable of handling these things and if yes, how?<\/p>\n<p>Thanks,<\/p>\n<p>Alex<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534925717183,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"indeterminist output pytorch pipelin train model random model time random seed set practic indeterminist random output share pipelin data metric output capabl",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/handling-indeterministic-output\/78",
        "Challenge_link_count":0,
        "Challenge_original_content":"indeterminist output nice pytorch pipelin step pipelin train pickl model train model start random set random seed end dfifer model train run hash model file time set seed model end valu pytorch pickl model exactli equal previou run term hash practic indeterminist random output pytorch gener share pipelin data metric output capabl alex",
        "Challenge_participation_count":5,
        "Challenge_preprocessed_content":"indeterminist output nice pytorch pipelin step pipelin train pickl model train model start random set random seed end dfifer model train hash model file time set seed model end valu pytorch pickl model exactli equal previou run term hash practic output share pipelin data metric output capabl alex",
        "Challenge_readability":8.1,
        "Challenge_reading_time":12.15,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":3.0,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Handling indeterministic output",
        "Challenge_topic":"Pipeline Automation",
        "Challenge_topic_macro":"Lifecycle Management",
        "Challenge_view_count":937.0,
        "Challenge_word_count":169,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":134322282817
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":0,
        "Challenge_body":"<p>I have a classifier working with XGBoost in sagemaker, but despite the training set only having 1s and 0s in the first column (csv file, first column is assumed to be target in sagemaker xgboost), the algorithm returns a decimal. <\/p>\n\n<p>First 3 records return 1.08, 0.34, and 0.91. I'd assume probabilities but 1.08? If these are rounded to 0 or 1 then they're all correct, but why is it returning non-class values?<\/p>\n\n<p>Furthermore, the class only contains a predict method - is a predict probability method not possible without using your own model?<\/p>\n\n<p>The code calling this is:<\/p>\n\n<pre><code>from flask import Flask\nfrom flask import request\nimport boto3\nfrom sagemaker.predictor import csv_serializer\nimport sagemaker\n\napp = Flask(__name__)\n\n@app.route(\"\/\")\ndef hello():\n    numbers = request.args.get('numbers')\n\n    #session\n    boto_session = boto3.Session(profile_name=\"profilename\",\n                          region_name='regionname')\n\n    #sagemaker session\n    sagemaker_session = sagemaker.Session(boto_session=boto_session)\n\n    #endpoint\n    predictor = sagemaker.predictor.RealTimePredictor(endpoint=\"modelname\", \n        sagemaker_session=sagemaker_session)\n    predictor.content_type=\"text\/csv\"\n    predictor.serializer=csv_serializer\n    predictor.deserializer=None\n\n    #result\n    result=predictor.predict(numbers)\n    result=result.decode(\"utf-8\")\n    return f'Output: {result}'\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=5000)\n<\/code><\/pre>\n\n<p>The flask section works fine, I can retrieve predictions at 127.0.0.1:5000.<\/p>\n\n<p>Sagemaker Version 1.3.0. Version 1.9.0 does not work - it requires fcntrl which is mac\/linux only - see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/311\" rel=\"nofollow noreferrer\">this on their repo<\/a>, apparently it's fixed on pypi but I've tried and the version doesn't change or fix the issue, so I'm stuck on 1.3.0 until they resolve it. Version 1.3.0 does not have a predict_proba method.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":6,
        "Challenge_created_time":1534940431650,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"invok endpoint return decim valu binari valu classifi class valu return predict probabl model call endpoint version version",
        "Challenge_last_edit_time":1534965275236,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51966783",
        "Challenge_link_count":1,
        "Challenge_original_content":"invok endpoint return valu type classifi train set column csv file column target algorithm return decim record return probabl round return class valu furthermor class predict predict probabl model call flask import flask flask import request import boto predictor import csv serial import app flask app rout request arg session boto session boto session profil profilenam region regionnam session session session boto session boto session endpoint predictor predictor realtimepredictor endpoint modelnam session session predictor type text csv predictor serial csv serial predictor deseri predictor predict decod utf return output app run debug port flask section retriev predict version version fcntrl mac linux repo appar pypi tri version stuck version predict proba",
        "Challenge_participation_count":6,
        "Challenge_preprocessed_content":"invok endpoint return valu type classifi train set column algorithm return decim record return probabl round return valu furthermor class predict predict probabl model call flask section retriev predict version version fcntrl repo appar pypi tri version stuck version",
        "Challenge_readability":11.9,
        "Challenge_reading_time":25.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":22,
        "Challenge_solved_time":null,
        "Challenge_title":"Sagemaker invoke endpoint returned value type?",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":990.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":134307568350
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":6,
        "Challenge_body":"<p>We have an Azure Machine Learning web service that is called fine from a C# program.  And it works fine when called as an HTML post (with Headers and a JSON string in the body).  However, in Azure Stream Analytics you have to create a Function to call an ML service.  And when this function is called in ASA, it fails with Bad Request.<\/p>\n\n<p>The documentation for the ML service gives the following documentation:<\/p>\n\n<p>Request Body\nSample Request<\/p>\n\n<pre><code>{\n   \"Inputs\":{\n      \"input\":[\n         {\n            \"device\":\"60-1-94-49-36-c5\",\n            \"uid\":\"5f4736aabfc1312385ea09805cc922\",\n            \"weight\":\"9-9-9-9-9-8-9-8-9-9-9-9-9-9-9-9-9-8-9-9-8-8-9-9-9-9-9- \n9-9-9-9-9-9-9-8-9-9-9-9-9-9-9-9-9-9-9-9-9-8-9-9-9-9-9-9-9-9-9-9-9-9-9-9-9-9- \n9-9-8-9-9-9-9-8-9-9-9-8-9-9-9-9-9-9-9-9-9-8-9-9-9-9-8-8-16-16-15-16-16-15- \n15-16-15-15-15-15-16-15-15-16-15-15-9-15-15-15-15-15-15-15-9-15-16-15-15-9- \n15-16-16-16-15-15-15-15-15-15-15-15-16-16-15-9-15-15-15-16-15-16-15-15-15- \n15-15-16-15-15-16-16-15-15-15\"\n         }\n      ]\n   },\n   \"GlobalParameters\":{\n\n   }\n}\n<\/code><\/pre>\n\n<p>The Azure Stream Analytics function (that calls the ML service above) has this signature:<\/p>\n\n<pre><code>FUNCTION SIGNATURE\nSmartStokML2018Aug17 ( device NVARCHAR(MAX) , \n                       uid    NVARCHAR(MAX) , \n                       weight NVARCHAR(MAX) ) RETURNS RECORD\n<\/code><\/pre>\n\n<p>Here the function is expecting 3 string arguments and NOT a full JSON string.  The 3 parameters are strings (NVARCHAR as shown).<\/p>\n\n<p>The 3 parameters have been passed in: device, uid and weight.  And in different string formats.  This includes passing the string arguments as JSON strings, using JSON.stringify() in a UDF, or sending in arguments with just data, no headers (\"device\", \"uid\", \"weight\").  But all calls to the ML service fail.<\/p>\n\n<pre><code>WITH QUERY1 AS ( \nSELECT DEVICE, UID, WEIGHT, \n       udf.jsonstringify( concat('{\"device\": \"',try_cast(device as nvarchar(max)), '\"}')) jsondevice,\n       udf.jsonstringify( concat('{\"uid\": \"',try_cast(uid as nvarchar(max)), '\"}')) jsonuid,\n       udf.jsonstringify( concat('{\"weight\": \"',try_cast(weight as nvarchar(max)), '\"}')) jsonweight\nFROM iothubinput2018aug21 ),\n\nQUERY2 AS (\nSELECT IntellistokML2018Aug21(JSONDEVICE, JSONUID, JSONWEIGHT) AS RESULT\nFROM QUERY1\n)\n\nSELECT *    \nINTO OUT2BLOB20                \nFROM QUERY2\n<\/code><\/pre>\n\n<p>Most of the errors are:\n    ValueError: invalid literal for int() with base 10: '\\\\\" {weight:9'\\n\\r\\n\\r\\n<\/p>\n\n<blockquote>\n  <blockquote>\n    <p>In what format does the ML Service expect these parameters to be passed in?<\/p>\n  <\/blockquote>\n<\/blockquote>\n\n<p>Note: the queries have been tried with ASA Compatibility Level 1 and 1.1.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1534940837563,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"call function stream analyt request servic call program html function asa request function argument servic json tri pass paramet format pass argument json json stringifi udf send argument data header devic uid weight call servic valueerror liter base",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/51966899",
        "Challenge_link_count":0,
        "Challenge_original_content":"stream analyt request call function servic call web servic call program call html header json bodi stream analyt creat function servic function call asa request document servic document request bodi sampl request input input devic uid faabfceacc weight globalparamet stream analyt function call servic signatur function signatur smartstokmlaug devic nvarchar uid nvarchar weight nvarchar return record function argument json paramet nvarchar shown paramet pass devic uid weight format pass argument json json stringifi udf send argument data header devic uid weight call servic queri select devic uid weight udf jsonstringifi concat devic cast devic nvarchar jsondevic udf jsonstringifi concat uid cast uid nvarchar jsonuid udf jsonstringifi concat weight cast weight nvarchar jsonweight iothubinputaug queri select intellistokmlaug jsondevic jsonuid jsonweight queri select outblob queri valueerror liter base weight format servic paramet pass note queri tri asa compat level",
        "Challenge_participation_count":7,
        "Challenge_preprocessed_content":"stream analyt request call function servic call web servic call program call html stream analyt creat function servic function call asa request document servic document request bodi sampl request stream analyt function signatur function argument json paramet paramet pass devic uid weight format pass argument json udf send argument data header call servic valueerror liter base format servic paramet pass note queri tri asa compat level",
        "Challenge_readability":10.2,
        "Challenge_reading_time":34.65,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"In Azure Stream Analytics Bad Request results when calling Azure Machine Learning function even though Azure ML service is called fine from C#",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":588.0,
        "Challenge_word_count":299,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":134307162437
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"Hi,\n\n\nI meet Matei and Mani during spark summit. We are currently evaluating MLFlow on Kubernetes and had some questions about it.\u00a0\n\n\nScenario: We have MLFlow Tracking server deployed in Kubernetes we also have a Jupyter notebook to run MLFlow Training.\u00a0\n\n\nHowever, if we don't provide s3 credentials in Jupyter Notebook container it sends an error. Is it required for both Jupyter Notebook and MLFlow Tracking Server to have s3 credentials in the container?\u00a0\n\n\nApache Spark allows for us to specify an non s3 endpoint other than aws. That way we can use systems like Ceph to store our models. Is this something that will work for MLFlow.\u00a0\n\n\n\n\nThanks,\u00a0\n\nZak Hassan\n\nEngineer - Artificial Intelligence -\u00a0 Center Of Excellence, CTO Office\nhttp:\/\/radanalytics.io\/ - Machine Learning On OpenShift",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1534951986000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"evalu kubernet relat credenti jupyt notebook track server explor endpoint ceph store model",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/vx1uqw2GsFk",
        "Challenge_link_count":1,
        "Challenge_original_content":"evalu kubernet meet matei mani spark summit evalu kubernet track server deploi kubernet jupyt notebook run train credenti jupyt notebook send jupyt notebook track server credenti apach spark allow specifi endpoint system ceph store model zak hassan engin artifici intellig center excel cto offic http radanalyt openshift",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"evalu kubernet meet matei mani spark summit evalu kubernet track server deploi kubernet jupyt notebook run train credenti jupyt notebook send jupyt notebook track server credenti apach spark allow specifi endpoint system ceph store model zak hassan engin artifici intellig center excel cto offic openshift",
        "Challenge_readability":7.8,
        "Challenge_reading_time":9.92,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":11,
        "Challenge_solved_time":null,
        "Challenge_title":"Evaluating MLFlow on Kubernetes",
        "Challenge_topic":"Kubernetes Service",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":411.0,
        "Challenge_word_count":127,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":134296014000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":9,
        "Challenge_body":"Hey folks,\n\n\nI got the mlflow ui server set up, and its saving everything except log_artifact.\n\n\nThe server is running mlflow in a docker container with continuum\/miniconda3 as the base image.\n\n\nFrom the command line, I'm launching the server like this:\n\n\n$ mlflow ui -h 0.0.0.0 -p 5000\n\n\nI can see and interact with the data.\n\n\nWhen I try to save the params, metrics and artifacts, I do this:\n\n\n# neumann\nmlflow_server = '52.89....'\n\n# Tracking URI\nmlflow_tracking_URI = 'http:\/\/' + mlflow_server + ':5000'\nprint (\"MLflow Tracking URI: %s\" % (mlflow_tracking_URI))\n\n\n# set tracking URI\nmlflow.set_tracking_uri(mlflow_tracking_URI)\n\n\nwith mlflow.start_run(experiment_id=3):\n\u00a0 \u00a0 mlflow.log_param(\"depth\", 5)\n\u00a0 \u00a0 mlflow.log_metric(\"roc_auc\", 0.8)\n\u00a0 \u00a0 mlflow.log_artifact(local_path='curve.png')\n\nThis is my FileNotFoundError error message:\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n<ipython-input-113-f5870bc80ffe> in <module>()\n      2     mlflow.log_param(\"depth\", 5)\n      3     mlflow.log_metric(\"roc_auc\", 0.8)\n----> 4     mlflow.log_artifact(local_path='curve.png')\n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/tracking\/fluent.py in log_artifact(local_path, artifact_path)\n    131     \"\"\"Log a local file or directory as an artifact of the currently active run.\"\"\"\n    132     artifact_uri = _get_or_start_run().info.artifact_uri\n--> 133     get_service().log_artifact(artifact_uri, local_path, artifact_path)\n    134 \n    135 \n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/tracking\/service.py in log_artifact(self, artifact_uri, local_path, artifact_path)\n    105         :param artifact_path: If provided, will be directory in artifact_uri to write to\"\"\"\n    106         artifact_repo = ArtifactRepository.from_artifact_uri(artifact_uri, self.store)\n--> 107         artifact_repo.log_artifact(local_path, artifact_path)\n    108 \n    109     def log_artifacts(self, artifact_uri, local_dir, artifact_path=None):\n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/store\/local_artifact_repo.py in log_artifact(self, local_file, artifact_path)\n     14             if artifact_path else self.artifact_uri\n     15         if not exists(artifact_dir):\n---> 16             mkdir(artifact_dir)\n     17         shutil.copy(local_file, artifact_dir)\n     18 \n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/utils\/file_utils.py in mkdir(root, name)\n     99             return target\n    100     except OSError as e:\n--> 101         raise e\n    102 \n    103 \n\n~\/py3\/lib\/python3.7\/site-packages\/mlflow\/utils\/file_utils.py in mkdir(root, name)\n     96     try:\n     97         if not exists(target):\n---> 98             os.mkdir(target)\n     99             return target\n    100     except OSError as e:\n\nFileNotFoundError: [Errno 2] No such file or directory: '\/mlruns\/3\/1053b732c0a14d6cb8c07ee4320fd781\/artifacts'\n\n\n\nWhen I look at the filesystem, everything is saving except artifacts:\n\n\n~\/mlruns\/3$ tree\n.\n\u251c\u2500\u2500 5dcd18160aa74e6e8e405a6257a13177\n\u2502 \u00a0 \u251c\u2500\u2500 artifacts\n\u2502 \u00a0 \u251c\u2500\u2500 meta.yaml\n\u2502 \u00a0 \u251c\u2500\u2500 metrics\n\u2502 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 roc_auc\n\u2502 \u00a0 \u2514\u2500\u2500 params\n\u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 depth\n\n\nAny suggestions?\n\n\nThanks!\nFranklin",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535031783000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"server log artifact function server run docker continuum miniconda base imag save param metric artifact messag filenotfounderror filesystem save artifact",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/vyq2eJqDU0k",
        "Challenge_link_count":1,
        "Challenge_original_content":"log artifact folk server set save log artifact server run docker continuum miniconda base imag line launch server interact data save param metric artifact neumann server track uri track uri http server print track uri track uri set track uri set track uri track uri start run log param depth log metric roc auc log artifact local path curv png filenotfounderror messag filenotfounderror traceback log param depth log metric roc auc log artifact local path curv png lib site packag track fluent log artifact local path artifact path log local file directori artifact activ run artifact uri start run artifact uri servic log artifact artifact uri local path artifact path lib site packag track servic log artifact artifact uri local path artifact path param artifact path directori artifact uri write artifact repo artifactrepositori artifact uri artifact uri store artifact repo log artifact local path artifact path log artifact artifact uri local dir artifact path lib site packag store local artifact repo log artifact local file artifact path artifact path artifact uri artifact dir mkdir artifact dir shutil copi local file artifact dir lib site packag util file util mkdir root return target oserror rais lib site packag util file util mkdir root target mkdir target return target oserror filenotfounderror errno file directori mlrun bcadcbceefd artifact filesystem save artifact mlrun tree dcdaaeeeaa artifact meta yaml metric roc auc param depth franklin",
        "Challenge_participation_count":9,
        "Challenge_preprocessed_content":"folk server set save server run docker base imag line launch server interact data save param metric artifact neumann track uri print set track uri filenotfounderror messag filenotfounderror traceback log local file directori artifact activ param directori write mkdir return target oserror rais mkdir return target oserror filenotfounderror file directori filesystem save artifact tree dcd artifact metric param depth franklin",
        "Challenge_readability":13.1,
        "Challenge_reading_time":37.31,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":27,
        "Challenge_solved_time":null,
        "Challenge_title":"log_artifact not working",
        "Challenge_topic":"Artifact Tracking",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":2390.0,
        "Challenge_word_count":270,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":134216217000
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. And i will push the data to S3 using dvc push. In our project multiple people are working together if another person already push the data changes to dvc if im going to push same it will through dvc conflict. In git we are getting git conflict<\/p>\n<p>please let me know<\/p>\n<p>Thanks.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535600671377,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"multipl peopl repositori push conflict push advic",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/dvc-conflict-may-arrive-if-multiple-user-working-on-same-repository\/86",
        "Challenge_link_count":0,
        "Challenge_original_content":"conflict arriv multipl repositori pipelin poc add track data file push data push multipl peopl push data push conflict git git conflict",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"conflict arriv multipl repositori pipelin poc add track data file push data push multipl peopl push data push conflict git git conflict",
        "Challenge_readability":5.5,
        "Challenge_reading_time":5.83,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":0.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC conflict may arrive if multiple user working on same repository?",
        "Challenge_topic":"Multi-Model Endpoint",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":821.0,
        "Challenge_word_count":88,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":133647328623
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"<p>Hi Ruslan,<\/p>\n<p>I would like to ask the best practice on how to handle the following situation with DVC.<br>\nLet\u2019s suppose we have a big raw dataset stored in the cloud.<\/p>\n<ul>\n<li>So, we initialize a project (git + dvc) to work with this dataset. At first we would like to trace how to get the dataset locally. For this we use <code>dvc run<\/code> with commands as <code>wget url<\/code> to download the raw dataset. We commit changes etc.<\/li>\n<li>Next we develop some scripts to preprocess the raw dataset into a dataset we call v0.1.0. Then we use <code>dvc run<\/code> to run the preprocessing on the raw dataset and store the way to obtain the dataset v0.1.0. Thus, we have two folders like <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>. We commit and push with git and dvc. At this step we would like to keep only <code>dataset_v0.1.0<\/code> in the cache and remove <code>raw_data<\/code> from cache.<\/li>\n<li>We have another developer to work on the project, so he\/she uses git to clone the project and run <code>dvc checkout<\/code> or <code>dvc pull<\/code> to get the latest data version state. After the last command he\/she gets from remote both data: <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>.<\/li>\n<\/ul>\n<p>Is it possible to handle the cache in such manner that we keep only the latest version without using in-place preprocessing for a given commit ?<\/p>\n<p>Thank you<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535720101051,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"advic cach larg raw dataset store cloud initi git preprocess raw dataset dataset call latest version dataset cach remov raw data cach cach latest version preprocess commit",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/handle-cache-to-keep-the-latest-version-of-data\/87",
        "Challenge_link_count":0,
        "Challenge_original_content":"cach latest version data ruslan practic suppos big raw dataset store cloud initi git dataset trace dataset local run wget url download raw dataset commit preprocess raw dataset dataset run run preprocess raw dataset store dataset folder raw data dataset commit push git step dataset cach remov raw data cach git clone run checkout pull latest data version state remot data raw data dataset cach latest version preprocess commit",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"cach latest version data ruslan practic suppos big raw dataset store cloud initi dataset trace dataset local download raw dataset commit preprocess raw dataset dataset run preprocess raw dataset store dataset folder commit push git step cach remov cach git clone run latest data version state remot data cach latest version preprocess commit",
        "Challenge_readability":6.2,
        "Challenge_reading_time":18.0,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":17,
        "Challenge_solved_time":null,
        "Challenge_title":"Handle cache to keep the latest version of data",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":631.0,
        "Challenge_word_count":231,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":133527898949
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I created one Experiment and hosted as web service in Azure ML Stdio<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a><\/p>\n\n<p>However, I have installed Azure Machine Learning Workbench and logging into same account. It says:<\/p>\n\n<p><strong>No Experimentation Account found in your Azure Subscriptions\nYou can create one in the Microsoft Azure Management Portal.<\/strong><\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1535807224517,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"experiment account subscript log workbench creat host web servic studio prompt creat experiment account portal",
        "Challenge_last_edit_time":1535822103232,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52128396",
        "Challenge_link_count":2,
        "Challenge_original_content":"experiment account subscript workbench creat host web servic stdio http studio net instal workbench log account sai experiment account subscript creat portal",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"experiment account subscript workbench creat host web servic stdio instal workbench log account sai experiment account subscript creat portal",
        "Challenge_readability":11.7,
        "Challenge_reading_time":6.85,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":5,
        "Challenge_solved_time":null,
        "Challenge_title":"No Experimentation Account found in your Azure Subscriptions in Azure Machine Learning Workbench",
        "Challenge_topic":"Role Permission",
        "Challenge_topic_macro":"Security Management",
        "Challenge_view_count":113.0,
        "Challenge_word_count":62,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":133440775483
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<pre><code>def input_csv_fn():\n    #filenames = np.load(file_io.FileIO(npy_file, 'r'))\n    Dataset = tf.data.TextLineDataset(csv_file).skip(1).shuffle(buffer_size = 2000000).map(parser_csv, num_parallel_calls = cpu_count())\n    #Dataset = Dataset.prefetch(2560)\n    #Dataset = Dataset.shuffle(buffer_size = 1280)\n    Dataset = Dataset.map(input_parser_plain, num_parallel_calls = cpu_count())\n    Dataset = Dataset.apply(tf.contrib.data.ignore_errors())\n    Dataset = Dataset.repeat(epochs)\n    Dataset = Dataset.batch(batch_size)\n    Dataset = Dataset.prefetch(batch_size)\n    iterator = Dataset.make_one_shot_iterator()\n    feats, labs = iterator.get_next()\n    return feats, labs\ndef aggregate_csv_batches():\n    features = []\n    labels = []\n    # add if GPU exists condition here to fit GPU and CPU data processing\n    if num_gpus &gt; 0:\n        num_devices = num_gpus\n    else:\n        num_devices = 1\n    for i in range(num_devices):\n        _features, _labels = input_csv_fn()\n        features.append(_features)\n        labels.append(_labels)\n    return features, labels\nreturn aggregate_csv_batches \n<\/code><\/pre>\n\n<p>Above is the code for reading the dataset from S3 bucket via CSV, but when I am trying to do that I while I am creating the training job on AWS Sagemaker I am getting the following error constantly<\/p>\n\n<blockquote>\n  <p>TypeError: Failed to convert object of type <code>&lt;type 'function'&gt;<\/code> to\n  Tensor. Contents: function aggregate_csv_batches at 0x7f1559eeaaa0.\n  Consider casting elements to a supported type.<\/p>\n<\/blockquote>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536130429443,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"creat train job messag type convert function tensor function aggreg csv batch",
        "Challenge_last_edit_time":1536145696943,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52178872",
        "Challenge_link_count":0,
        "Challenge_original_content":"start train input csv filenam load file fileio npy file dataset data textlinedataset csv file skip shuffl buffer size map parser csv num parallel call cpu count dataset dataset prefetch dataset dataset shuffl buffer size dataset dataset map input parser plain num parallel call cpu count dataset dataset appli contrib data ignor dataset dataset repeat epoch dataset dataset batch batch size dataset dataset prefetch batch size iter dataset shot iter feat lab iter return feat lab aggreg csv batch featur label add gpu condit fit gpu cpu data process num gpu num devic num gpu num devic rang num devic featur label input csv featur append featur label append label return featur label return aggreg csv batch read dataset bucket csv creat train job constantli typeerror convert object type tensor function aggreg csv batch xfee cast element type",
        "Challenge_participation_count":1,
        "Challenge_preprocessed_content":"start train read dataset bucket csv creat train job constantli typeerror convert object type tensor function cast element type",
        "Challenge_readability":11.6,
        "Challenge_reading_time":19.38,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":1.0,
        "Challenge_sentence_count":16,
        "Challenge_solved_time":null,
        "Challenge_title":"Cannot start training on Amazon SageMaker",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":158.0,
        "Challenge_word_count":145,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":133117570557
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>I am trying to train a model with Amazon Sagemaker and I want serve it using with Tensorflow serving. To achieve that, I am downloading the model to a Tensorflow serving docker and I am trying to serve it from there.<\/p>\n<p>The Sagemaker's training and evaluating stages are completed without errors, but when I load my model to the Tensorflow serving server and try to invoke it I get Tensorflow serving errors that suggest that my model has no defined inputs. It can be seen that the Tensorflow serving server that the model is being served.<\/p>\n<p>For debugging purposes, I tried to serve it with Sagemaker but all I got was a vague error message saying I have an error invoking the endpoint.<\/p>\n<p>I think that the problem is that I am not defining well either the serving_input_fn or invoking it wrong or both. Can anyone help?<\/p>\n<h3>Tensorflow serving server invocation curl:<\/h3>\n<pre><code>curl -d '{&quot;instances&quot;: [{&quot;col3&quot;: 1.0}]}' -X POST http:\/\/localhost:8501\/v1\/models\/test_model:predict\n<\/code><\/pre>\n<h3>The error I receive from Tensorflow serving:<\/h3>\n<pre><code>{ &quot;error&quot;: &quot;Failed to process element: 0 key: col3 of \\'instances\\' list. Error: Invalid argument: JSON object: does not have named input: col3&quot; }%    \n<\/code><\/pre>\n<h3>Sagemaker's training python file:<\/h3>\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.ops import nn\n\n\nTRAIN_FILENAME = 'test.csv'\nTEST_FILENAME = 'train.csv'\n\nNODES_IN_LAYER = 6\nLAYERS_NUM = 10\nNUM_LINES_TO_SKIP = 1\n\nCSV_COLUMNS = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'label']\nRECORDS_DEFAULTS = [[0], [0], [0.0], [0.0], [0], [0.0], [0.0], [0], [0.0]]\n\nBATCH_SIZE = 32\n\nFEATURE_SPEC = {\n    'col3': tf.FixedLenFeature(dtype=tf.float32, shape=[]),\n}\n\n\ndef estimator_fn(run_config, params):\n    feature_columns = [\n        tf.feature_column.numeric_column('col3')]\n    return tf.estimator.DNNRegressor(feature_columns=feature_columns,\n                                     hidden_units=[NODES_IN_LAYER] * LAYERS_NUM,\n                                     activation_fn=nn.tanh,\n                                     config=run_config)\n\n\ndef serving_input_fn(params):\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(FEATURE_SPEC)\n\n\ndef train_input_fn(training_dir, params):\n    &quot;&quot;&quot;Returns input function that would feed the model during training&quot;&quot;&quot;\n    return _generate_input_fn(training_dir, TRAIN_FILENAME)\n\n\ndef eval_input_fn(training_dir, params):\n    &quot;&quot;&quot;Returns input function that would feed the model during evaluation&quot;&quot;&quot;\n    return _generate_input_fn(training_dir, TEST_FILENAME)\n\n\ndef parse_csv(line):\n    columns = tf.decode_csv(line, record_defaults=RECORDS_DEFAULTS)\n    line_features = dict(zip(CSV_COLUMNS, columns))\n    line_label = line_features.pop('label')\n    return {'col3': line_features.pop('col3')}, line_label\n\n\ndef _generate_input_fn(training_dir, training_filename):\n    filename = os.path.join(training_dir, training_filename)\n    dataset = tf.data.TextLineDataset(filename)\n    dataset = dataset.skip(NUM_LINES_TO_SKIP).map(parse_csv).batch(BATCH_SIZE)\n    return dataset\n<\/code><\/pre>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536251773583,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"serv model train tensorflow serv relat serv input defin properli invok messag receiv tensorflow serv model defin input train model tensorflow serv server invoc curl",
        "Challenge_last_edit_time":1592644375060,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52208668",
        "Challenge_link_count":1,
        "Challenge_original_content":"tensorflow serv argument json object input train model serv tensorflow serv achiev download model tensorflow serv docker serv train evalu stage complet load model tensorflow serv server invok tensorflow serv model defin input tensorflow serv server model serv debug purpos tri serv vagu messag sai invok endpoint defin serv input invok tensorflow serv server invoc curl curl instanc col http localhost model test model predict receiv tensorflow serv process element kei col instanc list argument json object input col train file import import tensorflow tensorflow op import train filenam test csv test filenam train csv node layer layer num num line skip csv column col col col col col col col col label record default batch size featur spec col fixedlenfeatur dtype shape estim run config param featur column featur column numer column col return estim dnnregressor featur column featur column hidden unit node layer layer num activ tanh config run config serv input param return estim export build raw serv input receiv featur spec train input train dir param return input function feed model train return gener input train dir train filenam eval input train dir param return input function feed model evalu return gener input train dir test filenam pars csv line column decod csv line record default record default line featur dict zip csv column column line label line featur pop label return col line featur pop col line label gener input train dir train filenam filenam path train dir train filenam dataset data textlinedataset filenam dataset dataset skip num line skip map pars csv batch batch size return dataset",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":"tensorflow serv argument json object input train model serv tensorflow serv achiev download model tensorflow serv docker serv train evalu stage complet load model tensorflow serv server invok tensorflow serv model defin input tensorflow serv server model serv debug purpos tri serv vagu messag sai invok endpoint defin invok tensorflow serv server invoc curl receiv tensorflow serv train file",
        "Challenge_readability":11.4,
        "Challenge_reading_time":40.53,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"tensorflow serving Error: Invalid argument: JSON object: does not have named input",
        "Challenge_topic":"TensorFlow Model",
        "Challenge_topic_macro":"Model Management",
        "Challenge_view_count":5954.0,
        "Challenge_word_count":327,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":132996226417
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":3,
        "Challenge_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use Mercurial for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versionioning software that DVC can work with? The only part that I\u2019ve spotted in the docs that really relies on Git is the <code>dvc init<\/code> command that creates <code>.dvc\/.gitignore<\/code>. Would adding contents of this file to hgignore (equivalent of gitignore in Mercurial) solve the problem? Are there other caveats that would prevent me from using Mercurial with DVC?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536316201269,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"organ team mercuri version mercuri git identifi init reli git creat gitignor file hgignor equival gitignor mercuri clarif caveat prevent mercuri",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-version-control-systems-other-than-git\/90",
        "Challenge_link_count":0,
        "Challenge_original_content":"version control system git team organ mercuri version read depend git git versionion softwar iv spot doc reli git init creat gitignor file hgignor equival gitignor mercuri caveat prevent mercuri",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"version control system git team organ mercuri version read depend git git versionion softwar iv spot doc reli git creat file hgignor caveat prevent mercuri",
        "Challenge_readability":7.6,
        "Challenge_reading_time":8.34,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":8,
        "Challenge_solved_time":null,
        "Challenge_title":"DVC and version control systems other than Git",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":733.0,
        "Challenge_word_count":113,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":132931798731
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I need to read a csv file, which is saved in my local computer, from code within an \"Execute R\/Python Script\" in an experiment of Azure Machine Learning Studio. I don't have to upload the data as usually, i.e. from Datasets -> New -> Load from local file or with an Import Data module. I must do it with code. In principle this is not possible, neither from an experiment nor from a notebook, and in fact I always got error. But I'm confused because the documentation about <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts\" rel=\"nofollow noreferrer\">Execute Python Script<\/a> module says (among other things):<\/p>\n\n<p><strong>Limitations<\/strong><\/p>\n\n<p>The Execute Python Script currently has the following limitations:<\/p>\n\n<p>Sandboxed execution. The Python runtime is currently sandboxed and, as a result, does not allow access to the network or to the local file system in a persistent manner. All files saved locally are isolated and deleted once the module finishes. <strong>The Python code cannot access most directories on the machine it runs on, the exception being the current directory and its subdirectories.<\/strong><\/p>\n\n<p>According to the highlighted text, it should be possible to access and load a file from current directory, using for instance the pandas function read_csv. But actually no. There is some trick to accomplish this?<\/p>\n\n<p>Thanks.<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":2,
        "Challenge_created_time":1536334608003,
        "Challenge_favorite_count":1.0,
        "Challenge_gpt_summary":"read csv file save local execut studio upload data document state access directori run directori subdirectori",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52225743",
        "Challenge_link_count":1,
        "Challenge_original_content":"read local file studio read csv file save local execut studio upload data dataset load local file import data modul principl notebook document execut modul sai limit execut limit sandbox execut runtim sandbox allow access network local file persist file save local isol delet modul finish access directori run except directori subdirectori accord highlight text access load file directori instanc panda function read csv trick accomplish",
        "Challenge_participation_count":3,
        "Challenge_preprocessed_content":"read local file studio read csv file save local execut studio upload data dataset load local file import data modul principl notebook document execut modul sai limit execut limit sandbox execut runtim sandbox allow access network local file persist file save local isol delet modul finish access directori run except directori subdirectori accord highlight text access load file directori instanc panda function trick accomplish",
        "Challenge_readability":9.5,
        "Challenge_reading_time":18.82,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":4.0,
        "Challenge_sentence_count":14,
        "Challenge_solved_time":null,
        "Challenge_title":"How can I read a local file from an R or Python script in Azure Machine Learning Studio?",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":894.0,
        "Challenge_word_count":223,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Azure Machine Learning",
        "Challenge_open_time":132913391997
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":1,
        "Challenge_body":"<p>I'm testing Amazon SageMaker service with NodeJS + AWS SDK and after create a new model and endpoint based on <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">this<\/a> example (everything works well in the notebook, including the request to the endpoint), I'm trying to create requests from my Express application, but I'm getting the following error:<\/p>\n\n<pre><code>Error during recognition: { InvalidImageFormatException: Request has Invalid image format\n  at Request.extractError (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/protocol\/json.js:48:27)\n  at Request.callListeners (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/sequential_executor.js:109:20)\n  at Request.emit (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/sequential_executor.js:81:10)\n  at Request.emit (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:683:14)\n  at Request.transition (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:22:10)\n  at AcceptorStateMachine.runTo (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/state_machine.js:14:12)\n  at \/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/state_machine.js:26:10\n  at Request.&lt;anonymous&gt; (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:38:9)\n  at Request.&lt;anonymous&gt; (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:685:12)\n  at Request.callListeners (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/sequential_executor.js:119:18)\n  at Request.emit (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/sequential_executor.js:81:10)\n  at Request.emit (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:683:14)\n  at Request.transition (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:22:10)\n  at AcceptorStateMachine.runTo (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/state_machine.js:14:12)\n  at \/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/state_machine.js:26:10\n  at Request.&lt;anonymous&gt; (\/Users\/pdonaire\/Documents\/workspaceNode\/trsps-controller\/node_modules\/aws-sdk\/lib\/request.js:38:9)\n\nmessage: 'Request has Invalid image format',\ncode: 'InvalidImageFormatException',\ntime: 2018-09-10T04:42:07.530Z,\nrequestId: 'de3a04ff-b4b3-11e8-9bd8-8b88f803570c',\nstatusCode: 400,\nretryable: false,\nretryDelay: 55.860720412209794 }\n<\/code><\/pre>\n\n<p>My code is as follows:<\/p>\n\n<pre><code>export function sendRequestToSageMaker(base64image) {\n  const params = {\n    Body: new Buffer(base64image, 'base64') , \/* Strings will be Base-64 encoded on your behalf *\/ \/* required *\/\n    EndpointName: 'DEMO-imageclassification-ep--XXXX', \/* required *\/\n    Accept: 'application\/json',\n    ContentType: 'application\/x-image'\n  };\n  sagemakerruntime.invokeEndpoint(params, function(err, data) {\n    if (err) \n      console.error(err, err.stack); \/\/ an error occurred\n    else     \n      console.log(data);           \/\/ successful response\n   });\n   return null;\n}\n<\/code><\/pre>\n\n<p><code>base64image<\/code> is <code>req.body.photo<\/code> from a request that I'm doing with Postman with a JSON and just a single <code>photo<\/code> property with a base64 string that I've made with <a href=\"https:\/\/www.base64-image.de\/\" rel=\"nofollow noreferrer\">base64-image.de<\/a> website.<\/p>\n\n<p>Any help will be helpful! Thank you so much! :-)<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":1,
        "Challenge_created_time":1536555463237,
        "Challenge_favorite_count":0.0,
        "Challenge_gpt_summary":"test servic nodej sdk messag request imag format creat request express persist function base imag send base imag request postman",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/stackoverflow.com\/questions\/52251328",
        "Challenge_link_count":2,
        "Challenge_original_content":"request imag format test servic nodej sdk creat model endpoint base notebook request endpoint creat request express recognit invalidimageformatexcept request imag format request extracterror pdonair document workspacenod trsp control node modul sdk lib protocol json request caisten pdonair document workspacenod trsp control node modul sdk lib sequenti executor request emit pdonair document workspacenod trsp control node modul sdk lib sequenti executor request emit pdonair document workspacenod trsp control node modul sdk lib request request transit pdonair document workspacenod trsp control node modul sdk lib request acceptorstatemachin runto pdonair document workspacenod trsp control node modul sdk lib state pdonair document workspacenod trsp control node modul sdk lib state request pdonair document workspacenod trsp control node modul sdk lib request request pdonair document workspacenod trsp control node modul sdk lib request request caisten pdonair document workspacenod trsp control node modul sdk lib sequenti executor request emit pdonair document workspacenod trsp control node modul sdk lib sequenti executor request emit pdonair document workspacenod trsp control node modul sdk lib request request transit pdonair document workspacenod trsp control node modul sdk lib request acceptorstatemachin runto pdonair document workspacenod trsp control node modul sdk lib state pdonair document workspacenod trsp control node modul sdk lib state request pdonair document workspacenod trsp control node modul sdk lib request messag request imag format invalidimageformatexcept time requestid deaff bfc statuscod retryabl retrydelai export function sendrequestto baseimag const param bodi buffer baseimag base base encod behalf endpointnam imageclassif accept json contenttyp imag runtim invokeendpoint param function err data err consol err err stack consol log data respons return null baseimag req bodi photo request postman json singl photo properti base base imag websit",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"request imag format test servic nodej sdk creat model endpoint base creat request express request postman json singl properti base websit",
        "Challenge_readability":31.2,
        "Challenge_reading_time":53.02,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":2.0,
        "Challenge_sentence_count":28,
        "Challenge_solved_time":null,
        "Challenge_title":"AWS SageMaker - Request has Invalid image format",
        "Challenge_topic":"JSON Payload",
        "Challenge_topic_macro":"Service Management",
        "Challenge_view_count":739.0,
        "Challenge_word_count":221,
        "Platform":"Stack Overflow",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"Amazon SageMaker",
        "Challenge_open_time":132692536763
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":4,
        "Challenge_body":"<p>What are the main differences between DVC and DataLad\/git-annex ( <a href=\"https:\/\/www.datalad.org\/\" rel=\"nofollow noopener\">https:\/\/www.datalad.org\/<\/a> , <a href=\"https:\/\/git-annex.branchable.com\/\" rel=\"nofollow noopener\">https:\/\/git-annex.branchable.com\/<\/a> )?  What would be reasons to use one or the other?<\/p>",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536601415671,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"datalad git annex reason",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/discuss.dvc.org\/t\/comparison-to-datalad-and-git-annex\/92",
        "Challenge_link_count":4,
        "Challenge_original_content":"comparison datalad git annex datalad git annex http datalad org http git annex branchabl com reason",
        "Challenge_participation_count":4,
        "Challenge_preprocessed_content":null,
        "Challenge_readability":17.1,
        "Challenge_reading_time":4.72,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":7.0,
        "Challenge_sentence_count":6,
        "Challenge_solved_time":null,
        "Challenge_title":"Comparison to DataLad and git-annex",
        "Challenge_topic":"Remote Persistence",
        "Challenge_topic_macro":"Data Management",
        "Challenge_view_count":1861.0,
        "Challenge_word_count":31,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"DVC",
        "Challenge_open_time":132646584329
    },
    {
        "Challenge_adjusted_solved_time":null,
        "Challenge_answer_count":2,
        "Challenge_body":"MLflow 0.6.0 has been released: https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v0.6.0\n\n\nMLflow 0.6.0 introduces several major features:\n\n\n- A Java client API (to be published on Maven within the next day or two)\n- Support for saving and serving SparkML models as MLeap for low-latency serving\n- Support for tagging runs with metadata, during and after the run completion\n- Support for deleting (and restoring deleted) experiments\n\n\nIn addition to these features, there are a host of improvements and bugfixes to the REST API, Python API, tracking UI, and documentation.",
        "Challenge_closed_time":null,
        "Challenge_comment_count":0,
        "Challenge_created_time":1536613157000,
        "Challenge_favorite_count":null,
        "Challenge_gpt_summary":"introduct featur java client api save serv sparkml model mleap tag run metadata delet improv bugfix rest api api track document",
        "Challenge_last_edit_time":null,
        "Challenge_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/kJz7T4052RM",
        "Challenge_link_count":1,
        "Challenge_original_content":"releas releas http github com releas tag introduc featur java client api publish maven dai save serv sparkml model mleap low latenc serv tag run metadata run complet delet restor delet addit featur host improv bugfix rest api api track document",
        "Challenge_participation_count":2,
        "Challenge_preprocessed_content":"releas releas introduc featur java client api save serv sparkml model mleap serv tag run metadata run complet delet addit featur host improv bugfix rest api api track document",
        "Challenge_readability":14.1,
        "Challenge_reading_time":7.33,
        "Challenge_repo_contributor_count":null,
        "Challenge_repo_fork_count":null,
        "Challenge_repo_issue_count":null,
        "Challenge_repo_star_count":null,
        "Challenge_repo_watch_count":null,
        "Challenge_score":null,
        "Challenge_sentence_count":3,
        "Challenge_solved_time":null,
        "Challenge_title":"MLflow 0.6.0 released!",
        "Challenge_topic":"Spark Configuration",
        "Challenge_topic_macro":"Computation Management",
        "Challenge_view_count":29.0,
        "Challenge_word_count":83,
        "Platform":"Tool-specific",
        "Solution_body":null,
        "Solution_comment_count":null,
        "Solution_gpt_summary":null,
        "Solution_last_edit_time":null,
        "Solution_link_count":null,
        "Solution_original_content":null,
        "Solution_preprocessed_content":null,
        "Solution_readability":null,
        "Solution_reading_time":null,
        "Solution_score":null,
        "Solution_sentence_count":null,
        "Solution_topic":null,
        "Solution_topic_macro":null,
        "Solution_word_count":null,
        "Tool":"MLflow",
        "Challenge_open_time":132634843000
    }
]