{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import openai\n",
    "import textstat\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../Dataset'\n",
    "path_result = '../../Result'\n",
    "\n",
    "path_rq12 = os.path.join(path_result, 'RQ12')\n",
    "path_rq3 = os.path.join(path_result, 'RQ3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_resolution_summary(link):\n",
    "    webbrowser.open(link)\n",
    "    user_input = input(\"Please input a summary for the opened link: \")\n",
    "    return user_input\n",
    "\n",
    "def find_duplicates(in_list):  \n",
    "    duplicates = []\n",
    "    unique = set(in_list)\n",
    "    for each in unique:\n",
    "        count = in_list.count(each)\n",
    "        if count > 1:\n",
    "            duplicates.append(each)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create parallel coordinates plot\n",
      "compare using artifacts tab\n",
      "Use software for inference\n",
      "install previous software version\n",
      "install and import software\n",
      "Use get sweep URL function\n",
      "Use learning rate monitor\n",
      "use controller for debugging\n",
      "Use run comparer table\n",
      "Specify new run identifiers\n",
      "implement separate model tuning\n",
      "use latest versioned tag\n",
      "reinstall machine learning extension\n",
      "Move to Python SDK\n",
      "Modify deep copy process\n",
      "Plan MLflow 2.0 support\n",
      "bundle static project data\n",
      "implement lazy run instance\n",
      "stop using comet logger\n",
      "adjust row log interval\n",
      "increase row log interval\n",
      "Install old Optuna version\n",
      "Use older software version\n",
      "create new docker image\n",
      "use earlier transformers version\n",
      "use get local copy\n",
      "Use command line interface\n",
      "added search runs iterator\n",
      "use updated pull request\n",
      "set job existence check false\n",
      "Assessing multiple model support\n",
      "install lower software version\n",
      "Install compatible JupyterLab version\n",
      "Use sharded data distribution\n",
      "Use conda python3 kernel\n",
      "update software development kit\n",
      "specify task time limit\n",
      "use older software version\n",
      "Convert records into rows\n",
      "convert dictionary to list\n",
      "Recommend v2 SDK learning\n",
      "Force Unix line endings\n",
      "Use S3 for Spark-based processing\n",
      "use tracker close function\n",
      "provide empty model data\n",
      "use binary logistic workaround\n",
      "close due to low model utilization\n",
      "try fully replicated distribution\n",
      "implement global sample step\n",
      "Use Kedro with Notebooks\n",
      "pass enable web access\n",
      "include events in namespace\n",
      "implement model logging system\n",
      "Run prep librispeech data\n",
      "use older software version\n",
      "fix relative path bug\n",
      "change online store type\n",
      "Disable functionalities or obtain key\n",
      "enable multi label parameter\n",
      "Switch to HuggingFace Estimator\n",
      "Suggest using maintained optimizer libraries\n",
      "Set report logging off\n",
      "set parameters to none\n",
      "Switch to HTTP remote\n",
      "Create compatibility pull request\n",
      "use previous software version\n",
      "implement custom load method\n",
      "Apply for API key\n",
      "Identify path conversion issue\n",
      "install specific software version\n",
      "implement alternative model retrieval\n",
      "set max iteration constraint\n",
      "implement set output function\n",
      "compare machine learning interfaces\n",
      "Improve exit code usage\n",
      "Install correct SDK version\n",
      "adjust watcher exclude settings\n",
      "inform user connection steps\n",
      "uninstall remote removes main\n",
      "supports default branch triggers\n",
      "use search experiments method\n",
      "refuse incomplete pod start\n",
      "extract using container property\n",
      "implement git hash solution\n",
      "remove rarely used token\n",
      "Use compatible software versions\n",
      "clean database before startup\n",
      "fix log plot error\n",
      "Move model to CPU\n",
      "propose clear user notification\n",
      "include configuration in tuner\n",
      "Add sleep decrease checkpoints\n",
      "stop instances before deletion\n",
      "try different software version\n",
      "Avoid ending unstarted run\n",
      "use tables in builders\n",
      "install specific software version\n",
      "shorten training job names\n",
      "store plots in cache\n",
      "recommend anomaly detection resources\n",
      "Use folder URI dataset\n",
      "Use training data property\n",
      "Establish SQL Database connection\n",
      "Enable ONNX compatible models\n",
      "check file share name\n",
      "store secrets in vault\n",
      "use azure for big data\n",
      "use edit metadata module\n",
      "use qualified image name\n",
      "forward feedback to team\n",
      "Convert Excel to CSV\n",
      "Choose AUC Weighted metric\n",
      "use machine learning activity\n",
      "develop deep learning model\n",
      "Deploy using Kubernetes service\n",
      "Disallow creation new workspaces\n",
      "store models in container\n",
      "convert notebook to script\n",
      "consume model using client\n",
      "Explaining k-means pros and cons\n",
      "explain bounding box coordinates\n",
      "migrate to new studio\n",
      "use set property method\n",
      "deploy using different ports\n",
      "charge for compute use\n",
      "Use Azure command line\n",
      "forward feedback to team\n",
      "post on certification forum\n",
      "Implement YOLOv5 in Azure\n",
      "Enable long term metrics\n",
      "use image retrieval method\n",
      "Deploy model on SageMaker\n",
      "Create model, start job\n",
      "ensure unique task names\n",
      "Use AWS signature authentication\n",
      "transmit image via lambda\n",
      "create new endpoint configuration\n",
      "implement tracking scaling policy\n",
      "choose appropriate image and instance\n",
      "use universal inference script\n",
      "specify network access type\n",
      "enable service catalog access\n",
      "Save locally upload via API\n",
      "use launch terminal button\n",
      "use fast launch instances\n",
      "Use direct storage access\n",
      "install specific software version\n",
      "Check docker build files\n",
      "test locally using docker\n",
      "Use machine learning service\n",
      "Prefer FSX over pipemode\n",
      "Convert JSON to CSV\n",
      "Use API management service\n",
      "implement full outer join\n",
      "use batch execution service\n",
      "use batch execution service\n",
      "bring data onto servers\n",
      "use get blob function\n",
      "identify service instances impossible\n",
      "change datatype to float\n",
      "use permutation feature importance\n",
      "Delete and upload dataset\n",
      "implement web service parameters\n",
      "use import images module\n",
      "use r script module\n",
      "use permutation feature importance\n",
      "use azure data factory\n",
      "use scripting to remove rows\n",
      "Use multi-column data frame\n",
      "use edit metadata module\n",
      "use web service parameters\n",
      "save images using azure\n",
      "Export plot to PDF\n",
      "Rebuild model using libraries\n",
      "merge rows outside loop\n",
      "Train model with active and inactive members\n",
      "use older package version\n",
      "replace semicolons with tabs\n",
      "use command line compression\n",
      "use log accuracy method\n",
      "use anomaly detection module\n",
      "use flag for files\n",
      "write serving input function\n",
      "Install updated software development kit.\n",
      "add symbol before date\n",
      "Convert DataFrame to bytes\n",
      "preserve relative folder structure\n",
      "use sparse tensor function\n",
      "Use bounding box template\n",
      "implement custom input function\n",
      "use source directory parameter\n",
      "use serving input function\n",
      "create permanent ssh key\n",
      "use batch transform jobs\n",
      "implement wait for creation\n",
      "use specific docker image\n",
      "deploy updated software version\n",
      "Use AWS Secrets Manager\n",
      "Use size filtering function\n",
      "use import data module\n",
      "use code location argument\n",
      "Use data importing code\n",
      "use best training job\n",
      "Use alternative software development kit\n",
      "Save upload through API\n",
      "specify initialization during prediction\n",
      "Set CSV data format\n",
      "Charge for data calls\n",
      "set user managed parameter\n",
      "Download files using code\n",
      "close and reinitialize tasks\n",
      "specify private image registry\n",
      "Use s3 input objects\n",
      "Unprotect file, delete files, remove cache\n",
      "Update software development kit\n",
      "reopen code in script folder\n",
      "use set pip option method\n",
      "use start run function\n",
      "Use Ubuntu virtual machine\n",
      "Convert dataframe to array\n",
      "Convert TSV to CSV\n",
      "use write bytes mode\n",
      "install specific pandas version\n",
      "use mount point attribute\n",
      "cache data in storage\n",
      "Switch higher powered system\n",
      "create new endpoint configuration\n",
      "use get metric history\n",
      "Configure S3 output path\n",
      "\n",
      "choose platform based on data size\n",
      "Use vision AI model\n",
      "Run interface in directory\n",
      "Convert dataframe to CSV\n",
      "Link notebook to GitHub\n",
      "use get context method\n",
      "specify correct tenant id\n",
      "set environment before import\n",
      "upgrade software development kit\n",
      "use reduce max function\n",
      "enable continue on failure\n",
      "utilize single node effectively\n",
      "Use file or SQL for backend store\n",
      "modify split parameters function\n",
      "Use data transfer step\n",
      "execute entry point script\n",
      "add storage contributor role\n",
      "create deploy machine learning model\n",
      "use update endpoint parameter\n",
      "Use lowercase attribute names\n",
      "pass parameters to script\n",
      "Create matching region instance\n",
      "use older software version\n",
      "Use Step Functions SDK\n",
      "Load and deploy model\n",
      "Try Google Cloud TPUs\n",
      "use load model function\n",
      "Upload manually to S3\n",
      "remove unsupported data types\n",
      "Use internal load balancer\n",
      "use log text function\n",
      "Use SageMaker Python SDK\n",
      "pass interrupted run id\n",
      "Use special URL format\n",
      "Install rclone in WSL\n",
      "use local deployment configuration\n",
      "Use conda for R\n",
      "charge for running models\n",
      "use log artifact function\n",
      "create shared key object\n",
      "use download artifacts function\n",
      "use cache false command\n",
      "add time series column\n",
      "create new service account\n",
      "use load model function\n",
      "Use register pandas dataframe\n",
      "add content type support\n",
      "use older framework version\n",
      "create session with bucket\n",
      "upload file to blob\n",
      "Enable Cloud Build API\n",
      "use set experiment tag\n",
      "use create children method\n",
      "implement custom server logic\n",
      "implement before pipeline hook\n",
      "use update start method\n",
      "use session load context\n",
      "Implement custom SageMaker training\n",
      "use return all scores parameter\n",
      "use get model path\n",
      "Test virtual machine connection\n",
      "convert number to string\n",
      "support lower python versions\n",
      "create single training job\n",
      "use load model function\n",
      "use custom inference script\n",
      "Add instance IP to whitelist\n",
      "implement custom search space\n",
      "stream custom job logs\n",
      "use log figure function\n",
      "Upload notebook to SageMaker\n",
      "use start pipeline execution\n",
      "use user managed notebooks\n",
      "Install both Graphviz systems\n",
      "stage and commit files\n",
      "use docker build tool\n",
      "Use undeploy all method\n",
      "disable public network access\n",
      "Create specific MLTable file\n",
      "switch to service account\n",
      "Use internal gRPC connections\n",
      "create local virtual environment\n",
      "bake packages into container\n",
      "use relative model path\n",
      "Neo lacks quantization support\n",
      "Return path with object\n",
      "pass files during initialization\n",
      "use escape hatch syntax\n",
      "add security group rules\n",
      "create token manager class\n",
      "use load from disk function\n",
      "use compatible base image\n",
      "use command line interface\n",
      "Create metadata enforcing plugin\n",
      "switch off preview mode\n",
      "use specific software version\n",
      "use output serve kernel\n",
      "use ssh helper library\n",
      "use central google deployment\n",
      "use different feature sets\n",
      "Use mathematical null values\n",
      "Use Dataset register function\n",
      "Deploy real time endpoint\n",
      "Create valid API key\n",
      "Determine specific code size\n",
      "Propose container packing workaround\n",
      "Use Document AI processing\n",
      "lacks spot instance support\n",
      "import and export model\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if len(row['Resolution_summary'].split()) > 3:\n",
    "        print(row['Resolution_summary'])\n",
    "# df.to_json(os.path.join(path_result_rq3, 'labels_closed.json'), indent=4, orient='records') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click \"Custom Charts\"\n",
      "Plan MLflow 2.0 support\n",
      "access learner's loss\n",
      "Use \"Boolean\" type\n",
      "Implement Shantnu's code\n",
      "Add \"params\" field\n",
      "Use R/python within\n",
      "Install updated software development kit.\n",
      "Use aws.s3 package\n",
      "Use plt.savefig function\n",
      "Set verbose_eval None\n",
      "\"Create DVC pipeline\"\n",
      "Install updated charset_normalizer\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "\n",
    "punctuation_marks = ['.', '\"', '`', '_', '(', ')', '\\'', '/']\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for mark in punctuation_marks:\n",
    "        if mark in row['Resolution_summary']:\n",
    "            print(row['Resolution_summary'])\n",
    "            break\n",
    "# df.to_json(os.path.join(path_result_rq3, 'labels_closed.json'), indent=4, orient='records') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_new = 'preprocessed'\n",
    "# file_old = 'labels'\n",
    "\n",
    "# df_new = pd.read_json(os.path.join(path_dataset, f'{file_new}.json'))\n",
    "# df_new = df_new[df_new['Challenge_resolved_time'].notna()]\n",
    "# df_old = pd.read_json(os.path.join(path_rq3, f'{file_old}.json'))\n",
    "\n",
    "# df_git = df_old[df_old['Platform'].str.contains('Git')]\n",
    "# df_stack = df_old[df_old['Platform'].str.contains('Stack')]\n",
    "# df_tool = df_old[df_old['Platform'].str.contains('Tool')]\n",
    "\n",
    "# for index, row in df_new.iterrows():\n",
    "#     if 'Git' in row['Platform']:\n",
    "#         for i2, r2 in df_git.iterrows():\n",
    "#             if row['Challenge_link'] == r2['Challenge_link']:\n",
    "#                 df_new.at[index, 'Resolution_summary'] = r2['Resolution_summary']\n",
    "#                 break\n",
    "#     elif 'Stack' in row['Platform']:\n",
    "#         for i2, r2 in df_stack.iterrows():\n",
    "#             if row['Challenge_link'] == r2['Challenge_link']:\n",
    "#                 df_new.at[index, 'Resolution_summary'] = r2['Resolution_summary']\n",
    "#                 break\n",
    "#     else:\n",
    "#         for i2, r2 in df_tool.iterrows():\n",
    "#             if row['Challenge_link'] == r2['Challenge_link']:\n",
    "#                 df_new.at[index, 'Resolution_summary'] = r2['Resolution_summary']\n",
    "#                 break\n",
    "\n",
    "# df_new.to_json(os.path.join(path_rq3, f'{file_old}+.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "df = df[df['Challenge_solved_time'].notna()]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Resolution_summary'] = input_resolution_summary(row['Challenge_link'])\n",
    "    if index % 50 == 49:\n",
    "        df.to_json(os.path.join(path_rq3, 'labels.json'), indent=4, orient='records')\n",
    "\n",
    "df.to_json(os.path.join(path_rq3, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_rq12, 'macro-topics.json'))\n",
    "# df = df[df['Challenge_solved_time'].notna()]\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if 'Issue' not in row['Platform']:\n",
    "#         continue\n",
    "#     df.at[index, 'Resolution_summary'] = input_resolution_summary(row['Challenge_link'])\n",
    "#     if index % 50 == 49:\n",
    "#         df.to_json(os.path.join(path_rq3, 'labels.json'), indent=4, orient='records')\n",
    "\n",
    "# df.to_json(os.path.join(path_rq3, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if 'Issue' in row['Platform']:\n",
    "#         continue\n",
    "#     if row['Resolution_summary'] == 'na':\n",
    "#         print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_dataset = '../../Dataset'\n",
    "# df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "# df = df[df['Challenge_solved_time'].notna()]\n",
    "# df.to_json(os.path.join(path_rq3, 'labels_new.json'), indent=4, orient='records')\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "# df_new = pd.read_json(os.path.join(path_rq3, 'labels_new.json'))\n",
    "\n",
    "# # df_all = pd.concat([df, df_new], ignore_index=True)\n",
    "# # df_diff = df_all.drop_duplicates(subset=['Challenge_link'], keep=False)\n",
    "# df_diff = pd.concat([df, df_new, df_new]).drop_duplicates(subset=['Challenge_link'], keep=False)\n",
    "# # df_diff = pd.concat([df_new, df, df]).drop_duplicates(subset=['Challenge_link'], keep=False)\n",
    "# df_diff['Challenge_link'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_dataset = '../../Dataset'\n",
    "# df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "# df = df[df['Challenge_solved_time'].notna()]\n",
    "# df['Resolution_summary'] = 'na'\n",
    "# df.to_json(os.path.join(path_rq3, 'labels.json'), orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_old = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "# df_old.sort_values(by=['Challenge_link'], inplace=True)\n",
    "# df_old.to_json(os.path.join(path_rq3, 'labels.json'), orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_rq3, 'labels_closed.json'))\n",
    "\n",
    "# # df_difference = pd.concat([df_old, df, df]).drop_duplicates('Challenge_link', keep=False, ignore_index=True)\n",
    "# df_difference = pd.concat([df, df_old, df_old]).drop_duplicates('Challenge_link', keep=False, ignore_index=True)\n",
    "\n",
    "# df_all = pd.concat([df_old, df], ignore_index=True)\n",
    "# df_duplicate = df_all[df_all.duplicated(['Challenge_link'], keep='last')]\n",
    "\n",
    "# df_new = pd.concat([df_difference, df_duplicate], ignore_index=True)\n",
    "# df_new.to_json(os.path.join(path_rq3, 'labels+.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "\n",
    "# regex_digit = r\"[0-9]\"\n",
    "\n",
    "# regex_error = r\"[a-zA-Z0-9]+[eE]rror[^a-zA-Z]\"\n",
    "# regex_exception = r\"[a-zA-Z0-9]+[eE]xception[^a-zA-Z]\"\n",
    "\n",
    "# regex_error_leading = r\"[a-zA-Z0-9]+[eE]rror[a-zA-Z]+\"\n",
    "# regex_exception_leading = r\"[a-zA-Z0-9]+[eE]xception[a-zA-Z]+\"\n",
    "\n",
    "# false_positive_list = []\n",
    "\n",
    "# def camel_case_split(str):\n",
    "#     words = [[str[0].lower()]]\n",
    " \n",
    "#     for c in str[1:]:\n",
    "#         if (words[-1][-1].islower() or words[-1][-1].isdigit()) and c.isupper():\n",
    "#             words.append(list(c.lower()))\n",
    "#         else:\n",
    "#             words[-1].append(c)\n",
    "#     return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     challenge = row['Challenge_title'] + ' ' + row['Challenge_body']\n",
    "#     challenge = challenge.replace('\\n', ' ')\n",
    "#     error_list = re.findall(regex_error, challenge)\n",
    "#     if len(error_list):\n",
    "#         if row['Challenge_type'] != 'problem':\n",
    "#             df.at[index, 'Challenge_type'] = 'problem'\n",
    "#             false_positive_list.append(row['Challenge_link'])\n",
    "#         error = max(error_list, key = len)\n",
    "#         if len(re.findall(regex_digit, error)):\n",
    "#             print(row['Challenge_title'])\n",
    "#         else:\n",
    "#             error = re.sub(r'error.+', 'error', camel_case_split(error))\n",
    "#             df.at[index, 'Challenge_summary'] = error\n",
    "#     else:\n",
    "#         exception_list = re.findall(regex_exception, challenge)\n",
    "#         if len(exception_list):\n",
    "#             if row['Challenge_type'] != 'problem':\n",
    "#                 df.at[index, 'Challenge_type'] = 'problem'\n",
    "#                 false_positive_list.append(row['Challenge_link'])\n",
    "#             exception = max(exception_list, key = len)\n",
    "#             if len(re.findall(regex_digit, exception)):\n",
    "#                 print(row['Challenge_title'])\n",
    "#             else:\n",
    "#                 exception = re.sub(r'exception.+', 'exception', camel_case_split(exception))\n",
    "#                 df.at[index, 'Challenge_summary'] = exception\n",
    "#         else:\n",
    "#             error_list_leading = re.findall(regex_error_leading, challenge)\n",
    "#             if len(error_list_leading):\n",
    "#                 print(row['Challenge_title'])\n",
    "#             else:\n",
    "#                 exception_list_leading = re.findall(regex_exception_leading, challenge)\n",
    "#                 if len(exception_list_leading):\n",
    "#                     print(row['Challenge_title'])\n",
    "                    \n",
    "# df.to_json(os.path.join(path_rq3, 'anomaly.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_rq3, 'labels.json'))\n",
    "\n",
    "# regex_digit = r\"[0-9]\"\n",
    "\n",
    "# regex_error = r\"[a-zA-Z0-9]+[eE]rror[^a-zA-Z]\"\n",
    "# regex_exception = r\"[a-zA-Z0-9]+[eE]xception[^a-zA-Z]\"\n",
    "\n",
    "# regex_error_leading = r\"[a-zA-Z0-9]+[eE]rror[a-zA-Z]+\"\n",
    "# regex_exception_leading = r\"[a-zA-Z0-9]+[eE]xception[a-zA-Z]+\"\n",
    "\n",
    "# def camel_case_split(str):\n",
    "#     words = [[str[0].lower()]]\n",
    " \n",
    "#     for c in str[1:]:\n",
    "#         if (words[-1][-1].islower() or words[-1][-1].isdigit()) and c.isupper():\n",
    "#             words.append(list(c.lower()))\n",
    "#         else:\n",
    "#             words[-1].append(c)\n",
    "#     return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "# titles = []\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Challenge_title'] in titles:\n",
    "#         continue\n",
    "#     challenge = row['Challenge_title'] + ' ' + row['Challenge_body']\n",
    "#     challenge = challenge.replace('\\n', ' ').lower()\n",
    "#     if (' 403 ' in challenge) or ('[403]' in challenge) or ('(403)' in challenge) or (' 403,' in challenge) or ('forbidden' in challenge):\n",
    "#         pass\n",
    "#         # print(row['Challenge_title'])\n",
    "#         # df.at[index, 'Challenge_type'] = 'problem'\n",
    "#         # df.at[index, 'Challenge_summary'] = 'forbidden error'\n",
    "#     elif (' 404 ' in challenge) or ('[404]' in challenge) or ('(404)' in challenge) or (' 404,' in challenge) or ('not found' in challenge):\n",
    "#         print(row['Challenge_title'])\n",
    "        \n",
    "# # df.to_json(os.path.join(path_rq3, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a set of topics refering to specific empirical software engineering resolution. Please summarize each topic in a phrase and attach one sentence description in the MLOps context. Also, you must guarantee that those phrases are not duplicate with one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_rq3, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Resolution {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_entries = [topic for topic in topics.split('Topic ') if topic]\n",
    "\n",
    "topic_list = []\n",
    "topic_mapping = {}\n",
    "\n",
    "for index, topic_entry in enumerate(topic_entries):\n",
    "    topic_name, topic_info = topic_entry.split(' - ')\n",
    "    topic_name = topic_name.split(': ')[-1]\n",
    "    topic_description, topic_description_mlops = topic_info.split('MLOps Context: ')\n",
    "    topic = {\n",
    "        'Index': index + 1,\n",
    "        'Topic': topic_name,\n",
    "        'Description': topic_description.strip(),\n",
    "        # 'Description (MLOps)': topic_description_mlops.strip(),\n",
    "    }\n",
    "    topic_list.append(topic)\n",
    "    topic_mapping[index] = topic_name\n",
    "    \n",
    "topic_df = pd.DataFrame(topic_list)\n",
    "print(topic_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_topic2index_list = [\n",
    "    ('Code Development', []),\n",
    "    # ('Code Management', []),\n",
    "    ('Cost Management', []),\n",
    "    ('Compute Management', []),\n",
    "    ('Data Development', []),\n",
    "    ('Data Management', []),\n",
    "    ('Environment Management', []),\n",
    "    ('Experiment Management', []),\n",
    "    ('File Management', []),\n",
    "    ('Model Development', []),\n",
    "    ('Model Management', []),\n",
    "    ('Model Serving', []),\n",
    "    ('Network Management', []),\n",
    "    ('Observability Management', []),\n",
    "    ('Pipeline Management', []),\n",
    "    ('Quality Assurance Management', []),\n",
    "    ('Security Management', []),\n",
    "    # ('User Interface Management', []),\n",
    "]\n",
    "\n",
    "topic_list = []\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_indexing = {}\n",
    "macro_topic2index_dict = {}\n",
    "for index, topic_set in enumerate(macro_topic2index_list):\n",
    "    macro_topic2index_dict[topic_set[0]] = topic_set[1]\n",
    "    macro_topic_indexing[index] = topic_set[0]\n",
    "    topic_list.extend(topic_set[1])\n",
    "    for topic in topic_set[1]:\n",
    "        macro_topic_mapping[topic] = index\n",
    "\n",
    "print(find_duplicates(topic_list))\n",
    "print(len(topic_df) == len(topic_list))\n",
    "print(set(range(len(topic_list))).difference(set(range(topic_df.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # topic_list = [topic for topic in solution_topics.split('\\n') if topic]\n",
    "# solution_macro_topic_mapping_inverse = {\n",
    "#     \"-1: No Solution\": [-1],\n",
    "#     \"1: Artifact Management\": [1,15,36,42,44,48,49],\n",
    "#     \"2: Dependency and Environment Configuration\": [2,3,6,7,8,11,12,17,18,20,22,32,34,38,39,45,47],\n",
    "#     \"4: Deployment and Lifecycle Management\": [10,16,24,41],\n",
    "#     \"5: Maintenance and Support Management\": [5,26,29,30,43],\n",
    "#     \"6: Recommandation and Best Practices\": [9,21,23,25,35],\n",
    "#     \"7: Network and Access Control\": [4,14,27,31,33],\n",
    "#     \"8: Observability Management\": [0,28],\n",
    "#     \"10: Compute and Resource Management\": [13,37,40],\n",
    "#     \"11: Script Handling\": [19,46],\n",
    "#     # \"9: Experiment Management\": [],\n",
    "#     # \"12: Function Usage\": [],#???\n",
    "#     # \"13: Algorithm Improvement\": [],\n",
    "#     # \"14: Difference Comparison\": [],#?\n",
    "#     # \"15: Account Management\": [],\n",
    "#     # \"16: Details Request\": [54],#?\n",
    "#     # \"17: Exception handling\": [],\n",
    "#     # \"Identifier Management\": [],\n",
    "# }\n",
    "\n",
    "# solution_topic_indexing = {}\n",
    "# solution_macro_topic_list = []\n",
    "# solution_macro_topic_mapping = {}\n",
    "# solution_macro_topic_indexing = {}\n",
    "# for macro_topic, sub_topics in solution_macro_topic_mapping_inverse.items():\n",
    "#     index, name = int(macro_topic.split(': ')[0]), macro_topic.split(': ')[1]\n",
    "#     solution_macro_topic_indexing[index] = name\n",
    "#     solution_macro_topic_list.extend(sub_topics)\n",
    "#     # macro_topic_list = []\n",
    "#     for topic in sub_topics:\n",
    "#         # macro_topic_list.append(topic_list[topic].split(' -')[0].split(': ')[-1])\n",
    "#         solution_macro_topic_mapping[topic] = macro_topic\n",
    "        \n",
    "# # print(find_duplicates(solution_macro_topic_list))\n",
    "# # print(len(solution_macro_topic_list) == 50)\n",
    "# # print(set(range(50)).difference(set(solution_macro_topic_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq3, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']])\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_rq3, 'macro-topics.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "\n",
    "values = []\n",
    "labels = []\n",
    "\n",
    "for index, group in df.groupby('Challenge_topic_macro'):\n",
    "    topic_list = [topic + 1 for topic in macro_topic2index_dict[macro_topic_indexing[index]]]\n",
    "    entry = {\n",
    "        'Index': index + 1,\n",
    "        'Macro-topic': macro_topic_indexing[index],\n",
    "        'Percentage (%)': round(len(group)/len(df)*100, 2),\n",
    "        'Topic list': topic_list,\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "    labels.append(macro_topic_indexing[index])\n",
    "    values.append(len(group))\n",
    "\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_rq12, 'macro-topics.json'))\n",
    "\n",
    "for macro_name, macro_group in df.groupby('Challenge_topic_macro'):\n",
    "    categories = []\n",
    "    frequency_p = []\n",
    "    frequency_k = [] \n",
    "\n",
    "    for name, group in macro_group.groupby('Challenge_topic'):\n",
    "        categories.append(topic_mapping[name])\n",
    "        frequency_p.append(len(group[group['Challenge_type'] == 'problem']))\n",
    "        frequency_k.append(len(group[group['Challenge_type'] == 'knowledge']))\n",
    "    \n",
    "    # Create a stacked bar chart\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(name='Problem', x=categories, y=frequency_p),\n",
    "        go.Bar(name='Knowledge', x=categories, y=frequency_k)\n",
    "    ])\n",
    "\n",
    "    # Change the bar mode\n",
    "    fig.update_layout(barmode='stack', title=f\"{macro_topic_indexing[macro_name]}\")\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_json(os.path.join(path_rq3, 'macro-topics.json'))\n",
    "df_topics = df_topics[df_topics['Challenge_type'] == 'problem']\n",
    "df_topics = df_topics[df_topics['Resolution_summary_topic_macro'] != -1]\n",
    "\n",
    "df = df_topics[['Challenge_topic_macro', 'Resolution_summary_topic_macro']].value_counts().reset_index(name='count')\n",
    "df_grouped = df.groupby('Challenge_topic_macro')['count'].sum().reset_index()\n",
    "df_grouped.rename(columns={'count': 'sum'}, inplace=True)\n",
    "df_merged = pd.merge(df, df_grouped, on='Challenge_topic_macro')\n",
    "df_merged['normalized_count'] = df_merged['count'] / df_merged['sum']\n",
    "\n",
    "df_heatmap = df_merged.pivot_table(values='normalized_count', index='Resolution_summary_topic_macro', columns='Challenge_topic_macro', aggfunc=np.mean)\n",
    "ax = sns.heatmap(df_heatmap, cmap=\"GnBu\")\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel('Problem')\n",
    "plt.ylabel('Resolution')\n",
    "plt.savefig(os.path.join(path_rq3, 'Problem_resolution_heatmap_column.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics = pd.read_json(os.path.join(path_rq3, 'macro-topics.json'))\n",
    "# df_topics = df_topics[df_topics['Challenge_type'] == 'problem']\n",
    "# df_topics = df_topics[df_topics['Resolution_summary_topic_macro'] != -1]\n",
    "\n",
    "# df = df_topics[['Challenge_topic_macro', 'Resolution_summary_topic_macro']].value_counts().reset_index(name='count')\n",
    "# df_grouped = df.groupby('Resolution_summary_topic_macro')['count'].sum().reset_index()\n",
    "# df_grouped.rename(columns={'count': 'sum'}, inplace=True)\n",
    "# df_merged = pd.merge(df, df_grouped, on='Resolution_summary_topic_macro')\n",
    "# df_merged['normalized_count'] = df_merged['count'] / df_merged['sum']\n",
    "\n",
    "# df_heatmap = df_merged.pivot_table(values='normalized_count', index='Resolution_summary_topic_macro', columns='Challenge_topic_macro', aggfunc=np.mean)\n",
    "# ax = sns.heatmap(df_heatmap, cmap=\"GnBu\")\n",
    "# ax.invert_yaxis()\n",
    "# plt.xlabel('Problem')\n",
    "# plt.ylabel('Resolution')\n",
    "# plt.savefig(os.path.join(path_rq3, 'Problem_resolution_heatmap_row.pdf'), bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_json(os.path.join(path_rq3, 'macro-topics.json'))\n",
    "df_topics = df_topics[df_topics['Challenge_type'] == 'knowledge']\n",
    "df_topics = df_topics[df_topics['Resolution_summary_topic_macro'] != -1]\n",
    "\n",
    "df = df_topics[['Challenge_topic_macro', 'Resolution_summary_topic_macro']].value_counts().reset_index(name='count')\n",
    "df_grouped = df.groupby('Challenge_topic_macro')['count'].sum().reset_index()\n",
    "df_grouped.rename(columns={'count': 'sum'}, inplace=True)\n",
    "df_merged = pd.merge(df, df_grouped, on='Challenge_topic_macro')\n",
    "df_merged['normalized_count'] = df_merged['count'] / df_merged['sum']\n",
    "\n",
    "df_heatmap = df_merged.pivot_table(values='normalized_count', index='Resolution_summary_topic_macro', columns='Challenge_topic_macro', aggfunc=np.mean)\n",
    "ax = sns.heatmap(df_heatmap, cmap=\"GnBu\")\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel('Knowledge')\n",
    "plt.ylabel('Resolution')\n",
    "plt.savefig(os.path.join(path_rq3, 'Knowledge_resolution_heatmap_column.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics = pd.read_json(os.path.join(path_rq3, 'macro-topics.json'))\n",
    "# df_topics = df_topics[df_topics['Challenge_type'] == 'knowledge']\n",
    "# df_topics = df_topics[df_topics['Resolution_summary_topic_macro'] != -1]\n",
    "\n",
    "# df = df_topics[['Challenge_topic_macro', 'Resolution_summary_topic_macro']].value_counts().reset_index(name='count')\n",
    "# df_grouped = df.groupby('Resolution_summary_topic_macro')['count'].sum().reset_index()\n",
    "# df_grouped.rename(columns={'count': 'sum'}, inplace=True)\n",
    "# df_merged = pd.merge(df, df_grouped, on='Resolution_summary_topic_macro')\n",
    "# df_merged['normalized_count'] = df_merged['count'] / df_merged['sum']\n",
    "\n",
    "# df_heatmap = df_merged.pivot_table(values='normalized_count', index='Resolution_summary_topic_macro', columns='Challenge_topic_macro', aggfunc=np.mean)\n",
    "# ax = sns.heatmap(df_heatmap, cmap=\"GnBu\")\n",
    "# ax.invert_yaxis()\n",
    "# plt.xlabel('Knowledge')\n",
    "# plt.ylabel('Resolution')\n",
    "# plt.savefig(os.path.join(path_rq3, 'Knowledge_resolution_heatmap_row.pdf'), bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
