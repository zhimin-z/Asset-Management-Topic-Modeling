{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = df_issues['Issue_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_issues['Issue_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_issues['Issue_preprocessed_content'] = df_issues['Issue_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_questions['Question_original_content'] = df_questions['Question_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_issues['Original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "\n",
    "df_questions['Original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "df_challenges = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_challenges.to_json(os.path.join(path_dataset, 'challenges_original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the best topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=20, n_components=10,\n",
    "                  metric='manhattan', low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, max_cluster_size=1500)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 5))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "    # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    representation_model=representation_model,\n",
    "    # verbose=True                              # Step 7 - Track model stages\n",
    ")\n",
    "\n",
    "df_challenges = pd.read_json(os.path.join(path_dataset, 'challenges_original.json'))\n",
    "docs = df_challenges['Original_content_gpt_summary'].tolist()\n",
    "\n",
    "topic_model = topic_model.fit(docs)\n",
    "topic_model.save(os.path.join(path_dataset, 'Topic model'))\n",
    "\n",
    "fig = topic_model.visualize_topics()\n",
    "fig.write_html(os.path.join(path_dataset, 'Topic visualization.html'))\n",
    "\n",
    "fig = topic_model.visualize_barchart()\n",
    "fig.write_html(os.path.join(path_dataset, 'Term visualization.html'))\n",
    "\n",
    "fig = topic_model.visualize_heatmap()\n",
    "fig.write_html(os.path.join(path_dataset, 'Topic similarity visualization.html'))\n",
    "\n",
    "fig = topic_model.visualize_term_rank()\n",
    "fig.write_html(os.path.join(path_dataset, 'Term score decline visualization.html'))\n",
    "\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "fig.write_html(os.path.join(path_dataset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "fig.write_html(os.path.join(path_dataset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = '9963fa73f81aa361bdbaf545857e1230fc74094c'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "wandb_project = 'asset-management-project'\n",
    "wandb.login()\n",
    "\n",
    "df_challenges = pd.read_json(os.path.join(path_dataset, 'challenges_original.json'))\n",
    "docs = df_challenges['Original_content_gpt_summary'].tolist()\n",
    "\n",
    "# set general sweep configuration\n",
    "sweep_configuration = {\n",
    "    \"name\": \"experiment-2\",\n",
    "    \"metric\": {\n",
    "        'name': 'CoherenceCV',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        'n_neighbors': {\n",
    "            'values': list(range(10, 110, 10))\n",
    "        },\n",
    "        'n_components': {\n",
    "            'values': list(range(2, 12, 2))\n",
    "        },\n",
    "        'ngram_range': {\n",
    "            'values': list(range(3, 6))\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# set default sweep configuration\n",
    "config_defaults = {\n",
    "    'model_name': 'all-mpnet-base-v2',\n",
    "    'metric_distane': 'manhattan',\n",
    "    'low_memory': True,\n",
    "    'max_cluster_size': 1500,\n",
    "    'min_cluster_size': 50,\n",
    "    'stop_words': 'english',\n",
    "    'reduce_frequent_words': True\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=(1, wandb.config.ngram_range))\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: j7pnz7gn\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
