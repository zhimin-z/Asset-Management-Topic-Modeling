{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dateset = '../Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dateset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(path_dateset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = df_issues['Issue_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_issues['Issue_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_issues['Issue_preprocessed_content'] = df_issues['Issue_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_questions['Question_original_content'] = df_questions['Question_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_issues['Original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "\n",
    "df_questions['Original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "df_challenges = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_challenges.to_json(os.path.join(path_dateset, 'challenges_original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the best topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=20, n_components=10,\n",
    "                  metric='manhattan', low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, max_cluster_size=1500)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "    # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    representation_model=representation_model,\n",
    "    # verbose=True                              # Step 7 - Track model stages\n",
    ")\n",
    "\n",
    "df_challenges = pd.read_json(os.path.join(path_dateset, 'challenges_original.json'))\n",
    "docs = df_challenges['Original_content_gpt_summary'].tolist()\n",
    "\n",
    "topic_model = topic_model.fit(docs)\n",
    "# topic_model.save(os.path.join(path_labeling_issue, 'Topic model'))\n",
    "\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Topic visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_barchart()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Term visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_heatmap()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Topic similarity visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_term_rank()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Term score decline visualization.html'))\n",
    "\n",
    "# hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "# fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "# fig.write_html(os.path.join(path_dateset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "# embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "# fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "# fig.write_html(os.path.join(path_dateset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fe901fabd79984eef85d4a4d266a0ea0891ed091a1944f51c1dc9dfad01a082"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
