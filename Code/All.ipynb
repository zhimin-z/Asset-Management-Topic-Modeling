{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = df_issues['Issue_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_issues['Issue_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_issues['Issue_preprocessed_content'] = df_issues['Issue_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_questions['Question_original_content'] = df_questions['Question_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_issues['Challenge_original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Challenge_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Challenge_preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "\n",
    "df_questions['Challenge_original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Challenge_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Challenge_preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "df_questions['Solution_original_content'] = df_questions['Answer_original_content']\n",
    "df_questions['Solution_original_content_gpt_summary'] = df_questions['Answer_original_content_gpt_summary']\n",
    "df_questions['Solution_preprocessed_content'] = df_questions['Answer_preprocessed_content']\n",
    "\n",
    "del df_questions['Answer_original_content']\n",
    "del df_questions['Answer_original_content_gpt_summary']\n",
    "del df_questions['Answer_preprocessed_content']\n",
    "\n",
    "df_all = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>5050</td>\n",
       "      <td>-1_azure_attempting deploy_kubeflow_mlflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>0_logging trained model mlflow_using mlflow_lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>322</td>\n",
       "      <td>1_git repository_git lfs_git repo_data version...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>220</td>\n",
       "      <td>2_jupyter notebook instance_run jupyter notebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>207</td>\n",
       "      <td>3_challenges accessing s3_s3 bucket encounteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>204</td>\n",
       "      <td>4_gpu utilization_average gpu utilization_aver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>5_features improvements bug fixes_major featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>174</td>\n",
       "      <td>6_azure data lake gen2_azure data lake storage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>168</td>\n",
       "      <td>7_deploy trained model endpoint_multi model en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>162</td>\n",
       "      <td>8____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>156</td>\n",
       "      <td>9_azure machine learning service_deployed azur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>147</td>\n",
       "      <td>10_current aws identity role_challenges connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>134</td>\n",
       "      <td>11_challenges deploying tensorflow model_tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>12_apache spark_sparkmagic pyspark_pysparkproc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>119</td>\n",
       "      <td>13_trained keras model aws_scikit learn model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>115</td>\n",
       "      <td>14_custom docker image_custom docker_model doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>112</td>\n",
       "      <td>15_automl forecasting_forecasting automl_using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>102</td>\n",
       "      <td>16_model registry_model versioning_model signa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>101</td>\n",
       "      <td>17_xgboost model encountering error_xgboost mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>96</td>\n",
       "      <td>18_google cloud translate api_google translate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>92</td>\n",
       "      <td>19_create pipeline_run pipeline_pipeline run_r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>20_error hyperparameter tuning job_creating hy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>82</td>\n",
       "      <td>21_studio lab unable_studio lab unable access_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>75</td>\n",
       "      <td>22_build image classification_azure custom vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>72</td>\n",
       "      <td>23_existing conda environment_creating new con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>72</td>\n",
       "      <td>24_running hyperparameter sweep sweep_running ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>70</td>\n",
       "      <td>25_challenges azure kubernetes service_encount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>26_azure compute_compute azure_unable create c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "      <td>27_aws ground truth labeling_aws ground truth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>58</td>\n",
       "      <td>28_launch tensorboard aws_tensorboard logs_iss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>55</td>\n",
       "      <td>29_encountering modulenotfounderror attempting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>30_running guild run command_running script gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>31_environment run id_environment run id despi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "      <td>32_encountering challenges batch transform_cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>50</td>\n",
       "      <td>33_dataset to_pandas_dataframe_spark studio un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   5050         -1_azure_attempting deploy_kubeflow_mlflow\n",
       "1       0    368  0_logging trained model mlflow_using mlflow_lo...\n",
       "2       1    322  1_git repository_git lfs_git repo_data version...\n",
       "3       2    220  2_jupyter notebook instance_run jupyter notebo...\n",
       "4       3    207  3_challenges accessing s3_s3 bucket encounteri...\n",
       "5       4    204  4_gpu utilization_average gpu utilization_aver...\n",
       "6       5    191  5_features improvements bug fixes_major featur...\n",
       "7       6    174  6_azure data lake gen2_azure data lake storage...\n",
       "8       7    168  7_deploy trained model endpoint_multi model en...\n",
       "9       8    162                                              8____\n",
       "10      9    156  9_azure machine learning service_deployed azur...\n",
       "11     10    147  10_current aws identity role_challenges connec...\n",
       "12     11    134  11_challenges deploying tensorflow model_tenso...\n",
       "13     12    122  12_apache spark_sparkmagic pyspark_pysparkproc...\n",
       "14     13    119  13_trained keras model aws_scikit learn model ...\n",
       "15     14    115  14_custom docker image_custom docker_model doc...\n",
       "16     15    112  15_automl forecasting_forecasting automl_using...\n",
       "17     16    102  16_model registry_model versioning_model signa...\n",
       "18     17    101  17_xgboost model encountering error_xgboost mo...\n",
       "19     18     96  18_google cloud translate api_google translate...\n",
       "20     19     92  19_create pipeline_run pipeline_pipeline run_r...\n",
       "21     20     85  20_error hyperparameter tuning job_creating hy...\n",
       "22     21     82  21_studio lab unable_studio lab unable access_...\n",
       "23     22     75  22_build image classification_azure custom vis...\n",
       "24     23     72  23_existing conda environment_creating new con...\n",
       "25     24     72  24_running hyperparameter sweep sweep_running ...\n",
       "26     25     70  25_challenges azure kubernetes service_encount...\n",
       "27     26     65  26_azure compute_compute azure_unable create c...\n",
       "28     27     60  27_aws ground truth labeling_aws ground truth ...\n",
       "29     28     58  28_launch tensorboard aws_tensorboard logs_iss...\n",
       "30     29     55  29_encountering modulenotfounderror attempting...\n",
       "31     30     53  30_running guild run command_running script gu...\n",
       "32     31     51  31_environment run id_environment run id despi...\n",
       "33     32     51  32_encountering challenges batch transform_cha...\n",
       "34     33     50  33_dataset to_pandas_dataframe_spark studio un..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the best topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=10, n_components=2,\n",
    "                  metric='manhattan', low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, max_cluster_size=1500)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 4))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "    # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    representation_model=representation_model,\n",
    "    # verbose=True                              # Step 7 - Track model stages\n",
    ")\n",
    "\n",
    "df_challenges = pd.read_json(os.path.join(path_dataset, 'all_original.json'))\n",
    "docs = df_challenges['Challenge_original_content_gpt_summary'].tolist()\n",
    "\n",
    "topic_model = topic_model.fit(docs)\n",
    "# topic_model.save(os.path.join(path_dataset, 'Topic model'))\n",
    "\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Topic visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_barchart()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Term visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_heatmap()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Topic similarity visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_term_rank()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Term score decline visualization.html'))\n",
    "\n",
    "# hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "# fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "# fig.write_html(os.path.join(path_dataset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "# embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "# fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "# fig.write_html(os.path.join(path_dataset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = '9963fa73f81aa361bdbaf545857e1230fc74094c'\n",
    "os.environ[\"WANDB_AGENT_MAX_INITIAL_FAILURES\"]= \"150\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "wandb_project = 'asset-management-project'\n",
    "wandb.login()\n",
    "\n",
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_original.json'))\n",
    "docs = df_all['Challenge_original_content_gpt_summary'].tolist()\n",
    "\n",
    "# set general sweep configuration\n",
    "sweep_configuration = {\n",
    "    \"name\": \"experiment-2\",\n",
    "    \"metric\": {\n",
    "        'name': 'CoherenceCV',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        'n_neighbors': {\n",
    "            'values': list(range(10, 110, 10))\n",
    "        },\n",
    "        'n_components': {\n",
    "            'values': list(range(2, 12, 2))\n",
    "        },\n",
    "        'ngram_range': {\n",
    "            'values': list(range(3, 6))\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# set default sweep configuration\n",
    "config_defaults = {\n",
    "    'model_name': 'all-mpnet-base-v2',\n",
    "    'metric_distane': 'manhattan',\n",
    "    'low_memory': True,\n",
    "    'max_cluster_size': 1500,\n",
    "    'min_cluster_size': 50,\n",
    "    'stop_words': 'english',\n",
    "    'reduce_frequent_words': True\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=(1, wandb.config.ngram_range))\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: j7pnz7gn\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
