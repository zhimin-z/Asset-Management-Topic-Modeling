{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dateset = '../Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dateset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(path_dateset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = df_issues['Issue_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_issues['Issue_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_issues['Issue_preprocessed_content'] = df_issues['Issue_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_questions['Question_original_content'] = df_questions['Question_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_issues['Original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "\n",
    "df_questions['Original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "df_challenges = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_challenges.to_json(os.path.join(path_dateset, 'challenges_original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>140</td>\n",
       "      <td>-1_attributeerror attempting_encountered attri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0_git_local s3_file git_gitignore file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1_inconsistency logger_challenge logger_inside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2_issue azure_azure account_subscriptions azur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3_issue user_interface checkpoints_issue code_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>4_kubeflow_pipelines_kubeflow pipeline_kubeflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>5_logger pytorch_bug pytorch_causing pytorch_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>6_pipeline_ml_factory_generated pipeline_ml_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7_kedro cli_failed kedro_kedro init_kedro yml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8_pycaret runs_issue pycaret_challenges pycare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9_mlflow installed_mlflow cli_running mlflow_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10_bug workspace_updated workspace_access work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11_modulenotfounderror attempting_encountered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>12_error tensorflow_runtimeerror training_tens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1    140  -1_attributeerror attempting_encountered attri...\n",
       "1       0     40             0_git_local s3_file git_gitignore file\n",
       "2       1     32  1_inconsistency logger_challenge logger_inside...\n",
       "3       2     20  2_issue azure_azure account_subscriptions azur...\n",
       "4       3     20  3_issue user_interface checkpoints_issue code_...\n",
       "5       4     19  4_kubeflow_pipelines_kubeflow pipeline_kubeflo...\n",
       "6       5     19  5_logger pytorch_bug pytorch_causing pytorch_p...\n",
       "7       6     15  6_pipeline_ml_factory_generated pipeline_ml_fa...\n",
       "8       7     13      7_kedro cli_failed kedro_kedro init_kedro yml\n",
       "9       8     10  8_pycaret runs_issue pycaret_challenges pycare...\n",
       "10      9      9  9_mlflow installed_mlflow cli_running mlflow_m...\n",
       "11     10      9  10_bug workspace_updated workspace_access work...\n",
       "12     11      6  11_modulenotfounderror attempting_encountered ...\n",
       "13     12      6  12_error tensorflow_runtimeerror training_tens..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the best topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=20, n_components=10,\n",
    "                  metric='manhattan', low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, max_cluster_size=100)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "    # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    representation_model=representation_model,\n",
    "    # verbose=True                              # Step 7 - Track model stages\n",
    ")\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dateset, 'issues_original.json'))\n",
    "docs = df_issues['Issue_original_content_gpt_summary'].tolist()\n",
    "\n",
    "topic_model = topic_model.fit(docs)\n",
    "# topic_model.save(os.path.join(path_labeling_issue, 'Topic model'))\n",
    "\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Topic visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_barchart()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Term visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_heatmap()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Topic similarity visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_term_rank()\n",
    "# fig.write_html(os.path.join(path_dateset, 'Term score decline visualization.html'))\n",
    "\n",
    "# hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "# fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "# fig.write_html(os.path.join(path_dateset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "# embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "# fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "# fig.write_html(os.path.join(path_dateset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fe901fabd79984eef85d4a4d266a0ea0891ed091a1944f51c1dc9dfad01a082"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
