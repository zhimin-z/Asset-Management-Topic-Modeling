{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = df_issues['Issue_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_issues['Issue_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_issues['Issue_preprocessed_content'] = df_issues['Issue_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_questions['Question_original_content'] = df_questions['Question_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_issues['Challenge_original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Challenge_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Challenge_preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "\n",
    "df_questions['Challenge_original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Challenge_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Challenge_preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "df_questions['Solution_original_content'] = df_questions['Answer_original_content']\n",
    "df_questions['Solution_original_content_gpt_summary'] = df_questions['Answer_original_content_gpt_summary']\n",
    "df_questions['Solution_preprocessed_content'] = df_questions['Answer_preprocessed_content']\n",
    "\n",
    "del df_questions['Answer_original_content']\n",
    "del df_questions['Answer_original_content_gpt_summary']\n",
    "del df_questions['Answer_preprocessed_content']\n",
    "\n",
    "df_all = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "100%|██████████| 27/27 [00:50<00:00,  1.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4907</td>\n",
       "      <td>-1_mlflow_azure devops_azure_kubeflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>0_logged metrics_summary metrics_metrics tab_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>1_data version control_git lfs_git repository_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>281</td>\n",
       "      <td>2_challenges accessing s3_aws s3 bucket_s3 buc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>3_tensorflow gpu_average gpu utilization_gpu u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>232</td>\n",
       "      <td>4_jupyter notebook instance_run jupyter notebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>191</td>\n",
       "      <td>5_python conda environment_creating conda envi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>187</td>\n",
       "      <td>6_labelling job_custom labeling job_labeling j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>186</td>\n",
       "      <td>7_deploying model azure_model azure kubernetes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>181</td>\n",
       "      <td>8_multi model endpoint_model endpoint_model en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>181</td>\n",
       "      <td>9_steps pipeline_run pipeline_valueerror runni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>181</td>\n",
       "      <td>10_hyperparameter tuning job_hyperparameter tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>180</td>\n",
       "      <td>11_columns imported data_dataset module_datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>162</td>\n",
       "      <td>12____</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>150</td>\n",
       "      <td>13_package azure ml_related rlang package_load...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>147</td>\n",
       "      <td>14_tensorflow model aws_deploying tensorflow m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>126</td>\n",
       "      <td>15_apache spark_serverless apache spark_spark ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>115</td>\n",
       "      <td>16_aws lambda endpoint_endpoint aws lambda_end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>114</td>\n",
       "      <td>17_files azure blob_file azure blob_data azure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>113</td>\n",
       "      <td>18_flow error authorization_serviceprincipalau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>19_automl forecasting_using automl forecasting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>20_custom docker image_docker image environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>108</td>\n",
       "      <td>21_models training_train multiple models_train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>22_encountering issue studio_studio lab unable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>100</td>\n",
       "      <td>23_xgboost model encountered_xgboost model enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>88</td>\n",
       "      <td>24_batch prediction job_challenges batch trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "      <td>25_guild run code_running script guild_pythonp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>57</td>\n",
       "      <td>26_tensorboard custom_tensorboard logs_run ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "      <td>27_aws sagemaker_use aws sagemaker_aws sagemak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   4907              -1_mlflow_azure devops_azure_kubeflow\n",
       "1       0    311  0_logged metrics_summary metrics_metrics tab_m...\n",
       "2       1    293  1_data version control_git lfs_git repository_...\n",
       "3       2    281  2_challenges accessing s3_aws s3 bucket_s3 buc...\n",
       "4       3    237  3_tensorflow gpu_average gpu utilization_gpu u...\n",
       "5       4    232  4_jupyter notebook instance_run jupyter notebo...\n",
       "6       5    191  5_python conda environment_creating conda envi...\n",
       "7       6    187  6_labelling job_custom labeling job_labeling j...\n",
       "8       7    186  7_deploying model azure_model azure kubernetes...\n",
       "9       8    181  8_multi model endpoint_model endpoint_model en...\n",
       "10      9    181  9_steps pipeline_run pipeline_valueerror runni...\n",
       "11     10    181  10_hyperparameter tuning job_hyperparameter tu...\n",
       "12     11    180  11_columns imported data_dataset module_datase...\n",
       "13     12    162                                             12____\n",
       "14     13    150  13_package azure ml_related rlang package_load...\n",
       "15     14    147  14_tensorflow model aws_deploying tensorflow m...\n",
       "16     15    126  15_apache spark_serverless apache spark_spark ...\n",
       "17     16    115  16_aws lambda endpoint_endpoint aws lambda_end...\n",
       "18     17    114  17_files azure blob_file azure blob_data azure...\n",
       "19     18    113  18_flow error authorization_serviceprincipalau...\n",
       "20     19    110  19_automl forecasting_using automl forecasting...\n",
       "21     20    110  20_custom docker image_docker image environmen...\n",
       "22     21    108  21_models training_train multiple models_train...\n",
       "23     22    100  22_encountering issue studio_studio lab unable...\n",
       "24     23    100  23_xgboost model encountered_xgboost model enc...\n",
       "25     24     88  24_batch prediction job_challenges batch trans...\n",
       "26     25     59  25_guild run code_running script guild_pythonp...\n",
       "27     26     57  26_tensorboard custom_tensorboard logs_run ten...\n",
       "28     27     54  27_aws sagemaker_use aws sagemaker_aws sagemak..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the best topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=10, n_components=4,\n",
    "                  metric='manhattan', low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=50, max_cluster_size=1500)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "    # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    representation_model=representation_model,\n",
    "    # verbose=True                              # Step 7 - Track model stages\n",
    ")\n",
    "\n",
    "df_challenges = pd.read_json(os.path.join(path_dataset, 'all_original.json'))\n",
    "docs = df_challenges['Challenge_original_content_gpt_summary'].tolist()\n",
    "\n",
    "topic_model = topic_model.fit(docs)\n",
    "topic_model.save(os.path.join(path_dataset, 'Topic model'))\n",
    "\n",
    "fig = topic_model.visualize_topics()\n",
    "fig.write_html(os.path.join(path_dataset, 'Topic visualization.html'))\n",
    "\n",
    "fig = topic_model.visualize_barchart()\n",
    "fig.write_html(os.path.join(path_dataset, 'Term visualization.html'))\n",
    "\n",
    "fig = topic_model.visualize_heatmap()\n",
    "fig.write_html(os.path.join(path_dataset, 'Topic similarity visualization.html'))\n",
    "\n",
    "fig = topic_model.visualize_term_rank()\n",
    "fig.write_html(os.path.join(path_dataset, 'Term score decline visualization.html'))\n",
    "\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "fig.write_html(os.path.join(path_dataset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "fig.write_html(os.path.join(path_dataset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = '9963fa73f81aa361bdbaf545857e1230fc74094c'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "wandb_project = 'asset-management-project'\n",
    "wandb.login()\n",
    "\n",
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_original.json'))\n",
    "docs = df_all['Challenge_original_content_gpt_summary'].tolist()\n",
    "\n",
    "# set general sweep configuration\n",
    "sweep_configuration = {\n",
    "    \"name\": \"experiment-2\",\n",
    "    \"metric\": {\n",
    "        'name': 'CoherenceCV',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        'n_neighbors': {\n",
    "            'values': list(range(10, 110, 10))\n",
    "        },\n",
    "        'n_components': {\n",
    "            'values': list(range(2, 12, 2))\n",
    "        },\n",
    "        'ngram_range': {\n",
    "            'values': list(range(3, 6))\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# set default sweep configuration\n",
    "config_defaults = {\n",
    "    'model_name': 'all-mpnet-base-v2',\n",
    "    'metric_distane': 'manhattan',\n",
    "    'low_memory': True,\n",
    "    'max_cluster_size': 1500,\n",
    "    'min_cluster_size': 50,\n",
    "    'stop_words': 'english',\n",
    "    'reduce_frequent_words': True\n",
    "}\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=(1, wandb.config.ngram_range))\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: j7pnz7gn\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
