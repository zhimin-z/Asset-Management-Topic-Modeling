{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import openai\n",
    "import random\n",
    "import enchant\n",
    "import textstat\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, strip_punctuation, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "prompt_summary = 'Concisely convey the most significant points about the post in one or two brief sentences.\\n###'\n",
    "\n",
    "tools_keyword_mapping = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sagemaker', 'amazon', 'aws'],\n",
    "    'Azure Machine Learning': ['azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow', 'databricks'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex', 'google'],\n",
    "    'Weights & Biases': ['weights', 'biases', 'wandb']\n",
    "}\n",
    "\n",
    "stop_words_custom = {\n",
    "    'action',\n",
    "    'actions',\n",
    "    'activity',\n",
    "    'advance',\n",
    "    'advice',\n",
    "    'analysis',\n",
    "    'analyses',\n",
    "    'answer',\n",
    "    'answers',\n",
    "    'approach',\n",
    "    'approaches',\n",
    "    'article',\n",
    "    'assertion',\n",
    "    'behavior',\n",
    "    'bit',\n",
    "    'block',\n",
    "    'body',\n",
    "    'bug',\n",
    "    'build',\n",
    "    'building',\n",
    "    'case',\n",
    "    'cases',\n",
    "    'cause',\n",
    "    'change',\n",
    "    'changes',\n",
    "    'char',\n",
    "    'character',\n",
    "    'characters',\n",
    "    'classification',\n",
    "    'collection',\n",
    "    'com',\n",
    "    'combination',\n",
    "    'commmunication',\n",
    "    'community',\n",
    "    'company',\n",
    "    'confusion',\n",
    "    'content',\n",
    "    'contents',\n",
    "    'control',\n",
    "    'count',\n",
    "    'couple',\n",
    "    'couples',\n",
    "    'course',\n",
    "    'courses',\n",
    "    'cross',\n",
    "    'custom',\n",
    "    'customer',\n",
    "    'customers',\n",
    "    'day',\n",
    "    'days',\n",
    "    'decision',\n",
    "    'default',\n",
    "    'differ',\n",
    "    'difference',\n",
    "    'description',\n",
    "    'desktop',\n",
    "    'detail',\n",
    "    'details',\n",
    "    'edit',\n",
    "    'end',\n",
    "    'error',\n",
    "    'errors',\n",
    "    'example',\n",
    "    'examples',\n",
    "    'exception',\n",
    "    'exceptions',\n",
    "    'experience',\n",
    "    'explanation',\n",
    "    'explanations',\n",
    "    'exit',\n",
    "    'face',\n",
    "    'fact',\n",
    "    'facts',\n",
    "    'fail',\n",
    "    'feature',\n",
    "    'features',\n",
    "    'feedback',\n",
    "    'feedbacks',\n",
    "    'fix',\n",
    "    'fixes',\n",
    "    'float',\n",
    "    'follow',\n",
    "    'following',\n",
    "    'forecast',\n",
    "    'forecasting',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'functionality',\n",
    "    'functionalities',\n",
    "    'future',\n",
    "    'goal',\n",
    "    'goals',\n",
    "    'guidance',\n",
    "    'guide',\n",
    "    'guy',\n",
    "    'guys',\n",
    "    'help',\n",
    "    'hour',\n",
    "    'hours',\n",
    "    'idea',\n",
    "    'ideas',\n",
    "    'info',\n",
    "    'information',\n",
    "    'instruction',\n",
    "    'instructions',\n",
    "    'int',\n",
    "    'issue',\n",
    "    'issues',\n",
    "    'kind',\n",
    "    'kinds',\n",
    "    'language',\n",
    "    'languages',\n",
    "    'laptop',\n",
    "    'learn',\n",
    "    'learning',\n",
    "    'level',\n",
    "    'levels',\n",
    "    # 'location',\n",
    "    # 'locations',\n",
    "    'look',\n",
    "    'looks',\n",
    "    'lot',\n",
    "    'lots',\n",
    "    'luck',\n",
    "    'machine',\n",
    "    'machines',\n",
    "    'message',\n",
    "    'messages',\n",
    "    'method',\n",
    "    'methods',\n",
    "    'minute',\n",
    "    'minutes',\n",
    "    'mistake',\n",
    "    'mistakes',\n",
    "    'moment',\n",
    "    'month',\n",
    "    'months',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'note',\n",
    "    'number',\n",
    "    'numbers',\n",
    "    'one',\n",
    "    'ones',\n",
    "    'org',\n",
    "    'organization',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'people',\n",
    "    'person',\n",
    "    'picture',\n",
    "    'place',\n",
    "    'plan',\n",
    "    'post',\n",
    "    'posts',\n",
    "    'price',\n",
    "    'problem',\n",
    "    'problems',\n",
    "    'processing',\n",
    "    'product',\n",
    "    'products',\n",
    "    'program',\n",
    "    'project',\n",
    "    'projects',\n",
    "    'purpose',\n",
    "    'purposes',\n",
    "    'question',\n",
    "    'questions',\n",
    "    'raise',\n",
    "    'reason',\n",
    "    'reasons',\n",
    "    'recommendation',\n",
    "    'recommendations',\n",
    "    'regression',\n",
    "    'research',\n",
    "    'result',\n",
    "    'results',\n",
    "    'return',\n",
    "    'returns',\n",
    "    'scenario',\n",
    "    'scenarios',\n",
    "    'science',\n",
    "    'screen',\n",
    "    'screenshot',\n",
    "    'screenshots',\n",
    "    'second',\n",
    "    'seconds',\n",
    "    'section',\n",
    "    'self',\n",
    "    'sense',\n",
    "    'sentence',\n",
    "    'setup',\n",
    "    'shape',\n",
    "    'site',\n",
    "    'situation',\n",
    "    'software',\n",
    "    'solution',\n",
    "    'solutions',\n",
    "    'speech',\n",
    "    'start',\n",
    "    'state',\n",
    "    'statement',\n",
    "    'states',\n",
    "    'status',\n",
    "    'step',\n",
    "    'steps',\n",
    "    'string',\n",
    "    'study',\n",
    "    'stuff',\n",
    "    'success',\n",
    "    'suggest',\n",
    "    'suggestion',\n",
    "    'suggestions',\n",
    "    'summary',\n",
    "    'summaries',\n",
    "    'support',\n",
    "    'task',\n",
    "    'tasks',\n",
    "    'text',\n",
    "    'time',\n",
    "    'times',\n",
    "    'thank',\n",
    "    'thanks',\n",
    "    'thing',\n",
    "    'things',\n",
    "    'three',\n",
    "    'title',\n",
    "    'today',\n",
    "    'tomorrow',\n",
    "    'tool',\n",
    "    'tools',\n",
    "    'trouble',\n",
    "    'truth',\n",
    "    'tutorial',\n",
    "    'tutorials',\n",
    "    'two',\n",
    "    'understand',\n",
    "    'understanding',\n",
    "    'url',\n",
    "    'urls',\n",
    "    'use',\n",
    "    'user',\n",
    "    'users',\n",
    "    'uses',\n",
    "    'value',\n",
    "    'values',\n",
    "    'video',\n",
    "    'videos',\n",
    "    'vision',\n",
    "    'voice',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'week',\n",
    "    'weeks',\n",
    "    'word',\n",
    "    'words',\n",
    "    'work',\n",
    "    'workaround',\n",
    "    'workarounds',\n",
    "    'works',\n",
    "    'year',\n",
    "    'years',\n",
    "    'yesterday',\n",
    "}\n",
    "\n",
    "tools_keyword_list = set(itertools.chain(*tools_keyword_mapping.values()))\n",
    "stop_words_list = STOPWORDS.union(tools_keyword_list).union(stop_words_custom)\n",
    "\n",
    "\n",
    "keywords_image = {\n",
    "    \".jpg\", \n",
    "    \".png\", \n",
    "    \".jpeg\", \n",
    "    \".gif\", \n",
    "    \".bmp\", \n",
    "    \".webp\", \n",
    "    \".svg\", \n",
    "    \".tiff\"\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(block_list):\n",
    "    total_loc = 0\n",
    "    for blocks in block_list:\n",
    "        for block in blocks:\n",
    "            for line in block.splitlines():\n",
    "                if line.strip():\n",
    "                    total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    # extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract code blocks type 1\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code blocks type 2\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    return clean_text, links, code_line\n",
    "\n",
    "def extract_code(content):\n",
    "    code_patterns = [r'```.+?```', r'``.+?``', r'`.+?`']\n",
    "    clean_text = content\n",
    "    code_line = 0\n",
    "\n",
    "    for code_pattern in code_patterns:\n",
    "        code_snippets = re.findall(code_pattern, clean_text, flags=re.DOTALL)\n",
    "        code_line += count_code_line(code_snippets)\n",
    "        clean_text = re.sub(code_pattern, '', clean_text, flags=re.DOTALL)\n",
    "    \n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern1 = r\"\\!?\\[.*?\\]\\((.*?)\\)\"\n",
    "    links1 = re.findall(link_pattern1, text)\n",
    "    clean_text = re.sub(link_pattern1, '', text)\n",
    "    link_pattern2 = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links2 = re.findall(link_pattern2, clean_text)\n",
    "    clean_text = re.sub(link_pattern2, '', clean_text)\n",
    "    links = links1 + links2\n",
    "    return clean_text, links\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_line1 = extract_styles(content)\n",
    "    clean_text, code_line2 = extract_code(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_line = code_line1 + code_line2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_line'])\n",
    "    return content_collection(clean_text, links, code_line)\n",
    "\n",
    "def word_frequency(text):\n",
    "    word_counts = collections.Counter(text.split())\n",
    "    return word_counts\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "def extract_english(text):\n",
    "    words = [word for word in text.split() if spell_checker.check(word)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = strip_punctuation(clean_text)\n",
    "    clean_text = extract_english(clean_text)\n",
    "    clean_text = extract_nouns(clean_text)\n",
    "    clean_text = strip_short(clean_text)\n",
    "    clean_text = remove_stopwords(clean_text, stop_words_list)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_links(links):\n",
    "    image_links = 0\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([image in link for image in keywords_image]):\n",
    "            image_links += 1\n",
    "        elif any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['image', 'documentation', 'tool', 'issue', 'patch', 'tutorial', 'example'])\n",
    "    return link_analysis(image_links, documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_counts = collections.Counter()\n",
    "# df_issues = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "# for index, row in df_issues.iterrows():\n",
    "#     text = split_content(row['Challenge_body']).text\n",
    "#     # print(text)\n",
    "#     text = preprocess_text(text)\n",
    "#     total_counts.update(word_frequency(text))\n",
    "# total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_comment_body'] = row['Issue_comment_body']\n",
    "    df_issues.at[index, 'Challenge_comment_score'] = row['Issue_comment_score']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_comment_body'] = row['Question_comment_body']\n",
    "    df_questions.at[index, 'Challenge_comment_score'] = row['Question_comment_score']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_comment_score'] = row['Answer_comment_score']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df = df[df.columns.drop(list(df.filter(regex=r'Issue|Question|Answer')))]\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 & 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    clean_title = preprocess_text(title_analyzer.text)\n",
    "    \n",
    "    challenge_analyzer = split_content(row['Challenge_title'] + row['Challenge_body'])\n",
    "    link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    clean_text = preprocess_text(challenge_analyzer.text)\n",
    "    \n",
    "    df.at[index, 'Challenge_preprocessed_title'] = clean_title\n",
    "    df.at[index, 'Challenge_preprocessed_content'] = clean_text\n",
    "    df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Challenge_comment_body']):\n",
    "        comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Challenge_closed_time']):\n",
    "        solution_analyzer = split_content(row['Solution_body'])\n",
    "        link_analyzer = analyze_links(solution_analyzer.links)\n",
    "        text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "        df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "        if pd.notna(row['Solution_comment_body']):\n",
    "            comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "            link_analyzer = analyze_links(comment_analyzer.links)\n",
    "            text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "            df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "            df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "            df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "            df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "            df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "            df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "            df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "            df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "            df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "            df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "            df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 200 == 199:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    # if pd.notna(row['Challenge_gpt_summary']):\n",
    "    #     continue\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + ' Body: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-4',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=100,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clean_summary = preprocess_text(row['Challenge_gpt_summary'])\n",
    "    df.at[index, 'Challenge_preprocessed_summary'] = clean_summary\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
