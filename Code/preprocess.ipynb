{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import openai\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "style_post_tools = ['Azure Machine Learning', 'DVC', 'Guild AI', 'SigOpt', 'Weights & Biases']\n",
    "\n",
    "regex = r'''([a-z]*([a-z])\\2[a-z]*)|(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+-[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]<>]+)'''\n",
    "\n",
    "prompt_summary = 'Your task is to provide an accurate summary of the text within 100 words. Your summary should highlight only the most important aspects regarding the anomaly or inquiries.\\n###'\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sage maker', 'sagemaker', 'aws', 'amazon'],\n",
    "    'Azure Machine Learning': ['azure machine learning', 'azure ml', 'azureml', 'azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai', 'guildai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow', 'databricks'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex ai', 'vertexai', 'google'],\n",
    "    'Weights & Biases': ['weights and biases', 'wandb', 'weights & biases', 'weights&biases', 'w & b', 'w&b']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content preprocessing patterns\n",
    "\n",
    "def preprocess_normal_post(text, remove_code=False):\n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in text:\n",
    "                text = text.replace(tool_keyword, '')\n",
    "\n",
    "    text = re.sub(regex, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "            \n",
    "    text = preprocess_string(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_style_post(text, remove_code=False):          \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    remove_tags = ['script', 'style']\n",
    "    remove_tags.append('code') if remove_code else None\n",
    "    for tag in soup(remove_tags):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in text:\n",
    "                text = text.replace(tool_keyword, '')\n",
    "    \n",
    "    text = re.sub(regex, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "    \n",
    "    text = preprocess_string(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# expential backoff\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "\n",
    "    df_issues.at[index, 'Discussion_body'] = row['Comment_body']\n",
    "    df_issues.at[index, 'Discussion_score_count'] = row['Comment_score_count']\n",
    "\n",
    "del df_issues['Issue_title']\n",
    "del df_issues['Issue_body']\n",
    "del df_issues['Issue_link']\n",
    "del df_issues['Issue_created_time']\n",
    "del df_issues['Issue_comment_count']\n",
    "del df_issues['Issue_score_count']\n",
    "del df_issues['Issue_closed_time']\n",
    "del df_issues['Issue_repo_issue_count']\n",
    "del df_issues['Issue_repo_star_count']\n",
    "del df_issues['Issue_repo_watch_count']\n",
    "del df_issues['Issue_repo_fork_count']\n",
    "del df_issues['Issue_repo_contributor_count']\n",
    "del df_issues['Issue_self_closed']\n",
    "\n",
    "del df_issues['Comment_body']\n",
    "del df_issues['Comment_score_count']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    \n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "\n",
    "del df_questions['Question_title']\n",
    "del df_questions['Question_body']\n",
    "del df_questions['Question_link']\n",
    "del df_questions['Question_created_time']\n",
    "del df_questions['Question_last_edit_time']\n",
    "del df_questions['Question_answer_count']\n",
    "del df_questions['Question_comment_count']\n",
    "del df_questions['Question_score_count']\n",
    "del df_questions['Question_closed_time']\n",
    "del df_questions['Question_view_count']\n",
    "del df_questions['Question_favorite_count']\n",
    "del df_questions['Question_self_closed']\n",
    "\n",
    "del df_questions['Answer_body']\n",
    "del df_questions['Answer_comment_count']\n",
    "del df_questions['Answer_last_edit_time']\n",
    "del df_questions['Answer_score_count']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485032/4014615791.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Platform'] == 'Stack Overflow') or ((row['Platform'] == 'Tool-specific') and (row['Tool'] in style_post_tools)):\n",
    "        content = preprocess_style_post(row['Challenge_title']) + ' ' + preprocess_style_post(row['Challenge_body'])\n",
    "    else:\n",
    "        content = preprocess_normal_post(row['Challenge_title']) + ' ' + preprocess_normal_post(row['Challenge_body'])\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 16720 tokens. Please reduce the length of the messages. on post https://github.com/kedro-org/kedro/issues/308\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 42298 tokens. Please reduce the length of the messages. on post https://github.com/Azure/MachineLearningNotebooks/issues/1668\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 16742 tokens. Please reduce the length of the messages. on post https://stackoverflow.com/questions/73624005\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 52631 tokens. Please reduce the length of the messages. on post https://community.wandb.ai/t/oserror-could-not-find-a-suitable-tls-ca-certificate/3913\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 19521 tokens. Please reduce the length of the messages. on post https://discuss.dvc.org/t/mlem-modal-nanogpt-blog-code/1657\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 162573 tokens. Please reduce the length of the messages. on post https://groups.google.com/g/mlflow-users/c/GrCd-t0gx8U\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 99:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    if pd.notna(row['Challenge_gpt_summary_original_content']):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + ' Body: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-4',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary_original_content'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485032/4014615791.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Platform'] == 'Stack Overflow') or ((row['Platform'] == 'Tool-specific') and (row['Tool'] in style_post_tools)):\n",
    "        df.at[index, 'Challenge_gpt_summary_preprocessed_content'] = preprocess_style_post(row['Challenge_gpt_summary_original_content'])\n",
    "    else:\n",
    "        df.at[index, 'Challenge_gpt_summary_preprocessed_content'] = preprocess_normal_post(row['Challenge_gpt_summary_original_content'])\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_485032/4014615791.py:18: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Platform'] == 'Stack Overflow') or ((row['Platform'] == 'Tool-specific') and (row['Tool'] in style_post_tools)):\n",
    "        content = preprocess_style_post(row['Challenge_title'], remove_code=True) + ' ' + preprocess_style_post(row['Challenge_body'], remove_code=True)\n",
    "    else:\n",
    "        content = preprocess_normal_post(row['Challenge_title'], remove_code=True) + ' ' + preprocess_normal_post(row['Challenge_body'], remove_code=True)\n",
    "        \n",
    "    df.at[index, 'Challenge_preprocessed_content'] = content\n",
    "        \n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unknown']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "preprocess_string('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove custom stop words from challenges and solutions\n",
    "\n",
    "stop_words_custom = [\n",
    "    'abl',\n",
    "    'abnorm',\n",
    "    'acknowledg',\n",
    "    'actual',\n",
    "    'ad',\n",
    "    'addition',\n",
    "    'admit',\n",
    "    'advis',\n",
    "    'alright',\n",
    "    'altern',\n",
    "    'amaz',\n",
    "    'answer',\n",
    "    'anomali'\n",
    "    'api',\n",
    "    'appear',\n",
    "    'applic',\n",
    "    'appreci',\n",
    "    'approach',\n",
    "    'appropri',\n",
    "    'aris',\n",
    "    'ask',\n",
    "    'assum',\n",
    "    'astonish',\n",
    "    'attempt',\n",
    "    'avail',\n",
    "    'aw',\n",
    "    'awesom',\n",
    "    'azur',\n",
    "    'bad',\n",
    "    'basic',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'best',\n",
    "    'better',\n",
    "    'bring',\n",
    "    'bug',\n",
    "    'case',\n",
    "    'categori',\n",
    "    'caus',\n",
    "    'certain',\n",
    "    'challeng',\n",
    "    'chang',\n",
    "    'check',\n",
    "    'cloudera',\n",
    "    'code',\n",
    "    # 'colab',\n",
    "    'command',\n",
    "    'concern',\n",
    "    'confirm',\n",
    "    'confus',\n",
    "    'consid',\n",
    "    'consult',\n",
    "    'contain',\n",
    "    'content',\n",
    "    'continu',\n",
    "    'correct',\n",
    "    'correctli',\n",
    "    'correspond',\n",
    "    'couldn',\n",
    "    'crash',\n",
    "    'curiou',\n",
    "    'current',\n",
    "    'custom',\n",
    "    'deep',\n",
    "    'demand',\n",
    "    'demo',\n",
    "    'deni',\n",
    "    'depict',\n",
    "    'describ',\n",
    "    'despit',\n",
    "    'detail',\n",
    "    'develop',\n",
    "    'differ',\n",
    "    'differenti',\n",
    "    'difficult',\n",
    "    'difficulti',\n",
    "    'discov',\n",
    "    'discuss',\n",
    "    'distinguish',\n",
    "    'easi',\n",
    "    'east',\n",
    "    'effect',\n",
    "    'emerg',\n",
    "    'encount',\n",
    "    # 'end',\n",
    "    'enquiri',\n",
    "    'ensur',\n",
    "    'error',\n",
    "    'especi',\n",
    "    'exampl',\n",
    "    'exception',\n",
    "    'excit',\n",
    "    'exist',\n",
    "    'expect',\n",
    "    'experi',\n",
    "    'eventu',\n",
    "    'databrick',\n",
    "    'def',\n",
    "    'domo',\n",
    "    'dont',\n",
    "    'face',\n",
    "    'fact',\n",
    "    'fascin',\n",
    "    'fail',\n",
    "    'failur',\n",
    "    'fairli',\n",
    "    'fals',\n",
    "    'far',\n",
    "    'fault',\n",
    "    'favorit',\n",
    "    'favourit',\n",
    "    'feel',\n",
    "    'find',\n",
    "    'fine',\n",
    "    'firstli',\n",
    "    'fix',\n",
    "    'float',\n",
    "    'follow',\n",
    "    'form',\n",
    "    'frustrat',\n",
    "    'gcp',\n",
    "    'get',\n",
    "    'give',\n",
    "    'given',\n",
    "    'go',\n",
    "    'good',\n",
    "    'googl',\n",
    "    'got',\n",
    "    'guarante',\n",
    "    'handl',\n",
    "    'happen',\n",
    "    'hard',\n",
    "    'have',\n",
    "    'hear',\n",
    "    'hei',\n",
    "    'hello',\n",
    "    'help',\n",
    "    'ibm',\n",
    "    'impli',\n",
    "    'implic',\n",
    "    'includ',\n",
    "    'incorrect',\n",
    "    'incorrectli',\n",
    "    'incred',\n",
    "    'indic',\n",
    "    'info',\n",
    "    'inform',\n",
    "    'inner',\n",
    "    'inquiri',\n",
    "    'insight',\n",
    "    'instead',\n",
    "    'int',\n",
    "    'interest',\n",
    "    'invalid',\n",
    "    'investig',\n",
    "    'issu',\n",
    "    'join',\n",
    "    # 'jupyter',\n",
    "    # 'keras',\n",
    "    'kind',\n",
    "    'know',\n",
    "    'known',\n",
    "    'lead',\n",
    "    'learn',\n",
    "    'let',\n",
    "    'like',\n",
    "    'long',\n",
    "    'look',\n",
    "    'lot',\n",
    "    'machin',\n",
    "    'malfunct',\n",
    "    'make',\n",
    "    'main',\n",
    "    'major',\n",
    "    'manag',\n",
    "    'manner',\n",
    "    'marvel',\n",
    "    'max',\n",
    "    'mean',\n",
    "    'meaning',\n",
    "    'meaningfulli',\n",
    "    'meaningless',\n",
    "    'meantim',\n",
    "    'mention',\n",
    "    'method',\n",
    "    'min',\n",
    "    'mind',\n",
    "    'mistak',\n",
    "    'mistakenli',\n",
    "    # 'multipl',\n",
    "    'name',\n",
    "    'near',\n",
    "    'necessari',\n",
    "    'need',\n",
    "    'new',\n",
    "    'non',\n",
    "    'north',\n",
    "    'notice',\n",
    "    'number',\n",
    "    'obtain',\n",
    "    'occas',\n",
    "    'occasion',\n",
    "    'occur',\n",
    "    'offer',\n",
    "    'old',\n",
    "    'opinion',\n",
    "    'own',\n",
    "    # 'open',\n",
    "    'oracl',\n",
    "    'ought',\n",
    "    'outcom',\n",
    "    'part',\n",
    "    'particip',\n",
    "    'particular',\n",
    "    'particularli',\n",
    "    'perceive',\n",
    "    # 'perform',\n",
    "    'permit',\n",
    "    'person',\n",
    "    'perspect',\n",
    "    'place',\n",
    "    'point',\n",
    "    'pointless',\n",
    "    'possibl',\n",
    "    'post',\n",
    "    'pretty',\n",
    "    'problem',\n",
    "    'product',\n",
    "    'program',\n",
    "    'project',\n",
    "    'proper',\n",
    "    'provid',\n",
    "    'python',\n",
    "    # 'pytorch',\n",
    "    'question',\n",
    "    'real',\n",
    "    'realize',\n",
    "    'recent',\n",
    "    'recognize',\n",
    "    'recommend',\n",
    "    'refer',\n",
    "    'regard',\n",
    "    'requir',\n",
    "    'resolv',\n",
    "    'respond',\n",
    "    'result',\n",
    "    'right',\n",
    "    'rightli',\n",
    "    'satisfi',\n",
    "    'saw',\n",
    "    'scenario',\n",
    "    # 'scikit',\n",
    "    'script',\n",
    "    'second',\n",
    "    'secondli',\n",
    "    'seek',\n",
    "    'seen',\n",
    "    'self',\n",
    "    'shall',\n",
    "    'shan',\n",
    "    'shock',\n",
    "    'shouldn',\n",
    "    'show',\n",
    "    'similar',\n",
    "    'simpl',\n",
    "    'situat',\n",
    "    # 'sklearn',\n",
    "    'snippet',\n",
    "    'snowflak',\n",
    "    'solut',\n",
    "    'solv',\n",
    "    'sound',\n",
    "    'sourc',\n",
    "    'south',\n",
    "    'special',\n",
    "    'specif',\n",
    "    # 'start',\n",
    "    'startl',\n",
    "    'strang',\n",
    "    'string',\n",
    "    'struggl',\n",
    "    'stun',\n",
    "    'succe',\n",
    "    'success',\n",
    "    'suggest',\n",
    "    'super',\n",
    "    # 'support',\n",
    "    'sure',\n",
    "    'suspect',\n",
    "    'take',\n",
    "    'talk',\n",
    "    'tell',\n",
    "    # 'tensorflow',\n",
    "    'text',\n",
    "    'thank',\n",
    "    'thing',\n",
    "    'think',\n",
    "    'thirdli',\n",
    "    'thought',\n",
    "    'tool',\n",
    "    'topic',\n",
    "    'total',\n",
    "    'true',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'unabl',\n",
    "    'understand',\n",
    "    'unexpect',\n",
    "    'unknown',\n",
    "    'unsur',\n",
    "    'upcom',\n",
    "    'us',\n",
    "    'user',\n",
    "    'usual',\n",
    "    'valid',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'wai',\n",
    "    'want',\n",
    "    'weird',\n",
    "    'west',\n",
    "    'will',\n",
    "    'word',\n",
    "    'worst',\n",
    "    'won',\n",
    "    'wonder',\n",
    "    'work',\n",
    "    'wors',\n",
    "    'wouldn',\n",
    "    'wrong',\n",
    "    'wrongli',\n",
    "    'xgboost',\n",
    "    'ye',\n",
    "] \n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Challenge_original_content'] = remove_stopwords(row['Challenge_original_content'], stopwords=stop_words_custom)\n",
    "    df.at[index, 'Challenge_preprocessed_content'] = remove_stopwords(row['Challenge_preprocessed_content'], stopwords=stop_words_custom)\n",
    "    df.at[index, 'Challenge_gpt_summary_preprocessed_content'] = remove_stopwords(row['Challenge_gpt_summary_preprocessed_content'], stopwords=stop_words_custom)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
