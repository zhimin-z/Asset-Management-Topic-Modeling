{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import openai\n",
    "import random\n",
    "import string\n",
    "import enchant\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "english = enchant.Dict(\"en_US\")\n",
    "\n",
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "# style_post_tools = ['Azure Machine Learning', 'DVC', 'Guild AI', 'SigOpt', 'Weights & Biases']\n",
    "\n",
    "# regex_char = r'[^a-z]'\n",
    "# regex_code = r'''((.)\\2{2,})|(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+-[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]<>]+)'''\n",
    "\n",
    "prompt_summary = 'Concisely convey the most significant points about the text in one or two brief sentences.\\n###'\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sagemaker', 'amazon', 'aws'],\n",
    "    'Azure Machine Learning': ['azure machine learning', 'azure ml', 'azure-ml', 'azureml', 'azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai', 'guild-ai', 'guildai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow', 'databricks'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex ai', 'vertex-ai', 'vertexai', 'google'],\n",
    "    'Weights & Biases': ['weights and biases', 'wandb', 'weights & biases', 'weights&biases', 'w & b', 'w&b']\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(code_blocks):\n",
    "    total_loc = 0\n",
    "    for block in code_blocks:\n",
    "        for line in block.splitlines():\n",
    "            if line.strip():\n",
    "                total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    # extract code\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    # extract link\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract image\n",
    "    image_count = len([link for link in links if link.startswith('https://i.stack.imgur.com')])\n",
    "    non_image_links = [link for link in links if not link.startswith('https://i.stack.imgur.com')]\n",
    "    for tag in soup.find_all('img'):  # remove images\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('code'):  # remove code blocks type 1\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('blockquote'):  # remove code blocks type 2\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('a'):  # remove URLs\n",
    "        tag.decompose()\n",
    "    clean_text = soup.get_text(separator=' ', strip=True)\n",
    "    return clean_text, non_image_links, code_line, image_count\n",
    "\n",
    "def extract_code(content):\n",
    "    code_pattern = r\"`([^`]+)`\"\n",
    "    code_line = count_code_line(re.findall(code_pattern, content, flags=re.DOTALL))\n",
    "    clean_text = re.sub(code_pattern, '', content, flags=re.DOTALL)\n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links = re.findall(link_pattern, text)\n",
    "    clean_text = re.sub(link_pattern, '', text)\n",
    "    return clean_text, links\n",
    "\n",
    "def count_image_number(content):\n",
    "    image_pattern = r\"\\!\\[.*?\\]\\(.*?\\)\"\n",
    "    image_count = len(re.findall(image_pattern, content))\n",
    "    clean_text = re.sub(image_pattern, '', content)\n",
    "    return clean_text, image_count\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_count1, image_count1 = extract_styles(content)\n",
    "    clean_text, code_count2 = extract_code(clean_text)\n",
    "    clean_text, image_count2 = count_image_number(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_count = code_count1 + code_count2\n",
    "    image_count = image_count1 + image_count2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_count', 'image_count'])\n",
    "    return content_collection(clean_text, links, code_count, image_count)\n",
    "\n",
    "def analyze_links(links):\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['documentation', 'tool', 'issue', 'patch', 'example'])\n",
    "    return link_analysis(documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "def check_english(content):\n",
    "    if detect(content) == 'en':\n",
    "        return True\n",
    "    return False\n",
    "            \n",
    "def preprocess_content(content):\n",
    "    clean_text = content.lower()\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in content:\n",
    "                clean_text = clean_text.replace(tool_keyword, ' ')\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "\n",
    "\n",
    "# def remove_non_printable(content):\n",
    "#     return ''.join(c for c in content if c not in string.printable)\n",
    "\n",
    "# def remove_words_containing_string(word, string):\n",
    "#     words = word.split()\n",
    "#     pattern = re.compile(string)\n",
    "#     filtered_words = [word for word in words if not pattern.search(word)]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# def remove_keywords(content):\n",
    "#     word_list = []\n",
    "#     for word in content.split():\n",
    "#         if english.check(word):\n",
    "#             word_list.append(word)\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "# df = df[df['Platform'].str.contains('Git')]\n",
    "# for index, row in df.iterrows():\n",
    "#     print(analyze_content(row['Issue_body']).text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "might want to add this to the 01_setup/ Check Dependencies, but only if we want to enforce this.\n",
      "[]\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/tmp/ipykernel_3063001/3898695132.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "# for index, row in df.iterrows():\n",
    "#     if 'Check for Athena => SageMaker region when' in row['Issue_title']:\n",
    "#         content_collection = split_content(row['Issue_body'])\n",
    "#         print(content_collection.text)\n",
    "#         print(content_collection.links)\n",
    "#         print(content_collection.code_count)\n",
    "#         print(content_collection.image_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content preprocessing patterns\n",
    "\n",
    "def remove_non_printable(s):\n",
    "    return ''.join(c for c in s if c not in string.printable)\n",
    "\n",
    "def remove_words_containing_string(text, string):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Create a regular expression pattern to match the string\n",
    "    pattern = re.compile(string)\n",
    "    # Filter out the words that contain the string\n",
    "    filtered_words = [word for word in words if not pattern.search(word)]\n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_normal_post(text, remove_code=False):\n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in text:\n",
    "                text = text.replace(tool_keyword, '')\n",
    "\n",
    "    text = re.sub(regex_code, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "    text = re.sub(regex_char, ' ', text)\n",
    "    \n",
    "    word_list = []\n",
    "    for word in text.split():\n",
    "        if english.check(word):\n",
    "            word_list.append(word)\n",
    "            \n",
    "    text = ' '.join(preprocess_string(' '.join(word_list)))\n",
    "    return text\n",
    "\n",
    "def preprocess_style_post(text, remove_code=False):          \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    remove_tags = ['script', 'style']\n",
    "    remove_tags.append('code') if remove_code else None\n",
    "    for tag in soup(remove_tags):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    text = preprocess_normal_post(text, remove_code)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_answer_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "\n",
    "    df_issues.at[index, 'Solution_body'] = row['Comment_body']\n",
    "    df_issues.at[index, 'Solution_score_count'] = row['Comment_score_count']\n",
    "\n",
    "del df_issues['Issue_title']\n",
    "del df_issues['Issue_body']\n",
    "del df_issues['Issue_link']\n",
    "del df_issues['Issue_created_time']\n",
    "del df_issues['Issue_comment_count']\n",
    "del df_issues['Issue_score_count']\n",
    "del df_issues['Issue_closed_time']\n",
    "del df_issues['Issue_repo_issue_count']\n",
    "del df_issues['Issue_repo_star_count']\n",
    "del df_issues['Issue_repo_watch_count']\n",
    "del df_issues['Issue_repo_fork_count']\n",
    "del df_issues['Issue_repo_contributor_count']\n",
    "del df_issues['Issue_self_closed']\n",
    "\n",
    "del df_issues['Comment_body']\n",
    "del df_issues['Comment_score_count']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "del df_questions['Question_title']\n",
    "del df_questions['Question_body']\n",
    "del df_questions['Question_link']\n",
    "del df_questions['Question_created_time']\n",
    "del df_questions['Question_last_edit_time']\n",
    "del df_questions['Question_answer_count']\n",
    "del df_questions['Question_comment_count']\n",
    "del df_questions['Question_score_count']\n",
    "del df_questions['Question_closed_time']\n",
    "del df_questions['Question_view_count']\n",
    "del df_questions['Question_favorite_count']\n",
    "del df_questions['Question_self_closed']\n",
    "\n",
    "del df_questions['Answer_body']\n",
    "del df_questions['Answer_comment_count']\n",
    "del df_questions['Answer_last_edit_time']\n",
    "del df_questions['Answer_comment_body']\n",
    "del df_questions['Answer_score_count']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1333676/1106343772.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Platform'] == 'Stack Overflow') or ((row['Platform'] == 'Tool-specific') and (row['Tool'] in style_post_tools)):\n",
    "        content = preprocess_style_post(row['Challenge_title']) + ' ' + preprocess_style_post(row['Challenge_body'])\n",
    "    else:\n",
    "        content = preprocess_normal_post(row['Challenge_title']) + ' ' + preprocess_normal_post(row['Challenge_body'])\n",
    "    df.at[index, 'Challenge_original_content'] = content\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 16720 tokens. Please reduce the length of the messages. on post https://github.com/kedro-org/kedro/issues/308\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 42298 tokens. Please reduce the length of the messages. on post https://github.com/Azure/MachineLearningNotebooks/issues/1668\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 16742 tokens. Please reduce the length of the messages. on post https://stackoverflow.com/questions/73624005\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 52631 tokens. Please reduce the length of the messages. on post https://community.wandb.ai/t/oserror-could-not-find-a-suitable-tls-ca-certificate/3913\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 19521 tokens. Please reduce the length of the messages. on post https://discuss.dvc.org/t/mlem-modal-nanogpt-blog-code/1657\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 162573 tokens. Please reduce the length of the messages. on post https://groups.google.com/g/mlflow-users/c/GrCd-t0gx8U\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 99:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    if pd.notna(row['Challenge_gpt_summary_original_content']):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + ' Body: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-4',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary_original_content'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1333676/1106343772.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Platform'] == 'Stack Overflow') or ((row['Platform'] == 'Tool-specific') and (row['Tool'] in style_post_tools)):\n",
    "        df.at[index, 'Challenge_gpt_summary_preprocessed_content'] = preprocess_style_post(row['Challenge_gpt_summary_original_content'])\n",
    "    else:\n",
    "        df.at[index, 'Challenge_gpt_summary_preprocessed_content'] = preprocess_normal_post(row['Challenge_gpt_summary_original_content'])\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1333676/1106343772.py:38: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Platform'] == 'Stack Overflow') or ((row['Platform'] == 'Tool-specific') and (row['Tool'] in style_post_tools)):\n",
    "        content = preprocess_style_post(row['Challenge_title'], remove_code=True) + ' ' + preprocess_style_post(row['Challenge_body'], remove_code=True)\n",
    "    else:\n",
    "        content = preprocess_normal_post(row['Challenge_title'], remove_code=True) + ' ' + preprocess_normal_post(row['Challenge_body'], remove_code=True)\n",
    "        \n",
    "    df.at[index, 'Challenge_preprocessed_content'] = content\n",
    "        \n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove custom stop words from challenges and solutions\n",
    "\n",
    "stop_words_custom = [\n",
    "    'abil',\n",
    "    'abl',\n",
    "    'abnorm',\n",
    "    'accid',\n",
    "    'achiev',\n",
    "    'acknowledg',\n",
    "    'activ',\n",
    "    'actual',\n",
    "    'ad',\n",
    "    'addition',\n",
    "    'admit',\n",
    "    'advis',\n",
    "    'alright',\n",
    "    'altern',\n",
    "    'amaz',\n",
    "    'announc',\n",
    "    'anomali',\n",
    "    'answer',\n",
    "    'anomali'\n",
    "    'api',\n",
    "    'app',\n",
    "    'appear',\n",
    "    'applic',\n",
    "    'appreci',\n",
    "    'approach',\n",
    "    'appropri',\n",
    "    'aris',\n",
    "    'artifici',\n",
    "    'ask',\n",
    "    'assist',\n",
    "    'assum',\n",
    "    'astonish',\n",
    "    'attempt',\n",
    "    'attent',\n",
    "    'author',\n",
    "    'avail',\n",
    "    'avoid',\n",
    "    'aw',\n",
    "    'awesom',\n",
    "    'azur',\n",
    "    'bad',\n",
    "    'basic',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'benefici',\n",
    "    'benefit',\n",
    "    'best',\n",
    "    'better',\n",
    "    'blog',\n",
    "    'bring',\n",
    "    'bug',\n",
    "    'case',\n",
    "    'categori',\n",
    "    'cau',\n",
    "    'caus',\n",
    "    'certain',\n",
    "    'challeng',\n",
    "    'chang',\n",
    "    'check',\n",
    "    'choic',\n",
    "    'choos',\n",
    "    'chose',\n",
    "    'chosen',\n",
    "    'clarif',\n",
    "    'clear',\n",
    "    'cloud',\n",
    "    'cloudera',\n",
    "    'code',\n",
    "    'com',\n",
    "    'command',\n",
    "    'commun',\n",
    "    'compani',\n",
    "    'concept',\n",
    "    'concern',\n",
    "    'concis',\n",
    "    'condit',\n",
    "    'confirm',\n",
    "    'confus',\n",
    "    'consid',\n",
    "    'consist',\n",
    "    'consciou',\n",
    "    'consult',\n",
    "    'contact',\n",
    "    'contain',\n",
    "    'content',\n",
    "    'continu',\n",
    "    'correct',\n",
    "    'correctli',\n",
    "    'correspond',\n",
    "    'couldn',\n",
    "    'crash',\n",
    "    'curiou',\n",
    "    'current',\n",
    "    'custom',\n",
    "    'decid',\n",
    "    'deep',\n",
    "    'deliber',\n",
    "    'demand',\n",
    "    'demo',\n",
    "    'deni',\n",
    "    'depict',\n",
    "    'describ',\n",
    "    'desir',\n",
    "    'despit',\n",
    "    'detail',\n",
    "    'develop',\n",
    "    'differ',\n",
    "    'differenti',\n",
    "    'difficult',\n",
    "    'difficulti',\n",
    "    'discov',\n",
    "    'discrep',\n",
    "    'discuss',\n",
    "    'dislik',\n",
    "    'distinguish',\n",
    "    'easi',\n",
    "    'east',\n",
    "    'effect',\n",
    "    'emerg',\n",
    "    'encount',\n",
    "    # 'end',\n",
    "    'engin',\n",
    "    'enquiri',\n",
    "    'ensur',\n",
    "    'error',\n",
    "    'especi',\n",
    "    'exampl',\n",
    "    'except',\n",
    "    'exception',\n",
    "    'excit',\n",
    "    'exist',\n",
    "    'expect',\n",
    "    'experi',\n",
    "    'expert',\n",
    "    'explain',\n",
    "    'express',\n",
    "    'eventu',\n",
    "    'evid',\n",
    "    'dai',\n",
    "    'databrick',\n",
    "    'def',\n",
    "    'direct',\n",
    "    'directli',\n",
    "    'domo',\n",
    "    'dont',\n",
    "    'face',\n",
    "    'fact',\n",
    "    'fascin',\n",
    "    'fail',\n",
    "    'failur',\n",
    "    'fairli',\n",
    "    'fals',\n",
    "    'fanci',\n",
    "    'far',\n",
    "    'fault',\n",
    "    'favorit',\n",
    "    'favourit',\n",
    "    'feedback',\n",
    "    'feel',\n",
    "    'final',\n",
    "    'find',\n",
    "    'fine',\n",
    "    'firstli',\n",
    "    'fix',\n",
    "    'float',\n",
    "    'follow',\n",
    "    'form',\n",
    "    'frustrat',\n",
    "    'futur',\n",
    "    'gcp',\n",
    "    'get',\n",
    "    'give',\n",
    "    'given',\n",
    "    'go',\n",
    "    'good',\n",
    "    'googl',\n",
    "    'got',\n",
    "    'guarante',\n",
    "    'guidanc',\n",
    "    'guidelin'\n",
    "    'handl',\n",
    "    'hang',\n",
    "    'happen',\n",
    "    'happi',\n",
    "    'hard',\n",
    "    'harm',\n",
    "    'have',\n",
    "    'hear',\n",
    "    'hei',\n",
    "    'hello',\n",
    "    'help',\n",
    "    'highlight',\n",
    "    'hinder',\n",
    "    'horribl',\n",
    "    'hour',\n",
    "    'ibm',\n",
    "    'immedi',\n",
    "    'impli',\n",
    "    'implic',\n",
    "    'improv',\n",
    "    'includ',\n",
    "    'incorrect',\n",
    "    'incorrectli',\n",
    "    'incred',\n",
    "    'indic',\n",
    "    'individu',\n",
    "    'info',\n",
    "    'inform',\n",
    "    'inner',\n",
    "    'inquir',\n",
    "    'inquiri',\n",
    "    'insid',\n",
    "    'insight',\n",
    "    'instead',\n",
    "    'instruct',\n",
    "    'int',\n",
    "    'intellig',\n",
    "    'interest',\n",
    "    'introduc',\n",
    "    'invalid',\n",
    "    'investig',\n",
    "    'invit',\n",
    "    'involv',\n",
    "    'issu',\n",
    "    'java',\n",
    "    'join',\n",
    "    'keep',\n",
    "    'kind',\n",
    "    'know',\n",
    "    'known',\n",
    "    'lack',\n",
    "    'lastli',\n",
    "    'late',\n",
    "    'later',\n",
    "    'latest',\n",
    "    'lazi',\n",
    "    'lead',\n",
    "    'learn',\n",
    "    'let',\n",
    "    'like',\n",
    "    'long',\n",
    "    'look',\n",
    "    'lot',\n",
    "    'machin',\n",
    "    'malfunct',\n",
    "    'make',\n",
    "    'mail',\n",
    "    'main',\n",
    "    'major',\n",
    "    'manag',\n",
    "    'manner',\n",
    "    'manual',\n",
    "    'mark',\n",
    "    'marvel',\n",
    "    'max',\n",
    "    'mayb',\n",
    "    'mean',\n",
    "    'meaning',\n",
    "    'meaningfulli',\n",
    "    'meaningless',\n",
    "    'meantim',\n",
    "    'mention',\n",
    "    'method',\n",
    "    'min',\n",
    "    'mind',\n",
    "    'mistak',\n",
    "    'mistakenli',\n",
    "    'month',\n",
    "    'name',\n",
    "    'near',\n",
    "    'necessari',\n",
    "    'need',\n",
    "    'neg',\n",
    "    'network',\n",
    "    'neural',\n",
    "    'new',\n",
    "    'non',\n",
    "    'north',\n",
    "    'note',\n",
    "    'notic',\n",
    "    'number',\n",
    "    'obtain',\n",
    "    'occas',\n",
    "    'occasion',\n",
    "    'occur',\n",
    "    'offer',\n",
    "    'offici',\n",
    "    'old',\n",
    "    'opinion',\n",
    "    'option',\n",
    "    'org',\n",
    "    'organ',\n",
    "    'outsid',\n",
    "    'overal',\n",
    "    'own',\n",
    "    # 'open',\n",
    "    'oracl',\n",
    "    'ought',\n",
    "    'outcom',\n",
    "    'part',\n",
    "    'particip',\n",
    "    'particular',\n",
    "    'particularli',\n",
    "    'past',\n",
    "    'perceive',\n",
    "    'perform',\n",
    "    'permit',\n",
    "    'person',\n",
    "    'perspect',\n",
    "    'place',\n",
    "    'plan',\n",
    "    'point',\n",
    "    'pointless',\n",
    "    'poor',\n",
    "    'posit',\n",
    "    'possibl',\n",
    "    'post',\n",
    "    'potenti',\n",
    "    'practic',\n",
    "    'pretty',\n",
    "    'prevent',\n",
    "    'previou',\n",
    "    'primari',\n",
    "    'problem',\n",
    "    'product',\n",
    "    'program',\n",
    "    'project',\n",
    "    'proper',\n",
    "    'propos',\n",
    "    'provid',\n",
    "    'purpos',\n",
    "    'python',\n",
    "    'question',\n",
    "    'randomli',\n",
    "    'real',\n",
    "    'realize',\n",
    "    'recent',\n",
    "    'recognize',\n",
    "    'recommend',\n",
    "    'refer',\n",
    "    'regard',\n",
    "    'relat',\n",
    "    'relev',\n",
    "    'repeatedli',\n",
    "    'requir',\n",
    "    'research',\n",
    "    'resolv',\n",
    "    'respond',\n",
    "    'result',\n",
    "    'return',\n",
    "    'right',\n",
    "    'rightli',\n",
    "    'satisfi',\n",
    "    'saw',\n",
    "    'scenario',\n",
    "    'scienc',\n",
    "    'screenshot',\n",
    "    'script',\n",
    "    'second',\n",
    "    'secondli',\n",
    "    'see',\n",
    "    'seek',\n",
    "    'seen',\n",
    "    'self',\n",
    "    'shall',\n",
    "    'shan',\n",
    "    'shock',\n",
    "    'shouldn',\n",
    "    'show',\n",
    "    'similar',\n",
    "    'simpl',\n",
    "    'situat',\n",
    "    'snippet',\n",
    "    'snowflak',\n",
    "    'solut',\n",
    "    'solv',\n",
    "    'soon',\n",
    "    'sound',\n",
    "    'sourc',\n",
    "    'south',\n",
    "    'special',\n",
    "    'specif',\n",
    "    'spend',\n",
    "    'spent',\n",
    "    # 'start',\n",
    "    'startl',\n",
    "    'straight',\n",
    "    'straightforward',\n",
    "    'strang',\n",
    "    'string',\n",
    "    'struggl',\n",
    "    'stuck',\n",
    "    'studi',\n",
    "    'studio',\n",
    "    'stun',\n",
    "    'succe',\n",
    "    'success',\n",
    "    'successfulli',\n",
    "    'suggest',\n",
    "    'summari',\n",
    "    'super',\n",
    "    'surpris',\n",
    "    'support',\n",
    "    'suppos',\n",
    "    'sure',\n",
    "    'survei',\n",
    "    'suspect',\n",
    "    'take',\n",
    "    'talk',\n",
    "    'task',\n",
    "    'technic',\n",
    "    'technolog',\n",
    "    'tell',\n",
    "    'temporari',\n",
    "    'temporarili',\n",
    "    'term',\n",
    "    'text',\n",
    "    'thank',\n",
    "    'thing',\n",
    "    'think',\n",
    "    'thirdli',\n",
    "    'thought',\n",
    "    'time',\n",
    "    'tool',\n",
    "    'topic',\n",
    "    'total',\n",
    "    'tri',\n",
    "    'troubl',\n",
    "    'true',\n",
    "    'truli',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'tutori',\n",
    "    'unabl',\n",
    "    'unclear',\n",
    "    'underli',\n",
    "    'understand',\n",
    "    'unexpect',\n",
    "    'unknown',\n",
    "    'unsur',\n",
    "    'upcom',\n",
    "    'us',\n",
    "    'user',\n",
    "    'usual',\n",
    "    'valid',\n",
    "    'valu',\n",
    "    'variant',\n",
    "    'vertex',\n",
    "    'versu',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'wai',\n",
    "    'want',\n",
    "    'weak',\n",
    "    'websit',\n",
    "    'weird',\n",
    "    'west',\n",
    "    'will',\n",
    "    'word',\n",
    "    'worst',\n",
    "    'won',\n",
    "    'wonder',\n",
    "    'work',\n",
    "    'wors',\n",
    "    'worth',\n",
    "    'wouldn',\n",
    "    'written',\n",
    "    'wrong',\n",
    "    'wrongli',\n",
    "    'www',\n",
    "    'xgboost',\n",
    "    'ye',\n",
    "] \n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Challenge_original_content']\n",
    "    for stop_word in stop_words_custom:\n",
    "        text = remove_words_containing_string(text, stop_word)\n",
    "    df.at[index, 'Challenge_original_content'] = text\n",
    "                \n",
    "    text = row['Challenge_preprocessed_content']\n",
    "    for stop_word in stop_words_custom:\n",
    "        text = remove_words_containing_string(text, stop_word)\n",
    "    df.at[index, 'Challenge_preprocessed_content'] = text\n",
    "\n",
    "    text = row['Challenge_gpt_summary_preprocessed_content']\n",
    "    for stop_word in stop_words_custom:\n",
    "        text = remove_words_containing_string(text, stop_word)\n",
    "    df.at[index, 'Challenge_gpt_summary_preprocessed_content'] = text\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
