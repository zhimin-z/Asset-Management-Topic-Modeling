{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import openai\n",
    "import textstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The significance level indicates the probability of rejecting the null hypothesis when it is true.\n",
    "alpha = 0.05\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result = '../../Result'\n",
    "\n",
    "path_result_rq1 = os.path.join(path_result, 'RQ1')\n",
    "path_code_rq1 = os.path.join('..', 'RQ1')\n",
    "\n",
    "path_general_topic = os.path.join(path_code_rq1, 'General Topic Modeling')\n",
    "path_special_topic = os.path.join(path_code_rq1, 'Special Topic Modeling')\n",
    "\n",
    "path_anomaly = os.path.join(path_special_topic, 'Anomaly')\n",
    "path_root_cause = os.path.join(path_special_topic, 'Root Cause')\n",
    "path_solution = os.path.join(path_special_topic, 'Solution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Kubernetes Services - Discusses the deployment and modeling of services in Kubernetes, a popular container orchestration platform.\n",
      "Topic 1: SQL Database Operations - Covers the import, export, and manipulation of data in SQL databases, including handling CSV datasets.\n",
      "Topic 2: Data Pipelines - Focuses on the creation, execution, and management of data pipelines for efficient data processing.\n",
      "Topic 3: Git Version Control - Discusses the use of Git for version control, including handling files and repositories on GitHub.\n",
      "Topic 4: Dockerfile Management - Covers the creation, execution, and management of Dockerfiles, which are scripts that define the environment inside Docker containers.\n",
      "Topic 5: Conda Environments - Discusses the creation and management of Conda environments, which are isolated spaces where packages can be installed without interference.\n",
      "Topic 6: Workspace Configuration - Covers the setup and management of workspaces in software development environments like studios or workbenches.\n",
      "Topic 7: Data Visualization - Discusses the creation of various types of charts and plots for data visualization, including bar charts and line plots.\n",
      "Topic 8: PyTorch Operations - Covers the use of PyTorch, a popular machine learning library, including running and training models on different hardware like GPUs.\n",
      "Topic 9: Job Properties - Discusses the creation and management of jobs in a software system, including handling job properties and statuses.\n",
      "Topic 10: Hyperparameter Tuning - Focuses on the optimization of hyperparameters, which are parameters that are set before the learning process begins in machine learning models.\n",
      "Topic 11: Artifact Management - Covers the storage, versioning, and handling of artifacts, which are by-products of the software development process.\n",
      "Topic 12: TensorFlow Services - Discusses the use of TensorFlow, a popular machine learning library, including creating and invoking models.\n",
      "Topic 13: PySpark Jobs - Covers the creation and execution of jobs in PySpark, a Python library for Apache Spark, a big data processing engine.\n",
      "Topic 14: Model Training - Discusses the process of training machine learning models, including different training algorithms and techniques.\n",
      "Topic 15: Data Prediction - Covers the use of machine learning models for making predictions based on data.\n",
      "Topic 16: JupyterLab Operations - Discusses the use of JupyterLab, a web-based interactive development environment for Jupyter notebooks.\n",
      "Topic 17: Video Processing with AutoML - Covers the use of AutoML for video recognition, tracking, and other video processing tasks.\n",
      "Topic 18: Model File Management - Discusses the import, export, and saving of machine learning model files.\n",
      "Topic 19: Data Transformation - Covers the process of transforming data for machine learning tasks, including batch transformations.\n",
      "Topic 20: Forecasting with AutoML - Discusses the use of AutoML for forecasting tasks, including data prediction and model selection.\n",
      "Topic 21: Model Deployment - Covers the process of deploying machine learning models, including handling dependencies and configurations.\n",
      "Topic 22: Dialogflow Operations - Discusses the creation and management of agents in Dialogflow, a platform for building conversational interfaces.\n",
      "Topic 23: Company Websites - Discusses various aspects of company websites, including contributions, listings, and analytics.\n",
      "Topic 24: Cluster Computing - Covers the use of computing clusters for high-performance computing tasks.\n",
      "Topic 25: Instance Management - Discusses various operations related to instances, including renaming, enabling, and removing.\n",
      "Topic 26: TensorBoard Operations - Covers the use of TensorBoard, a visualization toolkit for TensorFlow.\n",
      "Topic 27: Service Quotas - Discusses the management of service quotas in cloud computing, including requests and increases.\n",
      "Topic 28: Language Translation - Covers the process of translating text between different languages.\n",
      "Topic 29: Document Metadata - Discusses the handling of metadata in documents, including versioning and dependencies.\n",
      "Topic 30: Lifecycle Configuration - Covers the configuration of lifecycles in software development, including instance lifecycles.\n",
      "Topic 31: Run Migration - Discusses the process of migrating runs, including tracking and team entities.\n",
      "Topic 32: Dependency Management - Covers the management of dependencies in software development, including conflict resolution and analysis.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that all summaries are different from each other.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_general_topic, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Kubernetes Services',\n",
       " 1: 'SQL Database',\n",
       " 2: 'Data Pipelines',\n",
       " 3: 'Git Version Control',\n",
       " 4: 'Dockerfile',\n",
       " 5: 'Conda Environment',\n",
       " 6: 'Workspace Configuration',\n",
       " 7: 'Data Visualization',\n",
       " 8: 'PyTorch',\n",
       " 9: 'Job Properties',\n",
       " 10: 'Hyperparameter Tuning',\n",
       " 11: 'Artifacts',\n",
       " 12: 'TensorFlow',\n",
       " 13: 'PySpark',\n",
       " 14: 'Model Training',\n",
       " 15: 'Data Prediction',\n",
       " 16: 'JupyterLab',\n",
       " 17: 'Video Processing',\n",
       " 18: 'Model Management',\n",
       " 19: 'Data Transformation',\n",
       " 20: 'Forecasting',\n",
       " 21: 'Model Deployment',\n",
       " 22: 'Dialogflow',\n",
       " 23: 'Company Websites',\n",
       " 24: 'Computing Clusters',\n",
       " 25: 'Instance Management',\n",
       " 26: 'TensorBoard',\n",
       " 27: 'Quota Management',\n",
       " 28: 'Language Translation',\n",
       " 29: 'Documentation',\n",
       " 30: 'Lifecycle Configuration',\n",
       " 31: 'Run Migration',\n",
       " 32: 'Dependency Management'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = '''Topic 0: Kubernetes Services - Discusses the deployment and modeling of services in Kubernetes, a popular container orchestration platform.\n",
    "Topic 1: SQL Database Operations - Covers the import, export, and manipulation of data in SQL databases, including handling CSV datasets.\n",
    "Topic 2: Data Pipelines - Focuses on the creation, execution, and management of data pipelines for efficient data processing.\n",
    "Topic 3: Git Version Control - Discusses the use of Git for version control, including handling files and repositories on GitHub.\n",
    "Topic 4: Dockerfile Management - Covers the creation, execution, and management of Dockerfiles, which are scripts that define the environment inside Docker containers.\n",
    "Topic 5: Conda Environments - Discusses the creation and management of Conda environments, which are isolated spaces where packages can be installed without interference.\n",
    "Topic 6: Workspace Configuration - Covers the setup and management of workspaces in software development environments like studios or workbenches.\n",
    "Topic 7: Data Visualization - Discusses the creation of various types of charts and plots for data visualization, including bar charts and line plots.\n",
    "Topic 8: PyTorch Operations - Covers the use of PyTorch, a popular machine learning library, including running and training models on different hardware like GPUs.\n",
    "Topic 9: Job Properties - Discusses the creation and management of jobs in a software system, including handling job properties and statuses.\n",
    "Topic 10: Hyperparameter Tuning - Focuses on the optimization of hyperparameters, which are parameters that are set before the learning process begins in machine learning models.\n",
    "Topic 11: Artifact Management - Covers the storage, versioning, and handling of artifacts, which are by-products of the software development process.\n",
    "Topic 12: TensorFlow Services - Discusses the use of TensorFlow, a popular machine learning library, including creating and invoking models.\n",
    "Topic 13: PySpark Jobs - Covers the creation and execution of jobs in PySpark, a Python library for Apache Spark, a big data processing engine.\n",
    "Topic 14: Model Training - Discusses the process of training machine learning models, including different training algorithms and techniques.\n",
    "Topic 15: Data Prediction - Covers the use of machine learning models for making predictions based on data.\n",
    "Topic 16: JupyterLab Operations - Discusses the use of JupyterLab, a web-based interactive development environment for Jupyter notebooks.\n",
    "Topic 17: Video Processing with AutoML - Covers the use of AutoML for video recognition, tracking, and other video processing tasks.\n",
    "Topic 18: Model File Management - Discusses the import, export, and saving of machine learning model files.\n",
    "Topic 19: Data Transformation - Covers the process of transforming data for machine learning tasks, including batch transformations.\n",
    "Topic 20: Forecasting with AutoML - Discusses the use of AutoML for forecasting tasks, including data prediction and model selection.\n",
    "Topic 21: Model Deployment - Covers the process of deploying machine learning models, including handling dependencies and configurations.\n",
    "Topic 22: Dialogflow Operations - Discusses the creation and management of agents in Dialogflow, a platform for building conversational interfaces.\n",
    "Topic 23: Company Websites - Discusses various aspects of company websites, including contributions, listings, and analytics.\n",
    "Topic 24: Cluster Computing - Covers the use of computing clusters for high-performance computing tasks.\n",
    "Topic 25: Instance Management - Discusses various operations related to instances, including renaming, enabling, and removing.\n",
    "Topic 26: TensorBoard Operations - Covers the use of TensorBoard, a visualization toolkit for TensorFlow.\n",
    "Topic 27: Service Quotas - Discusses the management of service quotas in cloud computing, including requests and increases.\n",
    "Topic 28: Language Translation - Covers the process of translating text between different languages.\n",
    "Topic 29: Document Metadata - Discusses the handling of metadata in documents, including versioning and dependencies.\n",
    "Topic 30: Lifecycle Configuration - Covers the configuration of lifecycles in software development, including instance lifecycles.\n",
    "Topic 31: Run Migration - Discusses the process of migrating runs, including tracking and team entities.\n",
    "Topic 32: Dependency Management - Covers the management of dependencies in software development, including conflict resolution and analysis.'''\n",
    "\n",
    "index = 0\n",
    "topic_dict = {}\n",
    "\n",
    "for topic in topics.split('\\n'):\n",
    "    topic_dict[index] = topic.split('-')[0].split(':')[-1].strip()\n",
    "    index += 1\n",
    "    \n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_mapping = {\n",
    "#     0: ('Pipeline Configuration', 'Implementing software pipelines for efficient workflow management.'),\n",
    "#     1: ('Package Installation', 'Installing and managing software packages using Conda and Pip.'),\n",
    "#     2: ('Hyperparameter Optimization', 'Tuning and optimizing hyperparameters for machine learning models.'),\n",
    "#     3: ('Docker Deployment', 'Creating and running Docker images for software deployment.'),\n",
    "#     4: ('Model Development', 'Training machine learning models to predict outcomes based on data.'),\n",
    "#     5: ('Lambda Endpoint', 'Implementing and invoking Lambda endpoints for serverless computing.'),\n",
    "#     6: ('Jupyter Configuration', 'Running and connecting to Jupyter notebooks for data analysis and visualization.'),\n",
    "#     7: ('Data Visualization', 'Creating various types of plots and charts for data visualization.'),\n",
    "#     8: ('Git Versioning', 'Managing and collaborating on software projects using Git repositories.'),\n",
    "#     9: ('TensorFlow Model Development', 'Training, deploying, and converting TensorFlow models for machine learning.'),\n",
    "#     10: ('GPU Acceleration', 'Utilizing GPUs for faster model training and processing.'),\n",
    "#     11: ('Spark Configuration', 'Configuring and using Apache Spark for big data processing.'),\n",
    "#     12: ('Role Management', 'Managing and assigning permissions for user roles in software projects.'),\n",
    "#     13: ('Artifact Management', 'Storing and accessing software artifacts such as code and data.'),\n",
    "#     14: ('Notebook Lifecycle Management', 'Configuring and managing the lifecycle of Jupyter notebooks.'),\n",
    "#     # 15: ('AutoML Forecasting', 'Using automated machine learning for forecasting and prediction.'),\n",
    "#     16: ('Data Labeling', 'Manually labeling data for machine learning models.'),\n",
    "#     17: ('Cluster Computing', 'Running software pipelines on high-performance computing clusters.'),\n",
    "#     18: ('Pandas Dataframe', 'Working with and manipulating data using Pandas dataframes.'),\n",
    "#     # 19: ('Communication', 'Utilizing various technologies for communication and data exchange.'),\n",
    "#     20: ('Database Connection', 'Connecting to and managing databases using ODBC and SQL.'),\n",
    "#     21: ('Datastore', 'Creating, uploading, and accessing datasets in data storage services.'),\n",
    "#     22: ('Model Troubleshooting', 'Troubleshooting and resolving errors in machine learning models.'),\n",
    "#     23: ('Model Deployment', 'Deploying machine learning models as endpoints for use in applications.'),\n",
    "#     24: ('Web Serving', 'Deploying web services for use in applications.'),\n",
    "#     # 25: ('Dialogflow Configuration', 'Creating and using chatbots and conversational agents using Dialogflow.'),\n",
    "#     26: ('YAML Configuration', 'Configuring software using YAML files.'),\n",
    "#     27: ('Columnar Data', 'Working with and manipulating data columns in datasets.'),\n",
    "#     # 28: ('Speech Recognition', 'Using speech recognition APIs and SDKs for audio transcription.'),\n",
    "#     29: ('Tensorboard Logging', 'Logging and visualizing machine learning experiments using Tensorboard.'),\n",
    "#     30: ('ModelChimp Configuration', 'Managing and tracking machine learning models using ModelChimp.'),\n",
    "#     31: ('PyTorch Model Development', 'Training and logging PyTorch models for machine learning.'),\n",
    "#     32: ('Logging', 'Logging and visualizing software output and data.'),\n",
    "#     33: ('SSH Connection', 'Connecting to and managing remote servers using SSH.'),\n",
    "#     # 34: ('Translation', 'Translating text and language using cloud APIs and services.'),\n",
    "#     # 35: ('Object Detection', 'Training and using machine learning models for object detection and tracking.'),\n",
    "#     36: ('JSON File', 'Working with and manipulating data in JSON format.'),\n",
    "#     # 37: ('Computer Vision', 'Using cloud vision APIs for image and text recognition.'),\n",
    "#     38: ('Kubernetes Deployment', 'Deploying and managing machine learning models using Kubernetes.'),\n",
    "#     39: ('Model Serving', 'Deploying and testing machine learning models as endpoints.'),\n",
    "#     40: ('Flask Configuration', 'Building and deploying Flask servers for web applications.'),\n",
    "#     41: ('File Management', 'Managing and organizing files and directories in software projects.'),\n",
    "#     42: ('Account Management', 'Managing and troubleshooting user accounts in software projects.'),\n",
    "#     # 43: ('Document Processing', 'Parsing and extracting data from PDF and other document file formats.'),\n",
    "#     44: ('Model Training', 'Training and evaluating machine learning models.'),\n",
    "#     45: ('Quota Management', 'Managing and requesting resource quotas for software projects.'),\n",
    "#     46: ('Job Management', 'Managing and troubleshooting software jobs and scheduling.'),\n",
    "#     47: ('Model Versioning', 'Managing and tracking versions of machine learning models.'),\n",
    "#     48: ('TensorFlow Configuration', 'Installing and configuring TensorFlow for machine learning.'),\n",
    "#     49: ('Workspace Management', 'Managing and configuring software workspaces.'),\n",
    "#     50: ('Terraform Configuration', 'Managing and deploying infrastructure using Terraform.')\n",
    "# }\n",
    "\n",
    "{0: 'Kubernetes Services',\n",
    " 1: 'SQL Database',\n",
    " 2: 'Data Pipelines',\n",
    " 3: 'Git Version Control',\n",
    " 4: 'Dockerfile',\n",
    " 5: 'Conda Environment',\n",
    " 6: 'Workspace Configuration',\n",
    " 7: 'Data Visualization',\n",
    " 8: 'PyTorch',\n",
    " 9: 'Job Properties',\n",
    " 10: 'Hyperparameter Tuning',\n",
    " 11: 'Artifacts',\n",
    " 12: 'TensorFlow',\n",
    " 13: 'PySpark',\n",
    " 14: 'Model Training',\n",
    " 15: 'Data Prediction',\n",
    " 16: 'JupyterLab',\n",
    " 17: 'Video Processing',\n",
    " 18: 'Model Management',\n",
    " 19: 'Data Transformation',\n",
    " 20: 'Forecasting',\n",
    " 21: 'Model Deployment',\n",
    " 22: 'Dialogflow',\n",
    " 23: 'Company Websites',\n",
    " 24: 'Computing Clusters',\n",
    " 25: 'Instance Management',\n",
    " 26: 'TensorBoard',\n",
    " 27: 'Quota Management',\n",
    " 28: 'Language Translation',\n",
    " 29: 'Documentation',\n",
    " 30: 'Lifecycle Configuration',\n",
    " 31: 'Run Migration',\n",
    " 32: 'Dependency Management'}\n",
    "\n",
    "macro_topic_mapping_inverse = {\n",
    "    # These topics are all related to the management of parallel computing resources.\n",
    "    1: ('Compute Management', ['Cluster Computing', 'GPU Acceleration', 'Model Training', 'Spark Configuration', 'TensorFlow Configuration']),\n",
    "    # These topics are all related to the management of logs and metrics.\n",
    "    2: ('Performance Management', ['Hyperparameter Optimization', 'Quota Management', 'Logging', 'Tensorboard Logging']),\n",
    "    # These topics are all related to the management of services.\n",
    "    3: ('Service Management', ['Flask Configuration', 'Lambda Endpoint', 'Model Serving', 'Web Serving']),\n",
    "    # These topics are all related to the management of data and datasets.\n",
    "    4: ('Data Management', ['Artifact Management', 'Columnar Data', 'Data Labeling', 'Data Visualization', 'Database Connection', 'Datastore', 'File Management', 'JSON File', 'Pandas Dataframe']),\n",
    "    # These topics are all related to the management of machine learning models.\n",
    "    5: ('Model Management', ['Model Deployment', 'Model Development', 'Model Training', 'Model Troubleshooting', 'Model Versioning', 'ModelChimp Configuration', 'PyTorch Model Development', 'TensorFlow Model Development']),\n",
    "    # These topics are all related to the management of packages and distributions.\n",
    "    6: ('Infrastructure Management', ['Docker Deployment', 'Jupyter Configuration', 'Kubernetes Deployment', 'Notebook Lifecycle Management', 'Package Installation', 'Terraform Configuration', 'Workspace Management', 'YAML Configuration']),\n",
    "    # These topics are all related to the management of pipelines.\n",
    "    7: ('Lifecycle Management', ['Job Management', 'Pipeline Configuration']),\n",
    "    # These topics are all related to the management of permissions and connectivity.\n",
    "    8: ('Access Management', ['Account Management', 'Database Connection', 'Role Management', 'SSH Connection']),\n",
    "    # These topics are all related to the management of source code.\n",
    "    9: ('Code Management', ['Git Versioning']),\n",
    "}\n",
    "\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_index_mapping = {}\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    macro_topic_index_mapping[macro_topic] = key\n",
    "    for topic in topics:\n",
    "        macro_topic_mapping[topic] = macro_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if exiting topics and chosen topics are the same\n",
    "\n",
    "# chosen_topics = set()\n",
    "# for topic in topic_mapping:\n",
    "#     chosen_topics.add(topic_mapping[topic][0])\n",
    "# existing_topics = set(macro_topic_mapping.keys())\n",
    "\n",
    "# print(len(topic_mapping) == len(macro_topic_mapping))\n",
    "# print(existing_topics.difference(chosen_topics))\n",
    "# print(chosen_topics.difference(existing_topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compute Management</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Performance Management</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Service Management</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Management</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Infrastructure Management</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lifecycle Management</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Access Management</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Code Management</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Topic  Number\n",
       "0         Compute Management     566\n",
       "1     Performance Management     708\n",
       "2         Service Management    1060\n",
       "3            Data Management    1752\n",
       "4           Model Management    2268\n",
       "5  Infrastructure Management    1655\n",
       "6       Lifecycle Management     722\n",
       "7          Access Management     710\n",
       "8            Code Management     241"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'topics.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in topic_mapping:\n",
    "        topic = topic_mapping[row['Challenge_topic']][0]\n",
    "        df.at[index, 'Challenge_topic'] = topic\n",
    "        df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic]\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_general, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    entry = {\n",
    "        'Topic': macro_topic,\n",
    "        'Number': len(df[df[\"Challenge_topic_macro\"] == macro_topic])\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "df_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "df['Challenge_type'] = 'na'\n",
    "df['Challenge_summary'] = 'na'\n",
    "df['Challenge_root_cause'] = 'na'\n",
    "df['Challenge_solution'] = 'na'\n",
    "df.to_json(os.path.join(path_cardsorting, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_cardsorting, 'labels.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "df_info.to_json(os.path.join(path_general, 'Tool platform state info.json'), indent=4, orient='records')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_general, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add difficlty and text complexity metrics to challenges & solutions\n",
    "\n",
    "link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "df = pd.read_json(os.path.join(path_cardsorting, 'labels.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['Solution_word_count'] = np.nan\n",
    "df['Solution_sentence_count'] = np.nan\n",
    "df['Solution_readability'] = np.nan\n",
    "df['Solution_reading_time'] = np.nan\n",
    "df['Solution_link_count'] = np.nan\n",
    "\n",
    "df['Challenge_solved_time'] = np.nan\n",
    "df['Challenge_adjusted_solved_time'] = np.nan\n",
    "\n",
    "df['Challenge_created_time'] = pd.to_datetime(df['Challenge_created_time'])\n",
    "df['Challenge_closed_time'] = pd.to_datetime(df['Challenge_closed_time'])\n",
    "df['Challenge_last_edit_time'] = pd.to_datetime(df['Challenge_last_edit_time'])\n",
    "df['Solution_last_edit_time'] = pd.to_datetime(df['Solution_last_edit_time'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    challenge_content = row['Challenge_title'] + '.' + str(row['Challenge_body'])\n",
    "    df.at[index, 'Challenge_word_count'] = textstat.lexicon_count(challenge_content)\n",
    "    df.at[index, 'Challenge_sentence_count'] = textstat.sentence_count(challenge_content)\n",
    "    df.at[index, 'Challenge_readability'] = textstat.flesch_kincaid_grade(challenge_content)\n",
    "    df.at[index, 'Challenge_reading_time'] = textstat.reading_time(challenge_content)\n",
    "    \n",
    "    links = list(set(re.findall(link_pattern, challenge_content)))\n",
    "    df.at[index, 'Challenge_link_count'] = len(links)\n",
    "\n",
    "    solution_content = row['Solution_body']\n",
    "\n",
    "    if pd.notna(solution_content):\n",
    "        df.at[index, 'Solution_word_count'] = textstat.lexicon_count(solution_content)\n",
    "        df.at[index, 'Solution_sentence_count'] = textstat.sentence_count(solution_content)\n",
    "        df.at[index, 'Solution_readability'] = textstat.flesch_kincaid_grade(solution_content)\n",
    "        df.at[index, 'Solution_reading_time'] = textstat.reading_time(solution_content)\n",
    "        \n",
    "        links = list(set(re.findall(link_pattern, solution_content)))\n",
    "        df.at[index, 'Solution_link_count'] = len(links)\n",
    "        \n",
    "    creation_time = row['Challenge_created_time']\n",
    "    closed_time = row['Challenge_closed_time']\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "\n",
    "    creation_time = row['Challenge_last_edit_time'] if pd.notna(row['Challenge_last_edit_time']) else creation_time\n",
    "    closed_time = row['Solution_last_edit_time'] if pd.notna(row['Solution_last_edit_time']) else closed_time\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = df.at[index, 'Challenge_solved_time']\n",
    "\n",
    "df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "df['Challenge_answer_count'] = df['Challenge_answer_count'].fillna(0)\n",
    "df['Challenge_participation_count'] = df['Challenge_answer_count'] + df['Challenge_comment_count']\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_json(os.path.join(path_general, 'metrics.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize links in the posts\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "\n",
    "link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['Solution_body']):\n",
    "        continue\n",
    "    \n",
    "    link_docs = 0\n",
    "    link_tools = 0\n",
    "    link_issues = 0\n",
    "    link_patches = 0\n",
    "    link_tutorials = 0\n",
    "    link_examples = 0\n",
    "        \n",
    "    links = list(set(re.findall(link_pattern, row['Solution_body'])))\n",
    "    \n",
    "    for link in links:\n",
    "        if any([patch in link for patch in keywords_patch]):\n",
    "            link_patches += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            link_issues += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            link_tools += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            link_docs += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            link_tutorials += 1\n",
    "        else:\n",
    "            link_examples += 1\n",
    "                \n",
    "    df.at[index, 'Solution_link_docs'] = link_docs\n",
    "    df.at[index, 'Solution_link_issues'] = link_issues\n",
    "    df.at[index, 'Solution_link_patches'] = link_patches\n",
    "    df.at[index, 'Solution_link_tools'] = link_tools\n",
    "    df.at[index, 'Solution_link_tutorials'] = link_tutorials\n",
    "    df.at[index, 'Solution_link_examples'] = link_examples\n",
    "    \n",
    "df.to_json(os.path.join(path_general, 'metrics.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Count'] = 1\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_topic_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge view count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_view_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge score count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_score_count'] = df_topics['Challenge_score_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_score_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge favorite count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_favorite_count'] = df_topics['Challenge_favorite_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_favorite_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge answer count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_answer_count'] = df_topics['Challenge_answer_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_answer_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge comment count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_comment_count'] = df_topics['Challenge_comment_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_comment_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_participation_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_rq1, 'Challenge_participation_count_distribution.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
