{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import openai\n",
    "import random\n",
    "import enchant\n",
    "import textstat\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, strip_punctuation, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../Dataset'\n",
    "path_result = '../../Result'\n",
    "path_rq1 = os.path.join(path_result, 'RQ1')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "prompt_summary = '''Summarize the core idea of the post into a succinct sentence, suppressing verbosity such as \"The core idea of the post is that...\". Use only common English words rather than unique symbols found in the text.\\n###'''\n",
    "\n",
    "tools_keyword_mapping = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sagemaker', 'amazon', 'aws'],\n",
    "    'Azure Machine Learning': ['azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow', 'databricks'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex', 'google', 'gcp'],\n",
    "    'Weights & Biases': ['weights', 'biases', 'wandb']\n",
    "}\n",
    "\n",
    "keywords_image = {\n",
    "    \".jpg\", \n",
    "    \".png\", \n",
    "    \".jpeg\", \n",
    "    \".gif\", \n",
    "    \".bmp\", \n",
    "    \".webp\", \n",
    "    \".svg\", \n",
    "    \".tiff\"\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n",
    "\n",
    "stop_words_custom = {\n",
    "    'ability',\n",
    "    'abilities',\n",
    "    'accident',\n",
    "    'accidents',\n",
    "    'acknowledgement',\n",
    "    'action',\n",
    "    'actions',\n",
    "    'activities',\n",
    "    'activity',\n",
    "    'ad',\n",
    "    'ads',\n",
    "    'advice',\n",
    "    'alternative',\n",
    "    'alternatives',\n",
    "    'analysis',\n",
    "    'analyses',\n",
    "    'announcement',\n",
    "    'anomaly'\n",
    "    'anomalies'\n",
    "    'answer',\n",
    "    'answers',\n",
    "    'appreciation',\n",
    "    'approach',\n",
    "    'approaches',\n",
    "    'article',\n",
    "    'articles',\n",
    "    'assertion',\n",
    "    'assistance',\n",
    "    'assumption',\n",
    "    'attempt',\n",
    "    'author',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'benefit',\n",
    "    'bit',\n",
    "    'bits',\n",
    "    'block',\n",
    "    'blocks',\n",
    "    'blog',\n",
    "    'blogs',\n",
    "    'body',\n",
    "    'bug',\n",
    "    'bugs',\n",
    "    'building',\n",
    "    'case',\n",
    "    'cases',\n",
    "    'categories',\n",
    "    'category',\n",
    "    'cause',\n",
    "    'causes',\n",
    "    'challenge',\n",
    "    'challenges',\n",
    "    'change',\n",
    "    'changes',\n",
    "    'char',\n",
    "    'character',\n",
    "    'characters',\n",
    "    'check',\n",
    "    'choice',\n",
    "    'choices',\n",
    "    'classification',\n",
    "    'cloud',\n",
    "    'collection',\n",
    "    'com',\n",
    "    'combination',\n",
    "    'commmunication',\n",
    "    'community',\n",
    "    'communities',\n",
    "    'company',\n",
    "    'companies',\n",
    "    'concept',\n",
    "    'concepts',\n",
    "    'concern',\n",
    "    'concerns',\n",
    "    'condition',\n",
    "    'conditions',\n",
    "    'confirmation',\n",
    "    'confusion',\n",
    "    'consideration',\n",
    "    'contact',\n",
    "    'content',\n",
    "    'contents',\n",
    "    'control',\n",
    "    'count',\n",
    "    'couple',\n",
    "    'couples',\n",
    "    'course',\n",
    "    'courses',\n",
    "    'crash',\n",
    "    'crashes',\n",
    "    'cross',\n",
    "    'current',\n",
    "    'custom',\n",
    "    'customer',\n",
    "    'customers',\n",
    "    'day',\n",
    "    'days',\n",
    "    'decision',\n",
    "    'default',\n",
    "    'demand',\n",
    "    'demo',\n",
    "    'description',\n",
    "    'desire',\n",
    "    'desktop',\n",
    "    'detail',\n",
    "    'details',\n",
    "    'differ',\n",
    "    'difference',\n",
    "    'differences',\n",
    "    'difficulties',\n",
    "    'difficulty',\n",
    "    'discrepancies',\n",
    "    'discrepancy',\n",
    "    'discussion',\n",
    "    'dislike',\n",
    "    'distinction',\n",
    "    'edit',\n",
    "    'effect',\n",
    "    'end',\n",
    "    'enquiries',\n",
    "    'enquiry',\n",
    "    'error',\n",
    "    'errors',\n",
    "    'evidence',\n",
    "    'example',\n",
    "    'examples',\n",
    "    'exception',\n",
    "    'exceptions',\n",
    "    'existence',\n",
    "    'exit',\n",
    "    'expectation',\n",
    "    'experience',\n",
    "    'expert',\n",
    "    'experts',\n",
    "    'explanation',\n",
    "    'face',\n",
    "    'fact',\n",
    "    'facts',\n",
    "    'fail',\n",
    "    'failure',\n",
    "    'favorite',\n",
    "    'favorites',\n",
    "    'fault',\n",
    "    'feature',\n",
    "    'features',\n",
    "    'feedback',\n",
    "    'feedbacks',\n",
    "    'fix',\n",
    "    'fixes',\n",
    "    'float',\n",
    "    'forecast',\n",
    "    'forecasting',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'functionality',\n",
    "    'functionalities',\n",
    "    'future',\n",
    "    'goal',\n",
    "    'goals',\n",
    "    'guarantee',\n",
    "    'guidance',\n",
    "    'guideline',\n",
    "    'guide',\n",
    "    'guy',\n",
    "    'guys',\n",
    "    'harm',\n",
    "    'help',\n",
    "    'hour',\n",
    "    'hours',\n",
    "    'ibm',\n",
    "    'idea',\n",
    "    'ideas',\n",
    "    'individual',\n",
    "    'individuals',\n",
    "    'info',\n",
    "    'information',\n",
    "    'inquiries',\n",
    "    'inquiry',\n",
    "    'insight',\n",
    "    'instruction',\n",
    "    'instructions',\n",
    "    'int',\n",
    "    'intelligence',\n",
    "    'interest',\n",
    "    'introduction',\n",
    "    'investigation',\n",
    "    'invitation',\n",
    "    'issue',\n",
    "    'issues',\n",
    "    'kind',\n",
    "    'kinds',\n",
    "    'lack',\n",
    "    'language',\n",
    "    'languages',\n",
    "    'laptop',\n",
    "    'learn',\n",
    "    'learning',\n",
    "    'level',\n",
    "    'levels',\n",
    "    # 'location',\n",
    "    # 'locations',\n",
    "    'look',\n",
    "    'looks',\n",
    "    'lot',\n",
    "    'lots',\n",
    "    'luck',\n",
    "    'machine',\n",
    "    'machines',\n",
    "    'major',\n",
    "    'manner',\n",
    "    'manners',\n",
    "    'manual',\n",
    "    'mark',\n",
    "    'meaning',\n",
    "    'message',\n",
    "    'messages',\n",
    "    'method',\n",
    "    'methods',\n",
    "    'mind',\n",
    "    'minute',\n",
    "    'minutes',\n",
    "    'mistake',\n",
    "    'mistakes',\n",
    "    'moment',\n",
    "    'month',\n",
    "    'months',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'note',\n",
    "    'notes',\n",
    "    'number',\n",
    "    'numbers',\n",
    "    'offer',\n",
    "    'one',\n",
    "    'ones',\n",
    "    'opinion',\n",
    "    'opinions',\n",
    "    'org',\n",
    "    'organization',\n",
    "    'outcome',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'past',\n",
    "    'people',\n",
    "    'permit',\n",
    "    'person',\n",
    "    'persons',\n",
    "    'perspective',\n",
    "    'perspectives',\n",
    "    'picture',\n",
    "    'pictures',\n",
    "    'place',\n",
    "    'places',\n",
    "    'plan',\n",
    "    'point',\n",
    "    'points',\n",
    "    'post',\n",
    "    'posts',\n",
    "    'price',\n",
    "    'problem',\n",
    "    'problems',\n",
    "    'processing',\n",
    "    'product',\n",
    "    'products',\n",
    "    'program',\n",
    "    'programs',\n",
    "    'project',\n",
    "    'projects',\n",
    "    'proposal',\n",
    "    'purpose',\n",
    "    'purposes',\n",
    "    # 'python',\n",
    "    'question',\n",
    "    'questions',\n",
    "    'raise',\n",
    "    'reason',\n",
    "    'reasons',\n",
    "    'recommendation',\n",
    "    'recommendations',\n",
    "    'regression',\n",
    "    'research',\n",
    "    'result',\n",
    "    'results',\n",
    "    'return',\n",
    "    'returns',\n",
    "    'scenario',\n",
    "    'scenarios',\n",
    "    'science',\n",
    "    'screen',\n",
    "    'screenshot',\n",
    "    'screenshots',\n",
    "    'second',\n",
    "    'seconds',\n",
    "    'section',\n",
    "    'self',\n",
    "    'sense',\n",
    "    'sentence',\n",
    "    'setup',\n",
    "    'shape',\n",
    "    'show',\n",
    "    'shows',\n",
    "    'site',\n",
    "    'situation',\n",
    "    'software',\n",
    "    'solution',\n",
    "    'solutions',\n",
    "    'speech',\n",
    "    'start',\n",
    "    'state',\n",
    "    'statement',\n",
    "    'states',\n",
    "    'status',\n",
    "    'step',\n",
    "    'steps',\n",
    "    'string',\n",
    "    'study',\n",
    "    'stuff',\n",
    "    'success',\n",
    "    'suggestion',\n",
    "    'suggestions',\n",
    "    'summary',\n",
    "    'summaries',\n",
    "    'surprise',\n",
    "    'support',\n",
    "    'talk',\n",
    "    'task',\n",
    "    'tasks',\n",
    "    'technique',\n",
    "    'techniques',\n",
    "    'technologies',\n",
    "    'technology',\n",
    "    'term',\n",
    "    'terms',\n",
    "    'text',\n",
    "    'time',\n",
    "    'times',\n",
    "    'thank',\n",
    "    'thanks',\n",
    "    'thing',\n",
    "    'things',\n",
    "    'thought',\n",
    "    'three',\n",
    "    'title',\n",
    "    'time',\n",
    "    'today',\n",
    "    'tomorrow',\n",
    "    'tool',\n",
    "    'tools',\n",
    "    'topic',\n",
    "    'topics',\n",
    "    'total',\n",
    "    'trouble',\n",
    "    'troubles',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'tutorial',\n",
    "    'tutorials',\n",
    "    'two',\n",
    "    'understand',\n",
    "    'understanding',\n",
    "    # 'url',\n",
    "    # 'urls',\n",
    "    'use',\n",
    "    'user',\n",
    "    'users',\n",
    "    'uses',\n",
    "    'value',\n",
    "    'values',\n",
    "    'variant',\n",
    "    'variants',\n",
    "    'versus',\n",
    "    'video',\n",
    "    'videos',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'vision',\n",
    "    'voice',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'week',\n",
    "    'weeks',\n",
    "    'word',\n",
    "    'words',\n",
    "    'work',\n",
    "    'workaround',\n",
    "    'workarounds',\n",
    "    'works',\n",
    "    'yeah',\n",
    "    'year',\n",
    "    'years',\n",
    "    'yesterday',\n",
    "}\n",
    "\n",
    "tools_keyword_list = set(itertools.chain(*tools_keyword_mapping.values()))\n",
    "stop_words_list = STOPWORDS.union(tools_keyword_list).union(stop_words_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(block_list):\n",
    "    total_loc = 0\n",
    "    for blocks in block_list:\n",
    "        for block in blocks:\n",
    "            for line in block.splitlines():\n",
    "                if line.strip():\n",
    "                    total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    # extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract code blocks type 1\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code blocks type 2\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    return clean_text, links, code_line\n",
    "\n",
    "def extract_code(content):\n",
    "    code_patterns = [r'```.+?```', r'``.+?``', r'`.+?`']\n",
    "    clean_text = content\n",
    "    code_line = 0\n",
    "\n",
    "    for code_pattern in code_patterns:\n",
    "        code_snippets = re.findall(code_pattern, clean_text, flags=re.DOTALL)\n",
    "        code_line += count_code_line(code_snippets)\n",
    "        clean_text = re.sub(code_pattern, '', clean_text, flags=re.DOTALL)\n",
    "    \n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern1 = r\"\\!?\\[.*?\\]\\((.*?)\\)\"\n",
    "    links1 = re.findall(link_pattern1, text)\n",
    "    clean_text = re.sub(link_pattern1, '', text)\n",
    "    link_pattern2 = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links2 = re.findall(link_pattern2, clean_text)\n",
    "    clean_text = re.sub(link_pattern2, '', clean_text)\n",
    "    links = links1 + links2\n",
    "    return clean_text, links\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_line1 = extract_styles(content)\n",
    "    clean_text, code_line2 = extract_code(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_line = code_line1 + code_line2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_line'])\n",
    "    return content_collection(clean_text, links, code_line)\n",
    "\n",
    "def word_frequency(text):\n",
    "    word_counts = collections.Counter(text.split())\n",
    "    return word_counts\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "def extract_english(text):\n",
    "    words = [word for word in text.split() if spell_checker.check(word)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = strip_punctuation(clean_text)\n",
    "    clean_text = extract_english(clean_text)\n",
    "    clean_text = extract_nouns(clean_text)\n",
    "    clean_text = strip_short(clean_text)\n",
    "    clean_text = remove_stopwords(clean_text, stop_words_list)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_links(links):\n",
    "    image_links = 0\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([image in link for image in keywords_image]):\n",
    "            image_links += 1\n",
    "        elif any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['image', 'documentation', 'tool', 'issue', 'patch', 'tutorial', 'example'])\n",
    "    return link_analysis(image_links, documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "\n",
    "def find_duplicates(in_list):  \n",
    "    duplicates = []\n",
    "    unique = set(in_list)\n",
    "    for each in unique:\n",
    "        count = in_list.count(each)\n",
    "        if count > 1:\n",
    "            duplicates.append(each)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_comment_body'] = row['Issue_comment_body']\n",
    "    df_issues.at[index, 'Challenge_comment_score'] = row['Issue_comment_score']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_comment_body'] = row['Question_comment_body']\n",
    "    df_questions.at[index, 'Challenge_comment_score'] = row['Question_comment_score']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_comment_score'] = row['Answer_comment_score']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df = df[df.columns.drop(list(df.filter(regex=r'Issue|Question|Answer')))]\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "df = df.explode('Tools')\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tools', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 & 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    clean_title = preprocess_text(title_analyzer.text)\n",
    "    \n",
    "    challenge_analyzer = split_content(row['Challenge_title'] + row['Challenge_body'])\n",
    "    link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    clean_text = preprocess_text(challenge_analyzer.text)\n",
    "    \n",
    "    df.at[index, 'Challenge_preprocessed_title'] = clean_title\n",
    "    df.at[index, 'Challenge_preprocessed_content'] = clean_text\n",
    "    df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Challenge_comment_body']):\n",
    "        comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Solution_body']):\n",
    "        solution_analyzer = split_content(row['Solution_body'])\n",
    "        link_analyzer = analyze_links(solution_analyzer.links)\n",
    "        text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "        df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "    if pd.notna(row['Solution_comment_body']):\n",
    "        comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 99:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    # if pd.notna(row['Challenge_gpt_summary']):\n",
    "    #     continue\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + ' Body: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=50,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clean_summary = preprocess_text(row['Challenge_gpt_summary'])\n",
    "    df.at[index, 'Challenge_preprocessed_summary'] = clean_summary\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from bertopic.backend import OpenAIBackend\n",
    "\n",
    "# # openai.api_key = MY_API_KEY\n",
    "# embedding_model = OpenAIBackend(delay_in_seconds=0.1, batch_size=10)\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# docs = df[df['Challenge_summary'] != 'na']['Challenge_summary'].tolist() + df[df['Challenge_root_cause'] != 'na']['Challenge_root_cause'].tolist()\n",
    "\n",
    "# topic_model = BERTopic(embedding_model=embedding_model)\n",
    "# topics, probs = topic_model.fit_transform(docs)\n",
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minimize_weighted_sum(df, sort_column):\n",
    "#     df_new = df.sort_values(sort_column, ascending=False)\n",
    "#     n = len(df)\n",
    "#     center_idx = (n - 1) // 2\n",
    "#     direction = -1\n",
    "#     distance = 0\n",
    "\n",
    "#     for _, row in df_new.iterrows():\n",
    "#         # Calculate the new index\n",
    "#         new_idx = center_idx + direction * distance\n",
    "        \n",
    "#         # Place the element from the sorted list into the new list\n",
    "#         df.iloc[new_idx] = row\n",
    "\n",
    "#         # If we've just moved to the left, increase the distance\n",
    "#         if direction == -1:\n",
    "#             distance += 1\n",
    "\n",
    "#         # Switch the direction\n",
    "#         direction *= -1\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Model Management - Handling and manipulation of models including training, saving, importing, and exporting.\n",
      "Topic 1: Data Pipelining - The process of managing and processing data through multiple pipelines.\n",
      "Topic 2: Package Installation - The process of installing, importing, and managing software packages using pip.\n",
      "Topic 3: Logging - The process of creating, tracking, and managing logs during model training.\n",
      "Topic 4: Docker Operations - Building, running, and managing Docker images and files.\n",
      "Topic 5: Access Management - Managing access permissions, roles, and tokens for secure operations.\n",
      "Topic 6: Data Labeling - The process of labeling data for training and object recognition.\n",
      "Topic 7: Git Operations - Managing data, files, and version control using Git.\n",
      "Topic 8: Bucket Operations - Managing files, data, and paths in storage buckets.\n",
      "Topic 9: Sweep Operations - Configuring, running, and managing multiple sweeps.\n",
      "Topic 10: Quota Management - Managing request quotas and handling limit exceptions.\n",
      "Topic 11: Remote Operations - Configuring, running, and connecting to remote files and executions.\n",
      "Topic 12: Batch Processing - Managing and processing data, files, and jobs in batches.\n",
      "Topic 13: Lambda Functions - Invoking and processing data using Lambda functions.\n",
      "Topic 14: Database Operations - Connecting, importing, and running operations on databases.\n",
      "Topic 15: Language Translation - Translating documents and languages using models.\n",
      "Topic 16: Panda Operations - Managing and converting files using Panda.\n",
      "Topic 17: Speech Processing - Handling audio files, generating speech, and transcribing services.\n",
      "Topic 18: Spark Operations - Configuring, implementing, and managing data using Spark.\n",
      "Topic 19: Instance Management - Creating, managing, and removing instances.\n",
      "Topic 20: Column Operations - Managing, cleaning, and visualizing data in columns.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description based on the stemmed words. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_rq1, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = '''Topic 0: Model Management - Handling and manipulation of models including training, saving, importing, and exporting.\n",
    "Topic 1: Data Pipelining - The process of managing and processing data through multiple pipelines.\n",
    "Topic 2: Package Installation - The process of installing, importing, and managing software packages using pip.\n",
    "Topic 3: Logging - The process of creating, tracking, and managing logs during model training.\n",
    "Topic 4: Docker Operations - Building, running, and managing Docker images and files.\n",
    "Topic 5: Access Management - Managing access permissions, roles, and tokens for secure operations.\n",
    "Topic 6: Data Labeling - The process of labeling data for training and object recognition.\n",
    "Topic 7: Git Operations - Managing data, files, and version control using Git.\n",
    "Topic 8: Bucket Operations - Managing files, data, and paths in storage buckets.\n",
    "Topic 9: Sweep Operations - Configuring, running, and managing multiple sweeps.\n",
    "Topic 10: Quota Management - Managing request quotas and handling limit exceptions.\n",
    "Topic 11: Remote Operations - Configuring, running, and connecting to remote files and executions.\n",
    "Topic 12: Batch Processing - Managing and processing data, files, and jobs in batches.\n",
    "Topic 13: Lambda Functions - Invoking and processing data using Lambda functions.\n",
    "Topic 14: Database Operations - Connecting, importing, and running operations on databases.\n",
    "Topic 15: Language Translation - Translating documents and languages using models.\n",
    "Topic 16: Panda Operations - Managing and converting files using Panda.\n",
    "Topic 17: Speech Processing - Handling audio files, generating speech, and transcribing services.\n",
    "Topic 18: Spark Operations - Configuring, implementing, and managing data using Spark.\n",
    "Topic 19: Instance Management - Creating, managing, and removing instances.\n",
    "Topic 20: Column Operations - Managing, cleaning, and visualizing data in columns.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "False\n",
      "{17, 15}\n"
     ]
    }
   ],
   "source": [
    "topic_list = [topic for topic in topics.split('\\n') if topic]\n",
    "macro_topic_mapping_inverse = {\n",
    "    '1: Observability Management': [3],\n",
    "    '2: Lifecycle Management': [1],\n",
    "    '3: Compute Management': [9, 10, 12, 13, 18],\n",
    "    '4: Environment Management': [2, 4, 19],\n",
    "    '5: Access Management': [5, 11],\n",
    "    '6: Model Management': [0],\n",
    "    '7: Data Management': [6, 8, 14, 16, 20],\n",
    "    '8: Code Management': [7],\n",
    "}\n",
    "        \n",
    "macro_topic_list = []\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_indexing = {}\n",
    "for macro_topic, sub_topics in macro_topic_mapping_inverse.items():\n",
    "    index, name = int(macro_topic.split(': ')[0]), macro_topic.split(': ')[1]\n",
    "    macro_topic_indexing[index] = name\n",
    "    macro_topic_list.extend(sub_topics)\n",
    "    for topic in sub_topics:\n",
    "        macro_topic_mapping[topic] = macro_topic\n",
    "\n",
    "print(find_duplicates(macro_topic_list))\n",
    "print(len(macro_topic_list) == len(topic_list))\n",
    "print(set(range(len(topic_list))).difference(set(macro_topic_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Challenge_topic'] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Topic & Percentage & Number \\\\\n",
      "\\midrule\n",
      "Model Management & 21.39 & 2378 \\\\\n",
      "Compute Management & 20.05 & 2229 \\\\\n",
      "Environment Management & 17.90 & 1990 \\\\\n",
      "Data Management & 13.13 & 1460 \\\\\n",
      "Lifecycle Management & 9.94 & 1105 \\\\\n",
      "Access Management & 7.84 & 872 \\\\\n",
      "Observability Management & 6.65 & 739 \\\\\n",
      "Code Management & 3.09 & 344 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_rq1, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "\n",
    "for name, group in df.groupby('Challenge_topic_macro'):\n",
    "    entry = {\n",
    "        'Topic': macro_topic_indexing[name],\n",
    "        'Percentage': round(len(group)/len(df)*100, 2),\n",
    "        'Number': len(group),\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "\n",
    "df_number = df_number.sort_values('Percentage', ascending=False)\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'filtered.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "df['Challenge_topic_macro'] = df['Challenge_topic_macro'].apply(lambda x: macro_topic_indexing[x])\n",
    "categories = ['Challenge_type', 'Challenge_topic_macro', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_rq1, 'State type topic sankey.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
