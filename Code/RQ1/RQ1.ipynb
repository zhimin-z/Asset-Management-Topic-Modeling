{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import openai\n",
    "import textstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The significance level indicates the probability of rejecting the null hypothesis when it is true.\n",
    "alpha = 0.05\n",
    "\n",
    "link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result = '../../Result'\n",
    "\n",
    "path_result_rq1 = os.path.join(path_result, 'RQ1')\n",
    "path_code_rq1 = os.path.join('..', 'RQ1')\n",
    "\n",
    "path_general_output = os.path.join(path_result_rq1, 'General Topics')\n",
    "path_special_output = os.path.join(path_result_rq1, 'Special Topics')\n",
    "\n",
    "path_general_topic = os.path.join(path_code_rq1, 'General Topic Modeling')\n",
    "path_special_topic = os.path.join(path_code_rq1, 'Special Topic Modeling')\n",
    "\n",
    "path_anomaly = os.path.join(path_special_topic, 'Anomaly')\n",
    "path_root_cause = os.path.join(path_special_topic, 'Root Cause')\n",
    "path_solution = os.path.join(path_special_topic, 'Solution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Data Pipelines - The process of creating, managing, and running data pipelines.\n",
      "Topic 1: Docker - A platform that uses OS-level virtualization to deliver software in packages called containers.\n",
      "Topic 2: Role and Permissions - The assignment and management of roles and permissions in a software system.\n",
      "Topic 3: Version Control - The practice of tracking and managing changes to software code.\n",
      "Topic 4: TensorFlow - An open-source platform for machine learning.\n",
      "Topic 5: Data Storage - The practice of storing and managing datasets.\n",
      "Topic 6: GPU Utilization - The use and management of GPU resources in a computing environment.\n",
      "Topic 7: Artifacts - The management and storage of software artifacts.\n",
      "Topic 8: Package Installation - The process of installing and managing software packages.\n",
      "Topic 9: Kubernetes - An open-source platform for managing containerized workloads and services.\n",
      "Topic 10: YAML in Pipelines - The use of YAML files in creating and managing data pipelines.\n",
      "Topic 11: Jupyter Notebooks - An open-source web application that allows the creation and sharing of documents that contain live code, equations, visualizations, and narrative text.\n",
      "Topic 12: Datasets - The creation, management, and use of datasets in software engineering.\n",
      "Topic 13: Logging - The practice of recording actions in a software system.\n",
      "Topic 14: PySpark - A Python library for Apache Spark, a fast and general-purpose cluster computing system.\n",
      "Topic 15: Forecasting - The use of machine learning to predict future data.\n",
      "Topic 16: Model Files - The creation, extraction, and management of model files in machine learning.\n",
      "Topic 17: VPC and Connections - The configuration and management of Virtual Private Clouds (VPC) and connections.\n",
      "Topic 18: Model Training - The process of training machine learning models.\n",
      "Topic 19: Data Visualization - The practice of visualizing data through various types of plots.\n",
      "Topic 20: API Endpoints - The points of interaction in an API.\n",
      "Topic 21: Pip Installation - The process of installing and managing software packages with pip.\n",
      "Topic 22: Regression - A type of predictive modelling technique which investigates the relationship between a dependent and independent variable.\n",
      "Topic 23: Predictive Endpoints - The use of endpoints in making predictions in machine learning.\n",
      "Topic 24: Database Connections - The process of connecting to and interacting with databases.\n",
      "Topic 25: Model Deployment - The process of making a machine learning model available for use.\n",
      "Topic 26: Notebook Execution - The process of running and managing Jupyter notebooks.\n",
      "Topic 27: Data Buckets - The use and management of data storage buckets.\n",
      "Topic 28: Hyperparameter Sweeping - The process of searching through a manual or predefined hyperparameter space for the best model hyperparameters.\n",
      "Topic 29: Training Arguments - The use and management of arguments in model training.\n",
      "Topic 30: Multi-model Endpoints - The use of endpoints that can deploy multiple models.\n",
      "Topic 31: Workspace Connections - The process of connecting to and managing workspaces.\n",
      "Topic 32: PyTorch Logging - The practice of recording actions in a PyTorch environment.\n",
      "Topic 33: Dependency Conflicts - The management and resolution of conflicts between software dependencies.\n",
      "Topic 34: Visual Studio Code - A free source-code editor made by Microsoft for Windows, Linux and macOS.\n",
      "Topic 35: CSV and Pandas - The use of CSV files and the pandas library in data processing.\n",
      "Topic 36: Hyperparameter Tuning - The process of optimizing the parameters of a machine learning model.\n",
      "Topic 37: Vision API - A tool that allows developers to understand the content of an image by encapsulating powerful machine learning models in an easy-to-use REST API.\n",
      "Topic 38: TensorBoard - A tool for providing the measurements and visualizations needed during the machine learning workflow.\n",
      "Topic 39: Conda - An open-source package management system and environment management system.\n",
      "Topic 40: Clustering - The task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group.\n",
      "Topic 41: Research and Collaboration - The process of conducting research and collaborating in a software engineering context.\n",
      "Topic 42: Endpoint Scaling - The process of managing the scale of API endpoints.\n",
      "Topic 43: Web Services - The use and management of web services in a software system.\n",
      "Topic 44: Speech Processing - The use of machine learning to process and transcribe speech.\n",
      "Topic 45: Error Handling - The process of handling and resolving errors in a software system.\n",
      "Topic 46: Dialogflow - A natural language understanding platform used to design and integrate a conversational user interface into mobile apps, web applications, devices, bots, interactive voice response systems, and so on.\n",
      "Topic 47: Language Translation - The use of machine learning to translate text from one language to another.\n",
      "Topic 48: Documentation and Release - The process of documenting software and managing its release.\n",
      "Topic 49: Directory Management - The process of managing and refactoring directories in a software system.\n",
      "Topic 50: Execution Speed - The speed at which a software system or process runs.\n",
      "Topic 51: Scheduling - The practice of scheduling tasks in a software system.\n",
      "Topic 52: Logging and Metrics - The practice of recording actions and metrics in a software system.\n",
      "Topic 53: Data Labeling - The process of labeling data for use in machine learning.\n",
      "Topic 54: CSV Processing - The process of processing CSV files.\n",
      "Topic 55: Convolutional Neural Networks - A class of deep neural networks, most commonly applied to analyzing visual imagery.\n",
      "Topic 56: PyTorch and CNNs - The use of PyTorch and Convolutional Neural Networks in machine learning.\n",
      "Topic 57: Flask - A micro web framework written in Python.\n",
      "Topic 58: Data Transformation - The process of converting data from one format or structure into another.\n",
      "Topic 59: File Upload and Sync - The process of uploading files and syncing data.\n",
      "Topic 60: Document Processing - The process of extracting information from documents.\n",
      "Topic 61: Model Deployment and Schema - The process of deploying models and defining schemas.\n",
      "Topic 62: Object Detection - The process of detecting objects within digital images and videos.\n",
      "Topic 63: PyTorch Environment - The use and management of the PyTorch machine learning platform.\n",
      "Topic 64: Quota Management - The process of managing quotas in a software system.\n",
      "Topic 65: Activity Panels and Computing - The use and management of activity panels and computing resources.\n",
      "Topic 66: Model Compilation - The process of compiling machine learning models.\n",
      "Topic 67: Web Service Deployment - The process of deploying web services.\n",
      "Topic 68: Disk Space Management - The process of managing disk space in a computing environment.\n",
      "Topic 69: Synchronization and Upload - The process of synchronizing and uploading data.\n",
      "Topic 70: Document Review - The process of reviewing and processing documents.\n",
      "Topic 71: Model Deployment and Services - The process of deploying models and managing services.\n",
      "Topic 72: Object Detection in Videos - The process of detecting objects in video data.\n",
      "Topic 73: PyTorch Environment - The use and management of the PyTorch machine learning platform.\n",
      "Topic 74: Quota Management - The process of managing quotas in a software system.\n",
      "Topic\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_general_topic, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 'NA',\n",
       " 0: 'Pipeline Configuration',\n",
       " 1: 'Docker Configuration',\n",
       " 2: 'Role Configuration',\n",
       " 3: 'Git Version Control',\n",
       " 4: 'TensorFlow Configuration',\n",
       " 5: 'Data Storage and Dataset',\n",
       " 6: 'GPU Configuration',\n",
       " 7: 'Artifact Management',\n",
       " 8: 'Package Management',\n",
       " 9: 'Kubernetes Configuration',\n",
       " 10: 'YAML Configuration',\n",
       " 11: 'Jupyter Configuration',\n",
       " 12: 'Dataset Management',\n",
       " 13: 'Log Management',\n",
       " 14: 'Spark Configuration',\n",
       " 15: 'AutoML Forecasting',\n",
       " 16: 'Model Storage and Conversion',\n",
       " 17: 'VPC and Connection',\n",
       " 18: 'Model Training',\n",
       " 19: 'Data Visualization',\n",
       " 20: 'Endpoint Configuration',\n",
       " 21: 'Pip Configuration',\n",
       " 22: 'Regression Analysis',\n",
       " 23: 'Endpoint Prediction',\n",
       " 24: 'Database Management',\n",
       " 25: 'Model Deployment',\n",
       " 26: 'Notebook Operation',\n",
       " 27: 'Online Storage and Bucket',\n",
       " 28: 'Hyperparameter Sweeping',\n",
       " 29: 'Training Configuration',\n",
       " 30: 'Multimodel Endpoint',\n",
       " 31: 'Workspace Configuration',\n",
       " 32: 'PyTorch and Logging',\n",
       " 33: 'Dependency Management',\n",
       " 34: 'VSCode Configuration',\n",
       " 35: 'Tabular Data Manipulation',\n",
       " 36: 'Hyperparameter Tuning',\n",
       " 37: 'Vision Processing',\n",
       " 38: 'TensorBoard Visualization',\n",
       " 39: 'Conda Configuration',\n",
       " 40: 'Cluster Management',\n",
       " 41: 'Research and Collaboration',\n",
       " 42: 'Endpoint Provisioning',\n",
       " 43: 'Web Service',\n",
       " 44: 'Speech Processing',\n",
       " 45: 'Error Handling',\n",
       " 46: 'Dialogflow Configuration',\n",
       " 47: 'Language Translation',\n",
       " 48: 'Documentation and Release',\n",
       " 49: 'Directory Management',\n",
       " 50: 'Execution Speed',\n",
       " 51: 'Job Scheduling',\n",
       " 52: 'Logging and Metrics',\n",
       " 53: 'Data Labeling',\n",
       " 54: 'CSV Processing',\n",
       " 55: 'Convolutional Neural Network',\n",
       " 56: 'Neural Network Architecture',\n",
       " 57: 'Flask Configuration',\n",
       " 58: 'Data Transformation',\n",
       " 59: 'File Syncing',\n",
       " 60: 'Document Processing',\n",
       " 61: 'Schema Configuration',\n",
       " 62: 'Object Detection',\n",
       " 63: 'PyTorch Configuration',\n",
       " 64: 'Quota Management',\n",
       " 65: 'Computing Activity',\n",
       " 66: 'Model Compilation',\n",
       " 67: 'Web Service Deployment',\n",
       " 68: 'Storage Management'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = '''Topic 0: Pipeline Configuration - Processes and tools for building, running, and managing data pipelines.\n",
    "Topic 1: Docker Configuration - Techniques and commands for creating and managing Docker containers and environments.\n",
    "Topic 2: Role Configuration - Management of roles and permissions in a software system, particularly in IAM.\n",
    "Topic 3: Git Version Control - Use of Git for version control of files and repositories.\n",
    "Topic 4: TensorFlow Configuration - Building, running, and managing models using TensorFlow.\n",
    "Topic 5: Data Storage and Dataset - Handling of datasets and data storage, including data lakes.\n",
    "Topic 6: GPU Configuration - Monitoring and managing GPU utilization, particularly in PyTorch and CUDA.\n",
    "Topic 7: Artifact Management - Management of artifacts in a software system, including storage and access.\n",
    "Topic 8: Package Management - Installation and management of software libraries and packages.\n",
    "Topic 9: Kubernetes Configuration - Deployment and management of services in a Kubernetes cluster.\n",
    "Topic 10: YAML Configuration - Use of YAML for configuring and structuring pipelines.\n",
    "Topic 11: Jupyter Configuration - Running and managing Jupyter notebooks.\n",
    "Topic 12: Dataset Management - Manipulation and management of datasets, including splitting and combining.\n",
    "Topic 13: Log Management - Configuration and management of logging in a software system.\n",
    "Topic 14: Spark Configuration - Running and managing PySpark jobs and dataframes.\n",
    "Topic 15: AutoML Forecasting - Use of AutoML for forecasting based on datasets.\n",
    "Topic 16: Model Storage and Transformation - Handling of model files, including extraction and import.\n",
    "Topic 17: VPC Connection - Configuration and management of VPC endpoints and connections.\n",
    "Topic 18: Model Training - Training and updating models, particularly with AutoML and sklearn.\n",
    "Topic 19: Data Visualization - Creation of various types of plots for data visualization.\n",
    "Topic 20: Endpoint Configuration - Configuration and invocation of API endpoints.\n",
    "Topic 21: Pip Configuration - Installation and management of pip, including version control.\n",
    "Topic 22: Regression Analysis - Use of regression models and evaluation of datasets.\n",
    "Topic 23: Endpoint Prediction - Making predictions using API endpoints and data.\n",
    "Topic 24: Database Management - Connecting to and managing databases, including SQL and MySQL.\n",
    "Topic 25: Model Deployment - Deployment and management of models in a machine learning project.\n",
    "Topic 26: Notebook Operation - Execution and management of notebooks.\n",
    "Topic 27: Online Storage and Bucket - Management of buckets and directories, including uploads and permissions.\n",
    "Topic 28: Hyperparameter Sweeping - Configuration and execution of hyperparameter sweeps.\n",
    "Topic 29: Training Configuration - Training models and handling training arguments.\n",
    "Topic 30: Multimodel Endpoint - Management of endpoints that host multiple models.\n",
    "Topic 31: Workspace Configuration - Connecting to and managing workspaces.\n",
    "Topic 32: PyTorch and Logging - Use of PyTorch for logging, including loss tracking.\n",
    "Topic 33: Dependency Management - Handling of dependencies and conflicts in software packages.\n",
    "Topic 34: VSCode Configuration - Use and management of Visual Studio Code.\n",
    "Topic 35: Tabular Data Manipulation - Handling of CSV data with pandas, including reading and writing.\n",
    "Topic 36: Hyperparameter Tuning - Tuning of hyperparameters in a machine learning model.\n",
    "Topic 37: Vision Processing - Use of vision APIs for image recognition and processing.\n",
    "Topic 38: TensorBoard Visualization - Use and configuration of TensorBoard for visualizing machine learning models.\n",
    "Topic 39: Conda Configuration - Use and management of Conda environments.\n",
    "Topic 40: Cluster Management - Running and managing computational clusters.\n",
    "Topic 41: Research and Collaboration - Collaboration and contribution in research and development.\n",
    "Topic 42: Endpoint Provisioning - Scaling and provisioning of API endpoints.\n",
    "Topic 43: Web Service - Deployment and management of web services.\n",
    "Topic 44: Speech Processing - Use of speech APIs for transcription and synthesis.\n",
    "Topic 45: Error Handling - Handling of common errors in machine learning processes.\n",
    "Topic 46: Dialogflow Configuration - Configuration and use of Dialogflow for conversational AI.\n",
    "Topic 47: Language Translation - Use of translation APIs for language translation.\n",
    "Topic 48: Documentation and Release - Updating and releasing documentation for software versions.\n",
    "Topic 49: Directory Management - Refactoring and management of directories and files.\n",
    "Topic 50: Execution Speed - Management of execution speed and time in software processes.\n",
    "Topic 51: Job Scheduling - Scheduling of jobs and executions in a software system.\n",
    "Topic 52: Logging and Metrics - Logging and fetching of metrics in a software system.\n",
    "Topic 53: Data Labeling - Processing and labeling of data for machine learning.\n",
    "Topic 54: CSV Processing - Processing and handling of CSV data.\n",
    "Topic 55: Convolutional Neural Network - Training and use of convolutional neural networks.\n",
    "Topic 56: Neural Network Architecture - Use of PyTorch for training CNNs and GANs.\n",
    "Topic 57: Flask Configuration - Use and management of the Flask web framework.\n",
    "Topic 58: Data Transformation - Transformation and conversion of data for machine learning.\n",
    "Topic 59: File Syncing - Uploading and syncing of files and artifacts.\n",
    "Topic 60: Document Processing - Processing and extraction of information from documents.\n",
    "Topic 61: Schema Configuration - Deployment of models and definition of schemas.\n",
    "Topic 62: Object Detection - Use of models for object detection in videos.\n",
    "Topic 63: PyTorch Configuration - Management and use of PyTorch environments.\n",
    "Topic 64: Quota Management - Management of quotas in a software system.\n",
    "Topic 65: Computing Activity  - Management of activity panels and computing resources.\n",
    "Topic 66: Model Compilation - Compilation of models, particularly with Neo.\n",
    "Topic 67: Web Service Deployment - Deployment and management of web services.\n",
    "Topic 68: Storage Management - Management of disk space and quotas.'''\n",
    "\n",
    "index = 0\n",
    "topic_dict = {-1: 'NA'}\n",
    "\n",
    "for topic in topics.split('\\n'):\n",
    "    topic_dict[index] = topic.split('-')[0].split(':')[-1].strip()\n",
    "    index += 1\n",
    "    \n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Git Version Control': 0,\n",
       " 'Role Configuration': 1,\n",
       " 'VPC and Connection': 1,\n",
       " 'Job Scheduling': 2,\n",
       " 'Pipeline Configuration': 2,\n",
       " 'YAML Configuration': 2,\n",
       " 'Conda Configuration': 3,\n",
       " 'Dependency Management': 3,\n",
       " 'Docker Configuration': 3,\n",
       " 'Jupyter Configuration': 3,\n",
       " 'Kubernetes Configuration': 3,\n",
       " 'Notebook Operation': 3,\n",
       " 'Package Management': 3,\n",
       " 'Pip Configuration': 3,\n",
       " 'VSCode Configuration': 3,\n",
       " 'Workspace Configuration': 3,\n",
       " 'Cluster Management': 4,\n",
       " 'Computing Activity': 4,\n",
       " 'Execution Speed': 4,\n",
       " 'GPU Configuration': 4,\n",
       " 'Hyperparameter Sweeping': 4,\n",
       " 'Hyperparameter Tuning': 4,\n",
       " 'Model Training': 4,\n",
       " 'PyTorch Configuration': 4,\n",
       " 'Spark Configuration': 4,\n",
       " 'TensorFlow Configuration': 4,\n",
       " 'Training Configuration': 4,\n",
       " 'Artifact Management': 5,\n",
       " 'CSV Processing': 5,\n",
       " 'Data Labeling': 5,\n",
       " 'Data Storage and Dataset': 5,\n",
       " 'Data Transformation': 5,\n",
       " 'Data Visualization': 5,\n",
       " 'Database Management': 5,\n",
       " 'Dataset Management': 5,\n",
       " 'Directory Management': 5,\n",
       " 'File Syncing': 5,\n",
       " 'Online Storage and Bucket': 5,\n",
       " 'Storage Management': 5,\n",
       " 'Tabular Data Manipulation': 5,\n",
       " 'Endpoint Prediction': 6,\n",
       " 'Endpoint Configuration': 6,\n",
       " 'Endpoint Provisioning': 6,\n",
       " 'Flask Configuration': 6,\n",
       " 'Multimodel Endpoint': 6,\n",
       " 'Schema Configuration': 6,\n",
       " 'Web Service': 6,\n",
       " 'Web Service Deployment': 6,\n",
       " 'Log Management': 7,\n",
       " 'Logging and Metrics': 7,\n",
       " 'PyTorch and Logging': 7,\n",
       " 'TensorBoard Visualization': 7,\n",
       " 'Model Compilation': 8,\n",
       " 'Model Deployment': 8,\n",
       " 'Model Storage and Conversion': 8,\n",
       " 'Error Handling': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_topic_mapping_inverse = {\n",
    "    # These topics are all related to the management of source code.\n",
    "    0: ('Code Management', ['Git Version Control']),\n",
    "    # These topics are all related to the management of permissions and connectivity.\n",
    "    1: ('Access Management', ['Role Configuration', 'VPC Connection']),\n",
    "    # These topics are all related to the management of pipelines.\n",
    "    2: ('Lifecycle Management', ['Job Scheduling', 'Pipeline Configuration', 'YAML Configuration']),\n",
    "    # These topics are all related to the management of packages and distributions.\n",
    "    3: ('Infrastructure Management', ['Conda Configuration', 'Dependency Management', 'Docker Configuration', 'Jupyter Configuration', 'Kubernetes Configuration', 'Notebook Operation', 'Package Management', 'Pip Configuration', 'VSCode Configuration', 'Workspace Configuration']),\n",
    "    # These topics are all related to the management of data and datasets.\n",
    "    4: ('Compute Management', ['Cluster Management', 'Computing Activity', 'Execution Speed', 'GPU Configuration', 'Hyperparameter Sweeping', 'Hyperparameter Tuning', 'Model Training', 'PyTorch Configuration', 'Spark Configuration', 'TensorFlow Configuration', 'Training Configuration']),\n",
    "    # These topics are all related to the management of services.\n",
    "    5: ('Data Management', ['Artifact Management', 'CSV Processing', 'Data Labeling', 'Data Storage and Dataset', 'Data Transformation', 'Data Visualization', 'Database Management', 'Dataset Management', 'Directory Management', 'File Syncing', 'Online Storage and Bucket', 'Storage Management', 'Tabular Data Manipulation']),\n",
    "    # These topics are all related to the management of parallel computing resources.\n",
    "    6: ('Service Management', ['Endpoint Prediction', 'Endpoint Configuration', 'Endpoint Provisioning', 'Flask Configuration', 'Multimodel Endpoint', 'Schema Configuration', 'Web Service', 'Web Service Deployment']),\n",
    "    # These topics are all related to the management of logs and metrics.\n",
    "    7: ('Performance Management', ['Log Management', 'Logging and Metrics', 'PyTorch and Logging', 'TensorBoard Visualization']),\n",
    "    # These topics are all related to the management of machine learning models.\n",
    "    8: ('Model Management', ['Model Compilation', 'Model Deployment', 'Model Storage and Transformation']),\n",
    "    # These topics are all related to miscellaneous software engineering steps.\n",
    "    9: ('Miscellaneous', ['Error Handling']),\n",
    "}\n",
    "\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_index_mapping = {}\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    macro_topic_index_mapping[macro_topic] = key\n",
    "    for topic in topics:\n",
    "        macro_topic_mapping[topic] = key\n",
    "\n",
    "macro_topic_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Management</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access Management</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lifecycle Management</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infrastructure Management</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compute Management</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Service Management</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Performance Management</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Management</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Topic  Number\n",
       "Index                                   \n",
       "0                Code Management     326\n",
       "1              Access Management     538\n",
       "2           Lifecycle Management     885\n",
       "3      Infrastructure Management    1858\n",
       "4             Compute Management    2142\n",
       "5                Data Management    1938\n",
       "6             Service Management    1307\n",
       "7         Performance Management     624\n",
       "8               Model Management     504\n",
       "9                  Miscellaneous     168"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general_output, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if topic_dict[row['Challenge_topic']] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict[row['Challenge_topic']]]\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_general_output, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    entry = {\n",
    "        'Index': key,\n",
    "        'Topic': macro_topic,\n",
    "        'Number': len(df[df[\"Challenge_topic_macro\"] == key])\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "df_number.set_index('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "# for index, row in df.iterrows():\n",
    "#     if topic_dict[row['Challenge_topic']] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict[row['Challenge_topic']]]\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_general_output, 'filtered.json'))\n",
    "\n",
    "df['Challenge_type'] = 'na'\n",
    "df['Challenge_summary'] = 'na'\n",
    "df['Challenge_root_cause'] = 'na'\n",
    "df['Challenge_solution'] = 'na'\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df['Challenge_summary'] = df['Challenge_summary'].str.lower()\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_special_output, 'anomaly.json'))\n",
    "\n",
    "regex_digit = r\"[0-9]\"\n",
    "\n",
    "regex_error = r\"[a-zA-Z0-9]+[eE]rror[^a-zA-Z]\"\n",
    "regex_exception = r\"[a-zA-Z0-9]+[eE]xception[^a-zA-Z]\"\n",
    "\n",
    "regex_error_leading = r\"[a-zA-Z0-9]+[eE]rror[a-zA-Z]+\"\n",
    "regex_exception_leading = r\"[a-zA-Z0-9]+[eE]xception[a-zA-Z]+\"\n",
    "\n",
    "false_positive_list = []\n",
    "\n",
    "def camel_case_split(str):\n",
    "    words = [[str[0].lower()]]\n",
    " \n",
    "    for c in str[1:]:\n",
    "        if (words[-1][-1].islower() or words[-1][-1].isdigit()) and c.isupper():\n",
    "            words.append(list(c.lower()))\n",
    "        else:\n",
    "            words[-1].append(c)\n",
    "    return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    challenge = row['Challenge_title'] + ' ' + row['Challenge_body']\n",
    "    challenge = challenge.replace('\\n', ' ')\n",
    "    error_list = re.findall(regex_error, challenge)\n",
    "    if len(error_list):\n",
    "        if row['Challenge_type'] != 'anomaly':\n",
    "            df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "            false_positive_list.append(row['Challenge_link'])\n",
    "        error = max(error_list, key = len)\n",
    "        if len(re.findall(regex_digit, error)):\n",
    "            print(row['Challenge_title'])\n",
    "        else:\n",
    "            error = re.sub(r'error.+', 'error', camel_case_split(error))\n",
    "            df.at[index, 'Challenge_summary'] = error\n",
    "    else:\n",
    "        exception_list = re.findall(regex_exception, challenge)\n",
    "        if len(exception_list):\n",
    "            if row['Challenge_type'] != 'anomaly':\n",
    "                df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "                false_positive_list.append(row['Challenge_link'])\n",
    "            exception = max(exception_list, key = len)\n",
    "            if len(re.findall(regex_digit, exception)):\n",
    "                print(row['Challenge_title'])\n",
    "            else:\n",
    "                exception = re.sub(r'exception.+', 'exception', camel_case_split(exception))\n",
    "                df.at[index, 'Challenge_summary'] = exception\n",
    "        else:\n",
    "            error_list_leading = re.findall(regex_error_leading, challenge)\n",
    "            if len(error_list_leading):\n",
    "                print(row['Challenge_title'])\n",
    "            else:\n",
    "                exception_list_leading = re.findall(regex_exception_leading, challenge)\n",
    "                if len(exception_list_leading):\n",
    "                    print(row['Challenge_title'])\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'anomaly.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'module not found'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_special_output, 'anomaly.json'))\n",
    "\n",
    "regex_digit = r\"[0-9]\"\n",
    "\n",
    "regex_error = r\"[a-zA-Z0-9]+[eE]rror[^a-zA-Z]\"\n",
    "regex_exception = r\"[a-zA-Z0-9]+[eE]xception[^a-zA-Z]\"\n",
    "\n",
    "regex_error_leading = r\"[a-zA-Z0-9]+[eE]rror[a-zA-Z]+\"\n",
    "regex_exception_leading = r\"[a-zA-Z0-9]+[eE]xception[a-zA-Z]+\"\n",
    "\n",
    "false_positive_list = []\n",
    "\n",
    "def camel_case_split(str):\n",
    "    words = [[str[0].lower()]]\n",
    " \n",
    "    for c in str[1:]:\n",
    "        if (words[-1][-1].islower() or words[-1][-1].isdigit()) and c.isupper():\n",
    "            words.append(list(c.lower()))\n",
    "        else:\n",
    "            words[-1].append(c)\n",
    "    return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_summary'] != 'internal server error':\n",
    "        continue\n",
    "    challenge = row['Challenge_title'] + ' ' + row['Challenge_body']\n",
    "    challenge = challenge.replace('\\n', ' ')\n",
    "    error_list = re.findall(regex_error, challenge)\n",
    "    if len(error_list):\n",
    "        if row['Challenge_type'] != 'anomaly':\n",
    "            df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "            false_positive_list.append(row['Challenge_link'])\n",
    "        error = max(error_list, key = len)\n",
    "        if len(re.findall(regex_digit, error)):\n",
    "            print(row['Challenge_title'])\n",
    "        else:\n",
    "            error = re.sub(r'error.+', 'error', camel_case_split(error))\n",
    "            df.at[index, 'Challenge_summary'] = error\n",
    "    else:\n",
    "        exception_list = re.findall(regex_exception, challenge)\n",
    "        if len(exception_list):\n",
    "            if row['Challenge_type'] != 'anomaly':\n",
    "                df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "                false_positive_list.append(row['Challenge_link'])\n",
    "            exception = max(exception_list, key = len)\n",
    "            if len(re.findall(regex_digit, exception)):\n",
    "                print(row['Challenge_title'])\n",
    "            else:\n",
    "                exception = re.sub(r'exception.+', 'exception', camel_case_split(exception))\n",
    "                df.at[index, 'Challenge_summary'] = exception\n",
    "        else:\n",
    "            error_list_leading = re.findall(regex_error_leading, challenge)\n",
    "            if len(error_list_leading):\n",
    "                print(row['Challenge_title'])\n",
    "            else:\n",
    "                exception_list_leading = re.findall(regex_exception_leading, challenge)\n",
    "                if len(exception_list_leading):\n",
    "                    print(row['Challenge_title'])\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'anomaly.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# for i,r in df.iterrows():\n",
    "#     if r['Challenge_type'] == 'inquiry':\n",
    "#         df.at[i, 'Challenge_summary'] = 'na'\n",
    "#         df.at[i, 'Challenge_root_cause'] = 'na'\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels+.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df = pd.concat([df_new, df_old], ignore_index=True)\n",
    "# df = df.drop_duplicates(['Challenge_link'], keep=False)\n",
    "\n",
    "# df = df[df['Challenge_type'].isna()]\n",
    "# df.to_json(os.path.join(path_special_output, 'extra.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels++.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_special_output, 'extra.json'))\n",
    "\n",
    "# df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "# df = df.drop_duplicates(['Challenge_link'])\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'extra+.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels+.json'))\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df_new[~((df1.City.isin(df2.City)) & (df1.State.isin(df2.State)))] \n",
    "\n",
    "# for index, row in df_new.iterrows():\n",
    "#     for i2,r2 in df.iterrows():\n",
    "#         if r2['Challenge_type'] == 'na':\n",
    "#             continue\n",
    "#         if r2['Challenge_link'] == row['Challenge_link']:\n",
    "#             df_new.at[index, 'Challenge_type'] = r2['Challenge_type']\n",
    "#             df_new.at[index, 'Challenge_summary'] = r2['Challenge_summary']\n",
    "#             df_new.at[index, 'Challenge_root_cause'] = r2['Challenge_root_cause']\n",
    "#             df_new.at[index, 'Challenge_solution'] = r2['Challenge_solution']\n",
    "#             break\n",
    "            \n",
    "# df_new.to_json(os.path.join(path_special_output, 'labels++.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Import and Module Issues - Problems related to importing modules or libraries and errors associated with these processes.\n",
      "Topic 1: Range of Errors - Various types of errors that are worrisome, inconsistent, unrecognized, confusing, or unexpected.\n",
      "Topic 2: Performance Issues - Problems related to slow execution, processing, and performance degradation.\n",
      "Topic 3: Connection Problems - Issues related to connection resets, bad gateways, misconfigured connections, and other connection errors.\n",
      "Topic 4: Project Modification Issues - Problems related to modifying, deleting, or coding projects and issues with files within these projects.\n",
      "Topic 5: Access Issues - Problems related to denied, forbidden, or unauthorized access.\n",
      "Topic 6: Installation Issues - Errors and issues related to software installation.\n",
      "Topic 7: Input Issues - Problems related to invalid, incorrect, unsupported, or unmatched inputs.\n",
      "Topic 8: Version Incompatibility - Issues related to incompatible, outdated, invalid, or conflicting software versions.\n",
      "Topic 9: Experiment Errors - Problems related to invalid, incorrect, failed, or unexpected experiments.\n",
      "Topic 10: File Availability Issues - Problems related to unavailable, missing, inaccessible, or ignored files.\n",
      "Topic 11: Artifact Issues - Problems related to unavailable, unlisted, invalid, incomplete, or broken artifacts.\n",
      "Topic 12: Request Issues - Problems related to bad, invalid, malformed, unavailable, or forbidden requests.\n",
      "Topic 13: Application Unresponsiveness - Issues related to unresponsive applications, processes, executions, or user interfaces.\n",
      "Topic 14: Deployment Errors - Problems related to unsuccessful, failed, invalid, or unavailable deployments.\n",
      "Topic 15: Dataset Availability Issues - Problems related to unavailable, inaccessible, unrecognized, unsupported, or invalid datasets.\n",
      "Topic 16: Configuration Issues - Problems related to unavailable, missing, incomplete, broken, or invalid configurations.\n",
      "Topic 17: Connection Timeout Issues - Problems related to connection timeouts, gateway timeouts, and service timeouts.\n",
      "Topic 18: Training Issues - Problems related to denied, failed, unresponsive, or invalid training jobs.\n",
      "Topic 19: Graph Issues - Problems related to unrendered, mismatched, large, blank, or inaccurate graphs and charts.\n",
      "Topic 20: Endpoint Issues - Problems related to unavailable, inaccessible, failed, invalid, or unimplemented endpoints.\n",
      "Topic 21: Command Issues - Problems related to unrecognized, missing, invalid, or outdated commands.\n",
      "Topic 22: Sweep Issues - Problems related to repeating, incorrect, or invalid sweeps and missing or skipped keys.\n",
      "Topic 23: Run Issues - Problems related to nested, restricted, grouped, repeated, indefinite, or unavailable runs.\n",
      "Topic 24: Package Issues - Problems related to package errors, invalid packages, incompatible packages, or issues with package installation.\n",
      "Topic 25: Permission Issues - Problems related to denied, insufficient, or forbidden permissions.\n",
      "Topic 26: Loading Errors - Problems related to loading errors, invalid loads, loading exceptions, or failed data loads.\n",
      "Topic 27: GPU Availability Issues - Problems related to unavailable, missing, unused, inactive, unrecognized, or unresponsive GPUs.\n",
      "Topic 28: Feature Inactivity - Problems related to inactive features, options, buttons, or unavailable features.\n",
      "Topic 29: Import/Export Errors - Problems related to errors in importing or exporting data.\n",
      "Topic 30: Column Issues - Problems related to unrecognized, unsupported, invalid, unavailable, unexpected, or improper columns.\n",
      "Topic 31: Metric Availability Issues - Problems related to unavailable, invisible, unlogged, unrecorded, or empty metrics.\n",
      "Topic 32: Notebook Unresponsiveness - Issues related to unresponsive, inactive, uninterrupted, or automated notebooks.\n",
      "Topic 33: Pipeline Issues - Problems related to pipeline errors, invalid pipelines, failed pipelines, or incorrect pipelines.\n",
      "Topic 34: Model Availability Issues - Problems related to unavailable, existing, lost, unsupported, or invalid models.\n",
      "Topic 35: Prediction Inconsistencies - Problems related to different, inconsistent, incorrect, invalid, or mismatching predictions.\n",
      "Topic 36: Training Errors - Problems related to training errors, invalid training, unsupported training, broken training, or unknown training.\n",
      "Topic 37: Label Issues - Problems related to unavailable, inaccessible, invisible, unpopulated, or invalid labels.\n",
      "Topic 38: Environment Issues - Problems related to missing, unavailable, invalid, or unconfigurable environments.\n",
      "Topic 39: Docker Issues - Problems related to building Docker images, Dockerfile errors, or Docker creation.\n",
      "Topic 40: Argument Issues - Problems related to unexpected, invalid, unused, unknown, or incompatible arguments.\n",
      "Topic 41: Server Errors - Problems related to server errors, internal errors, hosting errors, client errors, or unknown errors.\n",
      "Topic 42: Import and Module Issues - Problems related to importing modules and errors associated with these processes.\n",
      "Topic 43: Directory Issues - Problems related to unavailable, unrecognized, invalid, invisible, or incorrect directories.\n",
      "Topic 44: Media Issues - Problems related to unsupported, invalid, unexpected, unavailable, or error-prone media types.\n",
      "Topic 45: Serialization Issues - Problems related to unserializable, non-serializable, or serializable objects.\n",
      "Topic 46: Memory Issues - Problems related to memory overflow, overload, limit, leaks, or exceptions.\n",
      "Topic 47: Module Issues - Problems related to missing, unavailable, unexpected, or removed modules.\n",
      "Topic 48: Attribute Issues - Problems related to invalid, unrecognized, unexpected, unsupported, or incorrect attributes.\n",
      "Topic 49: Logging Inconsistencies - Problems related to inconsistent, malfunctioning, skipped, unpicklable, or incomplete logging.\n",
      "Topic 50: Data Availability Issues - Problems related to unavailable, malfunctioning, insufficient, inaccessible, incomplete, or disappearing data.\n",
      "Topic 51: Model Issues - Problems related to invalid, unrecognized, incompatible, unsupported, or incorrect models.\n",
      "Topic 52: Batch Issues - Problems related to batch errors, invalid batches, or batch transformations.\n",
      "Topic 53: Limit Exceedance - Problems related to exceeded size, limit, file, or exhausted errors.\n",
      "Topic 54: Login Issues - Problems related to interactive, disruptive, persistent, unexpected logins, or session errors.\n",
      "Topic 55: Filesystem Issues - Problems related to read-only filesystems, unavailable filesystems, write errors, or saving read issues.\n",
      "Topic 56: Pipeline Issues - Problems related to unavailable, unlogged, incomplete, unsuccessful, or unlabeled pipelines.\n",
      "Topic 57: Kernel Issues - Problems related to unstable, broken, crashed, invalid, or dying kernels.\n",
      "Topic 58: Run ID Issues - Problems related to invalid, unavailable, or unterminated run instances.\n",
      "Topic 59: Ping Issues - Problems related to failed, unresponsive pings, unavailable IPs, unresolved IPs, or unavailable hosts.\n",
      "Topic 60: Service Issues - Problems related to inaccessible, broken, incorrect, unknown, unavailable, or forbidden services.\n",
      "Topic 61: Model Loading Issues - Problems related to loading models, load errors, model errors, invalid models, or missing model functions.\n",
      "Topic 62: Upload Issues - Problems related to upload errors, insufficient uploads, denied uploads, or confused uploads.\n",
      "Topic 63: Training Speed Issues - Problems related to slow, empty, invalid, incomplete, delayed, or unsuccessful training.\n",
      "Topic 64: Job Issues - Problems related to job errors, scheduled errors, task errors, exceeded jobs, or job configurations.\n",
      "Topic 65: Domain Issues - Problems related to domain creation, existing domains, unauthorized domains, unsupported domains, or invisible domains.\n",
      "Topic 66: Model Deployment Issues - Problems related to deploying models, deployment errors, or unavailable deployed models.\n",
      "Topic 67: Endpoint Issues - Problems related to endpoint errors, invocations, invalid endpoints, or endpoint launches.\n",
      "Topic 68: Instance Issues - Problems related to invalid, failed, unterminated, or created instances.\n",
      "Topic 69: Dependency Conflicts - Problems related to conflicting, incompatible, or failed dependencies.\n",
      "Topic 70: Undefined Symbols - Problems related to undefined symbols, variables, functions, or names.\n",
      "Topic 71: Output Issues - Problems related to unavailable, disappearing, missing, ineffective, blocked, or unseen outputs.\n",
      "Topic 72: Deployment Unresponsiveness - Problems related to unresponsive, stalled, timed out, slow, unhealthy, or pending deployments.\n",
      "Topic 73: Conversion Errors - Problems related to conversion errors, float conversions, string conversions, or unsupported conversions.\n",
      "Topic 74: Model Saving Issues - Problems related to unsaved, unlocated, unsaveable, deleted, or inaccessible models.\n",
      "Topic 75: Requirement Issues - Problems related to unsatisfied requirements, constraints, unmet conditions, requirement conflicts, or misinterpreted requirements.\n",
      "Topic 76: Disk Space Issues - Problems related to insufficient disk space, full disks, insufficient storage, or disk errors.\n",
      "Topic 77: Access Restrictions - Problems related to restricted access, invite restrictions, restricted teams, versions, or environments.\n",
      "Topic 78: Credential Issues - Problems related to unavailable, invalid, incorrect, or unauthorized credentials.\n",
      "Topic 79: Role Issues - Problems related to unauthorized, invalid, unavailable, or incompatible roles.\n",
      "Topic 80: Syntax Issues - Problems related to invalid, incorrect, unsupported syntax, or parsing errors.\n",
      "Topic 81: Function Issues - Problems related to unimplemented, unavailable, unrecognized, undetected, incorrect, or unsupported functions.\n",
      "Topic 82: Operation Issues - Problems related to unsupported operations, errors, executions, schemes, operands, values, or functions.\n",
      "Topic 83: Parameter Issues - Problems related to invalid, unrecognized, disallowed, or incorrect parameters.\n",
      "Topic 84: Link Issues - Problems related to broken, unlinked, malfunctioning, unidentified, unclickable, or linking errors.\n",
      "Topic 85: Image Availability Issues - Problems related to unavailable, inaccessible, unattachable, invisible, or unpulled images.\n",
      "Topic 86: Attribute Recognition Issues - Problems related to unrecognized or nonexistent attributes.\n",
      "Topic 87: Transfer Errors - Problems related to errors in copying, transporting, cloning, or moving files.\n",
      "Topic 88: Path Issues - Problems related to incorrect, unrecognized, invalid, or unavailable paths.\n",
      "Topic 89: Token Issues - Problems related to invalid, missing, unavailable, unresolved, ignored, or unauthorized tokens.\n",
      "Topic 90: Log Availability Issues - Problems related to unavailable, missing, empty, or unresponsive logs.\n",
      "Topic 91: Encoding Issues - Problems related to invalid, unexpected, incorrectly encoded, or mismatched encodings.\n",
      "Topic 92: Authorization Issues - Problems related to pending, unauthorized, invalid, or error-prone authorizations.\n",
      "Topic 93: Result Inconsistencies - Problems related to inconsistent, incorrect, discrepant, different, misleading, or different results.\n",
      "Topic 94: Output Issues - Problems related to overlapping, duplicated, single, different, or consistent outputs.\n",
      "Topic 95: Null and Empty Issues - Problems related to null descriptions, empty data, objects, datasets, attributes, values, evaluations, or tasks.\n",
      "Topic 96: Tensorboard Issues - Problems related to unrecognized, unavailable, consuming, discrepant, incompatible, undesired, or inaccessible Tensorboards.\n",
      "Topic 97: Parameter Logging Issues - Problems related to unlogged, unadded, empty, unavailable, missing, or inaccessible parameters.\n",
      "Topic 98: Quota Exceedance - Problems related to exceeded, exhausted, insufficient, unlisted, or resource-exceeded quotas.\n",
      "Topic 99: Time and Date Issues - Problems related to incorrect epochs, times, datetimes, time precisions, erroneous dates, unstable timestamps, or time scales.\n",
      "Topic 100: Package Uninstallation Issues - Problems related to uninstallable, uninstalled, or error-prone packages or drivers.\n",
      "Topic 101: Download Errors - Problems related to download errors, failures, troubles, broken downloads, or unavailable downloads.\n",
      "Topic 102: Image Issues - Problems related to image errors, invalid images, unsupported images, broken images, or image creation.\n",
      "Topic 103: Compilation Issues - Problems related to compilation errors, fatal compilations, uncompiled, broken compiles, build errors, or incomplete compilations.\n",
      "Topic 104: Push Issues - Problems related to push errors, unsupported pushes, unexpected pushes, denied pushes, or unresponsive pushes.\n",
      "Topic 105: Loop and Backoff Issues - Problems related to loop backoffs, loop errors, retries loops, stop errors, or exceeded backoffs.\n",
      "Topic 106: Authentication Issues - Problems related to authentication errors, failed authentications, password errors, unexpected authentications, or proxy errors.\n",
      "Topic 107: Execution Issues - Problems related to execution errors, script errors, incompatible scripts, execute errors, unexecuted scripts, or failed executions.\n",
      "Topic 108: Initialization Issues - Problems related to initialization errors, invalid initializations, incorrect initializations, scoring initializations, inconsistent initializations, or incomplete initializations.\n",
      "Topic 109: Resource Issues - Problems related to insufficient, unavailable, exhausted, shortage, ineffective, or underutilized resources.\n",
      "Topic 110: Version Issues - Problems related to Python versions, outdated PyTorch, unsupported Python, Python upgrades, Python incompatibility, deprecated Python, or unavailable Python.\n",
      "Topic 111: Bucket Issues - Problems related to invalid, inaccessible, undefined, incorrect, or missing buckets.\n",
      "Topic 112: Hyperparameter Issues - Problems related to invalid, unsupported, unavailable, malfunctioning, incorrect, or tuning hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_anomaly, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_root_cause, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_solution, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_result_rq1, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different metrics to each post\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['Solution_word_count'] = np.nan\n",
    "df['Solution_sentence_count'] = np.nan\n",
    "df['Solution_readability'] = np.nan\n",
    "df['Solution_reading_time'] = np.nan\n",
    "df['Solution_link_count'] = np.nan\n",
    "\n",
    "df['Challenge_solved_time'] = np.nan\n",
    "df['Challenge_adjusted_solved_time'] = np.nan\n",
    "\n",
    "df['Solution_link_docs'] = np.nan\n",
    "df['Solution_link_issues'] = np.nan\n",
    "df['Solution_link_patches'] = np.nan\n",
    "df['Solution_link_tools'] = np.nan\n",
    "df['Solution_link_tutorials'] = np.nan\n",
    "df['Solution_link_examples'] = np.nan\n",
    "\n",
    "df['Challenge_created_time'] = pd.to_datetime(df['Challenge_created_time'])\n",
    "df['Challenge_closed_time'] = pd.to_datetime(df['Challenge_closed_time'])\n",
    "df['Challenge_last_edit_time'] = pd.to_datetime(df['Challenge_last_edit_time'])\n",
    "df['Solution_last_edit_time'] = pd.to_datetime(df['Solution_last_edit_time'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    challenge_content = row['Challenge_title'] + '.' + str(row['Challenge_body'])\n",
    "    df.at[index, 'Challenge_word_count'] = textstat.lexicon_count(challenge_content)\n",
    "    df.at[index, 'Challenge_sentence_count'] = textstat.sentence_count(challenge_content)\n",
    "    df.at[index, 'Challenge_readability'] = textstat.flesch_kincaid_grade(challenge_content)\n",
    "    df.at[index, 'Challenge_reading_time'] = textstat.reading_time(challenge_content)\n",
    "    \n",
    "    links = list(set(re.findall(link_pattern, challenge_content)))\n",
    "    df.at[index, 'Challenge_link_count'] = len(links)\n",
    "\n",
    "    solution_content = row['Solution_body']\n",
    "\n",
    "    if pd.notna(solution_content):\n",
    "        df.at[index, 'Solution_word_count'] = textstat.lexicon_count(solution_content)\n",
    "        df.at[index, 'Solution_sentence_count'] = textstat.sentence_count(solution_content)\n",
    "        df.at[index, 'Solution_readability'] = textstat.flesch_kincaid_grade(solution_content)\n",
    "        df.at[index, 'Solution_reading_time'] = textstat.reading_time(solution_content)\n",
    "        \n",
    "        links = list(set(re.findall(link_pattern, solution_content)))\n",
    "        df.at[index, 'Solution_link_count'] = len(links)\n",
    "        \n",
    "        link_docs = 0\n",
    "        link_tools = 0\n",
    "        link_issues = 0\n",
    "        link_patches = 0\n",
    "        link_tutorials = 0\n",
    "        link_examples = 0\n",
    "    \n",
    "        for link in links:\n",
    "            if any([patch in link for patch in keywords_patch]):\n",
    "                link_patches += 1\n",
    "            elif any([issue in link for issue in keywords_issue]):\n",
    "                link_issues += 1\n",
    "            elif any([tool in link for tool in keywords_tool]):\n",
    "                link_tools += 1\n",
    "            elif any([doc in link for doc in keywords_doc]):\n",
    "                link_docs += 1\n",
    "            elif any([tool in link for tool in keywords_tutorial]):\n",
    "                link_tutorials += 1\n",
    "            else:\n",
    "                link_examples += 1\n",
    "                \n",
    "        df.at[index, 'Solution_link_docs'] = link_docs\n",
    "        df.at[index, 'Solution_link_issues'] = link_issues\n",
    "        df.at[index, 'Solution_link_patches'] = link_patches\n",
    "        df.at[index, 'Solution_link_tools'] = link_tools\n",
    "        df.at[index, 'Solution_link_tutorials'] = link_tutorials\n",
    "        df.at[index, 'Solution_link_examples'] = link_examples\n",
    "        \n",
    "    creation_time = row['Challenge_created_time']\n",
    "    closed_time = row['Challenge_closed_time']\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "\n",
    "    creation_time = row['Challenge_last_edit_time'] if pd.notna(row['Challenge_last_edit_time']) else creation_time\n",
    "    closed_time = row['Solution_last_edit_time'] if pd.notna(row['Solution_last_edit_time']) else closed_time\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = df.at[index, 'Challenge_solved_time']\n",
    "\n",
    "df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "df['Challenge_answer_count'] = df['Challenge_answer_count'].fillna(0)\n",
    "df['Challenge_participation_count'] = df['Challenge_answer_count'] + df['Challenge_comment_count']\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_json(os.path.join(path_result_rq1, 'metrics.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Count'] = 1\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_topic_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge view count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_view_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge score count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_score_count'] = df_topics['Challenge_score_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_score_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge favorite count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_favorite_count'] = df_topics['Challenge_favorite_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_favorite_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge answer count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_answer_count'] = df_topics['Challenge_answer_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_answer_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge comment count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_comment_count'] = df_topics['Challenge_comment_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_comment_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_participation_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_participation_count_distribution.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
