{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import openai\n",
    "import random\n",
    "import enchant\n",
    "import textstat\n",
    "import itertools\n",
    "import collections\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, strip_punctuation, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../Dataset'\n",
    "path_result = '../../Result'\n",
    "path_rq1 = os.path.join(path_result, 'RQ1')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# subprocess.run(['python', '-m' 'spacy', 'download', 'en_core_web_sm'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "prompt_summary = '''Using simple English, write a brief sentence (fewer than 30 words) that captures the core idea of the given text.\\n###'''\n",
    "\n",
    "tools_keyword_mapping = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sagemaker', 'amazon', 'aws'],\n",
    "    'Azure Machine Learning': ['azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow', 'databricks'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex', 'google', 'gcp'],\n",
    "    'Weights & Biases': ['weights', 'biases', 'wandb']\n",
    "}\n",
    "\n",
    "keywords_image = {\n",
    "    \".jpg\", \n",
    "    \".png\", \n",
    "    \".jpeg\", \n",
    "    \".gif\", \n",
    "    \".bmp\", \n",
    "    \".webp\", \n",
    "    \".svg\", \n",
    "    \".tiff\"\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n",
    "\n",
    "stop_words_se = {\n",
    "    'ability',\n",
    "    'abilities',\n",
    "    'accident',\n",
    "    'accidents',\n",
    "    # 'acknowledgement',\n",
    "    'action',\n",
    "    'actions',\n",
    "    'activities',\n",
    "    'activity',\n",
    "    'advice',\n",
    "    'alternative',\n",
    "    'alternatives',\n",
    "    # 'announcement',\n",
    "    'anomaly'\n",
    "    'anomalies'\n",
    "    'answer',\n",
    "    'answers',\n",
    "    'appreciation',\n",
    "    'approach',\n",
    "    'approaches',\n",
    "    'article',\n",
    "    'articles',\n",
    "    'assistance',\n",
    "    'attempt',\n",
    "    'author',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'benefit',\n",
    "    'bit',\n",
    "    'bits',\n",
    "    'block',\n",
    "    'blocks',\n",
    "    # 'blog',\n",
    "    # 'blogs',\n",
    "    'body',\n",
    "    'bug',\n",
    "    'bugs',\n",
    "    'building',\n",
    "    'case',\n",
    "    'cases',\n",
    "    'categories',\n",
    "    'categorization',\n",
    "    'category',\n",
    "    'cause',\n",
    "    'causes',\n",
    "    'challenge',\n",
    "    'challenges',\n",
    "    'change',\n",
    "    'changes',\n",
    "    'char',\n",
    "    'character',\n",
    "    'characters',\n",
    "    'check',\n",
    "    'choice',\n",
    "    'choices',\n",
    "    'collection',\n",
    "    'com',\n",
    "    'combination',\n",
    "    # 'commmunication',\n",
    "    # 'community',\n",
    "    # 'communities',\n",
    "    # 'company',\n",
    "    # 'companies',\n",
    "    # 'computer',\n",
    "    # 'computers',\n",
    "    # 'concept',\n",
    "    # 'concepts',\n",
    "    'concern',\n",
    "    'concerns',\n",
    "    # 'condition',\n",
    "    # 'conditions',\n",
    "    'confirmation',\n",
    "    'confusion',\n",
    "    'consideration',\n",
    "    'contact',\n",
    "    # 'content',\n",
    "    # 'contents',\n",
    "    'context',\n",
    "    # 'count',\n",
    "    'couple',\n",
    "    'couples',\n",
    "    # 'course',\n",
    "    # 'courses',\n",
    "    'crash',\n",
    "    'crashes',\n",
    "    'cross',\n",
    "    # 'custom',\n",
    "    'customer',\n",
    "    'customers',\n",
    "    'day',\n",
    "    'days',\n",
    "    'demand',\n",
    "    # 'description',\n",
    "    'desire',\n",
    "    'detail',\n",
    "    'details',\n",
    "    'difference',\n",
    "    'differences',\n",
    "    'difficulties',\n",
    "    'difficulty',\n",
    "    'discrepancies',\n",
    "    'discrepancy',\n",
    "    'discussion',\n",
    "    'dislike',\n",
    "    'distinction',\n",
    "    'effect',\n",
    "    # 'engineering',\n",
    "    'enquiries',\n",
    "    'enquiry',\n",
    "    'error',\n",
    "    'errors',\n",
    "    'evidence',\n",
    "    'example',\n",
    "    'examples',\n",
    "    'exception',\n",
    "    'exceptions',\n",
    "    'existence',\n",
    "    'exit',\n",
    "    'expectation',\n",
    "    'experience',\n",
    "    'expert',\n",
    "    'experts',\n",
    "    # 'explanation',\n",
    "    'fact',\n",
    "    'facts',\n",
    "    'fail',\n",
    "    'failure',\n",
    "    'favorite',\n",
    "    'favorites',\n",
    "    'fault',\n",
    "    'faults',\n",
    "    # 'feature',\n",
    "    # 'features',\n",
    "    # 'feedback',\n",
    "    # 'feedbacks',\n",
    "    'fix',\n",
    "    'fixes',\n",
    "    'float',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'functionality',\n",
    "    'functionalities',\n",
    "    'future',\n",
    "    'goal',\n",
    "    'goals',\n",
    "    'guarantee',\n",
    "    # 'guidance',\n",
    "    # 'guideline',\n",
    "    # 'guide',\n",
    "    'guy',\n",
    "    'guys',\n",
    "    'harm',\n",
    "    'hello',\n",
    "    'help',\n",
    "    'hour',\n",
    "    'hours',\n",
    "    'ibm',\n",
    "    'idea',\n",
    "    'ideas',\n",
    "    'individual',\n",
    "    'individuals',\n",
    "    'info',\n",
    "    'information',\n",
    "    'inquiries',\n",
    "    'inquiry',\n",
    "    'insight',\n",
    "    # 'instruction',\n",
    "    # 'instructions',\n",
    "    'int',\n",
    "    'intelligence',\n",
    "    'intent',\n",
    "    'interest',\n",
    "    'introduction',\n",
    "    'investigation',\n",
    "    'invitation',\n",
    "    'issue',\n",
    "    'issues',\n",
    "    'kind',\n",
    "    'kinds',\n",
    "    'lack',\n",
    "    'learning',\n",
    "    'level',\n",
    "    'levels',\n",
    "    'look',\n",
    "    'looks',\n",
    "    'lot',\n",
    "    'lots',\n",
    "    'luck',\n",
    "    'machine',\n",
    "    'machines',\n",
    "    'major',\n",
    "    'manner',\n",
    "    'manners',\n",
    "    # 'manual',\n",
    "    'mark',\n",
    "    'meaning',\n",
    "    # 'message',\n",
    "    # 'messages',\n",
    "    'method',\n",
    "    'methods',\n",
    "    'minute',\n",
    "    'minutes',\n",
    "    'mistake',\n",
    "    'mistakes',\n",
    "    'month',\n",
    "    'months',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'number',\n",
    "    'numbers',\n",
    "    'offer',\n",
    "    'one',\n",
    "    'ones',\n",
    "    'opinion',\n",
    "    'opinions',\n",
    "    # 'org',\n",
    "    # 'organization',\n",
    "    'outcome',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'past',\n",
    "    'people',\n",
    "    'person',\n",
    "    'persons',\n",
    "    'perspective',\n",
    "    'perspectives',\n",
    "    'place',\n",
    "    'places',\n",
    "    'point',\n",
    "    'points',\n",
    "    'post',\n",
    "    'posts',\n",
    "    'practice',\n",
    "    'practices',\n",
    "    'problem',\n",
    "    'problems',\n",
    "    # 'product',\n",
    "    # 'products',\n",
    "    # 'program',\n",
    "    # 'programs',\n",
    "    # 'project',\n",
    "    # 'projects',\n",
    "    # 'proposal',\n",
    "    'purpose',\n",
    "    'purposes',\n",
    "    # 'python',\n",
    "    'question',\n",
    "    'questions',\n",
    "    'reason',\n",
    "    'reasons',\n",
    "    # 'recognition',\n",
    "    # 'recommendation',\n",
    "    # 'recommendations',\n",
    "    # 'recommender',\n",
    "    # 'regression',\n",
    "    # 'request',\n",
    "    'research',\n",
    "    'result',\n",
    "    'results',\n",
    "    'scenario',\n",
    "    'scenarios',\n",
    "    'science',\n",
    "    'screenshot',\n",
    "    'screenshots',\n",
    "    'second',\n",
    "    'seconds',\n",
    "    'section',\n",
    "    'sense',\n",
    "    'sentence',\n",
    "    'show',\n",
    "    'shows',\n",
    "    'situation',\n",
    "    'software',\n",
    "    'solution',\n",
    "    'solutions',\n",
    "    # 'start',\n",
    "    # 'state',\n",
    "    # 'statement',\n",
    "    # 'states',\n",
    "    # 'status',\n",
    "    # 'step',\n",
    "    # 'steps',\n",
    "    'string',\n",
    "    'study',\n",
    "    'stuff',\n",
    "    'success',\n",
    "    'suggestion',\n",
    "    'suggestions',\n",
    "    'summary',\n",
    "    'summaries',\n",
    "    'surprise',\n",
    "    # 'support',\n",
    "    'talk',\n",
    "    # 'task',\n",
    "    # 'tasks',\n",
    "    # 'technique',\n",
    "    # 'techniques',\n",
    "    # 'technologies',\n",
    "    # 'technology',\n",
    "    'term',\n",
    "    'terms',\n",
    "    'times',\n",
    "    'thank',\n",
    "    'thanks',\n",
    "    'thing',\n",
    "    'things',\n",
    "    'thought',\n",
    "    'three',\n",
    "    'title',\n",
    "    'today',\n",
    "    'tomorrow',\n",
    "    # 'tool',\n",
    "    # 'tools',\n",
    "    'topic',\n",
    "    'topics',\n",
    "    'total',\n",
    "    'trouble',\n",
    "    'troubles',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'two',\n",
    "    'understand',\n",
    "    'understanding',\n",
    "    'usage',\n",
    "    'use',\n",
    "    'user',\n",
    "    'users',\n",
    "    'uses',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'week',\n",
    "    'weeks',\n",
    "    'word',\n",
    "    'words',\n",
    "    'work',\n",
    "    'workaround',\n",
    "    'workarounds',\n",
    "    'works',\n",
    "    'yeah',\n",
    "    'year',\n",
    "    'years',\n",
    "    'yesterday',\n",
    "}\n",
    "\n",
    "stop_words_ml = {\n",
    "    'analysis',\n",
    "    'anomaly',\n",
    "    'audio',\n",
    "    'autopilot',\n",
    "    'chatbot',\n",
    "    'classification',\n",
    "    'classifier',\n",
    "    'clustering',\n",
    "    'decision',\n",
    "    'detection',\n",
    "    'dimensionality'\n",
    "    'forecasting',\n",
    "    'forest',\n",
    "    'fraud',\n",
    "    'gesture',\n",
    "    'language',\n",
    "    'phone',\n",
    "    'processing',\n",
    "    'recognition',\n",
    "    'recommendation',\n",
    "    'recommender',\n",
    "    'reduction',\n",
    "    'regression',\n",
    "    'regressor',\n",
    "    'reinforcement',\n",
    "    'segmentation',\n",
    "    'sentiment',\n",
    "    'series',\n",
    "    'spam',\n",
    "    'speech',\n",
    "    'sound',\n",
    "    'translation',\n",
    "    'tree',\n",
    "    'video',\n",
    "    'vision',\n",
    "    'voice',\n",
    "}\n",
    "\n",
    "tools_keyword_list = set(itertools.chain(*tools_keyword_mapping.values()))\n",
    "stop_words_basic = STOPWORDS.union(tools_keyword_list)\n",
    "stop_words_level1 = stop_words_basic.union(stop_words_se)\n",
    "stop_words_level2 = stop_words_level1.union(stop_words_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(block_list):\n",
    "    total_loc = 0\n",
    "    for blocks in block_list:\n",
    "        for block in blocks:\n",
    "            for line in block.splitlines():\n",
    "                if line.strip():\n",
    "                    total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    # extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract code blocks type 1\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code blocks type 2\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    return clean_text, links, code_line\n",
    "\n",
    "def extract_code(content):\n",
    "    code_patterns = [r'```.+?```', r'``.+?``', r'`.+?`']\n",
    "    clean_text = content\n",
    "    code_line = 0\n",
    "\n",
    "    for code_pattern in code_patterns:\n",
    "        code_snippets = re.findall(code_pattern, clean_text, flags=re.DOTALL)\n",
    "        code_line += count_code_line(code_snippets)\n",
    "        clean_text = re.sub(code_pattern, '', clean_text, flags=re.DOTALL)\n",
    "    \n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern1 = r\"\\!?\\[.*?\\]\\((.*?)\\)\"\n",
    "    links1 = re.findall(link_pattern1, text)\n",
    "    clean_text = re.sub(link_pattern1, '', text)\n",
    "    link_pattern2 = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links2 = re.findall(link_pattern2, clean_text)\n",
    "    clean_text = re.sub(link_pattern2, '', clean_text)\n",
    "    links = links1 + links2\n",
    "    return clean_text, links\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_line1 = extract_styles(content)\n",
    "    clean_text, code_line2 = extract_code(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_line = code_line1 + code_line2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_line'])\n",
    "    return content_collection(clean_text, links, code_line)\n",
    "\n",
    "def word_frequency(text):\n",
    "    word_counts = collections.Counter(text.split())\n",
    "    return word_counts\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def extract_english(text):\n",
    "    words = [word for word in text.split() if spell_checker.check(word)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_text(text, level=1):\n",
    "    clean_text = text.lower()\n",
    "    # clean_text = strip_punctuation(clean_text)\n",
    "    # clean_text = extract_english(clean_text)\n",
    "    # clean_text = strip_short(clean_text)\n",
    "    clean_text = extract_nouns(clean_text)\n",
    "    match level:\n",
    "        case 1:\n",
    "            clean_text = remove_stopwords(clean_text, stop_words_level1)\n",
    "        case 2:\n",
    "            clean_text = remove_stopwords(clean_text, stop_words_level2)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_links(links):\n",
    "    image_links = 0\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([image in link for image in keywords_image]):\n",
    "            image_links += 1\n",
    "        elif any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['image', 'documentation', 'tool', 'issue', 'patch', 'tutorial', 'example'])\n",
    "    return link_analysis(image_links, documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "\n",
    "def find_duplicates(in_list):  \n",
    "    duplicates = []\n",
    "    unique = set(in_list)\n",
    "    for each in unique:\n",
    "        count = in_list.count(each)\n",
    "        if count > 1:\n",
    "            duplicates.append(each)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_comment_body'] = row['Issue_comment_body']\n",
    "    df_issues.at[index, 'Challenge_comment_score'] = row['Issue_comment_score']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_comment_body'] = row['Question_comment_body']\n",
    "    df_questions.at[index, 'Challenge_comment_score'] = row['Question_comment_score']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_comment_score'] = row['Answer_comment_score']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df = df[df.columns.drop(list(df.filter(regex=r'(Issue|Question|Answer)_')))]\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "df = df.explode('Tools')\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tools', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.pdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "# del df['Challenge_preprocessed_content']\n",
    "# del df['Challenge_preprocessed_gpt_summary']\n",
    "# del df['Challenge_preprocessed_title']\n",
    "\n",
    "# df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p7/fg_w39cx6pq23vf3798tdmq00000gn/T/ipykernel_60381/824407263.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n",
      "/Users/jimmy/Documents/GitHub/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 & 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    clean_title1 = preprocess_text(title_analyzer.text)\n",
    "    clean_title2 = preprocess_text(title_analyzer.text, 2)\n",
    "    df.at[index, 'Challenge_preprocessed_title1'] = clean_title1\n",
    "    df.at[index, 'Challenge_preprocessed_title2'] = clean_title2\n",
    "    \n",
    "    challenge_analyzer = split_content(row['Challenge_title'] + row['Challenge_body'])\n",
    "    link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    clean_text1 = preprocess_text(challenge_analyzer.text)\n",
    "    clean_text2 = preprocess_text(challenge_analyzer.text, 2)\n",
    "    df.at[index, 'Challenge_preprocessed_content1'] = clean_text1\n",
    "    df.at[index, 'Challenge_preprocessed_content2'] = clean_text2\n",
    "    \n",
    "    df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Challenge_comment_body']):\n",
    "        comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Solution_body']):\n",
    "        solution_analyzer = split_content(row['Solution_body'])\n",
    "        link_analyzer = analyze_links(solution_analyzer.links)\n",
    "        text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "        df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "    if pd.notna(row['Solution_comment_body']):\n",
    "        comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 99:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    # if pd.notna(row['Challenge_gpt_summary']):\n",
    "    #     continue\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + '\\nBody: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo-16k',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=50,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clean_summary1 = preprocess_text(row['Challenge_gpt_summary'])\n",
    "    clean_summary2 = preprocess_text(row['Challenge_gpt_summary'], 2)\n",
    "    df.at[index, 'Challenge_preprocessed_gpt_summary1'] = clean_summary1\n",
    "    df.at[index, 'Challenge_preprocessed_gpt_summary2'] = clean_summary2\n",
    "\n",
    "df.sort_index(axis=1, inplace=True)\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if 'Stack' not in row['Platform']:\n",
    "#         continue\n",
    "#     for i2, r2 in df_old.iterrows():\n",
    "#         if 'Stack' not in r2['Platform']:\n",
    "#             continue\n",
    "#         if row['Challenge_link'] == r2['Challenge_link']:\n",
    "#             df.at[index, 'Tools'] = r2['Tools']\n",
    "#             break\n",
    "\n",
    "# df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from bertopic.backend import OpenAIBackend\n",
    "\n",
    "# # openai.api_key = MY_API_KEY\n",
    "# embedding_model = OpenAIBackend(delay_in_seconds=0.1, batch_size=10)\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# docs = df[df['Challenge_summary'] != 'na']['Challenge_summary'].tolist() + df[df['Challenge_root_cause'] != 'na']['Challenge_root_cause'].tolist()\n",
    "\n",
    "# topic_model = BERTopic(embedding_model=embedding_model)\n",
    "# topics, probs = topic_model.fit_transform(docs)\n",
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minimize_weighted_sum(df, sort_column):\n",
    "#     df_new = df.sort_values(sort_column, ascending=False)\n",
    "#     n = len(df)\n",
    "#     center_idx = (n - 1) // 2\n",
    "#     direction = -1\n",
    "#     distance = 0\n",
    "\n",
    "#     for _, row in df_new.iterrows():\n",
    "#         # Calculate the new index\n",
    "#         new_idx = center_idx + direction * distance\n",
    "        \n",
    "#         # Place the element from the sorted list into the new list\n",
    "#         df.iloc[new_idx] = row\n",
    "\n",
    "#         # If we've just moved to the left, increase the distance\n",
    "#         if direction == -1:\n",
    "#             distance += 1\n",
    "\n",
    "#         # Switch the direction\n",
    "#         direction *= -1\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Docker Image - A Docker image is a lightweight, standalone, executable package that includes everything needed to run a piece of software.\n",
      "1: Modeling - Modeling in software engineering refers to the process of creating a representation of a system or a subsystem to help with system design and decision making.\n",
      "2: Image Classification - Image classification is a process in computer vision that classifies images into predefined categories.\n",
      "3: Endpoint - An endpoint in software engineering refers to a remote computing device that communicates back and forth with a network to which it is connected.\n",
      "4: Feature Importance - Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.\n",
      "5: Studio - Studio in software engineering often refers to an integrated development environment (IDE) where developers can write, test, and debug their code.\n",
      "6: Pipeline - A pipeline in software engineering is a set of data processing elements connected in series, where the output of one element is the input of the next.\n",
      "7: Sweep - Sweep in software engineering often refers to a method of systematically searching through a space of parameter choices for machine learning models.\n",
      "8: Labeling Job - A labeling job in machine learning involves annotating data, like images or text, so that it can be used for model training.\n",
      "9: Log - Log refers to the record of events happening in the system, which is useful for debugging and monitoring purposes.\n",
      "10: Model Version - Model version refers to the different iterations or updates of a machine learning model.\n",
      "11: Notebook Instance - A notebook instance is a fully managed computing environment that enables users to easily develop and run Jupyter notebooks.\n",
      "12: Workspace - Workspace in software engineering is a set of related files and resources that are used for a particular project.\n",
      "13: Model - A model in software engineering often refers to a mathematical representation of a system or a process.\n",
      "14: Plot - Plot refers to a graphical representation of data, which is a crucial part of data analysis and machine learning.\n",
      "15: Instance - An instance in software engineering often refers to a single occurrence of an object or a class.\n",
      "16: Experiments - Experiments in software engineering often refer to the process of testing different configurations or versions of a system to evaluate their performance.\n",
      "17: Data - Data refers to the raw information or facts that are used as input for computations or analyses.\n",
      "18: Environment Variables - Environment variables are dynamic-named values that can affect the way running processes will behave on a computer.\n",
      "19: Import - Import refers to the process of bringing in code from other modules or libraries to use in your program.\n",
      "20: Spark - Spark is a fast and general-purpose cluster computing system for big data processing and machine learning.\n",
      "21: File Directory - A file directory is a storage structure which organizes files and folders in a hierarchical manner.\n",
      "22: Speech - Speech in software engineering often refers to technologies that enable computers to understand and respond to spoken language.\n",
      "23: Parameters - Parameters are the configurable parts of a process or function, allowing it to be tailored to specific needs.\n",
      "24: Pipeline - A pipeline in software engineering is a set of data processing elements connected in series.\n",
      "25: Prediction - Prediction refers to the output of an algorithm after it has been trained on a historical dataset and applied to new data.\n",
      "26: Training Job - A training job in machine learning refers to the process of teaching a model to make accurate predictions by feeding it data.\n",
      "27: Deployment - Deployment is the process of making a software system available for use.\n",
      "28: Python - Python is a high-level, interpreted programming language known for its readability and versatility.\n",
      "29: Cluster - A cluster in computing refers to a group of computers that work together to perform tasks.\n",
      "30: Scoring - Scoring in machine learning refers to the process of making predictions with a trained model.\n",
      "31: Web Service - A web service is a standardized way for software applications to communicate with each other over the internet.\n",
      "32: Script - A script is a program or sequence of instructions that is interpreted or carried out by another program.\n",
      "33: Model Training - Model training is the process of teaching a machine learning model to improve its accuracy by learning from data.\n",
      "34: Columns - Columns in software engineering often refer to the vertical segments of a database table that store specific information.\n",
      "35: Data Catalog - A data catalog is a service that provides a single, searchable index for data assets.\n",
      "36: Feature Store - A feature store is a centralized repository for storing, serving, and sharing machine learning features.\n",
      "37: Notebook - A notebook in software engineering is a web-based interface to a document that contains runnable code, visualizations, and narrative text.\n",
      "38: Service - A service in software engineering is a function that is well-defined, self-contained, and does not depend on the context or state of other services.\n",
      "39: Components Implementation - Components implementation refers to the process of defining and executing the individual parts of a software system.\n",
      "40: Output - Output in software engineering refers to the result that a software system produces.\n",
      "41: Bucket - A bucket in cloud computing is a basic container for storing data.\n",
      "42: Object Detection - Object detection is a computer vision technique for locating instances of objects in images or videos.\n",
      "43: Module - A module is a separate unit of software or hardware that can be used in combination with other units.\n",
      "44: Models - Models in software engineering often refer to representations of a system or a process.\n",
      "45: Configuration - Configuration in software engineering refers to the arrangement of each part of a software system.\n",
      "46: Pandas - Pandas is a software library for the Python programming language for data manipulation and analysis.\n",
      "47: Files - Files in software engineering refer to a container in a computer system for storing information.\n",
      "48: Execution - Execution in software engineering refers to the process of carrying out instructions by a computer.\n",
      "49: Jobs - Jobs in software engineering often refer to tasks or operations that a computer or a software system performs.\n",
      "50: REST - REST (Representational State Transfer) is a software architectural style that defines a set of constraints to be used for creating web services.\n",
      "51: Model Deployment - Model deployment is the process of making a machine learning model available in a production environment, where it can provide predictions.\n",
      "52: Autopilot - Autopilot in machine learning refers to automated processes that can handle tasks such as data preprocessing, model training, and model tuning.\n",
      "53: Model Endpoint - A model endpoint in machine learning is a route or URL where the model is deployed and can receive prediction requests.\n",
      "54: Storage - Storage in computing refers to devices or media that retain digital data.\n",
      "55: File - A file is a container in a computer system for storing information.\n",
      "56: Metrics - Metrics in software engineering are quantitative measures that help to assess the performance of a system.\n",
      "57: Server - A server is a computer or system that provides resources, data, services, or programs to other computers, known as clients, over a network.\n",
      "58: Object Attribute - An object attribute in programming refers to a variable or a value that belongs to an object.\n",
      "59: Ray Tune - Ray Tune is a Python library for experiment execution and hyperparameter tuning at any scale.\n",
      "60: Thread - A thread in computing is the smallest sequence of programmed instructions that can be managed independently by a scheduler.\n",
      "61: Git - Git is a distributed version-control system for tracking changes in source code during software development.\n",
      "62: Authentication - Authentication is the process of verifying the identity of a user, process, or device.\n",
      "63: Projects - Projects in software engineering refer to a planned set of interrelated tasks to be executed over a fixed period and within certain cost and other limitations.\n",
      "64: Model Registry - A model registry is a central hub where developers can manage and track their machine learning models.\n",
      "65: Training - Training in machine learning refers to the process of teaching a model to make accurate predictions by feeding it data.\n",
      "66: Integration - Integration in software engineering is the process of combining different computing systems and software applications physically or functionally.\n",
      "67: Batch Prediction - Batch prediction is a method of running predictions in which data is collected over time and predictions are made in groups.\n",
      "68: Account - An account in software engineering often refers to a record in a database that contains user-specific settings and information.\n",
      "69: Input - Input in software engineering refers to the data that is sent to a system for processing.\n",
      "70: Batch - Batch in software engineering often refers to a collection of jobs or tasks that are processed as a group.\n",
      "71: Transformers - Transformers in machine learning are a type of model that uses self-attention mechanisms and provide state-of-the-art results in many NLP tasks.\n",
      "72: Translation - Translation in machine learning refers to the process of converting text or speech from one language to another.\n",
      "73: Permission - Permission in software engineering refers to the authorization given to users or programs to access specific resources or perform specific actions.\n",
      "74: Inference - Inference in machine learning refers to the process of making predictions using a trained model.\n",
      "75: Quota - Quota in software engineering often refers to a limit on the number of resources that can be used.\n",
      "76: Argument - An argument in programming is a value that is passed between programs, subroutines or functions.\n",
      "77: Run - Run in software engineering refers to the process of executing a program or a piece of code.\n",
      "78: Code - Code in software engineering refers to the set of instructions written in a programming language that is executed by a computer.\n",
      "79: Training Directory - A training directory is a folder where training data for machine learning models is stored.\n",
      "80: Lambda - Lambda in software engineering often refers to a serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you.\n",
      "81: Validation - Validation in software engineering is the process of checking if a system meets the specifications and fulfills its intended purpose.\n",
      "82: Notebook Studio - Notebook Studio is an integrated development environment (IDE) for interactive computing and machine learning.\n",
      "83: File Format - A file format is a standard way in which information is encoded for storage in a computer file.\n",
      "84: Model Tar - Model tar refers to a compressed file that contains the trained model.\n",
      "85: Access - Access in software engineering refers to the permission to locate, view, or retrieve data.\n",
      "86: Tests - Tests in software engineering are used to check the functionality of a software program or a part of the program.\n",
      "87: Connection - Connection in software engineering often refers to the link between two or more devices or systems.\n",
      "88: Pipeline Model - A pipeline model is a sequence of data processing elements arranged so that the output of one element is the input of the next.\n",
      "89: Estimator - An estimator in machine learning is a type of algorithm used to estimate an unknown parameter based on observed data.\n",
      "90: Inference Pipeline - An inference pipeline is a sequence of steps used to make predictions with a trained machine learning model.\n",
      "91: Model Notebook - A model notebook is a document that contains both code (e.g. python or R) and rich text elements (paragraphs, equations, figures, links, etc…).\n",
      "92: Blob Storage - Blob storage is a service for storing large amounts of unstructured data, such as text or binary data, that can be accessed from anywhere in the world via HTTP or HTTPS.\n",
      "93: Tracking Server - A tracking server records and manages information about machine learning runs, including parameters, metrics, tags, and artifacts.\n",
      "94: Table - A table in software engineering often refers to a structured set of data.\n",
      "95: Experiment - An experiment in software engineering is a procedure carried out to verify, refute, or validate a hypothesis.\n",
      "96: Documentation - Documentation in software engineering is a written text or illustration that accompanies a software product or service.\n",
      "97: Artifacts - Artifacts in software engineering are the byproducts of the software development process, such as design documents, code listings, and executables.\n",
      "98: Memory - Memory in computing refers to the physical devices used to store programs or data on a temporary or permanent basis.\n",
      "99: Version - Version in software engineering refers to a particular state or variant of a software.\n",
      "100: Batch Transform - Batch transform is a method of running predictions in which data is collected over time and predictions are made in groups.\n",
      "101: Dependencies - Dependencies in software engineering are the relationships between different software modules where a change in one module may affect another.\n",
      "102: Artifact Store - An artifact store is a repository where build artifacts are stored and shared.\n",
      "103: Trials - Trials in machine learning refer to the different configurations of a model that are tested during hyperparameter tuning.\n",
      "104: Classification - Classification in machine learning is a type of supervised learning approach where the output is a category.\n",
      "105: Forecasting - Forecasting in machine learning is a type of modeling used to predict future values based on historical data.\n",
      "106: Network Retry - Network retry refers to the process of attempting a network request again if the first attempt fails.\n",
      "107: Notebook - A notebook in software engineering is a web-based interface to a document that contains runnable code, visualizations, and narrative text.\n",
      "108: Telemetry - Telemetry in software engineering is an automated communications process by which measurements and other data are collected at remote points and transmitted to receiving equipment for monitoring.\n",
      "109: Train Model - Train model refers to the process of teaching a machine learning model to improve its accuracy by learning from data.\n",
      "110: Model Package - A model package is a versioned bundle of artifacts used to create a machine learning model.\n",
      "111: Notebook Kernel - A notebook kernel is a “computational engine” that executes the code contained in a notebook document.\n",
      "112: Dependency - A dependency in software engineering is a relationship between two elements where one relies on the other.\n",
      "113: Metrics Run - Metrics run refers to the process of collecting and analyzing metrics during the execution of a software system.\n",
      "114: Pipeline Data - Pipeline data refers to the information that is processed in a series of stages, or a pipeline.\n",
      "115: Environment - An environment in software engineering refers to a collection of hardware and software tools a system uses to run.\n",
      "116: Job - A job in software engineering often refers to a single unit of work that a computer system can execute.\n",
      "117: Pricing - Pricing in software engineering often refers to the cost associated with using a particular software product or service.\n",
      "118: Resource - A resource in software engineering is any physical or virtual component of limited availability within a computer system.\n",
      "119: Batch Transform - Batch transform is a method of running predictions in which data is collected over time and predictions are made in groups.\n",
      "120: Runs - Runs in software engineering often refer to the execution of a program or a piece of code.\n",
      "121: Artifact - An artifact in software engineering is a byproduct of the software development process.\n",
      "122: Endpoint Model - An endpoint model in machine\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of words refering to specific software engineering topics. Please summarize each topic in two to three words and attach one sentence description. Also, you must guarantee that the terms are not duplicate with one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_rq1, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for index, term in enumerate(topic) if index < 5])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "False\n",
      "{17, 15}\n"
     ]
    }
   ],
   "source": [
    "topic_list = [topic for topic in topics.split('\\n') if topic]\n",
    "macro_topic_mapping_inverse = {\n",
    "    'Environment Management': [0,5,11,12,15,18,19,23,28,37,43,45,66,76,82,91,101,107,111,112,115,123,136,137,142,150,151,165,166,167],\n",
    "    'Pipeline Management': [6,24,30,49,70,88,114,116,138,140,148,153,155,158],\n",
    "    'Experiment Management': [16,48,77,95,103,120],\n",
    "    'Code Development': [32,39,58,61,130,141],\n",
    "    'Code Management': [78],\n",
    "    'Data Development': [8,20,100,119],\n",
    "    'Data Management': [4,8,17,34,35,36,41,46,92,94,97,102,121,125,154,160,168],\n",
    "    'Model Development': [1,7,26,33,59,65,79,109,128,139],\n",
    "    'Model Management': [13,44,71,84,89,110,126,156,162],\n",
    "    'Model Serving': [25,27,31,38,50,51,67,74,80,90],\n",
    "    'Network Management': [3,53,57,87,93,106,122,131,163],\n",
    "    'Observability Management': [9,56,108,113,127,132],\n",
    "    'Version Management': [10,61,64,99,157,169],\n",
    "    'Security Management': [62,68,73,85,129,135,164],\n",
    "    'Documentation Management': [96],\n",
    "    'Resource Management': [29,75,98,118,134,146,161],\n",
    "    'IO Management': [21,40,47,54,55,69,83,149],\n",
    "    'QA Management': [81,86,133],\n",
    "    'UI Management': [14,124],\n",
    "    'Discarded': [2,22,42,52,60,63,72,104,105,117,143,144,145,147,152,159],\n",
    "}\n",
    "        \n",
    "macro_topic_list = []\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_indexing = {}\n",
    "for macro_topic, sub_topics in macro_topic_mapping_inverse.items():\n",
    "    index, name = int(macro_topic.split(': ')[0]), macro_topic.split(': ')[1]\n",
    "    macro_topic_indexing[index] = name\n",
    "    macro_topic_list.extend(sub_topics)\n",
    "    for topic in sub_topics:\n",
    "        macro_topic_mapping[topic] = macro_topic\n",
    "\n",
    "print(find_duplicates(macro_topic_list))\n",
    "print(len(macro_topic_list) == len(topic_list))\n",
    "print(set(range(len(topic_list))).difference(set(macro_topic_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Challenge_topic'] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Topic & Percentage & Number \\\\\n",
      "\\midrule\n",
      "Model Management & 21.39 & 2378 \\\\\n",
      "Compute Management & 20.05 & 2229 \\\\\n",
      "Environment Management & 17.90 & 1990 \\\\\n",
      "Data Management & 13.13 & 1460 \\\\\n",
      "Lifecycle Management & 9.94 & 1105 \\\\\n",
      "Access Management & 7.84 & 872 \\\\\n",
      "Observability Management & 6.65 & 739 \\\\\n",
      "Code Management & 3.09 & 344 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_rq1, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "\n",
    "for name, group in df.groupby('Challenge_topic_macro'):\n",
    "    entry = {\n",
    "        'Topic': macro_topic_indexing[name],\n",
    "        'Percentage': round(len(group)/len(df)*100, 2),\n",
    "        'Number': len(group),\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "\n",
    "df_number = df_number.sort_values('Percentage', ascending=False)\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'filtered.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "df['Challenge_topic_macro'] = df['Challenge_topic_macro'].apply(lambda x: macro_topic_indexing[x])\n",
    "categories = ['Challenge_type', 'Challenge_topic_macro', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_rq1, 'State type topic sankey.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
