{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import openai\n",
    "import random\n",
    "import enchant\n",
    "import textstat\n",
    "import warnings\n",
    "import itertools\n",
    "import collections\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, strip_punctuation, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../Dataset'\n",
    "path_result = '../../Result'\n",
    "\n",
    "path_rq1 = os.path.join(path_result, 'RQ1')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# subprocess.run(['python', '-m' 'spacy', 'download', 'en_core_web_sm'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "prompt_summary = '''Refine the title of the post to make it short and clear in simple English.\\n###'''\n",
    "\n",
    "tools_keyword_mapping = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['amazon', 'aws', 'maker', 'sage'],\n",
    "    'Azure Machine Learning': ['azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['databricks', 'mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['google', 'gcp', 'vertex'],\n",
    "    'Weights & Biases': ['biases', 'wandb', 'weights']\n",
    "}\n",
    "\n",
    "tools_keyword_set = set(itertools.chain(*tools_keyword_mapping.values()))\n",
    "\n",
    "keywords_image = {\n",
    "    \".jpg\", \n",
    "    \".png\", \n",
    "    \".jpeg\", \n",
    "    \".gif\", \n",
    "    \".bmp\", \n",
    "    \".webp\", \n",
    "    \".svg\", \n",
    "    \".tiff\"\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n",
    "\n",
    "# error_words_basics = {\n",
    "#     'error',\n",
    "#     'fault',\n",
    "# }\n",
    "\n",
    "stop_words_se = {\n",
    "    'ability',\n",
    "    'abilities',\n",
    "    'accident',\n",
    "    'accidents',\n",
    "    # 'acknowledgement',\n",
    "    'action',\n",
    "    'actions',\n",
    "    'activities',\n",
    "    'activity',\n",
    "    'advice',\n",
    "    'ai',\n",
    "    'alternative',\n",
    "    'alternatives',\n",
    "    # 'announcement',\n",
    "    'anomaly'\n",
    "    'anomalies'\n",
    "    'answer',\n",
    "    'answers',\n",
    "    'appreciation',\n",
    "    'approach',\n",
    "    'approaches',\n",
    "    'article',\n",
    "    'articles',\n",
    "    'assistance',\n",
    "    'attempt',\n",
    "    'author',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'benefit',\n",
    "    'bit',\n",
    "    'bits',\n",
    "    'block',\n",
    "    'blocks',\n",
    "    # 'blog',\n",
    "    # 'blogs',\n",
    "    'body',\n",
    "    'bug',\n",
    "    'bugs',\n",
    "    'building',\n",
    "    'case',\n",
    "    'cases',\n",
    "    'categories',\n",
    "    'categorization',\n",
    "    'category',\n",
    "    'cause',\n",
    "    'causes',\n",
    "    'challenge',\n",
    "    'challenges',\n",
    "    'change',\n",
    "    'changes',\n",
    "    # 'char',\n",
    "    'check',\n",
    "    'choice',\n",
    "    'choices',\n",
    "    'clarification',\n",
    "    'collection',\n",
    "    'com',\n",
    "    'combination',\n",
    "    # 'commmunication',\n",
    "    # 'community',\n",
    "    # 'communities',\n",
    "    # 'company',\n",
    "    # 'companies',\n",
    "    # 'computer',\n",
    "    # 'computers',\n",
    "    # 'concept',\n",
    "    # 'concepts',\n",
    "    'concern',\n",
    "    'concerns',\n",
    "    # 'condition',\n",
    "    # 'conditions',\n",
    "    'confirmation',\n",
    "    'confusion',\n",
    "    'consideration',\n",
    "    # 'content',\n",
    "    # 'contents',\n",
    "    'context',\n",
    "    # 'count',\n",
    "    'couple',\n",
    "    'couples',\n",
    "    # 'course',\n",
    "    # 'courses',\n",
    "    'crash',\n",
    "    'crashes',\n",
    "    'cross',\n",
    "    # 'custom',\n",
    "    'customer',\n",
    "    'customers',\n",
    "    'day',\n",
    "    'days',\n",
    "    'demand',\n",
    "    # 'description',\n",
    "    'desire',\n",
    "    'detail',\n",
    "    'details',\n",
    "    'devops',\n",
    "    'difference',\n",
    "    'differences',\n",
    "    'difficulties',\n",
    "    'difficulty',\n",
    "    'discrepancies',\n",
    "    'discrepancy',\n",
    "    'discussion',\n",
    "    'dislike',\n",
    "    'distinction',\n",
    "    'effect',\n",
    "    'end',\n",
    "    # 'engineering',\n",
    "    'enquiries',\n",
    "    'enquiry',\n",
    "    'error',\n",
    "    'errors',\n",
    "    'evidence',\n",
    "    'example',\n",
    "    'examples',\n",
    "    'exception',\n",
    "    'exceptions',\n",
    "    'existence',\n",
    "    'exit',\n",
    "    'expectation',\n",
    "    'experience',\n",
    "    'expert',\n",
    "    'experts',\n",
    "    # 'explanation',\n",
    "    'fact',\n",
    "    'facts',\n",
    "    'fail',\n",
    "    'failure',\n",
    "    'fault',\n",
    "    'faults',\n",
    "    'favorite',\n",
    "    'favorites',\n",
    "    # 'feature',\n",
    "    # 'features',\n",
    "    # 'feedback',\n",
    "    # 'feedbacks',\n",
    "    'fix',\n",
    "    'fixes',\n",
    "    # 'float',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'functionality',\n",
    "    'functionalities',\n",
    "    'future',\n",
    "    'goal',\n",
    "    'goals',\n",
    "    'guarantee',\n",
    "    # 'guidance',\n",
    "    # 'guideline',\n",
    "    # 'guide',\n",
    "    'guy',\n",
    "    'guys',\n",
    "    'harm',\n",
    "    'hello',\n",
    "    'help',\n",
    "    'hour',\n",
    "    'hours',\n",
    "    'ibm',\n",
    "    'idea',\n",
    "    'ideas',\n",
    "    'individual',\n",
    "    'individuals',\n",
    "    'info',\n",
    "    'information',\n",
    "    'inquiries',\n",
    "    'inquiry',\n",
    "    'insight',\n",
    "    # 'instruction',\n",
    "    # 'instructions',\n",
    "    # 'int',\n",
    "    'intelligence',\n",
    "    'intent',\n",
    "    'interest',\n",
    "    'introduction',\n",
    "    'investigation',\n",
    "    'invitation',\n",
    "    'ipynb',\n",
    "    'issue',\n",
    "    'issues',\n",
    "    'kind',\n",
    "    'kinds',\n",
    "    'lack',\n",
    "    'learning',\n",
    "    'level',\n",
    "    'levels',\n",
    "    'look',\n",
    "    'looks',\n",
    "    'lot',\n",
    "    'lots',\n",
    "    'luck',\n",
    "    'machine',\n",
    "    'major',\n",
    "    'manner',\n",
    "    'manners',\n",
    "    # 'manual',\n",
    "    'mark',\n",
    "    'means',\n",
    "    'meaning',\n",
    "    # 'message',\n",
    "    # 'messages',\n",
    "    'method',\n",
    "    'methods',\n",
    "    'ml',\n",
    "    'mlops',\n",
    "    'minute',\n",
    "    'minutes',\n",
    "    'mistake',\n",
    "    'mistakes',\n",
    "    'month',\n",
    "    'months',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'number',\n",
    "    'numbers',\n",
    "    'offer',\n",
    "    'one',\n",
    "    'ones',\n",
    "    'opinion',\n",
    "    'opinions',\n",
    "    # 'org',\n",
    "    # 'organization',\n",
    "    'outcome',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'past',\n",
    "    'people',\n",
    "    'person',\n",
    "    'persons',\n",
    "    'perspective',\n",
    "    'perspectives',\n",
    "    'place',\n",
    "    'places',\n",
    "    'point',\n",
    "    'points',\n",
    "    'post',\n",
    "    'posts',\n",
    "    'practice',\n",
    "    'practices',\n",
    "    'problem',\n",
    "    'problems',\n",
    "    # 'product',\n",
    "    # 'products',\n",
    "    # 'program',\n",
    "    # 'programs',\n",
    "    # 'project',\n",
    "    # 'projects',\n",
    "    # 'proposal',\n",
    "    'purpose',\n",
    "    'purposes',\n",
    "    'py',\n",
    "    # 'python',\n",
    "    'qa',\n",
    "    'question',\n",
    "    'questions',\n",
    "    'reason',\n",
    "    'reasons',\n",
    "    # 'recognition',\n",
    "    # 'recommendation',\n",
    "    # 'recommendations',\n",
    "    # 'recommender',\n",
    "    # 'regression',\n",
    "    # 'request',\n",
    "    'research',\n",
    "    'result',\n",
    "    'results',\n",
    "    'scenario',\n",
    "    'scenarios',\n",
    "    'science',\n",
    "    'screenshot',\n",
    "    'screenshots',\n",
    "    'second',\n",
    "    'seconds',\n",
    "    'section',\n",
    "    'sense',\n",
    "    'sentence',\n",
    "    'show',\n",
    "    'shows',\n",
    "    'situation',\n",
    "    'software',\n",
    "    'solution',\n",
    "    'solutions',\n",
    "    'start',\n",
    "    # 'state',\n",
    "    # 'statement',\n",
    "    # 'states',\n",
    "    # 'status',\n",
    "    # 'step',\n",
    "    # 'steps',\n",
    "    # 'string',\n",
    "    'student',\n",
    "    'students',\n",
    "    'study',\n",
    "    'stuff',\n",
    "    'success',\n",
    "    'suggestion',\n",
    "    'suggestions',\n",
    "    'summary',\n",
    "    'summaries',\n",
    "    'surprise',\n",
    "    # 'support',\n",
    "    'talk',\n",
    "    # 'task',\n",
    "    # 'tasks',\n",
    "    'teacher',\n",
    "    'teachers',\n",
    "    # 'technique',\n",
    "    # 'techniques',\n",
    "    # 'technologies',\n",
    "    # 'technology',\n",
    "    'term',\n",
    "    'terms',\n",
    "    'tip',\n",
    "    'tips',\n",
    "    'thank',\n",
    "    'thanks',\n",
    "    'thing',\n",
    "    'things',\n",
    "    'thought',\n",
    "    'thoughts',\n",
    "    'three',\n",
    "    'title',\n",
    "    'today',\n",
    "    'tomorrow',\n",
    "    # 'tool',\n",
    "    # 'tools',\n",
    "    'total',\n",
    "    'trouble',\n",
    "    'troubles',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'two',\n",
    "    'understand',\n",
    "    'understanding',\n",
    "    'usage',\n",
    "    'use',\n",
    "    'user',\n",
    "    'users',\n",
    "    'uses',\n",
    "    # 'value',\n",
    "    # 'values',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'week',\n",
    "    'weeks',\n",
    "    'word',\n",
    "    'words',\n",
    "    'work',\n",
    "    'workaround',\n",
    "    'workarounds',\n",
    "    'works',\n",
    "    'yeah',\n",
    "    'year',\n",
    "    'years',\n",
    "    'yesterday',\n",
    "}\n",
    "\n",
    "stop_words_ml = {\n",
    "    'ad',\n",
    "    'ads',\n",
    "    'advertisement',\n",
    "    'advertisements',\n",
    "    'algorithm',\n",
    "    'algorithms',\n",
    "    'analysis',\n",
    "    'anomaly',\n",
    "    'asr',\n",
    "    'audio',\n",
    "    'autoencoder',\n",
    "    'automl',\n",
    "    'autopilot',\n",
    "    'bert',\n",
    "    'bi',\n",
    "    'chatbot',\n",
    "    'classification',\n",
    "    'classifier',\n",
    "    'clustering',\n",
    "    'cnn',\n",
    "    'cv',\n",
    "    'decision',\n",
    "    'detection',\n",
    "    'dimensionality',\n",
    "    'embedding',\n",
    "    'estimation',\n",
    "    'extraction',\n",
    "    'forecast',\n",
    "    'forecaster',\n",
    "    'forecasts',\n",
    "    'forecasting',\n",
    "    'forest',\n",
    "    'fraud',\n",
    "    'gan',\n",
    "    'gesture',\n",
    "    'gpt',\n",
    "    'ica',\n",
    "    'kmeans',\n",
    "    'k-means',\n",
    "    'knn',\n",
    "    'language',\n",
    "    'languages',\n",
    "    'lda',\n",
    "    'lstm',\n",
    "    'mining',\n",
    "    'modelling',\n",
    "    'ngram',\n",
    "    'n-gram',\n",
    "    'nlp',\n",
    "    'nmf',\n",
    "    'ocr',\n",
    "    'pca',\n",
    "    'processing',\n",
    "    'recognition',\n",
    "    'recommendation',\n",
    "    'recommendations',\n",
    "    'recommender',\n",
    "    'reduction',\n",
    "    'regression',\n",
    "    'regressor',\n",
    "    'reinforcement',\n",
    "    'rf',\n",
    "    'rl',\n",
    "    'rnn',\n",
    "    'segmentation',\n",
    "    'sentiment',\n",
    "    'series',\n",
    "    'sound',\n",
    "    'spam',\n",
    "    'speech',\n",
    "    'stt',\n",
    "    'strategy',\n",
    "    'strategies',\n",
    "    'svd',\n",
    "    'svm',\n",
    "    # 'text',\n",
    "    # 'texts',\n",
    "    'time',\n",
    "    'timeseries'\n",
    "    'topic',\n",
    "    'topics',\n",
    "    'transformer',\n",
    "    'translation',\n",
    "    'translator',\n",
    "    'tree',\n",
    "    'trend',\n",
    "    'trends',\n",
    "    'tsne',\n",
    "    'tts',\n",
    "    'vae',\n",
    "    'video',\n",
    "    'vision',\n",
    "    'voice',\n",
    "}\n",
    "\n",
    "stop_words = STOPWORDS.union(stop_words_se.union(stop_words_ml))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(block_list):\n",
    "    total_loc = 0\n",
    "    for blocks in block_list:\n",
    "        for block in blocks:\n",
    "            for line in block.splitlines():\n",
    "                if line.strip():\n",
    "                    total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    # extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract code blocks type 1\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code blocks type 2\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    return clean_text, links, code_line\n",
    "\n",
    "def extract_code(content):\n",
    "    code_patterns = [r'```.+?```', r'``.+?``', r'`.+?`']\n",
    "    clean_text = content\n",
    "    code_line = 0\n",
    "\n",
    "    for code_pattern in code_patterns:\n",
    "        code_snippets = re.findall(code_pattern, clean_text, flags=re.DOTALL)\n",
    "        code_line += count_code_line(code_snippets)\n",
    "        clean_text = re.sub(code_pattern, '', clean_text, flags=re.DOTALL)\n",
    "    \n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern1 = r\"\\!?\\[.*?\\]\\((.*?)\\)\"\n",
    "    links1 = re.findall(link_pattern1, text)\n",
    "    clean_text = re.sub(link_pattern1, '', text)\n",
    "    link_pattern2 = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links2 = re.findall(link_pattern2, clean_text)\n",
    "    clean_text = re.sub(link_pattern2, '', clean_text)\n",
    "    links = links1 + links2\n",
    "    return clean_text, links\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_line1 = extract_styles(content)\n",
    "    clean_text, code_line2 = extract_code(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_line = code_line1 + code_line2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_line'])\n",
    "    return content_collection(clean_text, links, code_line)\n",
    "\n",
    "def word_frequency(text):\n",
    "    word_counts = collections.Counter(text.split())\n",
    "    return word_counts\n",
    "\n",
    "def extract_nouns_verbs(text, verb):\n",
    "    doc = nlp(text)\n",
    "    if verb:\n",
    "        words = [token.text for token in doc if (token.pos_ == \"VERB\") or (token.pos_ == \"NOUN\")]\n",
    "    else:\n",
    "        words = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language == 'en'\n",
    "    except:\n",
    "        # In case the detection fails (e.g. if the text is too short or doesn't contain enough features)\n",
    "        return False\n",
    "\n",
    "def extract_english(text):\n",
    "    words = [word for word in text.split() if spell_checker.check(word)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_words_with_substring(text, substring_list):\n",
    "    words = text.split()\n",
    "    for substring in substring_list:\n",
    "        words = [word for word in words if substring not in word]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_text(text, verb=False):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = strip_punctuation(clean_text)\n",
    "    clean_text = extract_english(clean_text)\n",
    "    clean_text = remove_words_with_substring(clean_text, tools_keyword_set)\n",
    "    clean_text = remove_stopwords(clean_text, stop_words)\n",
    "    clean_text = extract_nouns_verbs(clean_text, verb)\n",
    "    # clean_text = strip_short(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_links(links):\n",
    "    image_links = 0\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([image in link for image in keywords_image]):\n",
    "            image_links += 1\n",
    "        elif any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['image', 'documentation', 'tool', 'issue', 'patch', 'tutorial', 'example'])\n",
    "    return link_analysis(image_links, documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "\n",
    "def find_duplicates(in_list):  \n",
    "    duplicates = []\n",
    "    unique = set(in_list)\n",
    "    for each in unique:\n",
    "        count = in_list.count(each)\n",
    "        if count > 1:\n",
    "            duplicates.append(each)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_comment_body'] = row['Issue_comment_body']\n",
    "    df_issues.at[index, 'Challenge_comment_score'] = row['Issue_comment_score']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_comment_body'] = row['Question_comment_body']\n",
    "    df_questions.at[index, 'Challenge_comment_score'] = row['Question_comment_score']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_comment_score'] = row['Answer_comment_score']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df = df[df.columns.drop(list(df.filter(regex=r'(Issue|Question|Answer)_')))]\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'topics'\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_dataset, f'{name}.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if 'Git' not in row['Platform']:\n",
    "#         continue\n",
    "#     for i2, r2 in df_old.iterrows():\n",
    "#         if 'Git' not in r2['Platform']:\n",
    "#             continue\n",
    "#         if row['Challenge_link'] == r2['Challenge_link']:\n",
    "#             df.at[index, 'Tools'] = r2['Tools']\n",
    "#             break\n",
    "\n",
    "# df.to_json(os.path.join(path_dataset, f'{name}.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-English posts\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    if not is_english(title_analyzer.text):\n",
    "        df.drop(index, inplace=True)\n",
    "        continue\n",
    "    \n",
    "    body_analyzer = split_content(row['Challenge_body'])\n",
    "    if not is_english(body_analyzer.text):\n",
    "        df.drop(index, inplace=True)\n",
    "        continue\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform:\n",
      "Github 2169\n",
      "Gitlab 1\n",
      "Stack Overflow 6051\n",
      "Tool-specific 4221\n",
      "Challenge type:\n",
      "knowledge 5384\n",
      "na 494\n",
      "problem 6564\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_dataset, 'labels.json'))\n",
    "\n",
    "print('Platform:')\n",
    "for name, group in df.groupby('Platform'):\n",
    "    print(name, group.shape[0])\n",
    "\n",
    "print('Challenge type:')\n",
    "for name, group in df.groupby('Challenge_type'):\n",
    "    print(name, group.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard all the rows that do not have a challenge type\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'labels.json'))\n",
    "\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "    \n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GPT summaries\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 99:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    if pd.notna(row['Challenge_gpt_summary']):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + '\\nBody: ' + row['Challenge_body'] + '###\\nTitle: '\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-4',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=50,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post level preprocessing\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clean_title = preprocess_text(row['Challenge_title'], verb=True)\n",
    "    df.at[index, 'Challenge_preprocessed_title'] = clean_title\n",
    "    \n",
    "    clean_summary = preprocess_text(row['Challenge_gpt_summary'], verb=True)\n",
    "    df.at[index, 'Challenge_preprocessed_gpt_summary'] = clean_summary\n",
    "    \n",
    "    body_analyzer = split_content(row['Challenge_body'])\n",
    "    clean_body = preprocess_text(body_analyzer.text, verb=True)\n",
    "    df.at[index, 'Challenge_preprocessed_body'] = clean_body\n",
    "    \n",
    "    df.at[index, 'Challenge_preprocessed_content'] = clean_title + ' ' + clean_body\n",
    "    \n",
    "    # link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    # text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    \n",
    "    # df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    # df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    # df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    # df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    # df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    # df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    # df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    # df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    # df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    # df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    # df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    # if pd.notna(row['Challenge_comment_body']):\n",
    "    #     comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "    #     link_analyzer = analyze_links(comment_analyzer.links)\n",
    "    #     text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "    #     df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "    #     df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "    #     df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "    #     df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "    #     df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "    #     df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "    #     df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "    #     df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "    #     df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "    #     df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "    #     df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    # if pd.notna(row['Solution_body']):\n",
    "    #     solution_analyzer = split_content(row['Solution_body'])\n",
    "    #     link_analyzer = analyze_links(solution_analyzer.links)\n",
    "    #     text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "    #     df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "    #     df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "    #     df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "    #     df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "    #     df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "    #     df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "    #     df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "    #     df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "    #     df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "    #     df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "    #     df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "    # if pd.notna(row['Solution_comment_body']):\n",
    "    #     comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "    #     link_analyzer = analyze_links(comment_analyzer.links)\n",
    "    #     text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "    #     df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "    #     df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "    #     df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "    #     df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "    #     df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "    #     df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "    #     df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "    #     df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "    #     df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "    #     df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "    #     df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "    \n",
    "    df.at[index, 'Challenge_solved_time'] = (row['Challenge_closed_time'] - row['Challenge_created_time']) / pd.Timedelta(hours=1)\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "df = df.explode('Tools')\n",
    "df['State'] = df['Challenge_solved_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tools', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.pdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "     Type &  Prevalence (\\%) &  Unresolved Rate (\\%) &  Median Resolved Time (hour) \\\\\n",
      "\\midrule\n",
      "knowledge &           45.06 &                67.76 &                        21.96 \\\\\n",
      "  problem &           54.94 &                65.40 &                        67.75 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "df_inquiry = []\n",
    "\n",
    "for name, group in df.groupby(['Challenge_type']):\n",
    "    closed = group[group['Challenge_closed_time'].notna()]\n",
    "    info = {\n",
    "        'Type': name,\n",
    "        'Prevalence (%)': round(len(group) / len(df) * 100, 2),\n",
    "        'Unresolved Rate (%)': 100 - round(len(closed) / len(group) * 100, 2),\n",
    "        'Median Resolved Time (hour)': round(group['Challenge_solved_time'].median(), 2),\n",
    "    }\n",
    "    df_inquiry.append(info)\n",
    "\n",
    "df_inquiry = pd.DataFrame(df_inquiry)\n",
    "print(df_inquiry.to_latex(index=False, float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value of problem inquiry: 0.0\n",
      "P-value of knowledge inquiry: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "df = df[df['Challenge_solved_time'].notna()]\n",
    "df_problem = df[df['Challenge_type'] == 'problem']['Challenge_solved_time'].tolist()\n",
    "df_knowledge = df[df['Challenge_type'] == 'knowledge']['Challenge_solved_time'].tolist()\n",
    "\n",
    "# Conduct Shapiro-Wilk test\n",
    "print(\"P-value of problem inquiry:\", shapiro(df_problem)[1])\n",
    "print(\"P-value of knowledge inquiry:\", shapiro(df_knowledge)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value of two inquiry: 8.888745701871212e-25\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "df = df[df['Challenge_solved_time'].notna()]\n",
    "df_problem = df[df['Challenge_type'] == 'problem']['Challenge_solved_time'].tolist()\n",
    "df_knowledge = df[df['Challenge_type'] == 'knowledge']['Challenge_solved_time'].tolist()\n",
    "\n",
    "# Conduct Shapiro-Wilk test\n",
    "print(\"P-value of two inquiry:\", mannwhitneyu(df_problem, df_knowledge)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a set of terms refering to specific software engineering topics. Please summarize each topic in a phrase and attach one sentence description in the asset management context. Also, you must guarantee that those phrases are not duplicate with one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_rq1, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for index, term in enumerate(topic)])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo-16k',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=8000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=1000,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = '''Topic 0: Docker - A platform for building, packaging, and distributing applications in containers. In the asset management context, Docker is used to create reproducible and isolated environments for deploying machine learning models.\n",
    "Topic 1: Columns - The vertical sections of a table that hold data of a specific type. In the asset management context, columns are used to represent features or attributes of a dataset.\n",
    "Topic 2: Model - A representation of a real-world process or system that can be used to make predictions or generate outputs. In the asset management context, models are trained and deployed to make predictions on new data.\n",
    "Topic 3: Labels - Tags or annotations assigned to data points to indicate their class or category. In the asset management context, labels are used for supervised learning tasks to train models.\n",
    "Topic 4: Model Prediction - The output or result generated by a trained machine learning model when given input data. In the asset management context, model predictions are used to make decisions or generate insights.\n",
    "Topic 5: API Gateway - A service that acts as a single entry point for multiple APIs, providing centralized management and control. In the asset management context, an API gateway can be used to expose machine learning models as RESTful APIs.\n",
    "Topic 6: Logs - Recorded events or messages that provide information about the execution of a program or system. In the ML asset managementset managementset managementset managementset managementset management context, logs are used to track and monitor the performance of machine learning models.\n",
    "Topic 7: Plots - Visual representations of data or mathematical functions. In the asset management context, plots are used to analyze and visualize the performance or behavior of machine learning models.\n",
    "Topic 8: PyTorch - An open-source machine learning framework that provides a flexible and efficient way to build and train deep learning models. In the asset management context, PyTorch is used for developing and deploying deep learning models.\n",
    "Topic 9: Custom Training - The process of training a machine learning model using custom algorithms or techniques tailored to a specific problem or domain. In the asset management context, custom training allows for more specialized and optimized models.\n",
    "Topic 10: Hyperparameter - A parameter whose value is set before the learning process begins and remains constant during training. In the asset management context, hyperparameters are tuned to optimize the performance of machine learning models.\n",
    "Topic 11: Parameters - Variables or values that are used as inputs to a function or model. In the asset management context, parameters are used to configure and customize the behavior of machine learning models.\n",
    "Topic 12: Bucket - A logical container for storing objects (files) in cloud storage systems. In the asset management context, buckets are used to store and organize data, models, and other artifacts.\n",
    "Topic 13: TensorFlow Model - A machine learning model built using the TensorFlow framework. In the asset management context, TensorFlow models are trained and deployed for various tasks such as image classification or natural language processing.\n",
    "Topic 14: Custom Model - A machine learning model that is built from scratch or customized to fit specific requirements or constraints. In the asset management context, custom models are developed and deployed to solve unique problems.\n",
    "Topic 15: Lookup Error - An error raised when a key or index used on a mapping or sequence is invalid, In the asset management context, lookup error can occur when incompatible data types are used in model training or inference.\n",
    "Topic 16: Notebook Instance - A virtual environment that provides an interactive interface for writing and running code, typically used for data exploration and experimentation. In the asset management context, notebook instances are used for developing and testing machine learning models.\n",
    "Topic 17: Pipeline Component - A modular unit or step in a data processing or machine learning pipeline. In the asset management context, pipeline components are used to define and execute the steps involved in training and deploying models.\n",
    "Topic 18: Version - A specific release or iteration of a software or model. In the asset management context, versions are used to track and manage changes to models, code, or configurations.\n",
    "Topic 19: Directory - A location or folder that stores files and other directories. In the asset management context, directories are used to organize and manage data, code, and other resources.\n",
    "Topic 20: Sweep - A process of systematically exploring a range of hyperparameter values to find the optimal configuration for a machine learning model. In the asset management context, sweeps are used to automate hyperparameter tuning.\n",
    "Topic 21: Web Service - A service or application that provides functionality or data over the internet using standard web protocols. In the asset management context, web services can be used to expose machine learning models as APIs.\n",
    "Topic 22: Authentication - The process of verifying the identity of a user or system. In the asset management context, authentication is used to control access to machine learning models and resources.\n",
    "Topic 23: Deployment - The process of making a machine learning model or application available for use in a production environment. In the asset management context, deployment involves deploying trained models to serve predictions or perform tasks.\n",
    "Topic 24: Git Repo - A repository or storage space for version-controlled code and project files using the Git version control system. In the asset management context, Git repos are used to manage and track changes to machine learning models and code.\n",
    "Topic 25: Port - A communication endpoint or interface in a computer network. In the asset management context, ports are used to specify the network location or address where machine learning models or services can be accessed.\n",
    "Topic 26: Keras - A high-level deep learning framework that runs on top of other deep learning libraries such as TensorFlow or Theano. In the asset management context, Keras is used for building and training neural networks.\n",
    "Topic 27: Compute Instances - Virtual machines or instances that provide computational resources for running applications or processes. In the asset management context, compute instances are used to train and deploy machine learning models.\n",
    "Topic 28: Cluster - A group of interconnected computers or servers that work together to perform a task or provide a service. In the asset management context, clusters can be used for distributed training or parallel processing of machine learning tasks.\n",
    "Topic 29: Permission - The authorization or access rights granted to a user or system to perform certain actions or access specific resources. In the ML asset managementset managementset managementset managementset managementset management context, permissions are used to control access to machine learning models, data, or resources.\n",
    "Topic 30: Team - A group of individuals working together on a project or towards a common goal. In the asset management context, teams collaborate to develop, train, and deploy machine learning models.\n",
    "Topic 31: Script RStudio - A script or program written in the R programming language and executed in the RStudio integrated development environment. In the asset management context, RStudio scripts can be used for data analysis, model training, or experimentation.\n",
    "Topic 32: Metrics Evaluation - The process of assessing or measuring the performance or quality of a machine learning model using various metrics or evaluation criteria. In the asset management context, metrics evaluation is used to assess the effectiveness of models.\n",
    "Topic 33: Data Studio - A platform or tool for visualizing, analyzing, and exploring data. In the asset management context, data studios are used to gain insights from data, perform data preprocessing, or create visualizations.\n",
    "Topic 34: Storage - The act or process of storing or saving data or information. In the asset management context, storage refers to the systems or services used to store and manage data, models, or artifacts.\n",
    "Topic 35: Inference Pipeline - A sequence or flow of steps or processes that transform input data into predictions or outputs. In the asset management context, inference pipelines are used to process and generate predictions from input data.\n",
    "Topic 36: Training - The process of teaching or learning from data to improve the performance or accuracy of a machine learning model. In the asset management context, training involves feeding data to models to optimize their parameters or weights.\n",
    "Topic 37: Endpoint - A specific URL or network address that provides access to a service or resource. In the asset management context, endpoints are used to expose machine learning models or APIs for making predictions or serving requests.\n",
    "Topic 38: Config - Short for configuration, it refers to the settings or parameters that define the behavior or operation of a system or application. In the asset management context, config files are used to specify the configuration of machine learning models or systems.\n",
    "Topic 39: Training Model - A machine learning model that has been trained on a specific dataset to make predictions or perform a task. In the asset management context, training models are developed and optimized for specific tasks or domains.\n",
    "Topic 40: Object Attribute - A characteristic or property of an object in object-oriented programming. In the asset management context, object attributes can represent features, parameters, or metadata associated with machine learning models or data.\n",
    "Topic 41: Model Registry - A centralized repository or database for storing and managing machine learning models. In the asset management context, model registries are used to track, version, and organize trained models.\n",
    "Topic 42: Batch Transform - The process of applying a trained machine learning model to a large batch or set of input data to generate predictions or outputs. In the asset management context, batch transforms are used for bulk inference or processing of data.\n",
    "Topic 43: Data - Information or facts that are collected, stored, or processed. In the asset management context, data refers to the input or training data used to develop and train machine learning models.\n",
    "Topic 44: Apache Spark - An open-source distributed computing system designed for big data processing and analytics. In the asset management context, Apache Spark can be used for distributed training or processing of large-scale machine learning tasks.\n",
    "Topic 45: Component - A modular or self-contained unit that performs a specific function or task. In the asset management context, components are used to build and assemble machine learning pipelines or systems.\n",
    "Topic 46: Memory - The temporary storage or workspace used by a computer or system to hold data or instructions. In the asset management context, memory is used to store and manipulate data during the training or inference process.\n",
    "Topic 47: Pipeline Data - The input or output data that flows through a machine learning pipeline or system. In the asset management context, pipeline data represents the data being processed or transformed by the pipeline.\n",
    "Topic 48: Huggingface Model - A machine learning model built using the Hugging Face library, which specializes in natural language processing tasks. In the asset management context, Hugging Face models are used for tasks such as text classification or language generation.\n",
    "Topic 49: Model Endpoint - The network address or URL where a trained machine learning model can be accessed or invoked to make predictions or serve requests. In the asset management context, model endpoints are used to expose models as APIs.\n",
    "Topic 50: Batch Predictions - The process of making predictions or generating outputs for a large batch or set of input data using a trained machine learning model. In the asset management context, batch predictions are used for bulk inference or processing of data.\n",
    "Topic 51: Network - A collection of interconnected nodes or devices that can communicate and exchange data. In the ML asset managementset managementset managementset managementset managementset management context, networks are used to connect and transfer data between different components or systems in a machine learning workflow.\n",
    "Topic 52: Environment Variables - Variables or settings that define the operating environment or behavior of a system or application. In the ML asset managementset management context, environment variables can be used to configure or customize the execution of machine learning models or workflows.\n",
    "Topic 53: Python - A popular programming language known for its simplicity and readability. In the ML asset managementset management context, Python is widely used for developing and implementing machine learning models and workflows.\n",
    "Topic 54: Pandas - A Python library for data manipulation and analysis, particularly for working with structured data. In the asset management context, Pandas is used for data preprocessing, cleaning, and transformation.\n",
    "Topic 55: Training Job - A specific instance or execution of the training process for a machine learning model. In the asset management context, training jobs are launched to train models on specific datasets or configurations.\n",
    "Topic 56: Blob Storage - A type of cloud storage service that stores unstructured data as blobs or binary large objects. In the asset management context, blob storage is used to store and manage large files or datasets.\n",
    "Topic 57: Stream Analytics - The process of analyzing and processing real-time or streaming data to extract insights or perform actions. In the asset management context, stream analytics can be used for real-time monitoring or processing of data in machine learning workflows.\n",
    "Topic 58: Model Neo - A framework or platform for optimizing and deploying machine learning models on edge devices or hardware accelerators. In the asset management context, Neo is used to optimize and deploy models for efficient inference.\n",
    "Topic 59: Script - A sequence of instructions or commands written in a programming language. In the asset management context, scripts are used to automate tasks, run experiments, or train machine learning models.\n",
    "Topic 60: TensorBoard - A web-based tool provided by TensorFlow for visualizing and analyzing the training process and performance of machine learning models. In the asset management context, TensorBoard is used for monitoring and debugging models.\n",
    "Topic 61: Server - A computer or system that provides resources or services to other computers or clients over a network. In the asset management context, servers are centralized service that tracks and manages various aspects of machine learning experiments.\n",
    "Topic 62: Custom Images - Customized or specialized images that are created for specific purposes or requirements. In the asset management context, custom images can be used to create reproducible and customized environments for training or deploying models.\n",
    "Topic 63: Loss - A measure of the error or difference between predicted and actual values in a machine learning model. In the asset management context, loss is used to evaluate and optimize the performance of models during training.\n",
    "Topic 64: PySpark - The Python API for Apache Spark, a distributed computing system for big data processing and analytics. In the asset management context, PySpark is used for distributed training or processing of machine learning tasks.\n",
    "Topic 65: Notebook - An interactive document or interface that combines code, text, and visualizations, typically used for data analysis, exploration, and documentation. In the asset management context, notebooks are used for developing, testing, and documenting machine learning models.\n",
    "Topic 66: API - Short for Application Programming Interface, it defines the methods and protocols for communication between software components or systems. In the asset management context, APIs are used to expose machine learning models or services for integration or consumption.\n",
    "Topic 67: Pipeline - A sequence or flow of steps or processes that transform input data into predictions or outputs. In the asset management context, pipelines are used to automate and streamline the end-to-end process of training and deploying machine learning models.\n",
    "Topic 68: File - A named collection of data or information stored on a computer or storage system. In the asset management context, files are used to store code, data, models, or other resources related to machine learning workflows.\n",
    "Topic 69: Dependencies - External libraries, packages, or modules that are required for a software or system to function properly. In the ML asset managementset managementset managementset managementset managementset management context, dependencies are used to specify and manage the required software components for running machine learning models or workflows.\n",
    "Topic 70: Studio - A platform or environment that provides tools, services, and resources for developing, training, and deploying machine learning models. In the asset management context, studios are used for end-to-end machine learning workflows and collaboration.\n",
    "Topic 71: Experiment - A systematic process or procedure carried out to test or validate a hypothesis or idea. In the asset management context, experiments are conducted to evaluate and compare different models, algorithms, or configurations.\n",
    "Topic 72: List Index - The position or location of an element in a list or array. In the asset management context, list indexes are used to access or manipulate specific elements or values in data structures.\n",
    "Topic 73: Model Deployment - The process of making a trained machine learning model available for use in a production environment or system. In the asset management context, model deployment involves deploying models to serve predictions or perform tasks.\n",
    "Topic 74: Database - A structured collection of data or information stored in a computer or system. In the asset management context, databases are used to store and manage structured data related to machine learning models or applications.\n",
    "Topic 75: ModuleNotFoundError - An error that occurs when a required module or package cannot be found or imported. In the asset management context, ModuleNotFoundError can occur when dependencies or required libraries are not properly installed or accessible.\n",
    "Topic 76: Dataset - A structured collection of data or information, typically organized in tabular form or as a set of files. In the asset management context, datasets are used for training, testing, or evaluating machine learning models.\n",
    "Topic 77: Metrics - Quantitative measures or indicators used to assess the performance or quality of a machine learning model. In the asset management context, metrics are used to evaluate and compare models based on their accuracy, precision, recall, or other criteria.\n",
    "Topic 78: Text - Written or printed words or characters that represent language or information. In the asset management context, text data is commonly used for natural language processing tasks, such as sentiment analysis or text classification.\n",
    "Topic 79: Files Studio - The file management or storage system within a machine learning studio or platform. In the asset management context, Files Studio is used to organize, store, and manage code, data, models, or other resources.\n",
    "Topic 80: Python Packages - Pre-built or reusable collections of Python modules or code that provide specific functionality or features. In the asset management context, Python packages are used to extend the capabilities of Python for machine learning tasks.\n",
    "Topic 81: Instance - A single occurrence or example of something. In the asset management context, instances refer to virtual machines or computing resources used for running applications or processes.\n",
    "Topic 82: Files - Collections of data or information stored on a computer or storage system. In the asset management context, files are used to store code, data, models, or other resources related to machine learning workflows.\n",
    "Topic 83: Experiment Workspace - A dedicated workspace or environment for conducting experiments, managing data, and developing machine learning models. In the asset management context, experiment workspaces provide a collaborative and organized environment for experimentation.\n",
    "Topic 84: Image - A visual representation or graphical file format that contains visual information or data. In the asset management context, images can represent visual data, such as images or charts, or be used to create visualizations or diagrams.\n",
    "Topic 85: Job - A task or process that is executed or performed by a computer or system. In the asset management context, jobs refer to specific tasks or operations related to training, deploying, or managing machine learning models.\n",
    "Topic 86: Athena Table - A table or data structure in Amazon Athena, a serverless query service for analyzing data in Amazon S3 using SQL. In the asset management context, Athena tables can be used to query and analyze data for machine learning tasks.\n",
    "Topic 87: Packages RPackage - Collections of pre-built or reusable code, functions, or modules in the R programming language. In the asset management context, R packages are used to extend the capabilities of R for machine learning tasks.\n",
    "Topic 88: Lambda - A serverless computing service that allows you to run code without provisioning or managing servers. In the asset management context, Lambdas can be used to execute functions or processes related to machine learning workflows.\n",
    "Topic 89: Studio Domain - A specific domain or environment within a machine learning studio or platform. In the asset management context, studio domains provide dedicated spaces or resources for different projects, teams, or purposes.\n",
    "Topic 90: Report - A document or presentation that provides information, analysis, or findings on a specific topic or subject. In the ML asset managementset managementset managementset managementset managementset management context, reports can be used to communicate and share insights or results from machine learning experiments or analyses.\n",
    "Topic 91: Model Artifacts - The files, data, or resources that are generated or produced during the training or development of a machine learning model. In the asset management context, model artifacts include trained weights, configurations, or metadata.\n",
    "Topic 92: Limit - A restriction or constraint on the resources, usage, or behavior of a system or application. In the asset management context, limits are used to control or manage the usage or availability of resources for machine learning tasks.\n",
    "Topic 93: Studio Model - A machine learning model that is developed, trained, or deployed within a machine learning studio or platform. In the asset management context, studio models are managed and monitored within the studio environment.\n",
    "Topic 94: Model Inference - The process of using a trained machine learning model to make predictions or generate outputs based on new or unseen data. In the asset management context, model inference is the primary task of deploying and using machine learning models.\n",
    "Topic 95: Batch Model - A machine learning model that is designed or optimized for batch processing or inference on large sets of data. In the asset management context, batch models are used for bulk inference or processing of data.\n",
    "Topic 96: Artifact Root - The root or top-level directory or folder where artifacts or files are stored or organized. In the asset management context, artifact roots are used to define the structure or hierarchy of stored artifacts.\n",
    "Topic 97: Files Pipeline - The files or data that flow through a machine learning pipeline or system. In the asset management context, files pipelines represent the input or output data being processed or transformed by the pipeline.\n",
    "Topic 98: Pipeline Parameters - The configurable settings or values that control the behavior or execution of a machine learning pipeline. In the asset management context, pipeline parameters are used to customize or adapt the pipeline for specific tasks or requirements.\n",
    "Topic 99: Artifacts - Files, data, or resources that are generated, produced, or used during the development, training, or deployment of machine learning models. In the asset management context, artifacts include code, models, data, or other resources related to machine learning workflows.\n",
    "Topic 100: Studio Notebook - A notebook or document within a machine learning studio or platform that provides an interactive interface for writing and running code, documenting processes, or sharing insights. In the asset management context, studio notebooks are used for developing, testing, and documenting machine learning models.\n",
    "Topic 101: Group - A collection or set of individuals or entities that are related or share common characteristics or goals. In the asset management context, groups can refer to teams, projects, or entities involved in machine learning workflows.\n",
    "Topic 102: Designer - A person or role responsible for designing or creating machine learning models, systems, or workflows. In the asset management context, designers play a key role in developing and optimizing machine learning solutions.\n",
    "Topic 103: Model Files - The files or resources that are associated with a machine learning model, such as the model architecture, weights, or configurations. In the asset management context, model files are used to store and manage the components of a trained model.\n",
    "Topic 104: Datasets - Collections of data or information that are used for training, testing, or evaluating machine learning models. In the asset management context, datasets are used to feed data into models and assess their performance.\n",
    "Topic 105: Workspace Access - The ability or permission to access and use a specific workspace or environment for machine learning tasks. In the asset management context, workspace access controls who can view, edit, or execute workflows within a workspace.\n",
    "Topic 106: Models - Machine learning models or algorithms that are trained or developed to perform specific tasks or make predictions. In the asset management context, models are the core components of machine learning workflows and systems.\n",
    "Topic 107: Loading Model - The process of loading or importing a trained machine learning model into memory or a runtime environment for inference or use. In the asset management context, loading models is a critical step in deploying and using machine learning models.\n",
    "Topic 108: Resource Students - The resources, materials, or tools provided to students for learning or practicing machine learning concepts or techniques. In the asset management context, resource students refer to the educational resources or materials available to students.\n",
    "Topic 109: Model Pipeline - A sequence or flow of steps or processes that transform input data into predictions or outputs using a machine learning model. In the asset management context, model pipelines are used to automate and streamline the process of training and deploying models.\n",
    "Topic 110: Files Notebook - The files or resources associated with a notebook within a machine learning studio or platform. In the asset management context, files notebooks store code, data, or other resources related to the development or execution of machine learning workflows.\n",
    "Topic 111: Module - A self-contained or reusable unit of code or functionality that can be imported or used in other programs or systems. In the asset management context, modules are used to organize and encapsulate code or functions related to machine learning tasks.\n",
    "Topic 112: Pipeline Step - A specific task or operation within a machine learning pipeline. In the asset management context, pipeline steps represent the individual components or processes involved in training, evaluating, or deploying machine learning models.\n",
    "Topic 113: Framework - A software or toolset that provides a foundation or structure for developing or implementing applications or systems. In the asset management context, frameworks provide the building blocks and libraries for developing and deploying machine learning models.\n",
    "Topic 114: Storage Accounts - Accounts or services that provide storage resources or capabilities for storing and managing data. In the ML asset managementset managementset managementset managementset managementset management context, storage accounts are used to store and manage data, models, or other resources related to machine learning workflows.\n",
    "Topic 115: Quota - A predefined or limited amount or quantity of resources or usage that is allocated or available for a specific purpose. In the asset management context, quotas are used to control or manage the usage or availability of resources for machine learning tasks.\n",
    "Topic 116: Output - The result or outcome generated by a machine learning model or system. In the asset management context, outputs can refer to predictions, classifications, or any other generated information or data.\n",
    "Topic 117: Hyperparameter Search - The process of systematically exploring a range of hyperparameter values to find the optimal configuration for a machine learning model. In the asset management context, hyperparameter search is used to automate the tuning of model parameters.\n",
    "Topic 118: Checkpoints - Saved or intermediate states of a machine learning model during the training process. In the asset management context, checkpoints are used to save and restore model weights or configurations for resuming or evaluating training.\n",
    "Topic 119: Model Data - The data or input used to train or evaluate a machine learning model. In the asset management context, model data represents the features, labels, or input used to develop and optimize machine learning models.\n",
    "Topic 120: File Format - The structure or organization of data or information stored in a file. In the asset management context, file formats define how data is stored, accessed, or interpreted by machine learning models or systems.\n",
    "Topic 121: Pipeline Endpoint - The network address or URL where a machine learning pipeline can be accessed or invoked to perform a specific task or operation. In the asset management context, pipeline endpoints are used to trigger or execute machine learning pipelines.\n",
    "Topic 122: Feature Store - A centralized repository or database for storing and managing features or attributes used in machine learning models. In the asset management context, feature stores are used to organize, share, and reuse features across different models or pipelines.\n",
    "Topic 123: Data Training - The process of using labeled or annotated data to train a machine learning model. In the asset management context, data training involves feeding data into models to optimize their parameters or weights.\n",
    "Topic 124: Runs - Instances or executions of a machine learning workflow or experiment. In the asset management context, runs represent specific iterations or versions of a model or pipeline.\n",
    "Topic 125: YML TemplatedConfigLoader - A configuration file format and loader that uses YAML syntax to define and load configuration settings or parameters. In the asset management context, YML TemplatedConfigLoader is used to specify and load configuration settings for machine learning workflows.\n",
    "Topic 126: Kernel - A software component or module that provides the core functionality or services of an operating system. In the asset management context, kernels are used to execute and run code within a notebook or development environment.\n",
    "Topic 127: Run Experiment - The process of executing or running a machine learning experiment to evaluate or validate a hypothesis or idea. In the asset management context, running experiments involves training models, evaluating performance, or generating insights.\n",
    "Topic 128: Workspace - A dedicated environment or space for developing, training, and deploying machine learning models. In the asset management context, workspaces provide the infrastructure, tools, and resources for end-to-end machine learning workflows.\n",
    "Topic 129: PyTorch Lightning - A lightweight PyTorch wrapper or library that simplifies the training and development of PyTorch models. In the asset management context, PyTorch Lightning is used to streamline and standardize the training process.\n",
    "Topic 130: CUDA Memory - The memory or storage space on a GPU (Graphics Processing Unit) that is used for processing and executing CUDA (Compute Unified Device Architecture) operations. In the asset management context, CUDA memory is used for training and running deep learning models.\n",
    "Topic 131: Estimator - An object or component that encapsulates the training, evaluation, and prediction capabilities of a machine learning model. In the asset management context, estimators are used to define and configure machine learning models for training or inference.\n",
    "Topic 132: Task - A specific job, operation, or activity that needs to be performed or completed. In the asset management context, tasks refer to the individual steps or processes involved in training, evaluating, or deploying machine learning models.\n",
    "Topic 133: Run Command - The execution or invocation of a specific command or instruction in a command-line interface or terminal. In the asset management context, run commands are used to trigger or execute machine learning workflows or processes.\n",
    "Topic 134: Model Output - The result or output generated by a machine learning model when given input data. In the asset management context, model outputs can be predictions, classifications, or any other generated information or data.\n",
    "Topic 135: Artifacts UI - The user interface or graphical interface for managing, viewing, or accessing artifacts or files within a machine learning platform or system. In the ML asset managementset managementset managementset managementset managementset management context, artifacts UI provides a visual interface for interacting with stored files or resources.\n",
    "Topic 136: Code - Instructions or commands written in a programming language that can be executed or interpreted by a computer or system. In the asset management context, code is used to develop, train, and deploy machine learning models and workflows.\n",
    "Topic 137: Account - A user account or profile that provides access and permissions to a system, platform, or service. In the asset management context, accounts are used to manage and control access to machine learning resources or platforms.\n",
    "Topic 138: Package - A collection or bundle of code, files, or resources that are distributed or installed together as a single unit. In the asset management context, packages are used to package and distribute machine learning models, libraries, or tools.\n",
    "Topic 139: Score - A numerical value or measure that represents the performance, quality, or effectiveness of a machine learning model. In the asset management context, scores are used to evaluate and compare models based on their accuracy, precision, recall, or other criteria.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "True\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "topic_list = [topic for topic in topics.split('\\n') if topic]\n",
    "macro_topic_mapping_inverse = [\n",
    "    ('Code Development', [15,40,59,66,72,133,136]),\n",
    "    ('Code Management', [24]),\n",
    "    ('Data Development', [1,7,44,54,57,64,86]),\n",
    "    ('Data Management', [12,34,43,56,74,76,78,84,96,99,104,122,123]),\n",
    "    # ('Documentation Management', []),\n",
    "    ('Environment Management', [0,11,16,18,31,33,38,45,52,53,62,65,69,70,75,79,80,87,89,100,102,105,111,113,125,126,128,138]),\n",
    "    ('Experiment Management', [61,71,83,101,118,124,127,132]),\n",
    "    ('Input/Output Management', [19,68,82,110,116,120,134]),\n",
    "    ('Compute Management', [27,28,46,81,92,115,130]),\n",
    "    ('Pipeline Management', [17,47,67,85,97,98,109,112,121]),\n",
    "    ('Model Development', [3,8,9,10,13,20,26,39,55,117,129,131]),\n",
    "    ('Model Management', [2,14,41,48,91,93,103,106,107,119]),\n",
    "    ('Model Serving', [4,5,21,23,35,37,42,49,50,58,73,88,94,95,139]),\n",
    "    ('Network Management', [25,51]),\n",
    "    ('Observability Management', [6,32,60,63,77,90]),\n",
    "    # ('QA Management', []),\n",
    "    ('Security Management', [22,29,114,137]),\n",
    "    ('User Interface Management', [135]),\n",
    "    ('Miscellaneous', [30,36,108]),\n",
    "]\n",
    "\n",
    "topic_list = []\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_indexing = {}\n",
    "for index, topic_set in enumerate(macro_topic_mapping_inverse):\n",
    "    macro_topic_indexing[index] = topic_set[0]\n",
    "    topic_list.extend(topic_set[1])\n",
    "    for topic in topic_set[1]:\n",
    "        macro_topic_mapping[topic] = index\n",
    "\n",
    "print(find_duplicates(topic_list))\n",
    "print(len(topic_list) == len(topic_list))\n",
    "print(set(range(len(topic_list))).difference(set(topic_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "                   Topic &  Percentage &  Number \\\\\n",
      "\\midrule\n",
      "  Environment Management &       22.04 &    2536 \\\\\n",
      "           Model Serving &       11.89 &    1368 \\\\\n",
      "       Model Development &       10.82 &    1245 \\\\\n",
      "         Data Management &        7.67 &     882 \\\\\n",
      "        Model Management &        7.16 &     824 \\\\\n",
      "     Pipeline Management &        5.50 &     633 \\\\\n",
      "        Code Development &        5.15 &     593 \\\\\n",
      "        Data Development &        4.71 &     542 \\\\\n",
      "           IO Management &        4.63 &     533 \\\\\n",
      "Observability Management &        4.42 &     508 \\\\\n",
      "      Compute Management &        4.33 &     498 \\\\\n",
      "   Experiment Management &        4.22 &     485 \\\\\n",
      "     Security Management &        2.60 &     299 \\\\\n",
      "           Miscellaneous &        2.09 &     241 \\\\\n",
      "      Network Management &        1.68 &     193 \\\\\n",
      "         Code Management &        0.74 &      85 \\\\\n",
      "           UI Management &        0.34 &      39 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hole": 0.3,
         "labels": [
          "Code Development",
          "Code Management",
          "Data Development",
          "Data Management",
          "Environment Management",
          "Experiment Management",
          "IO Management",
          "Compute Management",
          "Pipeline Management",
          "Model Development",
          "Model Management",
          "Model Serving",
          "Network Management",
          "Observability Management",
          "Security Management",
          "UI Management",
          "Miscellaneous"
         ],
         "type": "pie",
         "values": [
          593,
          85,
          542,
          882,
          2536,
          485,
          533,
          498,
          633,
          1245,
          824,
          1368,
          193,
          508,
          299,
          39,
          241
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"3241c6a8-5745-4d32-a694-37fd56e7fccb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3241c6a8-5745-4d32-a694-37fd56e7fccb\")) {                    Plotly.newPlot(                        \"3241c6a8-5745-4d32-a694-37fd56e7fccb\",                        [{\"hole\":0.3,\"labels\":[\"Code Development\",\"Code Management\",\"Data Development\",\"Data Management\",\"Environment Management\",\"Experiment Management\",\"IO Management\",\"Compute Management\",\"Pipeline Management\",\"Model Development\",\"Model Management\",\"Model Serving\",\"Network Management\",\"Observability Management\",\"Security Management\",\"UI Management\",\"Miscellaneous\"],\"values\":[593,85,542,882,2536,485,533,498,633,1245,824,1368,193,508,299,39,241],\"type\":\"pie\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('3241c6a8-5745-4d32-a694-37fd56e7fccb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']])\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_rq1, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "\n",
    "values = []\n",
    "labels = []\n",
    "\n",
    "for name, group in df.groupby('Challenge_topic_macro'):\n",
    "    entry = {\n",
    "        'Topic': macro_topic_indexing[name],\n",
    "        'Percentage': round(len(group)/len(df)*100, 2),\n",
    "        'Number': len(group),\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "    labels.append(macro_topic_indexing[name])\n",
    "    values.append(len(group))\n",
    "\n",
    "df_number = df_number.sort_values('Percentage', ascending=False)\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))\n",
    "\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'filtered.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "df['Challenge_topic_macro'] = df['Challenge_topic_macro'].apply(lambda x: macro_topic_indexing[x])\n",
    "categories = ['Challenge_type', 'Challenge_topic_macro', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_rq1, 'State type topic sankey.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
