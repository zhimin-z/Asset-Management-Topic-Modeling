{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import openai\n",
    "import textstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The significance level indicates the probability of rejecting the null hypothesis when it is true.\n",
    "alpha = 0.05\n",
    "\n",
    "link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result = '../../Result'\n",
    "\n",
    "path_result_rq1 = os.path.join(path_result, 'RQ1')\n",
    "path_code_rq1 = os.path.join('..', 'RQ1')\n",
    "\n",
    "path_general_output = os.path.join(path_result_rq1, 'General Topics')\n",
    "path_special_output = os.path.join(path_result_rq1, 'Special Topics')\n",
    "\n",
    "path_general_topic = os.path.join(path_code_rq1, 'General Topic Modeling')\n",
    "path_special_topic = os.path.join(path_code_rq1, 'Special Topic Modeling')\n",
    "\n",
    "path_anomaly = os.path.join(path_special_topic, 'Anomaly')\n",
    "path_root_cause = os.path.join(path_special_topic, 'Root Cause')\n",
    "path_solution = os.path.join(path_special_topic, 'Solution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Data Pipelines - The process of creating, managing, and running data pipelines.\n",
      "Topic 1: Docker - A platform that uses OS-level virtualization to deliver software in packages called containers.\n",
      "Topic 2: Role and Permissions - The assignment and management of roles and permissions in a software system.\n",
      "Topic 3: Version Control - The practice of tracking and managing changes to software code.\n",
      "Topic 4: TensorFlow - An open-source platform for machine learning.\n",
      "Topic 5: Data Storage - The practice of storing and managing datasets.\n",
      "Topic 6: GPU Utilization - The use and management of GPU resources in a computing environment.\n",
      "Topic 7: Artifacts - The management and storage of software artifacts.\n",
      "Topic 8: Package Installation - The process of installing and managing software packages.\n",
      "Topic 9: Kubernetes - An open-source platform for managing containerized workloads and services.\n",
      "Topic 10: YAML in Pipelines - The use of YAML files in creating and managing data pipelines.\n",
      "Topic 11: Jupyter Notebooks - An open-source web application that allows the creation and sharing of documents that contain live code, equations, visualizations, and narrative text.\n",
      "Topic 12: Datasets - The creation, management, and use of datasets in software engineering.\n",
      "Topic 13: Logging - The practice of recording actions in a software system.\n",
      "Topic 14: PySpark - A Python library for Apache Spark, a fast and general-purpose cluster computing system.\n",
      "Topic 15: Forecasting - The use of machine learning to predict future data.\n",
      "Topic 16: Model Files - The creation, extraction, and management of model files in machine learning.\n",
      "Topic 17: VPC and Connections - The configuration and management of Virtual Private Clouds (VPC) and connections.\n",
      "Topic 18: Model Training - The process of training machine learning models.\n",
      "Topic 19: Data Visualization - The practice of visualizing data through various types of plots.\n",
      "Topic 20: API Endpoints - The points of interaction in an API.\n",
      "Topic 21: Pip Installation - The process of installing and managing software packages with pip.\n",
      "Topic 22: Regression - A type of predictive modelling technique which investigates the relationship between a dependent and independent variable.\n",
      "Topic 23: Predictive Endpoints - The use of endpoints in making predictions in machine learning.\n",
      "Topic 24: Database Connections - The process of connecting to and interacting with databases.\n",
      "Topic 25: Model Deployment - The process of making a machine learning model available for use.\n",
      "Topic 26: Notebook Execution - The process of running and managing Jupyter notebooks.\n",
      "Topic 27: Data Buckets - The use and management of data storage buckets.\n",
      "Topic 28: Hyperparameter Sweeping - The process of searching through a manual or predefined hyperparameter space for the best model hyperparameters.\n",
      "Topic 29: Training Arguments - The use and management of arguments in model training.\n",
      "Topic 30: Multi-model Endpoints - The use of endpoints that can deploy multiple models.\n",
      "Topic 31: Workspace Connections - The process of connecting to and managing workspaces.\n",
      "Topic 32: PyTorch Logging - The practice of recording actions in a PyTorch environment.\n",
      "Topic 33: Dependency Conflicts - The management and resolution of conflicts between software dependencies.\n",
      "Topic 34: Visual Studio Code - A free source-code editor made by Microsoft for Windows, Linux and macOS.\n",
      "Topic 35: CSV and Pandas - The use of CSV files and the pandas library in data processing.\n",
      "Topic 36: Hyperparameter Tuning - The process of optimizing the parameters of a machine learning model.\n",
      "Topic 37: Vision API - A tool that allows developers to understand the content of an image by encapsulating powerful machine learning models in an easy-to-use REST API.\n",
      "Topic 38: TensorBoard - A tool for providing the measurements and visualizations needed during the machine learning workflow.\n",
      "Topic 39: Conda - An open-source package management system and environment management system.\n",
      "Topic 40: Clustering - The task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group.\n",
      "Topic 41: Research and Collaboration - The process of conducting research and collaborating in a software engineering context.\n",
      "Topic 42: Endpoint Scaling - The process of managing the scale of API endpoints.\n",
      "Topic 43: Web Services - The use and management of web services in a software system.\n",
      "Topic 44: Speech Processing - The use of machine learning to process and transcribe speech.\n",
      "Topic 45: Error Handling - The process of handling and resolving errors in a software system.\n",
      "Topic 46: Dialogflow - A natural language understanding platform used to design and integrate a conversational user interface into mobile apps, web applications, devices, bots, interactive voice response systems, and so on.\n",
      "Topic 47: Language Translation - The use of machine learning to translate text from one language to another.\n",
      "Topic 48: Documentation and Release - The process of documenting software and managing its release.\n",
      "Topic 49: Directory Management - The process of managing and refactoring directories in a software system.\n",
      "Topic 50: Execution Speed - The speed at which a software system or process runs.\n",
      "Topic 51: Scheduling - The practice of scheduling tasks in a software system.\n",
      "Topic 52: Logging and Metrics - The practice of recording actions and metrics in a software system.\n",
      "Topic 53: Data Labeling - The process of labeling data for use in machine learning.\n",
      "Topic 54: CSV Processing - The process of processing CSV files.\n",
      "Topic 55: Convolutional Neural Networks - A class of deep neural networks, most commonly applied to analyzing visual imagery.\n",
      "Topic 56: PyTorch and CNNs - The use of PyTorch and Convolutional Neural Networks in machine learning.\n",
      "Topic 57: Flask - A micro web framework written in Python.\n",
      "Topic 58: Data Transformation - The process of converting data from one format or structure into another.\n",
      "Topic 59: File Upload and Sync - The process of uploading files and syncing data.\n",
      "Topic 60: Document Processing - The process of extracting information from documents.\n",
      "Topic 61: Model Deployment and Schema - The process of deploying models and defining schemas.\n",
      "Topic 62: Object Detection - The process of detecting objects within digital images and videos.\n",
      "Topic 63: PyTorch Environment - The use and management of the PyTorch machine learning platform.\n",
      "Topic 64: Quota Management - The process of managing quotas in a software system.\n",
      "Topic 65: Activity Panels and Computing - The use and management of activity panels and computing resources.\n",
      "Topic 66: Model Compilation - The process of compiling machine learning models.\n",
      "Topic 67: Web Service Deployment - The process of deploying web services.\n",
      "Topic 68: Disk Space Management - The process of managing disk space in a computing environment.\n",
      "Topic 69: Synchronization and Upload - The process of synchronizing and uploading data.\n",
      "Topic 70: Document Review - The process of reviewing and processing documents.\n",
      "Topic 71: Model Deployment and Services - The process of deploying models and managing services.\n",
      "Topic 72: Object Detection in Videos - The process of detecting objects in video data.\n",
      "Topic 73: PyTorch Environment - The use and management of the PyTorch machine learning platform.\n",
      "Topic 74: Quota Management - The process of managing quotas in a software system.\n",
      "Topic\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_general_topic, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 'NA',\n",
       " 0: 'Pipeline Configuration',\n",
       " 1: 'Docker Configuration',\n",
       " 2: 'Role Configuration',\n",
       " 3: 'Git Version Control',\n",
       " 4: 'TensorFlow Configuration',\n",
       " 5: 'Data Storage and Dataset',\n",
       " 6: 'GPU Configuration',\n",
       " 7: 'Artifact Management',\n",
       " 8: 'Package Management',\n",
       " 9: 'Kubernetes Service',\n",
       " 10: 'YAML Configuration',\n",
       " 11: 'Jupyter Configuration',\n",
       " 12: 'Dataset Management',\n",
       " 13: 'Log Management',\n",
       " 14: 'Spark Configuration',\n",
       " 15: 'AutoML Forecasting',\n",
       " 16: 'Model Storage and Conversion',\n",
       " 17: 'VPC and Connection',\n",
       " 18: 'Model Training',\n",
       " 19: 'Data Visualization',\n",
       " 20: 'Endpoint Configuration',\n",
       " 21: 'Pip Configuration',\n",
       " 22: 'Regression',\n",
       " 23: 'Endpoint Prediction',\n",
       " 24: 'Database Management',\n",
       " 25: 'Model Deployment',\n",
       " 26: 'Notebook Operation',\n",
       " 27: 'Online Storage and Bucket',\n",
       " 28: 'Hyperparameter Sweeping',\n",
       " 29: 'Training Configuration',\n",
       " 30: 'Multimodel Endpoint',\n",
       " 31: 'Workspace Configuration',\n",
       " 32: 'PyTorch and Logging',\n",
       " 33: 'Dependency Management',\n",
       " 34: 'VSCode Configuration',\n",
       " 35: 'Tabular Data Manipulation',\n",
       " 36: 'Hyperparameter Tuning',\n",
       " 37: 'Computer Vision',\n",
       " 38: 'TensorBoard Visualization',\n",
       " 39: 'Conda Configuration',\n",
       " 40: 'Cluster Management',\n",
       " 41: 'Research and Collaboration',\n",
       " 42: 'Endpoint Provisioning',\n",
       " 43: 'Web Service',\n",
       " 44: 'Speech Processing',\n",
       " 45: 'Error Handling',\n",
       " 46: 'Dialogflow Configuration',\n",
       " 47: 'Language Translation',\n",
       " 48: 'Documentation and Release',\n",
       " 49: 'Directory Management',\n",
       " 50: 'Execution Speed',\n",
       " 51: 'Job Scheduling',\n",
       " 52: 'Logging and Metrics',\n",
       " 53: 'Data Labeling',\n",
       " 54: 'CSV Processing',\n",
       " 55: 'Convolutional Neural Network',\n",
       " 56: 'Neural Network Architecture',\n",
       " 57: 'Flask Configuration',\n",
       " 58: 'Data Transformation',\n",
       " 59: 'File Syncing',\n",
       " 60: 'Document Processing',\n",
       " 61: 'Schema Configuration',\n",
       " 62: 'Object Detection',\n",
       " 63: 'PyTorch Configuration',\n",
       " 64: 'Quota Management',\n",
       " 65: 'Activity Panel and Computing',\n",
       " 66: 'Model Compilation',\n",
       " 67: 'Web Service Deployment',\n",
       " 68: 'Storage Management'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = '''Topic 0: Pipeline Configuration - Processes and tools for building, running, and managing data pipelines.\n",
    "Topic 1: Docker Configuration - Techniques and commands for creating and managing Docker containers and environments.\n",
    "Topic 2: Role Configuration - Management of roles and permissions in a software system, particularly in IAM.\n",
    "Topic 3: Git Version Control - Use of Git for version control of files and repositories.\n",
    "Topic 4: TensorFlow Configuration - Building, running, and managing models using TensorFlow.\n",
    "Topic 5: Data Storage and Dataset - Handling of datasets and data storage, including data lakes.\n",
    "Topic 6: GPU Configuration - Monitoring and managing GPU utilization, particularly in PyTorch and CUDA.\n",
    "Topic 7: Artifact Management - Management of artifacts in a software system, including storage and access.\n",
    "Topic 8: Package Management - Installation and management of software libraries and packages.\n",
    "Topic 9: Kubernetes Service - Deployment and management of services in a Kubernetes cluster.\n",
    "Topic 10: YAML Configuration - Use of YAML for configuring and structuring pipelines.\n",
    "Topic 11: Jupyter Configuration - Running and managing Jupyter notebooks.\n",
    "Topic 12: Dataset Management - Manipulation and management of datasets, including splitting and combining.\n",
    "Topic 13: Log Management - Configuration and management of logging in a software system.\n",
    "Topic 14: Spark Configuration - Running and managing PySpark jobs and dataframes.\n",
    "Topic 15: AutoML Forecasting - Use of AutoML for forecasting based on datasets.\n",
    "Topic 16: Model Storage and Conversion - Handling of model files, including extraction and import.\n",
    "Topic 17: VPC and Connection - Configuration and management of VPC endpoints and connections.\n",
    "Topic 18: Model Training - Training and updating models, particularly with AutoML and sklearn.\n",
    "Topic 19: Data Visualization - Creation of various types of plots for data visualization.\n",
    "Topic 20: Endpoint Configuration - Configuration and invocation of API endpoints.\n",
    "Topic 21: Pip Configuration - Installation and management of pip, including version control.\n",
    "Topic 22: Regression - Use of regression models and evaluation of datasets.\n",
    "Topic 23: Endpoint Prediction - Making predictions using API endpoints and data.\n",
    "Topic 24: Database Management - Connecting to and managing databases, including SQL and MySQL.\n",
    "Topic 25: Model Deployment - Deployment and management of models in a machine learning project.\n",
    "Topic 26: Notebook Operation - Execution and management of notebooks.\n",
    "Topic 27: Online Storage and Bucket - Management of buckets and directories, including uploads and permissions.\n",
    "Topic 28: Hyperparameter Sweeping - Configuration and execution of hyperparameter sweeps.\n",
    "Topic 29: Training Configuration - Training models and handling training arguments.\n",
    "Topic 30: Multimodel Endpoint - Management of endpoints that host multiple models.\n",
    "Topic 31: Workspace Configuration - Connecting to and managing workspaces.\n",
    "Topic 32: PyTorch and Logging - Use of PyTorch for logging, including loss tracking.\n",
    "Topic 33: Dependency Management - Handling of dependencies and conflicts in software packages.\n",
    "Topic 34: VSCode Configuration - Use and management of Visual Studio Code.\n",
    "Topic 35: Tabular Data Manipulation - Handling of CSV data with pandas, including reading and writing.\n",
    "Topic 36: Hyperparameter Tuning - Tuning of hyperparameters in a machine learning model.\n",
    "Topic 37: Computer Vision - Use of vision APIs for image recognition and processing.\n",
    "Topic 38: TensorBoard Visualization - Use and configuration of TensorBoard for visualizing machine learning models.\n",
    "Topic 39: Conda Configuration - Use and management of Conda environments.\n",
    "Topic 40: Cluster Management - Running and managing computational clusters.\n",
    "Topic 41: Research and Collaboration - Collaboration and contribution in research and development.\n",
    "Topic 42: Endpoint Provisioning - Scaling and provisioning of API endpoints.\n",
    "Topic 43: Web Service - Deployment and management of web services.\n",
    "Topic 44: Speech Processing - Use of speech APIs for transcription and synthesis.\n",
    "Topic 45: Error Handling - Handling of common errors in model prediction.\n",
    "Topic 46: Dialogflow Configuration - Configuration and use of Dialogflow for conversational AI.\n",
    "Topic 47: Language Translation - Use of translation APIs for language translation.\n",
    "Topic 48: Documentation and Release - Updating and releasing documentation for software versions.\n",
    "Topic 49: Directory Management - Refactoring and management of directories and files.\n",
    "Topic 50: Execution Speed - Management of execution speed and time in software processes.\n",
    "Topic 51: Job Scheduling - Scheduling of jobs and executions in a software system.\n",
    "Topic 52: Logging and Metrics - Logging and fetching of metrics in a software system.\n",
    "Topic 53: Data Labeling - Processing and labeling of data for machine learning.\n",
    "Topic 54: CSV Processing - Processing and handling of CSV data.\n",
    "Topic 55: Convolutional Neural Network - Training and use of convolutional neural networks (CNNs).\n",
    "Topic 56: Neural Network Architecture - Use of PyTorch for training CNNs and GANs.\n",
    "Topic 57: Flask Configuration - Use and management of the Flask web framework.\n",
    "Topic 58: Data Transformation - Transformation and conversion of data for machine learning.\n",
    "Topic 59: File Syncing - Uploading and syncing of files and artifacts.\n",
    "Topic 60: Document Processing - Processing and extraction of information from documents.\n",
    "Topic 61: Schema Configuration - Deployment of models and definition of schemas.\n",
    "Topic 62: Object Detection - Use of models for object detection in videos.\n",
    "Topic 63: PyTorch Configuration - Management and use of PyTorch environments.\n",
    "Topic 64: Quota Management - Management of quotas in a software system.\n",
    "Topic 65: Activity Panel and Computing - Management of activity panels and computing resources.\n",
    "Topic 66: Model Compilation - Compilation of models, particularly with Neo.\n",
    "Topic 67: Web Service Deployment - Deployment and management of web services.\n",
    "Topic 68: Storage Management - Management of disk space and quotas.'''\n",
    "\n",
    "index = 0\n",
    "topic_dict = {-1: 'NA'}\n",
    "\n",
    "for topic in topics.split('\\n'):\n",
    "    topic_dict[index] = topic.split('-')[0].split(':')[-1].strip()\n",
    "    index += 1\n",
    "    \n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Git Version Control': 0,\n",
       " 'Role Configuration': 1,\n",
       " 'VPC and Connection': 1,\n",
       " 'Job Scheduling': 2,\n",
       " 'Pipeline Configuration': 2,\n",
       " 'YAML Configuration': 2,\n",
       " 'Conda Configuration': 3,\n",
       " 'Dependency Management': 3,\n",
       " 'Docker Configuration': 3,\n",
       " 'Jupyter Configuration': 3,\n",
       " 'Notebook Operation': 3,\n",
       " 'Package Management': 3,\n",
       " 'Pip Configuration': 3,\n",
       " 'VSCode Configuration': 3,\n",
       " 'Workspace Configuration': 3,\n",
       " 'Artifact Management': 4,\n",
       " 'CSV Processing': 4,\n",
       " 'Data Labeling': 4,\n",
       " 'Data Storage and Dataset': 4,\n",
       " 'Data Transformation': 4,\n",
       " 'Data Visualization': 4,\n",
       " 'Database Management': 4,\n",
       " 'Dataset Management': 4,\n",
       " 'Directory Management': 4,\n",
       " 'File Syncing': 4,\n",
       " 'Online Storage and Bucket': 4,\n",
       " 'Storage Management': 4,\n",
       " 'Tabular Data Manipulation': 4,\n",
       " 'Cluster Management': 5,\n",
       " 'Execution Speed': 5,\n",
       " 'GPU Configuration': 5,\n",
       " 'Hyperparameter Sweeping': 5,\n",
       " 'Hyperparameter Tuning': 5,\n",
       " 'Model Training': 5,\n",
       " 'PyTorch Configuration': 5,\n",
       " 'Spark Configuration': 5,\n",
       " 'TensorFlow Configuration': 5,\n",
       " 'Training Configuration': 5,\n",
       " 'Endpoint Prediction': 6,\n",
       " 'Endpoint Configuration': 6,\n",
       " 'Endpoint Provisioning': 6,\n",
       " 'Flask Configuration': 6,\n",
       " 'Multimodel Endpoint': 6,\n",
       " 'Schema Configuration': 6,\n",
       " 'Web Service': 6,\n",
       " 'Web Service Deployment': 6,\n",
       " 'Kubernetes Service': 7,\n",
       " 'Log Management': 7,\n",
       " 'Logging and Metrics': 7,\n",
       " 'PyTorch and Logging': 7,\n",
       " 'TensorBoard Visualization': 7,\n",
       " 'Model Compilation': 8,\n",
       " 'Model Deployment': 8,\n",
       " 'Model Storage and Conversion': 8,\n",
       " 'Error Handling': 9}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_topic_mapping_inverse = {\n",
    "    # These topics are all related to the management of source code.\n",
    "    0: ('Code Management', ['Git Version Control']),\n",
    "    # These topics are all related to the management of permissions and connectivity.\n",
    "    1: ('Access Management', ['Role Configuration', 'VPC and Connection']),\n",
    "    # These topics are all related to the management of pipelines.\n",
    "    2: ('Lifecycle Management', ['Job Scheduling', 'Pipeline Configuration', 'YAML Configuration']),\n",
    "    # These topics are all related to the management of packages and distributions.\n",
    "    3: ('Infrastructure Management', ['Conda Configuration', 'Dependency Management', 'Docker Configuration', 'Jupyter Configuration', 'Notebook Operation', 'Package Management', 'Pip Configuration', 'VSCode Configuration', 'Workspace Configuration']),\n",
    "    # These topics are all related to the management of data and datasets.\n",
    "    4: ('Data Management', ['Artifact Management', 'CSV Processing', 'Data Labeling', 'Data Storage and Dataset', 'Data Transformation', 'Data Visualization', 'Database Management', 'Dataset Management', 'Directory Management', 'File Syncing', 'Online Storage and Bucket', 'Storage Management', 'Tabular Data Manipulation']),\n",
    "    # These topics are all related to the management of parallel computing resources.\n",
    "    5: ('Compute Management', ['Cluster Management', 'Execution Speed', 'GPU Configuration', 'Hyperparameter Sweeping', 'Hyperparameter Tuning', 'Model Training', 'PyTorch Configuration', 'Spark Configuration', 'TensorFlow Configuration', 'Training Configuration']),\n",
    "    # These topics are all related to the management of services.\n",
    "    6: ('Service Management', ['Endpoint Prediction', 'Endpoint Configuration', 'Endpoint Provisioning', 'Flask Configuration', 'Multimodel Endpoint', 'Schema Configuration', 'Web Service', 'Web Service Deployment']),\n",
    "    # These topics are all related to the management of logs and metrics.\n",
    "    7: ('Performance Management', ['Kubernetes Service', 'Log Management', 'Logging and Metrics', 'PyTorch and Logging', 'TensorBoard Visualization']),\n",
    "    # These topics are all related to the management of machine learning models.\n",
    "    8: ('Model Management', ['Model Compilation', 'Model Deployment', 'Model Storage and Conversion']),\n",
    "    # These topics are all related to miscellaneous software engineering steps.\n",
    "    9: ('Miscellaneous', ['Error Handling']),\n",
    "}\n",
    "\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_index_mapping = {}\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    macro_topic_index_mapping[macro_topic] = key\n",
    "    for topic in topics:\n",
    "        macro_topic_mapping[topic] = key\n",
    "\n",
    "macro_topic_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Management</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access Management</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lifecycle Management</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infrastructure Management</td>\n",
       "      <td>1695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compute Management</td>\n",
       "      <td>1926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Service Management</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Performance Management</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Management</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Topic  Number\n",
       "Index                                   \n",
       "0                Code Management     326\n",
       "1              Access Management     538\n",
       "2           Lifecycle Management     885\n",
       "3      Infrastructure Management    1695\n",
       "4                Data Management    1938\n",
       "5             Compute Management    1926\n",
       "6             Service Management    1307\n",
       "7         Performance Management     787\n",
       "8               Model Management     504\n",
       "9                  Miscellaneous     168"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general_output, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if topic_dict[row['Challenge_topic']] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict[row['Challenge_topic']]]\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_general_output, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    entry = {\n",
    "        'Index': key,\n",
    "        'Topic': macro_topic,\n",
    "        'Number': len(df[df[\"Challenge_topic_macro\"] == key])\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "df_number.set_index('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "# for index, row in df.iterrows():\n",
    "#     if topic_dict[row['Challenge_topic']] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict[row['Challenge_topic']]]\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels+.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_general_output, 'filtered.json'))\n",
    "\n",
    "df['Challenge_type'] = 'na'\n",
    "df['Challenge_summary'] = 'na'\n",
    "df['Challenge_root_cause'] = 'na'\n",
    "df['Challenge_solution'] = 'na'\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# for i,r in df.iterrows():\n",
    "#     if r['Challenge_type'] == 'inquiry':\n",
    "#         df.at[i, 'Challenge_summary'] = 'na'\n",
    "#         df.at[i, 'Challenge_root_cause'] = 'na'\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels+.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df = pd.concat([df_new, df_old], ignore_index=True)\n",
    "# df = df.drop_duplicates(['Challenge_link'], keep=False)\n",
    "\n",
    "# df = df[df['Challenge_type'].isna()]\n",
    "# df.to_json(os.path.join(path_special_output, 'extra.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels++.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_special_output, 'extra.json'))\n",
    "\n",
    "# df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "# df = df.drop_duplicates(['Challenge_link'])\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'extra+.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels+.json'))\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df_new[~((df1.City.isin(df2.City)) & (df1.State.isin(df2.State)))] \n",
    "\n",
    "# for index, row in df_new.iterrows():\n",
    "#     for i2,r2 in df.iterrows():\n",
    "#         if r2['Challenge_type'] == 'na':\n",
    "#             continue\n",
    "#         if r2['Challenge_link'] == row['Challenge_link']:\n",
    "#             df_new.at[index, 'Challenge_type'] = r2['Challenge_type']\n",
    "#             df_new.at[index, 'Challenge_summary'] = r2['Challenge_summary']\n",
    "#             df_new.at[index, 'Challenge_root_cause'] = r2['Challenge_root_cause']\n",
    "#             df_new.at[index, 'Challenge_solution'] = r2['Challenge_solution']\n",
    "#             break\n",
    "            \n",
    "# df_new.to_json(os.path.join(path_special_output, 'labels++.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_result_rq1, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different metrics to each post\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['Solution_word_count'] = np.nan\n",
    "df['Solution_sentence_count'] = np.nan\n",
    "df['Solution_readability'] = np.nan\n",
    "df['Solution_reading_time'] = np.nan\n",
    "df['Solution_link_count'] = np.nan\n",
    "\n",
    "df['Challenge_solved_time'] = np.nan\n",
    "df['Challenge_adjusted_solved_time'] = np.nan\n",
    "\n",
    "df['Solution_link_docs'] = np.nan\n",
    "df['Solution_link_issues'] = np.nan\n",
    "df['Solution_link_patches'] = np.nan\n",
    "df['Solution_link_tools'] = np.nan\n",
    "df['Solution_link_tutorials'] = np.nan\n",
    "df['Solution_link_examples'] = np.nan\n",
    "\n",
    "df['Challenge_created_time'] = pd.to_datetime(df['Challenge_created_time'])\n",
    "df['Challenge_closed_time'] = pd.to_datetime(df['Challenge_closed_time'])\n",
    "df['Challenge_last_edit_time'] = pd.to_datetime(df['Challenge_last_edit_time'])\n",
    "df['Solution_last_edit_time'] = pd.to_datetime(df['Solution_last_edit_time'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    challenge_content = row['Challenge_title'] + '.' + str(row['Challenge_body'])\n",
    "    df.at[index, 'Challenge_word_count'] = textstat.lexicon_count(challenge_content)\n",
    "    df.at[index, 'Challenge_sentence_count'] = textstat.sentence_count(challenge_content)\n",
    "    df.at[index, 'Challenge_readability'] = textstat.flesch_kincaid_grade(challenge_content)\n",
    "    df.at[index, 'Challenge_reading_time'] = textstat.reading_time(challenge_content)\n",
    "    \n",
    "    links = list(set(re.findall(link_pattern, challenge_content)))\n",
    "    df.at[index, 'Challenge_link_count'] = len(links)\n",
    "\n",
    "    solution_content = row['Solution_body']\n",
    "\n",
    "    if pd.notna(solution_content):\n",
    "        df.at[index, 'Solution_word_count'] = textstat.lexicon_count(solution_content)\n",
    "        df.at[index, 'Solution_sentence_count'] = textstat.sentence_count(solution_content)\n",
    "        df.at[index, 'Solution_readability'] = textstat.flesch_kincaid_grade(solution_content)\n",
    "        df.at[index, 'Solution_reading_time'] = textstat.reading_time(solution_content)\n",
    "        \n",
    "        links = list(set(re.findall(link_pattern, solution_content)))\n",
    "        df.at[index, 'Solution_link_count'] = len(links)\n",
    "        \n",
    "        link_docs = 0\n",
    "        link_tools = 0\n",
    "        link_issues = 0\n",
    "        link_patches = 0\n",
    "        link_tutorials = 0\n",
    "        link_examples = 0\n",
    "    \n",
    "        for link in links:\n",
    "            if any([patch in link for patch in keywords_patch]):\n",
    "                link_patches += 1\n",
    "            elif any([issue in link for issue in keywords_issue]):\n",
    "                link_issues += 1\n",
    "            elif any([tool in link for tool in keywords_tool]):\n",
    "                link_tools += 1\n",
    "            elif any([doc in link for doc in keywords_doc]):\n",
    "                link_docs += 1\n",
    "            elif any([tool in link for tool in keywords_tutorial]):\n",
    "                link_tutorials += 1\n",
    "            else:\n",
    "                link_examples += 1\n",
    "                \n",
    "        df.at[index, 'Solution_link_docs'] = link_docs\n",
    "        df.at[index, 'Solution_link_issues'] = link_issues\n",
    "        df.at[index, 'Solution_link_patches'] = link_patches\n",
    "        df.at[index, 'Solution_link_tools'] = link_tools\n",
    "        df.at[index, 'Solution_link_tutorials'] = link_tutorials\n",
    "        df.at[index, 'Solution_link_examples'] = link_examples\n",
    "        \n",
    "    creation_time = row['Challenge_created_time']\n",
    "    closed_time = row['Challenge_closed_time']\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "\n",
    "    creation_time = row['Challenge_last_edit_time'] if pd.notna(row['Challenge_last_edit_time']) else creation_time\n",
    "    closed_time = row['Solution_last_edit_time'] if pd.notna(row['Solution_last_edit_time']) else closed_time\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = df.at[index, 'Challenge_solved_time']\n",
    "\n",
    "df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "df['Challenge_answer_count'] = df['Challenge_answer_count'].fillna(0)\n",
    "df['Challenge_participation_count'] = df['Challenge_answer_count'] + df['Challenge_comment_count']\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_json(os.path.join(path_result_rq1, 'metrics.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Count'] = 1\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_topic_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge view count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_view_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge score count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_score_count'] = df_topics['Challenge_score_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_score_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge favorite count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_favorite_count'] = df_topics['Challenge_favorite_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_favorite_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge answer count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_answer_count'] = df_topics['Challenge_answer_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_answer_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge comment count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_comment_count'] = df_topics['Challenge_comment_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_comment_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_participation_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_participation_count_distribution.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
