{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import openai\n",
    "import textstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The significance level indicates the probability of rejecting the null hypothesis when it is true.\n",
    "alpha = 0.05\n",
    "\n",
    "link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result = '../../Result'\n",
    "\n",
    "path_result_rq1 = os.path.join(path_result, 'RQ1')\n",
    "path_code_rq1 = os.path.join('..', 'RQ1')\n",
    "\n",
    "path_general_output = os.path.join(path_result_rq1, 'General Topics')\n",
    "path_special_output = os.path.join(path_result_rq1, 'Special Topics')\n",
    "\n",
    "path_general_topic = os.path.join(path_code_rq1, 'General Topic Modeling')\n",
    "path_special_topic = os.path.join(path_code_rq1, 'Special Topic Modeling')\n",
    "\n",
    "path_anomaly = os.path.join(path_special_topic, 'Anomaly')\n",
    "path_root_cause = os.path.join(path_special_topic, 'Root Cause')\n",
    "path_solution = os.path.join(path_special_topic, 'Solution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_general_topic, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 'NA',\n",
       " 0: 'Docker',\n",
       " 1: 'Pipeline',\n",
       " 2: 'Pandas DataFrame',\n",
       " 3: 'Git',\n",
       " 4: 'Access Control',\n",
       " 5: 'Jupyter Notebook',\n",
       " 6: 'Database Services',\n",
       " 7: 'Logging',\n",
       " 8: 'TensorFlow',\n",
       " 9: 'Web Service Deployment',\n",
       " 10: 'YAML',\n",
       " 11: 'Apache Spark',\n",
       " 12: 'Hyperparameter Tuning',\n",
       " 13: 'Remote Server',\n",
       " 14: 'PyTorch GPU',\n",
       " 15: 'Data Labeling',\n",
       " 16: 'Regression Analysis',\n",
       " 17: 'Dependency Management',\n",
       " 18: 'Model Saving',\n",
       " 19: 'Forecasting',\n",
       " 20: 'Hyperparameter Sweep',\n",
       " 21: 'Feature Store',\n",
       " 22: 'Environment Configuration',\n",
       " 23: 'Package Installation',\n",
       " 24: 'Dialogflow',\n",
       " 25: 'Speech API',\n",
       " 26: 'PyTorch',\n",
       " 27: 'Vision API',\n",
       " 28: 'Pip',\n",
       " 29: 'TensorBoard',\n",
       " 30: 'Data Bucket',\n",
       " 31: 'AWS Lambda',\n",
       " 32: 'Language Translation',\n",
       " 33: 'Quota Management',\n",
       " 34: 'JSON',\n",
       " 35: 'Model Deployment',\n",
       " 36: 'Prediction API',\n",
       " 37: 'OCR',\n",
       " 38: 'Loss Metrics',\n",
       " 39: 'Model Training',\n",
       " 40: 'Model Registry',\n",
       " 41: 'Data Visualization'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = '''Topic 0: Docker Configuration - A platform used to automate the deployment, scaling, and management of applications using containerization.\n",
    "Topic 1: Pipeline Configuration - A set of automated processes that allow developers and DevOps professionals to reliably and efficiently compile, build and deploy their code to their production compute platforms.\n",
    "Topic 2: Tabular Data Manipulation - A two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns) in Python.\n",
    "Topic 3: Git Version Control - A distributed version control system for tracking changes in source code during software development.\n",
    "Topic 4: Access Control - The selective restriction of access to a place or other resource while offering a simple setup and ease of use.\n",
    "Topic 5: Jupyter Configuration - An open-source web application that allows the creation and sharing of documents containing live code, equations, visualizations, and narrative text.\n",
    "Topic 6: Database Management - Services that offer a high-performance, reliable, and secure place to store data.\n",
    "Topic 7: Log Management - The act of keeping a log or record of all information or events that occur within a system.\n",
    "Topic 8: TensorFlow Configuration - An open-source software library for machine learning and artificial intelligence.\n",
    "Topic 9: Web Service - The process of making a web service and its constituent resources available for use.\n",
    "Topic 10: YAML Configuration - A human-readable data serialization standard used in configuration files and in applications where data is being stored or transmitted.\n",
    "Topic 11: Spark Configuration - An open-source distributed general-purpose cluster-computing framework.\n",
    "Topic 12: Hyperparameter Tuning - The process of choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "Topic 13: Remote Configuration - A server that is hosted by a company and only allows users to securely manage, organize, and share files over the internet.\n",
    "Topic 14: GPU Configuration - A deep learning platform that accelerates the path from research prototyping to production deployment with GPU support.\n",
    "Topic 15: Data Labeling - The process of tagging the data, like images or text, by humans to make it understandable for machines.\n",
    "Topic 16: Regression - A set of statistical processes for estimating the relationships between a dependent variable and one or more independent variables.\n",
    "Topic 17: Dependency Management - A technique for declaring, resolving and using dependencies required by the project in an automated fashion.\n",
    "Topic 18: Model Persistence - The process of persisting the state of a machine learning model in a file.\n",
    "Topic 19: Forecasting - The process of making predictions of the future based on past and present data and most commonly by analysis of trends.\n",
    "Topic 20: Hyperparameter Sweep - A technique to automate the process of tuning parameters in a model.\n",
    "Topic 21: Feature Store - A centralized, curated, and access-controlled repository for machine learning features.\n",
    "Topic 22: Environment Configuration - The process of setting up a system environment to run specific software.\n",
    "Topic 23: Package Installation - The process of installing and managing software packages.\n",
    "Topic 24: Dialogflow - A natural language understanding platform used to design and integrate a conversational user interface into mobile apps, web applications, devices, bots, and more.\n",
    "Topic 25: Speech API - A service that converts spoken language into written text.\n",
    "Topic 26: PyTorch - An open-source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing.\n",
    "Topic 27: Vision API - A tool that uses machine learning models to classify images into thousands of categories, detect individual objects and faces within images, and find and read printed words contained within images.\n",
    "Topic 28: Pip - A package installer for Python that provides a standard interface for installing Python packages.\n",
    "Topic 29: TensorBoard - A tool for providing the measurements and visualizations needed during the machine learning workflow.\n",
    "Topic 30: Data Bucket - A logical unit of storage in cloud object storage.\n",
    "Topic 31: AWS Lambda - A serverless compute service that runs your code in response to events and automatically manages the underlying compute resources for you.\n",
    "Topic 32: Language Translation - The process of changing the language of text from one language to another.\n",
    "Topic 33: Quota Management - The process of managing the users' access to the resources in terms of quotas.\n",
    "Topic 34: JSON - A lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate.\n",
    "Topic 35: Model Deployment - The process of making your model available in production environment, where it can provide predictions to other software systems.\n",
    "Topic 36: Prediction API - A cloud-based Machine Learning service that allows developers to build Machine Learning models with all kinds of data.\n",
    "Topic 37: OCR - Optical Character Recognition, a technology used to convert different types of documents, such as scanned paper documents, PDF files or images captured by a digital camera into editable and searchable data.\n",
    "Topic 38: Loss Metrics - A way to measure how well a machine learning model is performing.\n",
    "Topic 39: Model Training - The process of determining the ideal parameters for a machine learning model.\n",
    "Topic 40: Model Registry - A centralized model store to collaboratively manage machine learning models across their full lifecycle.\n",
    "Topic 41: Data Visualization - The graphical representation of information and data using graphical elements such as charts, graphs, and maps.'''\n",
    "\n",
    "index = 0\n",
    "topic_dict = {-1: 'NA'}\n",
    "\n",
    "for topic in topics.split('\\n'):\n",
    "    topic_dict[index] = topic.split('-')[0].split(':')[-1].strip()\n",
    "    index += 1\n",
    "    \n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Git Version Control': 0,\n",
       " 'Role Configuration': 1,\n",
       " 'VPC and Connection': 1,\n",
       " 'Job Scheduling': 2,\n",
       " 'Pipeline Configuration': 2,\n",
       " 'YAML Configuration': 2,\n",
       " 'Conda Configuration': 3,\n",
       " 'Dependency Management': 3,\n",
       " 'Docker Configuration': 3,\n",
       " 'Jupyter Configuration': 3,\n",
       " 'Kubernetes Configuration': 3,\n",
       " 'Notebook Operation': 3,\n",
       " 'Package Management': 3,\n",
       " 'Pip Configuration': 3,\n",
       " 'VSCode Configuration': 3,\n",
       " 'Workspace Configuration': 3,\n",
       " 'Cluster Management': 4,\n",
       " 'Computing Activity': 4,\n",
       " 'Execution Speed': 4,\n",
       " 'GPU Configuration': 4,\n",
       " 'Hyperparameter Sweeping': 4,\n",
       " 'Hyperparameter Tuning': 4,\n",
       " 'Model Training': 4,\n",
       " 'PyTorch Configuration': 4,\n",
       " 'Spark Configuration': 4,\n",
       " 'TensorFlow Configuration': 4,\n",
       " 'Training Configuration': 4,\n",
       " 'Artifact Management': 5,\n",
       " 'CSV Processing': 5,\n",
       " 'Data Labeling': 5,\n",
       " 'Data Storage and Dataset': 5,\n",
       " 'Data Transformation': 5,\n",
       " 'Data Visualization': 5,\n",
       " 'Database Management': 5,\n",
       " 'Dataset Management': 5,\n",
       " 'Directory Management': 5,\n",
       " 'File Syncing': 5,\n",
       " 'Online Storage and Bucket': 5,\n",
       " 'Storage Management': 5,\n",
       " 'Tabular Data Manipulation': 5,\n",
       " 'Endpoint Prediction': 6,\n",
       " 'Endpoint Configuration': 6,\n",
       " 'Endpoint Provisioning': 6,\n",
       " 'Flask Configuration': 6,\n",
       " 'Multimodel Endpoint': 6,\n",
       " 'Schema Configuration': 6,\n",
       " 'Web Service': 6,\n",
       " 'Web Service Deployment': 6,\n",
       " 'Log Management': 7,\n",
       " 'Logging and Metrics': 7,\n",
       " 'PyTorch and Logging': 7,\n",
       " 'TensorBoard Visualization': 7,\n",
       " 'Model Compilation': 8,\n",
       " 'Model Deployment': 8,\n",
       " 'Model Storage and Conversion': 8,\n",
       " 'Error Handling': 9}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_topic_mapping_inverse = {\n",
    "    # These topics are all related to the management of source code.\n",
    "    0: ('Code Management', ['Git Version Control']),\n",
    "    # These topics are all related to the management of permissions and connectivity.\n",
    "    1: ('Access Management', ['Access Control', 'Remote Configuration']),\n",
    "    # These topics are all related to the management of pipelines.\n",
    "    2: ('Lifecycle Management', ['Job Scheduling', 'Pipeline Configuration', 'YAML Configuration']),\n",
    "    # These topics are all related to the management of packages and distributions.\n",
    "    3: ('Infrastructure Management', ['Conda Configuration', 'Dependency Management', 'Docker Configuration', 'Jupyter Configuration', 'Kubernetes Configuration', 'Notebook Operation', 'Package Management', 'Pip Configuration', 'VSCode Configuration', 'Workspace Configuration']),\n",
    "    # These topics are all related to the management of data and datasets.\n",
    "    4: ('Compute Management', ['Cluster Management', 'Computing Activity', 'Execution Speed', 'GPU Configuration', 'Hyperparameter Sweep', 'Hyperparameter Tuning', 'Model Training', 'PyTorch Configuration', 'Spark Configuration', 'TensorFlow Configuration', 'Training Configuration']),\n",
    "    # These topics are all related to the management of services.\n",
    "    5: ('Data Management', ['Artifact Management', 'CSV Processing', 'Data Labeling', 'Data Storage and Dataset', 'Data Transformation', 'Data Visualization', 'Database Management', 'Dataset Management', 'Directory Management', 'File Syncing', 'Online Storage and Bucket', 'Storage Management', 'Tabular Data Manipulation']),\n",
    "    # These topics are all related to the management of parallel computing resources.\n",
    "    6: ('Service Management', ['Endpoint Prediction', 'Endpoint Configuration', 'Endpoint Provisioning', 'Flask Configuration', 'Multimodel Endpoint', 'Schema Configuration', 'Web Service']),\n",
    "    # These topics are all related to the management of logs and metrics.\n",
    "    7: ('Performance Management', ['Log Management', 'Logging and Metrics', 'PyTorch and Logging', 'TensorBoard Visualization']),\n",
    "    # These topics are all related to the management of machine learning models.\n",
    "    8: ('Model Management', ['Model Compilation', 'Model Deployment', 'Model Persistence']),\n",
    "    # These topics are all related to miscellaneous software engineering steps.\n",
    "    9: ('Miscellaneous', ['Error Handling']),\n",
    "}\n",
    "\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_index_mapping = {}\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    macro_topic_index_mapping[macro_topic] = key\n",
    "    for topic in topics:\n",
    "        macro_topic_mapping[topic] = key\n",
    "\n",
    "macro_topic_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Management</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access Management</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lifecycle Management</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infrastructure Management</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compute Management</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Service Management</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Performance Management</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model Management</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Topic  Number\n",
       "Index                                   \n",
       "0                Code Management     326\n",
       "1              Access Management     538\n",
       "2           Lifecycle Management     885\n",
       "3      Infrastructure Management    1858\n",
       "4             Compute Management    2142\n",
       "5                Data Management    1938\n",
       "6             Service Management    1307\n",
       "7         Performance Management     624\n",
       "8               Model Management     504\n",
       "9                  Miscellaneous     168"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general_output, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if topic_dict[row['Challenge_topic']] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict[row['Challenge_topic']]]\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_general_output, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    entry = {\n",
    "        'Index': key,\n",
    "        'Topic': macro_topic,\n",
    "        'Number': len(df[df[\"Challenge_topic_macro\"] == key])\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "df_number.set_index('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "# for index, row in df.iterrows():\n",
    "#     if topic_dict[row['Challenge_topic']] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict[row['Challenge_topic']]]\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_general_output, 'filtered.json'))\n",
    "\n",
    "df['Challenge_type'] = 'na'\n",
    "df['Challenge_summary'] = 'na'\n",
    "df['Challenge_root_cause'] = 'na'\n",
    "df['Challenge_solution'] = 'na'\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if (row['Challenge_root_cause'] != 'na') and (row['Challenge_root_cause'] == row['Challenge_summary']):\n",
    "#         df.at[index, 'Challenge_root_cause'] = 'na'\n",
    "#     if (row['Challenge_root_cause'] != 'na') and (row['Challenge_root_cause'] == row['Challenge_solution']):\n",
    "#         print(row['Challenge_root_cause'])\n",
    "        \n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "df['Challenge_summary'] = df['Challenge_summary'].str.lower()\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# regex_digit = r\"[0-9]\"\n",
    "\n",
    "# regex_error = r\"[a-zA-Z0-9]+[eE]rror[^a-zA-Z]\"\n",
    "# regex_exception = r\"[a-zA-Z0-9]+[eE]xception[^a-zA-Z]\"\n",
    "\n",
    "# regex_error_leading = r\"[a-zA-Z0-9]+[eE]rror[a-zA-Z]+\"\n",
    "# regex_exception_leading = r\"[a-zA-Z0-9]+[eE]xception[a-zA-Z]+\"\n",
    "\n",
    "# false_positive_list = []\n",
    "\n",
    "# def camel_case_split(str):\n",
    "#     words = [[str[0].lower()]]\n",
    " \n",
    "#     for c in str[1:]:\n",
    "#         if (words[-1][-1].islower() or words[-1][-1].isdigit()) and c.isupper():\n",
    "#             words.append(list(c.lower()))\n",
    "#         else:\n",
    "#             words[-1].append(c)\n",
    "#     return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     challenge = row['Challenge_title'] + ' ' + row['Challenge_body']\n",
    "#     challenge = challenge.replace('\\n', ' ')\n",
    "#     error_list = re.findall(regex_error, challenge)\n",
    "#     if len(error_list):\n",
    "#         if row['Challenge_type'] != 'anomaly':\n",
    "#             df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "#             false_positive_list.append(row['Challenge_link'])\n",
    "#         error = max(error_list, key = len)\n",
    "#         if len(re.findall(regex_digit, error)):\n",
    "#             print(row['Challenge_title'])\n",
    "#         else:\n",
    "#             error = re.sub(r'error.+', 'error', camel_case_split(error))\n",
    "#             df.at[index, 'Challenge_summary'] = error\n",
    "#     else:\n",
    "#         exception_list = re.findall(regex_exception, challenge)\n",
    "#         if len(exception_list):\n",
    "#             if row['Challenge_type'] != 'anomaly':\n",
    "#                 df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "#                 false_positive_list.append(row['Challenge_link'])\n",
    "#             exception = max(exception_list, key = len)\n",
    "#             if len(re.findall(regex_digit, exception)):\n",
    "#                 print(row['Challenge_title'])\n",
    "#             else:\n",
    "#                 exception = re.sub(r'exception.+', 'exception', camel_case_split(exception))\n",
    "#                 df.at[index, 'Challenge_summary'] = exception\n",
    "#         else:\n",
    "#             error_list_leading = re.findall(regex_error_leading, challenge)\n",
    "#             if len(error_list_leading):\n",
    "#                 print(row['Challenge_title'])\n",
    "#             else:\n",
    "#                 exception_list_leading = re.findall(regex_exception_leading, challenge)\n",
    "#                 if len(exception_list_leading):\n",
    "#                     print(row['Challenge_title'])\n",
    "                    \n",
    "# df.to_json(os.path.join(path_special_output, 'anomaly.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker TuningStep Fails to Download Source Code\n",
      "Sagemaker endpoint fails to respond then PipeModeDataset is used\n",
      "run-notebook command not found after install sagemaker-run-notebook\n",
      "[Error] Notebook: sentiment-analysis.ipynb in SageMaker Studio with kernel Python3 (Tensorflow2 GPU Optimized)\n",
      "Question : how/when is source_dir copied into the sagemaker training instance?\n",
      "Output 'SageMakerRoleArn' not found in stack\n",
      "How to verify pytorch model inference is running on Inf1 sagemaker endpoint properly\n",
      "Sagemaker Neo Compilation for ARM64\n",
      "Loading DLR models compiled with Sagemaker Neo\n",
      "Broken link for Azure ML on https://docs.fast.ai/\n",
      "[BUG] Broken link (run_notebook_on_azureml) in readme\n",
      "ImportError: No module named 'azureml.core'\n",
      "command 'vscodeai.azureml.toolbar.submit' not found\n",
      "Command 'Azure ML: Connect to Compute Instance' resulted in an error \n",
      "import `Workspace` in Azure ML\n",
      "MLOps with Azure Machine Learning Service and Azure DevOps workshop guide is not available\n",
      "clearml.storage - ERROR - Google cloud driver not found\n",
      "I am getting a lot of errors while running dvc repro especially for fake_news module not found\n",
      "kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog\n",
      "Kedro executable not found in docker image\n",
      "MLflow support for conda-pack environments\n",
      "Test failed on `pytest -s python/tests/spark/sql/codegen/test_mlflow_registry.py::test_mlflow_model_from_model_version`\n",
      "[DOC-FIX] `model_store` different between torchserve and mlflow-torchserve\n",
      "MLflow Server EKS Service API 500\n",
      "MLflow EKS Service API 500\n",
      "No default MLFlow run to serve\n",
      "Experiment entry not found in MLFlow\n",
      "mlflow.exceptions.MlflowException: Run 'LIFFireNet' not found\n",
      "Unable to `pip` install - mlflow_tools/make_exps_page doesn't exist\n",
      "mlflow sagemaker build-and-push-container gives executor failed running exit code 127\n",
      "wandb: ERROR Error while calling W&B API: project not found (<Response [404]>) \n",
      "wandb logging does not seem to work (on Colab at least)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://api.wandb.ai/graphql\n",
      "Error with wandb\n",
      "wandb error\n",
      "get_wandb_logger fails to retrieve WandbLogger when debug=True\n",
      "Reading Mp3 files from S3 to Sagemaker for feature extraction using LIBROSA\n",
      "Invoice parser \"invents\" non-existing string for supplier_name normalized value\n",
      "Error 404: AciDeploymentFailed\n",
      "How to get root access in SageMaker Studio Lab\n",
      "When pretrained model is not found, sagemaker falls into an infinite silent loop\n",
      "Sagemaker notebooks raise error for `pandas.CSVDataSet`\n",
      "[Bug] study fail to mount in SWB 5.2.6 SageMaker Jupyter Notebook\n",
      "[BUG] Error in some of the AzureML tests\n",
      "error when installing AZURE ML training model piece\n",
      "Fix the definition of pipelines/sentence_embedding/dvc.yaml\n",
      "dvc servername and url not found by calling \"dvc-cc run\"\n",
      "Can not create model in MLflowCatalog\n",
      "dbx deploy fails due to mlflow experiment not found\n",
      "[BUG]: Unable to Start DFP Production MLFlow Server\n",
      "running mlflow>1.28 projects causes mlflow not found error\n",
      "[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error\n",
      "MLFlow and Hydra causing crash when used together\n",
      "Programmatically enable installed extensions in Vertex AI Managed Notebook instance\n",
      "Install Python Packages in Azure ML?\n",
      "Broken DAG: urllib3 1.25.3 (/home/ubuntu/.local/lib/python3.7/site-packages), Requirement.parse('urllib3<1.25,>=1.21'), {'sagemaker'}\n",
      "Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?\n",
      "GCP AI Platform Vertex endpoint model undeploy : 404 The DeployedModel with ID `2367889687867` is missing\n",
      "Is there a way to pass arguments to our own docker container in sagemaker?\n",
      "Kedro can not find SQL Server table\n",
      "No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode\n",
      "sudo: not found on AWS Sagemaker Studio\n",
      "Using Sacred Module with iPython\n",
      "Azure ML endpoint 404 error\n",
      "Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)\n",
      "Error Tracking in Amazon SageMaker\n",
      "Error in connecting Azure SQL database from Azure Machine Learning Service using python\n",
      "In GCP Vertex AI, why is Delete Training Pipeline REST endpoint unimplemented?\n",
      "Trying to work on R using Azure ML Studio Notebook and facing challenges with ODBC package\n",
      "Problem trying to authenticate with bearer token on nginx + oauth2-proxy + docker\n",
      "Failed ping healthcheck after deploying TF2.1 model with TF-serving-container on AWS Sagemaker\n",
      "Pycaret MlFlow authentication\n",
      "AWS SageMaker Studio Lab - Permission denied\n",
      "mlflow static_prefix url in set_tracking_uri is not working\n",
      "azureml how to deploy docker image to webservice\n",
      "i am getting error when deploying machine learning model in aci\n",
      "Model.get_model_path(model_name=\"model\") throws an error: Model not found in cache or in root at\n",
      "How do you use pyodbc in Azure Machine Learning Workbench\n",
      "Mlflow download_artifacts giving Not Found error\n",
      "Sagemaker deploy model with inference code and requirements\n",
      "\"Entry Point Not Found\" Error LightGBM R package in Azure\n",
      "How to save parquet in S3 from AWS SageMaker?\n",
      "Why can't I access Output from Vertex pipeline kfp component?\n",
      "How to deploy a detectron2 model using file in azureML\n",
      "install python package in Azure ml\n",
      "Automatically Install OpenJDK into SageMaker Notebook\n",
      "jsonschema 4.4.0 does not provide the extra 'isoduration'\n",
      "Readding missing files to DVC\n",
      "Kubeflow vs Vertex AI Pipelines\n",
      "Sagemaker batch transform \"ValueError: could not convert string to float\"\n",
      "How to mock an S3AFileSystem locally for testing spark.read.csv with pytest?\n",
      "BERT model loading not working with pytorch 1.3.1-eia container\n",
      "Sagemaker directory opt/ml/models does not store models to load them for inference\n",
      "Google.Cloud.AIPlatform.V1 Received http2 header with status: 404\n",
      "SageMaker deploy error \"serve\" executable file not found in $PATH\n",
      "Ensure Java is installed and PATH is set for `java` in Amazon SageMaker Jupyter Notebook\n",
      "Wandb line plots only show bar charts after refresh\n",
      "pyodbc not working in web-service container, Azure Model Management\n",
      "mlflow / app engine error code 405 method not allowed, when using remote tracking server\n",
      "MLFlow - running \"mlflow ui\" throwing file not found error on windows 10\n",
      "Sagemaker: Specifying custom entry point gives not found error\n",
      "Job submittal fails with : CondaHTTPError: HTTP 000 CONNECTION FAILED\n",
      "Import azure.core not found issue in running Notebook through MachineLearningStudio\n",
      "Installing additional R package (ImputeTS R Package) in Azure ML\n",
      "How should Pubsub, acting a log sink, fire a function without sending the log?\n",
      "AWS Sagemaker : No response back from the endpoint \"HTTP 301 293\"\n",
      "How to specify a name for the output file of a SageMaker Batch Transform job?\n",
      "mlflow Exception: Run with UUID is already active\n",
      "How to serve daily precomputed predictions in aws sagemaker?\n",
      "Still on ML-Flow installation in R Studio\n",
      "Vertex Pipeline Metric values not being added to metrics artifact?\n",
      "Automatic hyperparameter tuning in Sagemaker aws failed to run\n",
      "Set up health check for Sagemaker endpoint in Postman\n",
      "Deploying Huggingface model for inference - pytorch-scatter issues\n",
      "Azure ML Internal Server Error and 404 Error\n",
      "How to connect AzureML (Machine Learning) with AzureVM (Virtual Machine)?\n",
      "Sagemaker AlgorithmError: ExecuteUserScriptError:\n",
      "SagemakerTraining job catboost-classification-model , ErrorMessage \"TypeError: Cannot convert 'xxx'' to float\n",
      "Unable to access to mlflow ui\n",
      "Trying to query Azure SQL Database with Azure ML / Docker Image\n",
      "IDtoken retrieval in Vertex AI pipeline fails randomly\n",
      "No such file or directory: '/opt/ml/input/data/test/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN]\n",
      "Can't access mounted Dataset on Azure Machine Learning Service Notebook\n",
      "PartitionedDataSet not found when Kedro pipeline is run in Docker\n",
      "SageMaker Studio Image - pip not found and no python 3 in terminal for Python 3 notebook instance\n",
      "sagmaker deploy() model gives error exec: \"serve\": executable file not found in $PATH\n",
      "How to use AWS Sagemaker XGBoost framework?\n",
      "How to delete a run_id from MLflow\n",
      "How to Set Java Home for Notebook in SageMaker\n",
      "Using ipyleaflet within a Vertex AI Managed Notebook running on a Docker image\n",
      "ERROR:root:Line magic function `%azureml` not found?\n",
      "Store scaler with mlflow keras-model\n",
      "MLflow saves models to relative place instead of tracking_uri\n",
      "How to observe and control how sagemaker multimodel server loads models in memory\n",
      "Read MobileNetSSd model files from azure ML Registered Models\n",
      "xgboost model prediction error : Input numpy.ndarray must be 2 dimensional\n",
      "How to load data from your S3 bucket to Sagemaker jupyter notebook to train the model?\n",
      "\"list index out of range\" error in AzureML inference schema\n",
      "Cannot use tensorboard with Vertex AI Custom job\n",
      "502 bad gateway error with upstream prematurely closed connection?\n",
      "AWS SageMaker models popping in and out of the vacuum\n",
      "Azure ML model deployment fail: Module not found error\n",
      "Sagemaker Batch Transform Error \"Model container failed to respond to ping; Ensure /ping endpoint is implemented and responds with an HTTP 200 status\"\n",
      "AzureML Model.profile() timeout without ever running the model\n",
      "Kedro 0.16.3 and kedro[spark.SparkDataSet] pip libraries cannot be installed together on databricks cluster\n",
      "%run works only once in Jupyter Notebook\n",
      "Amazon sagemaker Lifecycle configuration not working\n",
      "Azure Machine Learning: Remove pre-installed R packages\n",
      "How best to install dependencies in a Sagemaker PySpark cluster\n",
      "TemplatedConfigLoader in register_config_loader not replacing patterns in catalog.yml (kedro)\n",
      "Wandb throws Permission denied error although I am logged in\n",
      "Vertex AI Pipelines (Kubeflow) skip step with dependent outputs on later step\n",
      "How to upload .r file into azure ml studio and run it?\n",
      "No GPU detected on AWS SageMaker pytorch-1.8-gpu-py36 instance\n",
      "Azure ML Studio cannot load a installed package in R\n",
      "Can't connect to Azure ML Web Service in Azure Data Factory\n",
      "Load/access/mount directory to aws sagemaker from S3\n",
      "Cannot use `gcloud auth print-identity-token` from within Vertex AI Custom Job\n",
      "Mlflow not running on machine\n",
      "sagemaker batch transform breaks with upstream prematurely closed connection while reading upstream\n",
      "How can I use GPUs on Azure ML with a NVIDIA CUDA custom docker base image?\n",
      "AWS Sagemaker InvokeEndpoint: operation: Endpoint of account not found\n",
      "Converting PDF and PPTX files drawn from S3 into a JPG format\n",
      "Model pkl not found by SageMaker inference\n",
      "Vertex AI - Deployment failed\n",
      "Missing delimiter error when importing html text\n",
      "Installing private python wheel from a storage account\n",
      "Error loading model from mlflow: java.io.StreamCorruptedException: invalid type code: 00\n",
      "How to submit local jobs with dsl.pipeline\n",
      "Cannot import librosa on SageMaker Jupyter notebook instance \"OSError: sndfile library not found\"\n",
      "SageMaker Batch Transform fails to access nginx\n",
      "Getting this weird error when trying to run DVC pull\n",
      "AWS Sagemaker Spark S3 access issue\n",
      "Error in running Azure Data Factory Pipeline. Linked Service Reference not found\n",
      "AWS Lambda send image file to Amazon Sagemaker\n",
      "How do I setup the _SERVER_MODEL_PATH variable?\n",
      "In Azure ML - After Web service deployment, getting column name not found error\n",
      "how to use kedro.versioning in latest version of kedro?\n",
      "Azure ML R Model Train & Score Web Service\n",
      "Sagemaker suddenly unable to install python packages, missing python-dev\n",
      "How to deploy multiple ml models with scoring file using azure ml cli\n",
      "MLflow 1.2.0 define MLproject file\n",
      "Unable to connect Mlflow server to my mlflow project image\n",
      "AzureML schema \"list index out of range\" error\n",
      "Can't find scoring.py when using PythonScriptStep() in Databricks\n",
      "Facing this error : container_linux.go:367: starting container process caused: exec: \"python\": executable file not found in $PATH: unknown\n",
      "Creating a dataframe in Azure ML Notebook with R kernel\n",
      "Azure: importing not already existing packages in 'src'\n",
      "Running mlflow as a systemd service - gunicorn not found\n",
      "AWS CreateDeviceFleet operation fail because \"the account id does not have ownership on bucket\"\n",
      "Blankspace and colon not found in firstline\n",
      "Uniquely identify instances of VMs (Azure ML - web services)\n",
      "sagemaker notebook instance Elastic Inference tensorflow model local deployment\n",
      "Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench\n",
      "How to avoid error \"conda --version: conda not found\" in az ml run --submit-script command?\n",
      "Is it possible to \"apt install\" in SageMaker Studio Lab?\n",
      "AWS Sagemaker + AWS Lambda\n",
      "Where to perform the saving of an nodeoutput in Kedro?\n",
      "Installing additional R Package on Azure ML\n",
      "Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error\n",
      "Amazon SageMaker: ClientError: Data download failed:NoSuchKey (404): The specified key does not exist\n",
      "Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '\n",
      "Jobs-Cloud Scheduler (Google Cloud) fails to run scheduled pipelines\n",
      "kedro context and catalog missing from ipython session\n",
      "How to install OpenJDK library?\n",
      "how to set path of bucket in amazonsagemaker jupyter notebook?\n",
      "Adding a {serve} metagraph to existing Tensorflow model\n",
      "mlflow serving r models failed if use LDA instead of linear regression\n",
      "Azure ML Online Endpoint deployment DriverFileNotFound Error\n",
      "Errors while using sagemaker api to invoke endpoints\n",
      "How combine results from multiple models in Google Vertex AI?\n",
      "How to explicitly set sagemaker autopilot's validation set?\n",
      "Gluonnlp installation not found on Sagemaker jupyter notebook\n",
      "Compare String with list of strings in bash\n",
      "AzureML Environment for Inference : can't add pip packages to dependencies\n",
      "SQLAlchemy Oracle - InvalidRequestError: could not retrieve isolation level\n",
      "how SageMaker to access s3 bucket data\n",
      "Failed to pull existing files from SSH DVC Remote\n",
      "'azureml.logging' module not found\n",
      "Vertex AI custom prediction vs Google Kubernetes Engine\n",
      "Reading File from Vertex AI and Google Cloud Storage\n",
      "cloud 9 and sagemaker - hyper parameter optimisation\n",
      "AWS SageMaker S3 os.listdir() Access denied\n",
      "Docker image not found during local deployment (\"no such image\")\n",
      "How do you write lifecycle configurations for SageMaker on windows?\n",
      "What is the name of the driver to connect to Azure SQL Database from pyodbc in Azure ML?\n",
      "Is there a way to un-register an environment in Azure ML studio\n",
      "Azure-ML Deployment does NOT see AzureML Environment (wrong version number)\n",
      "How can we get the pipeline to read columns with special characters?\n",
      "how create azure machine learning scoring image using local package\n",
      "Simple but frustrating error: Google.cloud module not found\n",
      "Custom container image not found by Vertex AI for model upload\n",
      "Sweep creation down? (Resolved)\n",
      "404 response executing GraphQL\n",
      "Agent bug? File not found error\n",
      "404 error running sweep from local jupyter nb\n",
      "I think that W&B is having some connection issues since yesterday night\n",
      "ModuleNotFoundError: No module named 'nvgpu' in sagemaker batch transform\n",
      "install java dependency when building my own processing container\n",
      "Java not found when running Sagemaker Studio python notebooks\n",
      "Batch transform step not working\n",
      "Sagemaker Neo compilation fails\n",
      "NVMLError_FunctionNotFound: I was trying to deploy a PyTorch model in a ml.g4dn.xlarge instance\n",
      "Unable to select a compute type in SageMaker Studio Lab\n",
      "Sagemaker Batch Transform - \"upstream prematurely closed connection\" - Unable to serve requests that take longer than 30 minutes\n",
      "Issues with exposing SKLearn model as endpoint on AWS Sagemaker\n",
      "Error Creating Endpoint\n",
      "SQL Server driver issue on notebook instance running on AWS SageMaker\n",
      "No such file or directory: '/opt/ml/input/data/test/revenue_train.csv' Sagemaker [SM_CHANNEL_TRAIN]\n",
      "Error for Training job catboost-classification-model , ErrorMessage \"TypeError: Cannot convert 'xxx'' to float\n",
      "Sagemaker Endpoint is not created when deploying HuggingFace Model using it.\n",
      "Export Autopilot model to GovCloud region\n",
      "passing a numpy array to predict_fn when making inference for xgboost model\n",
      "Failed ping healthcheck after deploying TF2.1 model with TF-serving-contain\n",
      "Custom packages in Sagemaker studio\n",
      "Trouble deploying SageMaker trained model in DeepLens\n",
      "ERROR: unexpected error - [Errno 2] No such file or directory:\n",
      "Dvc remote drive\n",
      "Cannot apply the first exp after new commit\n",
      "I have added my google drive as a remote storage\n",
      "DVC list shows libssl-so.1.1.1k not found\n",
      "Problem with top-level plot definitions\n",
      "SSH remote: unexpected error - Permission denied\n",
      "What to do if the file is not found in the repo?\n",
      "Track remote data on Azure\n",
      "Can't go to former version of dataset with `dvc checkout`\n",
      "Documentation: tutorial problem?\n",
      "How to save or log pytorch model using MLflow?\n",
      "Local run in azureml sdk2\n",
      "Missing ODBC drivers in Azure ML Compute Instances\n",
      "Bing Search API not working\n",
      "How to fix Apply Transformation error?\n",
      "how to modify the template script on azure auto ml?\n",
      "While running an Azure ML Experiement, I get \"File Not found\" error when attempting to find ODBC driver for python pyodbc.connect command\n",
      "ClusterIdentityNotFound when submitting experiment.\n",
      "Using Azure ML Studio Designer with R script: package not found but I installed it on the compute instance\n",
      "Azure ML Studio can't generate job when datastore is used as input; failed string validation? Also, code files not being loaded on job.\n",
      "How to  give Source Directory on the step pipeline in Azure Machine Learning\n",
      "Azure machine learning samples 404\n",
      "Error 404 for design pipelines\n",
      "Pickle Load- File Not Found when deploying using Azure ML Studio\n",
      "azure machine learning SubscriptionNotFound\n",
      "Swagger file not present -- Azure Machine Learning\n",
      "Azureml compute instance spark dependencies missing\n",
      "Local compute not found error when running a hyperparameter search\n",
      "Multiple new errors when deploying to ACI webservice\n",
      "After Web service deployment, getting column name not found error\n",
      "Module Not Found Error when launching parameter study\n",
      "How to import Microsoft.RelInfra.Common.Exception so that it could be properly handled?\n",
      "The azure cli command \"az ml attach folder\" is directly adding .azureml directory to .amlignore , so where to put config.json when using Azure devops pipeline to submit script to aml workspace?\n",
      "Failure when submitting pipe line\n",
      "Issues about Azure Machine Learning Studio\n",
      "How to use a working pipeline on live dataset?\n",
      "Getting 500 errors after model deployment\n",
      "Azure-cli-ml Version: '1.33.0', 'Error': WebserviceException. Can't deploy model into ACI\n",
      "Machine Learning CI Pipeline: Submitting an experiment failed\n",
      "How to use a registred model in a python script(in Azure) ?\n",
      "i am getting error when deploying machine learning model in aci\n",
      "i am getting error when deploying machine learning model in aci\n",
      "Endopint not consumable after successful model deployment to azure instance container (machine learning studio - designer)\n",
      "Cannot use GPU on Azure Notebooks in Azure Machine Learning Studio\n",
      "Deploying spark-nlp model using custom docker image fails in Azure Machine Learning\n",
      "Import Data Error - DocumentDb library exception: DocumentDB client threw an exception . ( Error 1000 ) Using Machine learning studio (classic)\n",
      "azureml when deployment fails from local source directory\n",
      "Module not found when custom python package installed via shell script\n",
      "Registered AzureML Model from a NotebookVM can not be found\n",
      "Isn't Interactive login, default for Workspace.from_config()?\n",
      "Investigating AML workspace images crashes due to already deleted model\n",
      "Rest api to create or update azure ML workspace doesn't create dependant resources\n",
      "Model file is not found for Registration of model in training Pipeline.\n",
      "Opening source file causes File not found exception\n",
      "Guild view not working in jupyterhub\n",
      "Local run vs remote run dependencies\n",
      "Tensorboard FileNot found error on Windows-10, guild-0.7.3.dev1\n",
      "Guild runs on remote not found\n",
      "sqlite3.OperationalError: disk I/O error when using the scratch drive on Linux cluster for storage of guild runs\n",
      "MLflow-Docker Artifacts Model Not Found\n",
      "Model Deployment Issues\n",
      "Deploy problems\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "regex_digit = r\"[0-9]\"\n",
    "\n",
    "regex_error = r\"[a-zA-Z0-9]+[eE]rror[^a-zA-Z]\"\n",
    "regex_exception = r\"[a-zA-Z0-9]+[eE]xception[^a-zA-Z]\"\n",
    "\n",
    "regex_error_leading = r\"[a-zA-Z0-9]+[eE]rror[a-zA-Z]+\"\n",
    "regex_exception_leading = r\"[a-zA-Z0-9]+[eE]xception[a-zA-Z]+\"\n",
    "\n",
    "def camel_case_split(str):\n",
    "    words = [[str[0].lower()]]\n",
    " \n",
    "    for c in str[1:]:\n",
    "        if (words[-1][-1].islower() or words[-1][-1].isdigit()) and c.isupper():\n",
    "            words.append(list(c.lower()))\n",
    "        else:\n",
    "            words[-1].append(c)\n",
    "    return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "titles = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_title'] in titles:\n",
    "        continue\n",
    "    challenge = row['Challenge_title'] + ' ' + row['Challenge_body']\n",
    "    challenge = challenge.replace('\\n', ' ').lower()\n",
    "    if (' 403 ' in challenge) or ('[403]' in challenge) or ('(403)' in challenge) or (' 403,' in challenge) or ('forbidden' in challenge):\n",
    "        pass\n",
    "        # print(row['Challenge_title'])\n",
    "        # df.at[index, 'Challenge_type'] = 'anomaly'\n",
    "        # df.at[index, 'Challenge_summary'] = 'forbidden error'\n",
    "    elif (' 404 ' in challenge) or ('[404]' in challenge) or ('(404)' in challenge) or (' 404,' in challenge) or ('not found' in challenge):\n",
    "        print(row['Challenge_title'])\n",
    "        \n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# for i,r in df.iterrows():\n",
    "#     if r['Challenge_type'] == 'inquiry':\n",
    "#         df.at[i, 'Challenge_summary'] = 'na'\n",
    "#         df.at[i, 'Challenge_root_cause'] = 'na'\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels+.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df = pd.concat([df_new, df_old], ignore_index=True)\n",
    "# df = df.drop_duplicates(['Challenge_link'], keep=False)\n",
    "\n",
    "# df = df[df['Challenge_type'].isna()]\n",
    "# df.to_json(os.path.join(path_special_output, 'extra.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels++.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_special_output, 'extra.json'))\n",
    "\n",
    "# df = pd.concat([df_old, df_new], ignore_index=True)\n",
    "# df = df.drop_duplicates(['Challenge_link'])\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'extra+.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.read_json(os.path.join(path_special_output, 'labels+.json'))\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# df_new[~((df1.City.isin(df2.City)) & (df1.State.isin(df2.State)))] \n",
    "\n",
    "# for index, row in df_new.iterrows():\n",
    "#     for i2,r2 in df.iterrows():\n",
    "#         if r2['Challenge_type'] == 'na':\n",
    "#             continue\n",
    "#         if r2['Challenge_link'] == row['Challenge_link']:\n",
    "#             df_new.at[index, 'Challenge_type'] = r2['Challenge_type']\n",
    "#             df_new.at[index, 'Challenge_summary'] = r2['Challenge_summary']\n",
    "#             df_new.at[index, 'Challenge_root_cause'] = r2['Challenge_root_cause']\n",
    "#             df_new.at[index, 'Challenge_solution'] = r2['Challenge_solution']\n",
    "#             break\n",
    "            \n",
    "# df_new.to_json(os.path.join(path_special_output, 'labels++.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Import and Module Issues - Problems related to importing modules or libraries and errors associated with these processes.\n",
      "Topic 1: Range of Errors - Various types of errors that are worrisome, inconsistent, unrecognized, confusing, or unexpected.\n",
      "Topic 2: Performance Issues - Problems related to slow execution, processing, and performance degradation.\n",
      "Topic 3: Connection Problems - Issues related to connection resets, bad gateways, misconfigured connections, and other connection errors.\n",
      "Topic 4: Project Modification Issues - Problems related to modifying, deleting, or coding projects and issues with files within these projects.\n",
      "Topic 5: Access Issues - Problems related to denied, forbidden, or unauthorized access.\n",
      "Topic 6: Installation Issues - Errors and issues related to software installation.\n",
      "Topic 7: Input Issues - Problems related to invalid, incorrect, unsupported, or unmatched inputs.\n",
      "Topic 8: Version Incompatibility - Issues related to incompatible, outdated, invalid, or conflicting software versions.\n",
      "Topic 9: Experiment Errors - Problems related to invalid, incorrect, failed, or unexpected experiments.\n",
      "Topic 10: File Availability Issues - Problems related to unavailable, missing, inaccessible, or ignored files.\n",
      "Topic 11: Artifact Issues - Problems related to unavailable, unlisted, invalid, incomplete, or broken artifacts.\n",
      "Topic 12: Request Issues - Problems related to bad, invalid, malformed, unavailable, or forbidden requests.\n",
      "Topic 13: Application Unresponsiveness - Issues related to unresponsive applications, processes, executions, or user interfaces.\n",
      "Topic 14: Deployment Errors - Problems related to unsuccessful, failed, invalid, or unavailable deployments.\n",
      "Topic 15: Dataset Availability Issues - Problems related to unavailable, inaccessible, unrecognized, unsupported, or invalid datasets.\n",
      "Topic 16: Configuration Issues - Problems related to unavailable, missing, incomplete, broken, or invalid configurations.\n",
      "Topic 17: Connection Timeout Issues - Problems related to connection timeouts, gateway timeouts, and service timeouts.\n",
      "Topic 18: Training Issues - Problems related to denied, failed, unresponsive, or invalid training jobs.\n",
      "Topic 19: Graph Issues - Problems related to unrendered, mismatched, large, blank, or inaccurate graphs and charts.\n",
      "Topic 20: Endpoint Issues - Problems related to unavailable, inaccessible, failed, invalid, or unimplemented endpoints.\n",
      "Topic 21: Command Issues - Problems related to unrecognized, missing, invalid, or outdated commands.\n",
      "Topic 22: Sweep Issues - Problems related to repeating, incorrect, or invalid sweeps and missing or skipped keys.\n",
      "Topic 23: Run Issues - Problems related to nested, restricted, grouped, repeated, indefinite, or unavailable runs.\n",
      "Topic 24: Package Issues - Problems related to package errors, invalid packages, incompatible packages, or issues with package installation.\n",
      "Topic 25: Permission Issues - Problems related to denied, insufficient, or forbidden permissions.\n",
      "Topic 26: Loading Errors - Problems related to loading errors, invalid loads, loading exceptions, or failed data loads.\n",
      "Topic 27: GPU Availability Issues - Problems related to unavailable, missing, unused, inactive, unrecognized, or unresponsive GPUs.\n",
      "Topic 28: Feature Inactivity - Problems related to inactive features, options, buttons, or unavailable features.\n",
      "Topic 29: Import/Export Errors - Problems related to errors in importing or exporting data.\n",
      "Topic 30: Column Issues - Problems related to unrecognized, unsupported, invalid, unavailable, unexpected, or improper columns.\n",
      "Topic 31: Metric Availability Issues - Problems related to unavailable, invisible, unlogged, unrecorded, or empty metrics.\n",
      "Topic 32: Notebook Unresponsiveness - Issues related to unresponsive, inactive, uninterrupted, or automated notebooks.\n",
      "Topic 33: Pipeline Issues - Problems related to pipeline errors, invalid pipelines, failed pipelines, or incorrect pipelines.\n",
      "Topic 34: Model Availability Issues - Problems related to unavailable, existing, lost, unsupported, or invalid models.\n",
      "Topic 35: Prediction Inconsistencies - Problems related to different, inconsistent, incorrect, invalid, or mismatching predictions.\n",
      "Topic 36: Training Errors - Problems related to training errors, invalid training, unsupported training, broken training, or unknown training.\n",
      "Topic 37: Label Issues - Problems related to unavailable, inaccessible, invisible, unpopulated, or invalid labels.\n",
      "Topic 38: Environment Issues - Problems related to missing, unavailable, invalid, or unconfigurable environments.\n",
      "Topic 39: Docker Issues - Problems related to building Docker images, Dockerfile errors, or Docker creation.\n",
      "Topic 40: Argument Issues - Problems related to unexpected, invalid, unused, unknown, or incompatible arguments.\n",
      "Topic 41: Server Errors - Problems related to server errors, internal errors, hosting errors, client errors, or unknown errors.\n",
      "Topic 42: Import and Module Issues - Problems related to importing modules and errors associated with these processes.\n",
      "Topic 43: Directory Issues - Problems related to unavailable, unrecognized, invalid, invisible, or incorrect directories.\n",
      "Topic 44: Media Issues - Problems related to unsupported, invalid, unexpected, unavailable, or error-prone media types.\n",
      "Topic 45: Serialization Issues - Problems related to unserializable, non-serializable, or serializable objects.\n",
      "Topic 46: Memory Issues - Problems related to memory overflow, overload, limit, leaks, or exceptions.\n",
      "Topic 47: Module Issues - Problems related to missing, unavailable, unexpected, or removed modules.\n",
      "Topic 48: Attribute Issues - Problems related to invalid, unrecognized, unexpected, unsupported, or incorrect attributes.\n",
      "Topic 49: Logging Inconsistencies - Problems related to inconsistent, malfunctioning, skipped, unpicklable, or incomplete logging.\n",
      "Topic 50: Data Availability Issues - Problems related to unavailable, malfunctioning, insufficient, inaccessible, incomplete, or disappearing data.\n",
      "Topic 51: Model Issues - Problems related to invalid, unrecognized, incompatible, unsupported, or incorrect models.\n",
      "Topic 52: Batch Issues - Problems related to batch errors, invalid batches, or batch transformations.\n",
      "Topic 53: Limit Exceedance - Problems related to exceeded size, limit, file, or exhausted errors.\n",
      "Topic 54: Login Issues - Problems related to interactive, disruptive, persistent, unexpected logins, or session errors.\n",
      "Topic 55: Filesystem Issues - Problems related to read-only filesystems, unavailable filesystems, write errors, or saving read issues.\n",
      "Topic 56: Pipeline Issues - Problems related to unavailable, unlogged, incomplete, unsuccessful, or unlabeled pipelines.\n",
      "Topic 57: Kernel Issues - Problems related to unstable, broken, crashed, invalid, or dying kernels.\n",
      "Topic 58: Run ID Issues - Problems related to invalid, unavailable, or unterminated run instances.\n",
      "Topic 59: Ping Issues - Problems related to failed, unresponsive pings, unavailable IPs, unresolved IPs, or unavailable hosts.\n",
      "Topic 60: Service Issues - Problems related to inaccessible, broken, incorrect, unknown, unavailable, or forbidden services.\n",
      "Topic 61: Model Loading Issues - Problems related to loading models, load errors, model errors, invalid models, or missing model functions.\n",
      "Topic 62: Upload Issues - Problems related to upload errors, insufficient uploads, denied uploads, or confused uploads.\n",
      "Topic 63: Training Speed Issues - Problems related to slow, empty, invalid, incomplete, delayed, or unsuccessful training.\n",
      "Topic 64: Job Issues - Problems related to job errors, scheduled errors, task errors, exceeded jobs, or job configurations.\n",
      "Topic 65: Domain Issues - Problems related to domain creation, existing domains, unauthorized domains, unsupported domains, or invisible domains.\n",
      "Topic 66: Model Deployment Issues - Problems related to deploying models, deployment errors, or unavailable deployed models.\n",
      "Topic 67: Endpoint Issues - Problems related to endpoint errors, invocations, invalid endpoints, or endpoint launches.\n",
      "Topic 68: Instance Issues - Problems related to invalid, failed, unterminated, or created instances.\n",
      "Topic 69: Dependency Conflicts - Problems related to conflicting, incompatible, or failed dependencies.\n",
      "Topic 70: Undefined Symbols - Problems related to undefined symbols, variables, functions, or names.\n",
      "Topic 71: Output Issues - Problems related to unavailable, disappearing, missing, ineffective, blocked, or unseen outputs.\n",
      "Topic 72: Deployment Unresponsiveness - Problems related to unresponsive, stalled, timed out, slow, unhealthy, or pending deployments.\n",
      "Topic 73: Conversion Errors - Problems related to conversion errors, float conversions, string conversions, or unsupported conversions.\n",
      "Topic 74: Model Saving Issues - Problems related to unsaved, unlocated, unsaveable, deleted, or inaccessible models.\n",
      "Topic 75: Requirement Issues - Problems related to unsatisfied requirements, constraints, unmet conditions, requirement conflicts, or misinterpreted requirements.\n",
      "Topic 76: Disk Space Issues - Problems related to insufficient disk space, full disks, insufficient storage, or disk errors.\n",
      "Topic 77: Access Restrictions - Problems related to restricted access, invite restrictions, restricted teams, versions, or environments.\n",
      "Topic 78: Credential Issues - Problems related to unavailable, invalid, incorrect, or unauthorized credentials.\n",
      "Topic 79: Role Issues - Problems related to unauthorized, invalid, unavailable, or incompatible roles.\n",
      "Topic 80: Syntax Issues - Problems related to invalid, incorrect, unsupported syntax, or parsing errors.\n",
      "Topic 81: Function Issues - Problems related to unimplemented, unavailable, unrecognized, undetected, incorrect, or unsupported functions.\n",
      "Topic 82: Operation Issues - Problems related to unsupported operations, errors, executions, schemes, operands, values, or functions.\n",
      "Topic 83: Parameter Issues - Problems related to invalid, unrecognized, disallowed, or incorrect parameters.\n",
      "Topic 84: Link Issues - Problems related to broken, unlinked, malfunctioning, unidentified, unclickable, or linking errors.\n",
      "Topic 85: Image Availability Issues - Problems related to unavailable, inaccessible, unattachable, invisible, or unpulled images.\n",
      "Topic 86: Attribute Recognition Issues - Problems related to unrecognized or nonexistent attributes.\n",
      "Topic 87: Transfer Errors - Problems related to errors in copying, transporting, cloning, or moving files.\n",
      "Topic 88: Path Issues - Problems related to incorrect, unrecognized, invalid, or unavailable paths.\n",
      "Topic 89: Token Issues - Problems related to invalid, missing, unavailable, unresolved, ignored, or unauthorized tokens.\n",
      "Topic 90: Log Availability Issues - Problems related to unavailable, missing, empty, or unresponsive logs.\n",
      "Topic 91: Encoding Issues - Problems related to invalid, unexpected, incorrectly encoded, or mismatched encodings.\n",
      "Topic 92: Authorization Issues - Problems related to pending, unauthorized, invalid, or error-prone authorizations.\n",
      "Topic 93: Result Inconsistencies - Problems related to inconsistent, incorrect, discrepant, different, misleading, or different results.\n",
      "Topic 94: Output Issues - Problems related to overlapping, duplicated, single, different, or consistent outputs.\n",
      "Topic 95: Null and Empty Issues - Problems related to null descriptions, empty data, objects, datasets, attributes, values, evaluations, or tasks.\n",
      "Topic 96: Tensorboard Issues - Problems related to unrecognized, unavailable, consuming, discrepant, incompatible, undesired, or inaccessible Tensorboards.\n",
      "Topic 97: Parameter Logging Issues - Problems related to unlogged, unadded, empty, unavailable, missing, or inaccessible parameters.\n",
      "Topic 98: Quota Exceedance - Problems related to exceeded, exhausted, insufficient, unlisted, or resource-exceeded quotas.\n",
      "Topic 99: Time and Date Issues - Problems related to incorrect epochs, times, datetimes, time precisions, erroneous dates, unstable timestamps, or time scales.\n",
      "Topic 100: Package Uninstallation Issues - Problems related to uninstallable, uninstalled, or error-prone packages or drivers.\n",
      "Topic 101: Download Errors - Problems related to download errors, failures, troubles, broken downloads, or unavailable downloads.\n",
      "Topic 102: Image Issues - Problems related to image errors, invalid images, unsupported images, broken images, or image creation.\n",
      "Topic 103: Compilation Issues - Problems related to compilation errors, fatal compilations, uncompiled, broken compiles, build errors, or incomplete compilations.\n",
      "Topic 104: Push Issues - Problems related to push errors, unsupported pushes, unexpected pushes, denied pushes, or unresponsive pushes.\n",
      "Topic 105: Loop and Backoff Issues - Problems related to loop backoffs, loop errors, retries loops, stop errors, or exceeded backoffs.\n",
      "Topic 106: Authentication Issues - Problems related to authentication errors, failed authentications, password errors, unexpected authentications, or proxy errors.\n",
      "Topic 107: Execution Issues - Problems related to execution errors, script errors, incompatible scripts, execute errors, unexecuted scripts, or failed executions.\n",
      "Topic 108: Initialization Issues - Problems related to initialization errors, invalid initializations, incorrect initializations, scoring initializations, inconsistent initializations, or incomplete initializations.\n",
      "Topic 109: Resource Issues - Problems related to insufficient, unavailable, exhausted, shortage, ineffective, or underutilized resources.\n",
      "Topic 110: Version Issues - Problems related to Python versions, outdated PyTorch, unsupported Python, Python upgrades, Python incompatibility, deprecated Python, or unavailable Python.\n",
      "Topic 111: Bucket Issues - Problems related to invalid, inaccessible, undefined, incorrect, or missing buckets.\n",
      "Topic 112: Hyperparameter Issues - Problems related to invalid, unsupported, unavailable, malfunctioning, incorrect, or tuning hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_anomaly, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_root_cause, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_solution, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_result_rq1, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different metrics to each post\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['Solution_word_count'] = np.nan\n",
    "df['Solution_sentence_count'] = np.nan\n",
    "df['Solution_readability'] = np.nan\n",
    "df['Solution_reading_time'] = np.nan\n",
    "df['Solution_link_count'] = np.nan\n",
    "\n",
    "df['Challenge_solved_time'] = np.nan\n",
    "df['Challenge_adjusted_solved_time'] = np.nan\n",
    "\n",
    "df['Solution_link_docs'] = np.nan\n",
    "df['Solution_link_issues'] = np.nan\n",
    "df['Solution_link_patches'] = np.nan\n",
    "df['Solution_link_tools'] = np.nan\n",
    "df['Solution_link_tutorials'] = np.nan\n",
    "df['Solution_link_examples'] = np.nan\n",
    "\n",
    "df['Challenge_created_time'] = pd.to_datetime(df['Challenge_created_time'])\n",
    "df['Challenge_closed_time'] = pd.to_datetime(df['Challenge_closed_time'])\n",
    "df['Challenge_last_edit_time'] = pd.to_datetime(df['Challenge_last_edit_time'])\n",
    "df['Solution_last_edit_time'] = pd.to_datetime(df['Solution_last_edit_time'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    challenge_content = row['Challenge_title'] + '.' + str(row['Challenge_body'])\n",
    "    df.at[index, 'Challenge_word_count'] = textstat.lexicon_count(challenge_content)\n",
    "    df.at[index, 'Challenge_sentence_count'] = textstat.sentence_count(challenge_content)\n",
    "    df.at[index, 'Challenge_readability'] = textstat.flesch_kincaid_grade(challenge_content)\n",
    "    df.at[index, 'Challenge_reading_time'] = textstat.reading_time(challenge_content)\n",
    "    \n",
    "    links = list(set(re.findall(link_pattern, challenge_content)))\n",
    "    df.at[index, 'Challenge_link_count'] = len(links)\n",
    "\n",
    "    solution_content = row['Solution_body']\n",
    "\n",
    "    if pd.notna(solution_content):\n",
    "        df.at[index, 'Solution_word_count'] = textstat.lexicon_count(solution_content)\n",
    "        df.at[index, 'Solution_sentence_count'] = textstat.sentence_count(solution_content)\n",
    "        df.at[index, 'Solution_readability'] = textstat.flesch_kincaid_grade(solution_content)\n",
    "        df.at[index, 'Solution_reading_time'] = textstat.reading_time(solution_content)\n",
    "        \n",
    "        links = list(set(re.findall(link_pattern, solution_content)))\n",
    "        df.at[index, 'Solution_link_count'] = len(links)\n",
    "        \n",
    "        link_docs = 0\n",
    "        link_tools = 0\n",
    "        link_issues = 0\n",
    "        link_patches = 0\n",
    "        link_tutorials = 0\n",
    "        link_examples = 0\n",
    "    \n",
    "        for link in links:\n",
    "            if any([patch in link for patch in keywords_patch]):\n",
    "                link_patches += 1\n",
    "            elif any([issue in link for issue in keywords_issue]):\n",
    "                link_issues += 1\n",
    "            elif any([tool in link for tool in keywords_tool]):\n",
    "                link_tools += 1\n",
    "            elif any([doc in link for doc in keywords_doc]):\n",
    "                link_docs += 1\n",
    "            elif any([tool in link for tool in keywords_tutorial]):\n",
    "                link_tutorials += 1\n",
    "            else:\n",
    "                link_examples += 1\n",
    "                \n",
    "        df.at[index, 'Solution_link_docs'] = link_docs\n",
    "        df.at[index, 'Solution_link_issues'] = link_issues\n",
    "        df.at[index, 'Solution_link_patches'] = link_patches\n",
    "        df.at[index, 'Solution_link_tools'] = link_tools\n",
    "        df.at[index, 'Solution_link_tutorials'] = link_tutorials\n",
    "        df.at[index, 'Solution_link_examples'] = link_examples\n",
    "        \n",
    "    creation_time = row['Challenge_created_time']\n",
    "    closed_time = row['Challenge_closed_time']\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "\n",
    "    creation_time = row['Challenge_last_edit_time'] if pd.notna(row['Challenge_last_edit_time']) else creation_time\n",
    "    closed_time = row['Solution_last_edit_time'] if pd.notna(row['Solution_last_edit_time']) else closed_time\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = df.at[index, 'Challenge_solved_time']\n",
    "\n",
    "df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "df['Challenge_answer_count'] = df['Challenge_answer_count'].fillna(0)\n",
    "df['Challenge_participation_count'] = df['Challenge_answer_count'] + df['Challenge_comment_count']\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_json(os.path.join(path_result_rq1, 'metrics.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Count'] = 1\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_topic_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge view count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_view_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge score count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_score_count'] = df_topics['Challenge_score_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_score_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge favorite count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_favorite_count'] = df_topics['Challenge_favorite_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_favorite_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge answer count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_answer_count'] = df_topics['Challenge_answer_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_answer_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge comment count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_comment_count'] = df_topics['Challenge_comment_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_comment_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_participation_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_participation_count_distribution.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
