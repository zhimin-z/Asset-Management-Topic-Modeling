{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import openai\n",
    "import random\n",
    "import enchant\n",
    "import textstat\n",
    "import itertools\n",
    "import collections\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, strip_punctuation, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../Dataset'\n",
    "path_result = '../../Result'\n",
    "path_rq1 = os.path.join(path_result, 'RQ1')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# subprocess.run(['python', '-m' 'spacy', 'download', 'en_core_web_sm'])\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "prompt_summary = '''Refine the post title to make it short and clear in simple English.\\n###'''\n",
    "\n",
    "tools_keyword_mapping = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['amazon', 'aws', 'maker', 'sage'],\n",
    "    'Azure Machine Learning': ['azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['databricks', 'mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['google', 'gcp', 'vertex'],\n",
    "    'Weights & Biases': ['biases', 'wandb', 'weights']\n",
    "}\n",
    "\n",
    "tools_keyword_list = set(itertools.chain(*tools_keyword_mapping.values()))\n",
    "\n",
    "keywords_image = {\n",
    "    \".jpg\", \n",
    "    \".png\", \n",
    "    \".jpeg\", \n",
    "    \".gif\", \n",
    "    \".bmp\", \n",
    "    \".webp\", \n",
    "    \".svg\", \n",
    "    \".tiff\"\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n",
    "\n",
    "stop_words_se = {\n",
    "    'ability',\n",
    "    'abilities',\n",
    "    'accident',\n",
    "    'accidents',\n",
    "    # 'acknowledgement',\n",
    "    'action',\n",
    "    'actions',\n",
    "    'activities',\n",
    "    'activity',\n",
    "    'advice',\n",
    "    'ai',\n",
    "    'alternative',\n",
    "    'alternatives',\n",
    "    # 'announcement',\n",
    "    'anomaly'\n",
    "    'anomalies'\n",
    "    'answer',\n",
    "    'answers',\n",
    "    'appreciation',\n",
    "    'approach',\n",
    "    'approaches',\n",
    "    'article',\n",
    "    'articles',\n",
    "    'assistance',\n",
    "    'attempt',\n",
    "    'author',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'benefit',\n",
    "    'bit',\n",
    "    'bits',\n",
    "    'block',\n",
    "    'blocks',\n",
    "    # 'blog',\n",
    "    # 'blogs',\n",
    "    'body',\n",
    "    'bug',\n",
    "    'bugs',\n",
    "    'building',\n",
    "    'case',\n",
    "    'cases',\n",
    "    'categories',\n",
    "    'categorization',\n",
    "    'category',\n",
    "    'cause',\n",
    "    'causes',\n",
    "    'challenge',\n",
    "    'challenges',\n",
    "    'change',\n",
    "    'changes',\n",
    "    # 'char',\n",
    "    'check',\n",
    "    'choice',\n",
    "    'choices',\n",
    "    'clarification',\n",
    "    'collection',\n",
    "    'com',\n",
    "    'combination',\n",
    "    # 'commmunication',\n",
    "    # 'community',\n",
    "    # 'communities',\n",
    "    # 'company',\n",
    "    # 'companies',\n",
    "    # 'computer',\n",
    "    # 'computers',\n",
    "    # 'concept',\n",
    "    # 'concepts',\n",
    "    'concern',\n",
    "    'concerns',\n",
    "    # 'condition',\n",
    "    # 'conditions',\n",
    "    'confirmation',\n",
    "    'confusion',\n",
    "    'consideration',\n",
    "    # 'content',\n",
    "    # 'contents',\n",
    "    'context',\n",
    "    # 'count',\n",
    "    'couple',\n",
    "    'couples',\n",
    "    # 'course',\n",
    "    # 'courses',\n",
    "    'crash',\n",
    "    'crashes',\n",
    "    'cross',\n",
    "    # 'custom',\n",
    "    'customer',\n",
    "    'customers',\n",
    "    'day',\n",
    "    'days',\n",
    "    'demand',\n",
    "    # 'description',\n",
    "    'desire',\n",
    "    'detail',\n",
    "    'details',\n",
    "    'devops',\n",
    "    'difference',\n",
    "    'differences',\n",
    "    'difficulties',\n",
    "    'difficulty',\n",
    "    'discrepancies',\n",
    "    'discrepancy',\n",
    "    'discussion',\n",
    "    'dislike',\n",
    "    'distinction',\n",
    "    'effect',\n",
    "    'end',\n",
    "    # 'engineering',\n",
    "    'enquiries',\n",
    "    'enquiry',\n",
    "    'error',\n",
    "    'errors',\n",
    "    'evidence',\n",
    "    'example',\n",
    "    'examples',\n",
    "    'exception',\n",
    "    'exceptions',\n",
    "    'existence',\n",
    "    'exit',\n",
    "    'expectation',\n",
    "    'experience',\n",
    "    'expert',\n",
    "    'experts',\n",
    "    # 'explanation',\n",
    "    'fact',\n",
    "    'facts',\n",
    "    'fail',\n",
    "    'failure',\n",
    "    'favorite',\n",
    "    'favorites',\n",
    "    'fault',\n",
    "    'faults',\n",
    "    # 'feature',\n",
    "    # 'features',\n",
    "    # 'feedback',\n",
    "    # 'feedbacks',\n",
    "    'fix',\n",
    "    'fixes',\n",
    "    # 'float',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'functionality',\n",
    "    'functionalities',\n",
    "    'future',\n",
    "    'goal',\n",
    "    'goals',\n",
    "    'guarantee',\n",
    "    # 'guidance',\n",
    "    # 'guideline',\n",
    "    # 'guide',\n",
    "    'guy',\n",
    "    'guys',\n",
    "    'harm',\n",
    "    'hello',\n",
    "    'help',\n",
    "    'hour',\n",
    "    'hours',\n",
    "    'ibm',\n",
    "    'idea',\n",
    "    'ideas',\n",
    "    'individual',\n",
    "    'individuals',\n",
    "    'info',\n",
    "    'information',\n",
    "    'inquiries',\n",
    "    'inquiry',\n",
    "    'insight',\n",
    "    # 'instruction',\n",
    "    # 'instructions',\n",
    "    # 'int',\n",
    "    'intelligence',\n",
    "    'intent',\n",
    "    'interest',\n",
    "    'introduction',\n",
    "    'investigation',\n",
    "    'invitation',\n",
    "    'ipynb',\n",
    "    'issue',\n",
    "    'issues',\n",
    "    'kind',\n",
    "    'kinds',\n",
    "    'lack',\n",
    "    'learning',\n",
    "    'level',\n",
    "    'levels',\n",
    "    'look',\n",
    "    'looks',\n",
    "    'lot',\n",
    "    'lots',\n",
    "    'luck',\n",
    "    'machine',\n",
    "    'major',\n",
    "    'manner',\n",
    "    'manners',\n",
    "    # 'manual',\n",
    "    'mark',\n",
    "    'meaning',\n",
    "    # 'message',\n",
    "    # 'messages',\n",
    "    'method',\n",
    "    'methods',\n",
    "    'ml',\n",
    "    'mlops',\n",
    "    'minute',\n",
    "    'minutes',\n",
    "    'mistake',\n",
    "    'mistakes',\n",
    "    'month',\n",
    "    'months',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'number',\n",
    "    'numbers',\n",
    "    'offer',\n",
    "    'one',\n",
    "    'ones',\n",
    "    'opinion',\n",
    "    'opinions',\n",
    "    # 'org',\n",
    "    # 'organization',\n",
    "    'outcome',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'past',\n",
    "    'people',\n",
    "    'person',\n",
    "    'persons',\n",
    "    'perspective',\n",
    "    'perspectives',\n",
    "    'place',\n",
    "    'places',\n",
    "    'point',\n",
    "    'points',\n",
    "    'post',\n",
    "    'posts',\n",
    "    'practice',\n",
    "    'practices',\n",
    "    'problem',\n",
    "    'problems',\n",
    "    # 'product',\n",
    "    # 'products',\n",
    "    # 'program',\n",
    "    # 'programs',\n",
    "    # 'project',\n",
    "    # 'projects',\n",
    "    # 'proposal',\n",
    "    'purpose',\n",
    "    'purposes',\n",
    "    'py',\n",
    "    # 'python',\n",
    "    'qa',\n",
    "    'question',\n",
    "    'questions',\n",
    "    'reason',\n",
    "    'reasons',\n",
    "    # 'recognition',\n",
    "    # 'recommendation',\n",
    "    # 'recommendations',\n",
    "    # 'recommender',\n",
    "    # 'regression',\n",
    "    # 'request',\n",
    "    'research',\n",
    "    'result',\n",
    "    'results',\n",
    "    'scenario',\n",
    "    'scenarios',\n",
    "    'science',\n",
    "    'screenshot',\n",
    "    'screenshots',\n",
    "    'second',\n",
    "    'seconds',\n",
    "    'section',\n",
    "    'sense',\n",
    "    'sentence',\n",
    "    'show',\n",
    "    'shows',\n",
    "    'situation',\n",
    "    'software',\n",
    "    'solution',\n",
    "    'solutions',\n",
    "    'start',\n",
    "    # 'state',\n",
    "    # 'statement',\n",
    "    # 'states',\n",
    "    # 'status',\n",
    "    # 'step',\n",
    "    # 'steps',\n",
    "    # 'string',\n",
    "    'study',\n",
    "    'stuff',\n",
    "    'success',\n",
    "    'suggestion',\n",
    "    'suggestions',\n",
    "    'summary',\n",
    "    'summaries',\n",
    "    'surprise',\n",
    "    # 'support',\n",
    "    'talk',\n",
    "    # 'task',\n",
    "    # 'tasks',\n",
    "    # 'technique',\n",
    "    # 'techniques',\n",
    "    # 'technologies',\n",
    "    # 'technology',\n",
    "    'term',\n",
    "    'terms',\n",
    "    'times',\n",
    "    'thank',\n",
    "    'thanks',\n",
    "    'thing',\n",
    "    'things',\n",
    "    'thought',\n",
    "    'three',\n",
    "    'title',\n",
    "    'today',\n",
    "    'tomorrow',\n",
    "    # 'tool',\n",
    "    # 'tools',\n",
    "    'topic',\n",
    "    'topics',\n",
    "    'total',\n",
    "    'trouble',\n",
    "    'troubles',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'two',\n",
    "    'understand',\n",
    "    'understanding',\n",
    "    'usage',\n",
    "    'use',\n",
    "    'user',\n",
    "    'users',\n",
    "    'uses',\n",
    "    # 'value',\n",
    "    # 'values',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'week',\n",
    "    'weeks',\n",
    "    'word',\n",
    "    'words',\n",
    "    'work',\n",
    "    'workaround',\n",
    "    'workarounds',\n",
    "    'works',\n",
    "    'yeah',\n",
    "    'year',\n",
    "    'years',\n",
    "    'yesterday',\n",
    "}\n",
    "\n",
    "stop_words_ml = {\n",
    "    'ad',\n",
    "    'ads',\n",
    "    'algorithm',\n",
    "    'algorithms',\n",
    "    'analysis',\n",
    "    'anomaly',\n",
    "    'asr',\n",
    "    'audio',\n",
    "    'automl',\n",
    "    'autopilot',\n",
    "    'bert',\n",
    "    'bi',\n",
    "    'chatbot',\n",
    "    'classification',\n",
    "    'classifier',\n",
    "    'clustering',\n",
    "    'cnn',\n",
    "    'cv',\n",
    "    'decision',\n",
    "    'detection',\n",
    "    'dimensionality'\n",
    "    'forecast',\n",
    "    'forecasts',\n",
    "    'forecasting',\n",
    "    'forest',\n",
    "    'fraud',\n",
    "    'gan',\n",
    "    'gesture',\n",
    "    'gpt',\n",
    "    'ica',\n",
    "    'knn',\n",
    "    'language',\n",
    "    'languages',\n",
    "    'lda',\n",
    "    'lstm',\n",
    "    'nlp',\n",
    "    'nmf',\n",
    "    'ocr',\n",
    "    'pca',\n",
    "    'phone',\n",
    "    'processing',\n",
    "    'recognition',\n",
    "    'recommendation',\n",
    "    'recommendations',\n",
    "    'recommender',\n",
    "    'reduction',\n",
    "    'regression',\n",
    "    'regressor',\n",
    "    'reinforcement',\n",
    "    'rf',\n",
    "    'rl',\n",
    "    'rnn',\n",
    "    'segmentation',\n",
    "    'sentiment',\n",
    "    'series',\n",
    "    'sota',\n",
    "    'sound',\n",
    "    'spam',\n",
    "    'speech',\n",
    "    'stt',\n",
    "    'svd',\n",
    "    'svm',\n",
    "    'time',\n",
    "    'translation',\n",
    "    'translator',\n",
    "    'tree',\n",
    "    'tsne',\n",
    "    'tts',\n",
    "    'video',\n",
    "    'vision',\n",
    "    'voice',\n",
    "}\n",
    "\n",
    "stop_words_level1 = STOPWORDS.union(stop_words_se)\n",
    "stop_words_level2 = stop_words_level1.union(stop_words_ml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(block_list):\n",
    "    total_loc = 0\n",
    "    for blocks in block_list:\n",
    "        for block in blocks:\n",
    "            for line in block.splitlines():\n",
    "                if line.strip():\n",
    "                    total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    # extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract code blocks type 1\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code blocks type 2\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    return clean_text, links, code_line\n",
    "\n",
    "def extract_code(content):\n",
    "    code_patterns = [r'```.+?```', r'``.+?``', r'`.+?`']\n",
    "    clean_text = content\n",
    "    code_line = 0\n",
    "\n",
    "    for code_pattern in code_patterns:\n",
    "        code_snippets = re.findall(code_pattern, clean_text, flags=re.DOTALL)\n",
    "        code_line += count_code_line(code_snippets)\n",
    "        clean_text = re.sub(code_pattern, '', clean_text, flags=re.DOTALL)\n",
    "    \n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern1 = r\"\\!?\\[.*?\\]\\((.*?)\\)\"\n",
    "    links1 = re.findall(link_pattern1, text)\n",
    "    clean_text = re.sub(link_pattern1, '', text)\n",
    "    link_pattern2 = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links2 = re.findall(link_pattern2, clean_text)\n",
    "    clean_text = re.sub(link_pattern2, '', clean_text)\n",
    "    links = links1 + links2\n",
    "    return clean_text, links\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_line1 = extract_styles(content)\n",
    "    clean_text, code_line2 = extract_code(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_line = code_line1 + code_line2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_line'])\n",
    "    return content_collection(clean_text, links, code_line)\n",
    "\n",
    "def word_frequency(text):\n",
    "    word_counts = collections.Counter(text.split())\n",
    "    return word_counts\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language == 'en'\n",
    "    except:\n",
    "        # In case the detection fails (e.g. if the text is too short or doesn't contain enough features)\n",
    "        return False\n",
    "\n",
    "def extract_english(text):\n",
    "    words = [word for word in text.split() if spell_checker.check(word)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_words_with_substring(text, substring_list):\n",
    "    words = text.split()\n",
    "    for substring in substring_list:\n",
    "        words = [word for word in words if substring not in word]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_text(text, level=1):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = remove_words_with_substring(clean_text, tools_keyword_list)\n",
    "    clean_text = strip_punctuation(clean_text)\n",
    "    # clean_text = extract_english(clean_text)\n",
    "    # clean_text = strip_short(clean_text)\n",
    "    clean_text = extract_nouns(clean_text)\n",
    "    match level:\n",
    "        case 1:\n",
    "            clean_text = remove_stopwords(clean_text, stop_words_level1)\n",
    "        case 2:\n",
    "            clean_text = remove_stopwords(clean_text, stop_words_level2)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_links(links):\n",
    "    image_links = 0\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([image in link for image in keywords_image]):\n",
    "            image_links += 1\n",
    "        elif any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['image', 'documentation', 'tool', 'issue', 'patch', 'tutorial', 'example'])\n",
    "    return link_analysis(image_links, documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "\n",
    "def find_duplicates(in_list):  \n",
    "    duplicates = []\n",
    "    unique = set(in_list)\n",
    "    for each in unique:\n",
    "        count = in_list.count(each)\n",
    "        if count > 1:\n",
    "            duplicates.append(each)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_comment_body'] = row['Issue_comment_body']\n",
    "    df_issues.at[index, 'Challenge_comment_score'] = row['Issue_comment_score']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_comment_body'] = row['Question_comment_body']\n",
    "    df_questions.at[index, 'Challenge_comment_score'] = row['Question_comment_score']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_comment_score'] = row['Answer_comment_score']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df = df[df.columns.drop(list(df.filter(regex=r'(Issue|Question|Answer)_')))]\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "df = df.explode('Tools')\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tools', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.pdf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "# del df['Challenge_preprocessed_content']\n",
    "# del df['Challenge_preprocessed_gpt_summary']\n",
    "# del df['Challenge_preprocessed_title']\n",
    "\n",
    "# df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/tmp/ipykernel_3141408/4081503589.py:11: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Post level preprocessing\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    # clean_title1 = preprocess_text(title_analyzer.text)\n",
    "    clean_title2 = preprocess_text(title_analyzer.text, 2)\n",
    "    \n",
    "    # challenge_analyzer = split_content(row['Challenge_title'] + row['Challenge_body'])\n",
    "    # link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    # text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    # clean_text1 = preprocess_text(challenge_analyzer.text)\n",
    "    # clean_text2 = preprocess_text(challenge_analyzer.text, 2)\n",
    "    \n",
    "    # df.at[index, 'Challenge_preprocessed_title1'] = clean_title1\n",
    "    df.at[index, 'Challenge_preprocessed_title2'] = clean_title2\n",
    "    # df.at[index, 'Challenge_preprocessed_content1'] = clean_text1\n",
    "    # df.at[index, 'Challenge_preprocessed_content2'] = clean_text2\n",
    "    # df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    # df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    # df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    # df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    # df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    # df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    # df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    # df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    # df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    # df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    # df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    # if pd.notna(row['Challenge_comment_body']):\n",
    "    #     comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "    #     link_analyzer = analyze_links(comment_analyzer.links)\n",
    "    #     text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "    #     df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "    #     df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "    #     df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "    #     df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "    #     df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "    #     df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "    #     df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "    #     df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "    #     df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "    #     df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "    #     df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    # if pd.notna(row['Solution_body']):\n",
    "    #     solution_analyzer = split_content(row['Solution_body'])\n",
    "    #     link_analyzer = analyze_links(solution_analyzer.links)\n",
    "    #     text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "    #     df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "    #     df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "    #     df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "    #     df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "    #     df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "    #     df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "    #     df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "    #     df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "    #     df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "    #     df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "    #     df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "    # if pd.notna(row['Solution_comment_body']):\n",
    "    #     comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "    #     link_analyzer = analyze_links(comment_analyzer.links)\n",
    "    #     text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "    #     df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "    #     df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "    #     df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "    #     df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "    #     df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "    #     df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "    #     df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "    #     df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "    #     df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "    #     df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "    #     df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in cast\n",
      "\n",
      "/tmp/ipykernel_3141408/4081503589.py:11: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning:\n",
      "\n",
      "It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "/tmp/ipykernel_3141408/4081503589.py:11: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "\n",
      "/tmp/ipykernel_3141408/4081503589.py:11: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "\n",
      "/tmp/ipykernel_3141408/4081503589.py:11: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "\n",
      "/tmp/ipykernel_3141408/4081503589.py:11: MarkupResemblesLocatorWarning:\n",
      "\n",
      "The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Post level preprocessing\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    if not is_english(title_analyzer.text):\n",
    "        df.drop(index, inplace=True)\n",
    "        continue\n",
    "    clean_title1 = preprocess_text(title_analyzer.text)\n",
    "    clean_title2 = preprocess_text(title_analyzer.text, 2)\n",
    "    df.at[index, 'Challenge_preprocessed_title1'] = clean_title1\n",
    "    df.at[index, 'Challenge_preprocessed_title2'] = clean_title2\n",
    "    \n",
    "    challenge_analyzer = split_content(row['Challenge_title'] + row['Challenge_body'])\n",
    "    link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    clean_text1 = preprocess_text(challenge_analyzer.text)\n",
    "    clean_text2 = preprocess_text(challenge_analyzer.text, 2)\n",
    "    \n",
    "    df.at[index, 'Challenge_preprocessed_content1'] = clean_text1\n",
    "    df.at[index, 'Challenge_preprocessed_content2'] = clean_text2\n",
    "    df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Challenge_comment_body']):\n",
    "        comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Solution_body']):\n",
    "        solution_analyzer = split_content(row['Solution_body'])\n",
    "        link_analyzer = analyze_links(solution_analyzer.links)\n",
    "        text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "        df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "    if pd.notna(row['Solution_comment_body']):\n",
    "        comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on post 99\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/awslabs/gluonts/issues/426\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/huggingface/transformers/issues/13875\n",
      "persisting on post 199\n",
      "persisting on post 299\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/rom1504/img2dataset/issues/219\n",
      "persisting on post 399\n",
      "persisting on post 499\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/SeldonIO/seldon-core/issues/4497\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/SeldonIO/seldon-core/issues/4014\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/SeldonIO/seldon-core/issues/3846\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/SeldonIO/MLServer/issues/811\n",
      "persisting on post 599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/pycaret/pycaret/issues/3389\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/pycaret/pycaret/issues/3383\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/pycaret/pycaret/issues/2838\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/pycaret/pycaret/issues/931\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/prinz-nussknacker/prinz/issues/78\n",
      "persisting on post 699\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/neptune-ai/kedro-neptune/issues/70\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/nv-morpheus/Morpheus/issues/512\n",
      "persisting on post 799\n",
      "persisting on post 899\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/amesar/mlflow-tools/issues/21\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws/amazon-sagemaker-examples/issues/3521\n",
      "persisting on post 999\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws/amazon-sagemaker-examples/issues/2044\n",
      "persisting on post 1099\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws/amazon-sagemaker-examples/issues/702\n",
      "persisting on post 1199\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws/deep-learning-containers/issues/2611\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws/sagemaker-training-toolkit/issues/110\n",
      "persisting on post 1299\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws/sagemaker-scikit-learn-container/issues/82\n",
      "persisting on post 1399\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/aws-samples/amazon-a2i-sample-jupyter-notebooks/issues/5\n",
      "persisting on post 1499\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/Azure/MachineLearningNotebooks/issues/1882\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/Azure/MachineLearningNotebooks/issues/1837\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/Azure/MachineLearningNotebooks/issues/1775\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/Azure/MachineLearningNotebooks/issues/1668\n",
      "persisting on post 1599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/Azure/ai-toolkit-iot-edge/issues/22\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/ashleve/lightning-hydra-template/issues/478\n",
      "persisting on post 1699\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/facebookresearch/eai-vc/issues/12\n",
      "persisting on post 1799\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/Lightning-AI/lightning/issues/9879\n",
      "persisting on post 1899\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/lukas/ml-class/issues/50\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/openvinotoolkit/anomalib/issues/1009\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/openvinotoolkit/anomalib/issues/994\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/openclimatefix/nowcasting_dataset/issues/31\n",
      "persisting on post 1999\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/zenml-io/zenml/issues/1580\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/zenml-io/zenml/issues/1517\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/zenml-io/zenml/issues/767\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/nni/issues/3518\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/computervision-recipes/issues/404\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/computervision-recipes/issues/332\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/computervision-recipes/issues/320\n",
      "persisting on post 2099\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/MLOpsPython/issues/416\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/MLOpsPython/issues/383\n",
      "persisting on post 2199\n",
      "persisting on post 2299\n",
      "persisting on post 2399\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/equinor/gordo/issues/843\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/eto-ai/rikai/issues/372\n",
      "persisting on post 2499\n",
      "persisting on post 2599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/huggingface/notebooks/issues/396\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/huggingface/blog/issues/114\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/coli-saar/am-parser/issues/89\n",
      "persisting on post 2699\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/kedro-org/kedro-viz/issues/1276\n",
      "persisting on post 2799\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/microsoft/qlib/issues/1035\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://github.com/mlflow/mlflow-export-import/issues/33\n",
      "persisting on post 2899\n",
      "persisting on post 2999\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/74175401\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/73642527\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/76585219\n",
      "persisting on post 3099\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/76627676\n",
      "persisting on post 3199\n",
      "persisting on post 3299\n",
      "persisting on post 3399\n",
      "persisting on post 3499\n",
      "persisting on post 3599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/74962473\n",
      "persisting on post 3699\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/75814882\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/75078011\n",
      "persisting on post 3799\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/75570377\n",
      "persisting on post 3899\n",
      "persisting on post 3999\n",
      "persisting on post 4099\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/56046428\n",
      "persisting on post 4199\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/71652798\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/62422682\n",
      "persisting on post 4299\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/58592206\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/73085199\n",
      "persisting on post 4399\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/66561959\n",
      "persisting on post 4499\n",
      "persisting on post 4599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/65577286\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/75989861\n",
      "persisting on post 4699\n",
      "persisting on post 4799\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/75064559\n",
      "persisting on post 4899\n",
      "persisting on post 4999\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/70291455\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/68150444\n",
      "persisting on post 5099\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/68489311\n",
      "persisting on post 5199\n",
      "persisting on post 5299\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/73624005\n",
      "persisting on post 5399\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/74216681\n",
      "persisting on post 5499\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/74502427\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/75642137\n",
      "persisting on post 5599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/73027227\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/73080139\n",
      "persisting on post 5699\n",
      "persisting on post 5799\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/52760539\n",
      "persisting on post 5899\n",
      "persisting on post 5999\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/62962319\n",
      "persisting on post 6099\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/63492891\n",
      "persisting on post 6199\n",
      "persisting on post 6299\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/73462205\n",
      "persisting on post 6399\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/62813017\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/63116338\n",
      "persisting on post 6499\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/65235553\n",
      "persisting on post 6599\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/59762829\n",
      "persisting on post 6699\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/76178347\n",
      "persisting on post 6799\n",
      "Random.uniform() missing 1 required positional argument: 'b' on post https://stackoverflow.com/questions/69984504\n"
     ]
    }
   ],
   "source": [
    "# GPT summary\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 99:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    if pd.notna(row['Challenge_gpt_summary']):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + '\\nBody: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=50,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# GPT summary\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clean_summary1 = preprocess_text(row['Challenge_gpt_summary'])\n",
    "    clean_summary2 = preprocess_text(row['Challenge_gpt_summary'], 2)\n",
    "    df.at[index, 'Challenge_preprocessed_gpt_summary1'] = clean_summary1\n",
    "    df.at[index, 'Challenge_preprocessed_gpt_summary2'] = clean_summary2\n",
    "\n",
    "df.sort_index(axis=1, inplace=True)\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "# df_old = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if 'Stack' not in row['Platform']:\n",
    "#         continue\n",
    "#     for i2, r2 in df_old.iterrows():\n",
    "#         if 'Stack' not in r2['Platform']:\n",
    "#             continue\n",
    "#         if row['Challenge_link'] == r2['Challenge_link']:\n",
    "#             df.at[index, 'Tools'] = r2['Tools']\n",
    "#             break\n",
    "\n",
    "# df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from bertopic.backend import OpenAIBackend\n",
    "\n",
    "# # openai.api_key = MY_API_KEY\n",
    "# embedding_model = OpenAIBackend(delay_in_seconds=0.1, batch_size=10)\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# docs = df[df['Challenge_summary'] != 'na']['Challenge_summary'].tolist() + df[df['Challenge_root_cause'] != 'na']['Challenge_root_cause'].tolist()\n",
    "\n",
    "# topic_model = BERTopic(embedding_model=embedding_model)\n",
    "# topics, probs = topic_model.fit_transform(docs)\n",
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minimize_weighted_sum(df, sort_column):\n",
    "#     df_new = df.sort_values(sort_column, ascending=False)\n",
    "#     n = len(df)\n",
    "#     center_idx = (n - 1) // 2\n",
    "#     direction = -1\n",
    "#     distance = 0\n",
    "\n",
    "#     for _, row in df_new.iterrows():\n",
    "#         # Calculate the new index\n",
    "#         new_idx = center_idx + direction * distance\n",
    "        \n",
    "#         # Place the element from the sorted list into the new list\n",
    "#         df.iloc[new_idx] = row\n",
    "\n",
    "#         # If we've just moved to the left, increase the distance\n",
    "#         if direction == -1:\n",
    "#             distance += 1\n",
    "\n",
    "#         # Switch the direction\n",
    "#         direction *= -1\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_topic = '''You will be given a list of words refering to specific software engineering topics. Please summarize each topic in two to three words and attach one sentence description. Also, you must guarantee that the terms are not duplicate with one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_rq1, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for index, term in enumerate(topic) if index < 5])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "False\n",
      "{17, 15}\n"
     ]
    }
   ],
   "source": [
    "topic_list = [topic for topic in topics.split('\\n') if topic]\n",
    "macro_topic_mapping_inverse = {\n",
    "    'Environment Management': [0,8,21,28,44,56,58,60,62,75,80,83,86,95,97,107,112,118,122,138,141,156,158],#107?\n",
    "    'Resource Management': [23,36,38,49,81,96,117,119,123,159],\n",
    "    'Pipeline Management': [20,32,42,47,67,74,105,109,131,135],\n",
    "    'Experiment Management': [13,59,88,128,165],\n",
    "    'Code Development': [1,43,51,70,79,90,91,100,108,126,129,140,145,160],#129?\n",
    "    'Data Development': [24,26,27,30,46,53,82,84,89,130],#89?\n",
    "    'Data Management': [3,15,17,29,33,34,39,71,72,101,104,116,134,157,163],\n",
    "    'Model Development': [4,7,11,14,63,106,115,142,143,148,149],\n",
    "    'Model Management': [2,5,18,61,77,87,94,114,125,133],\n",
    "    'Model Serving': [6,16,22,31,40,52,57,64,69,103,113,124,127,139,144,152,166],\n",
    "    'Version Management': [45,65,78,120,161],\n",
    "    'Network Management': [25,66,93,121,132,164],#164?\n",
    "    'Observability Management': [10,12,35,54,76,137,147],\n",
    "    'Security Management': [9,73,99,153,155],\n",
    "    'Documentation Management': [],\n",
    "    'File Management': [37,41,50,92,111,136,146,150,151],#146?#150?\n",
    "    'QA Management': [98,110,162],\n",
    "    'Visualization Management': [19,154],\n",
    "    'Discarded': [48,55,68,85,102],\n",
    "}\n",
    "        \n",
    "macro_topic_list = []\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_indexing = {}\n",
    "for macro_topic, sub_topics in macro_topic_mapping_inverse.items():\n",
    "    index, name = int(macro_topic.split(': ')[0]), macro_topic.split(': ')[1]\n",
    "    macro_topic_indexing[index] = name\n",
    "    macro_topic_list.extend(sub_topics)\n",
    "    for topic in sub_topics:\n",
    "        macro_topic_mapping[topic] = macro_topic\n",
    "\n",
    "print(find_duplicates(macro_topic_list))\n",
    "print(len(macro_topic_list) == len(topic_list))\n",
    "print(set(range(len(topic_list))).difference(set(macro_topic_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Challenge_topic'] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Topic & Percentage & Number \\\\\n",
      "\\midrule\n",
      "Model Management & 21.39 & 2378 \\\\\n",
      "Compute Management & 20.05 & 2229 \\\\\n",
      "Environment Management & 17.90 & 1990 \\\\\n",
      "Data Management & 13.13 & 1460 \\\\\n",
      "Lifecycle Management & 9.94 & 1105 \\\\\n",
      "Access Management & 7.84 & 872 \\\\\n",
      "Observability Management & 6.65 & 739 \\\\\n",
      "Code Management & 3.09 & 344 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_rq1, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "\n",
    "for name, group in df.groupby('Challenge_topic_macro'):\n",
    "    entry = {\n",
    "        'Topic': macro_topic_indexing[name],\n",
    "        'Percentage': round(len(group)/len(df)*100, 2),\n",
    "        'Number': len(group),\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "\n",
    "df_number = df_number.sort_values('Percentage', ascending=False)\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'filtered.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "df['Challenge_topic_macro'] = df['Challenge_topic_macro'].apply(lambda x: macro_topic_indexing[x])\n",
    "categories = ['Challenge_type', 'Challenge_topic_macro', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_rq1, 'State type topic sankey.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
