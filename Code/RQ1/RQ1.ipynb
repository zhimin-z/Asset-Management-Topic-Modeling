{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import openai\n",
    "import random\n",
    "import enchant\n",
    "import textstat\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_short, strip_punctuation, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../../Dataset'\n",
    "path_result = '../../Result'\n",
    "path_rq1 = os.path.join(path_result, 'RQ1')\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spell_checker = enchant.Dict(\"en_US\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "prompt_summary = 'Concisely convey the most significant idea about the post in one brief sentence.\\n###'\n",
    "\n",
    "tools_keyword_mapping = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sagemaker', 'amazon', 'aws'],\n",
    "    'Azure Machine Learning': ['azure', 'microsoft'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow', 'databricks'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex', 'google', 'gcp'],\n",
    "    'Weights & Biases': ['weights', 'biases', 'wandb']\n",
    "}\n",
    "\n",
    "keywords_image = {\n",
    "    \".jpg\", \n",
    "    \".png\", \n",
    "    \".jpeg\", \n",
    "    \".gif\", \n",
    "    \".bmp\", \n",
    "    \".webp\", \n",
    "    \".svg\", \n",
    "    \".tiff\"\n",
    "}\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n",
    "\n",
    "stop_words_custom = {\n",
    "    'ability',\n",
    "    'abilities',\n",
    "    'accident',\n",
    "    'accidents',\n",
    "    'acknowledgement',\n",
    "    'action',\n",
    "    'actions',\n",
    "    'activities',\n",
    "    'activity',\n",
    "    'ad',\n",
    "    'ads',\n",
    "    'advice',\n",
    "    'alternative',\n",
    "    'alternatives',\n",
    "    'analysis',\n",
    "    'analyses',\n",
    "    'announcement',\n",
    "    'anomaly'\n",
    "    'anomalies'\n",
    "    'answer',\n",
    "    'answers',\n",
    "    'appreciation',\n",
    "    'approach',\n",
    "    'approaches',\n",
    "    'article',\n",
    "    'articles',\n",
    "    'assertion',\n",
    "    'assistance',\n",
    "    'assumption',\n",
    "    'attempt',\n",
    "    'author',\n",
    "    'behavior',\n",
    "    'behaviour',\n",
    "    'benefit',\n",
    "    'bit',\n",
    "    'bits',\n",
    "    'block',\n",
    "    'blocks',\n",
    "    'blog',\n",
    "    'blogs',\n",
    "    'body',\n",
    "    'bug',\n",
    "    'bugs',\n",
    "    'building',\n",
    "    'case',\n",
    "    'cases',\n",
    "    'categories',\n",
    "    'category',\n",
    "    'cause',\n",
    "    'causes',\n",
    "    'challenge',\n",
    "    'challenges',\n",
    "    'change',\n",
    "    'changes',\n",
    "    'char',\n",
    "    'character',\n",
    "    'characters',\n",
    "    'check',\n",
    "    'choice',\n",
    "    'choices',\n",
    "    'classification',\n",
    "    'cloud',\n",
    "    'collection',\n",
    "    'com',\n",
    "    'combination',\n",
    "    'commmunication',\n",
    "    'community',\n",
    "    'communities',\n",
    "    'company',\n",
    "    'companies',\n",
    "    'concept',\n",
    "    'concepts',\n",
    "    'concern',\n",
    "    'concerns',\n",
    "    'condition',\n",
    "    'conditions',\n",
    "    'confirmation',\n",
    "    'confusion',\n",
    "    'consideration',\n",
    "    'contact',\n",
    "    'content',\n",
    "    'contents',\n",
    "    'control',\n",
    "    'count',\n",
    "    'couple',\n",
    "    'couples',\n",
    "    'course',\n",
    "    'courses',\n",
    "    'crash',\n",
    "    'crashes',\n",
    "    'cross',\n",
    "    'current',\n",
    "    'custom',\n",
    "    'customer',\n",
    "    'customers',\n",
    "    'day',\n",
    "    'days',\n",
    "    'decision',\n",
    "    'default',\n",
    "    'demand',\n",
    "    'demo',\n",
    "    'description',\n",
    "    'desire',\n",
    "    'desktop',\n",
    "    'detail',\n",
    "    'details',\n",
    "    'differ',\n",
    "    'difference',\n",
    "    'differences',\n",
    "    'difficulties',\n",
    "    'difficulty',\n",
    "    'discrepancies',\n",
    "    'discrepancy',\n",
    "    'discussion',\n",
    "    'dislike',\n",
    "    'distinction',\n",
    "    'edit',\n",
    "    'effect',\n",
    "    'end',\n",
    "    'enquiries',\n",
    "    'enquiry',\n",
    "    'error',\n",
    "    'errors',\n",
    "    'evidence',\n",
    "    'example',\n",
    "    'examples',\n",
    "    'exception',\n",
    "    'exceptions',\n",
    "    'existence',\n",
    "    'exit',\n",
    "    'expectation',\n",
    "    'experience',\n",
    "    'expert',\n",
    "    'experts',\n",
    "    'explanation',\n",
    "    'face',\n",
    "    'fact',\n",
    "    'facts',\n",
    "    'fail',\n",
    "    'failure',\n",
    "    'favorite',\n",
    "    'favorites',\n",
    "    'fault',\n",
    "    'feature',\n",
    "    'features',\n",
    "    'feedback',\n",
    "    'feedbacks',\n",
    "    'fix',\n",
    "    'fixes',\n",
    "    'float',\n",
    "    'forecast',\n",
    "    'forecasting',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'functionality',\n",
    "    'functionalities',\n",
    "    'future',\n",
    "    'goal',\n",
    "    'goals',\n",
    "    'guarantee',\n",
    "    'guidance',\n",
    "    'guideline',\n",
    "    'guide',\n",
    "    'guy',\n",
    "    'guys',\n",
    "    'harm',\n",
    "    'help',\n",
    "    'hour',\n",
    "    'hours',\n",
    "    'ibm',\n",
    "    'idea',\n",
    "    'ideas',\n",
    "    'individual',\n",
    "    'individuals',\n",
    "    'info',\n",
    "    'information',\n",
    "    'inquiries',\n",
    "    'inquiry',\n",
    "    'insight',\n",
    "    'instruction',\n",
    "    'instructions',\n",
    "    'int',\n",
    "    'intelligence',\n",
    "    'interest',\n",
    "    'introduction',\n",
    "    'investigation',\n",
    "    'invitation',\n",
    "    'issue',\n",
    "    'issues',\n",
    "    'kind',\n",
    "    'kinds',\n",
    "    'lack',\n",
    "    'language',\n",
    "    'languages',\n",
    "    'laptop',\n",
    "    'learn',\n",
    "    'learning',\n",
    "    'level',\n",
    "    'levels',\n",
    "    # 'location',\n",
    "    # 'locations',\n",
    "    'look',\n",
    "    'looks',\n",
    "    'lot',\n",
    "    'lots',\n",
    "    'luck',\n",
    "    'machine',\n",
    "    'machines',\n",
    "    'major',\n",
    "    'manner',\n",
    "    'manners',\n",
    "    'manual',\n",
    "    'mark',\n",
    "    'meaning',\n",
    "    'message',\n",
    "    'messages',\n",
    "    'method',\n",
    "    'methods',\n",
    "    'mind',\n",
    "    'minute',\n",
    "    'minutes',\n",
    "    'mistake',\n",
    "    'mistakes',\n",
    "    'moment',\n",
    "    'month',\n",
    "    'months',\n",
    "    'need',\n",
    "    'needs',\n",
    "    'note',\n",
    "    'notes',\n",
    "    'number',\n",
    "    'numbers',\n",
    "    'offer',\n",
    "    'one',\n",
    "    'ones',\n",
    "    'opinion',\n",
    "    'opinions',\n",
    "    'org',\n",
    "    'organization',\n",
    "    'outcome',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'past',\n",
    "    'people',\n",
    "    'permit',\n",
    "    'person',\n",
    "    'persons',\n",
    "    'perspective',\n",
    "    'perspectives',\n",
    "    'picture',\n",
    "    'pictures',\n",
    "    'place',\n",
    "    'places',\n",
    "    'plan',\n",
    "    'point',\n",
    "    'points',\n",
    "    'post',\n",
    "    'posts',\n",
    "    'price',\n",
    "    'problem',\n",
    "    'problems',\n",
    "    'processing',\n",
    "    'product',\n",
    "    'products',\n",
    "    'program',\n",
    "    'programs',\n",
    "    'project',\n",
    "    'projects',\n",
    "    'proposal',\n",
    "    'purpose',\n",
    "    'purposes',\n",
    "    # 'python',\n",
    "    'question',\n",
    "    'questions',\n",
    "    'raise',\n",
    "    'reason',\n",
    "    'reasons',\n",
    "    'recommendation',\n",
    "    'recommendations',\n",
    "    'regression',\n",
    "    'research',\n",
    "    'result',\n",
    "    'results',\n",
    "    'return',\n",
    "    'returns',\n",
    "    'scenario',\n",
    "    'scenarios',\n",
    "    'science',\n",
    "    'screen',\n",
    "    'screenshot',\n",
    "    'screenshots',\n",
    "    'second',\n",
    "    'seconds',\n",
    "    'section',\n",
    "    'self',\n",
    "    'sense',\n",
    "    'sentence',\n",
    "    'setup',\n",
    "    'shape',\n",
    "    'show',\n",
    "    'shows',\n",
    "    'site',\n",
    "    'situation',\n",
    "    'software',\n",
    "    'solution',\n",
    "    'solutions',\n",
    "    'speech',\n",
    "    'start',\n",
    "    'state',\n",
    "    'statement',\n",
    "    'states',\n",
    "    'status',\n",
    "    'step',\n",
    "    'steps',\n",
    "    'string',\n",
    "    'study',\n",
    "    'stuff',\n",
    "    'success',\n",
    "    'suggestion',\n",
    "    'suggestions',\n",
    "    'summary',\n",
    "    'summaries',\n",
    "    'surprise',\n",
    "    'support',\n",
    "    'talk',\n",
    "    'task',\n",
    "    'tasks',\n",
    "    'technique',\n",
    "    'techniques',\n",
    "    'technologies',\n",
    "    'technology',\n",
    "    'term',\n",
    "    'terms',\n",
    "    'text',\n",
    "    'time',\n",
    "    'times',\n",
    "    'thank',\n",
    "    'thanks',\n",
    "    'thing',\n",
    "    'things',\n",
    "    'thought',\n",
    "    'three',\n",
    "    'title',\n",
    "    'time',\n",
    "    'today',\n",
    "    'tomorrow',\n",
    "    'tool',\n",
    "    'tools',\n",
    "    'topic',\n",
    "    'topics',\n",
    "    'total',\n",
    "    'trouble',\n",
    "    'troubles',\n",
    "    'truth',\n",
    "    'try',\n",
    "    'tutorial',\n",
    "    'tutorials',\n",
    "    'two',\n",
    "    'understand',\n",
    "    'understanding',\n",
    "    # 'url',\n",
    "    # 'urls',\n",
    "    'use',\n",
    "    'user',\n",
    "    'users',\n",
    "    'uses',\n",
    "    'value',\n",
    "    'values',\n",
    "    'variant',\n",
    "    'variants',\n",
    "    'versus',\n",
    "    'video',\n",
    "    'videos',\n",
    "    'view',\n",
    "    'viewpoint',\n",
    "    'vision',\n",
    "    'voice',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'week',\n",
    "    'weeks',\n",
    "    'word',\n",
    "    'words',\n",
    "    'work',\n",
    "    'workaround',\n",
    "    'workarounds',\n",
    "    'works',\n",
    "    'yeah',\n",
    "    'year',\n",
    "    'years',\n",
    "    'yesterday',\n",
    "}\n",
    "\n",
    "tools_keyword_list = set(itertools.chain(*tools_keyword_mapping.values()))\n",
    "stop_words_list = STOPWORDS.union(tools_keyword_list).union(stop_words_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_code_line(block_list):\n",
    "    total_loc = 0\n",
    "    for blocks in block_list:\n",
    "        for block in blocks:\n",
    "            for line in block.splitlines():\n",
    "                if line.strip():\n",
    "                    total_loc += 1\n",
    "    return total_loc\n",
    "\n",
    "def extract_styles(content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    clean_text = soup.get_text(separator=' ')\n",
    "    # extract links\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)] \n",
    "    # extract code blocks type 1\n",
    "    code_line1 = count_code_line([c.get_text() for c in soup.find_all('code')]) \n",
    "    # extract code blocks type 2\n",
    "    code_line2 = count_code_line([c.get_text() for c in soup.find_all('blockquote')]) \n",
    "    code_line = code_line1 + code_line2\n",
    "    return clean_text, links, code_line\n",
    "\n",
    "def extract_code(content):\n",
    "    code_patterns = [r'```.+?```', r'``.+?``', r'`.+?`']\n",
    "    clean_text = content\n",
    "    code_line = 0\n",
    "\n",
    "    for code_pattern in code_patterns:\n",
    "        code_snippets = re.findall(code_pattern, clean_text, flags=re.DOTALL)\n",
    "        code_line += count_code_line(code_snippets)\n",
    "        clean_text = re.sub(code_pattern, '', clean_text, flags=re.DOTALL)\n",
    "    \n",
    "    return clean_text, code_line\n",
    "\n",
    "def extract_links(text):\n",
    "    link_pattern1 = r\"\\!?\\[.*?\\]\\((.*?)\\)\"\n",
    "    links1 = re.findall(link_pattern1, text)\n",
    "    clean_text = re.sub(link_pattern1, '', text)\n",
    "    link_pattern2 = r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "    links2 = re.findall(link_pattern2, clean_text)\n",
    "    clean_text = re.sub(link_pattern2, '', clean_text)\n",
    "    links = links1 + links2\n",
    "    return clean_text, links\n",
    "\n",
    "def split_content(content):\n",
    "    clean_text, links1, code_line1 = extract_styles(content)\n",
    "    clean_text, code_line2 = extract_code(clean_text)\n",
    "    clean_text, links2 = extract_links(clean_text)\n",
    "    \n",
    "    links = links1 + links2\n",
    "    code_line = code_line1 + code_line2\n",
    "    \n",
    "    content_collection = namedtuple('Analyzer', ['text', 'links', 'code_line'])\n",
    "    return content_collection(clean_text, links, code_line)\n",
    "\n",
    "def word_frequency(text):\n",
    "    word_counts = collections.Counter(text.split())\n",
    "    return word_counts\n",
    "\n",
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return ' '.join(nouns)\n",
    "\n",
    "def extract_english(text):\n",
    "    words = [word for word in text.split() if spell_checker.check(word)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = strip_punctuation(clean_text)\n",
    "    clean_text = extract_english(clean_text)\n",
    "    clean_text = extract_nouns(clean_text)\n",
    "    clean_text = strip_short(clean_text)\n",
    "    clean_text = remove_stopwords(clean_text, stop_words_list)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_links(links):\n",
    "    image_links = 0\n",
    "    documentation_links = 0\n",
    "    tool_links = 0\n",
    "    issue_links = 0\n",
    "    patch_links = 0\n",
    "    tutorial_links = 0\n",
    "    example_links = 0\n",
    "    \n",
    "    for link in links:\n",
    "        if any([image in link for image in keywords_image]):\n",
    "            image_links += 1\n",
    "        elif any([patch in link for patch in keywords_patch]):\n",
    "            patch_links += 1\n",
    "        elif any([issue in link for issue in keywords_issue]):\n",
    "            issue_links += 1\n",
    "        elif any([tool in link for tool in keywords_tool]):\n",
    "            tool_links += 1\n",
    "        elif any([doc in link for doc in keywords_doc]):\n",
    "            documentation_links += 1\n",
    "        elif any([tool in link for tool in keywords_tutorial]):\n",
    "            tutorial_links += 1\n",
    "        else:\n",
    "            example_links += 1\n",
    "\n",
    "    link_analysis = namedtuple('Analyzer', ['image', 'documentation', 'tool', 'issue', 'patch', 'tutorial', 'example'])\n",
    "    return link_analysis(image_links, documentation_links, tool_links, issue_links, patch_links, tutorial_links, example_links)\n",
    "\n",
    "def analyze_text(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    readability = textstat.flesch_reading_ease(text)\n",
    "    reading_time = textstat.reading_time(text)\n",
    "    \n",
    "    text_analysis = namedtuple('Analyzer', ['word_count', 'readability', 'reading_time'])\n",
    "    return text_analysis(word_count, readability, reading_time)\n",
    "\n",
    "# expential backoff\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "\n",
    "def find_duplicates(in_list):  \n",
    "    duplicates = []\n",
    "    unique = set(in_list)\n",
    "    for each in unique:\n",
    "        count = in_list.count(each)\n",
    "        if count > 1:\n",
    "            duplicates.append(each)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues.json'))\n",
    "\n",
    "for index, row in df_issues.iterrows():\n",
    "    df_issues.at[index, 'Challenge_title'] = row['Issue_title']\n",
    "    df_issues.at[index, 'Challenge_body'] = row['Issue_body']\n",
    "    df_issues.at[index, 'Challenge_link'] = row['Issue_link']\n",
    "    df_issues.at[index, 'Challenge_tag_count'] = row['Issue_tag_count']\n",
    "    df_issues.at[index, 'Challenge_created_time'] = row['Issue_created_time']\n",
    "    df_issues.at[index, 'Challenge_score_count'] = row['Issue_score_count']\n",
    "    df_issues.at[index, 'Challenge_closed_time'] = row['Issue_closed_time']\n",
    "    df_issues.at[index, 'Challenge_repo_issue_count'] = row['Issue_repo_issue_count']\n",
    "    df_issues.at[index, 'Challenge_repo_star_count'] = row['Issue_repo_star_count']\n",
    "    df_issues.at[index, 'Challenge_repo_watch_count'] = row['Issue_repo_watch_count']\n",
    "    df_issues.at[index, 'Challenge_repo_fork_count'] = row['Issue_repo_fork_count']\n",
    "    df_issues.at[index, 'Challenge_repo_contributor_count'] = row['Issue_repo_contributor_count']\n",
    "    df_issues.at[index, 'Challenge_self_closed'] = row['Issue_self_closed']\n",
    "    df_issues.at[index, 'Challenge_comment_count'] = row['Issue_comment_count']\n",
    "    df_issues.at[index, 'Challenge_comment_body'] = row['Issue_comment_body']\n",
    "    df_issues.at[index, 'Challenge_comment_score'] = row['Issue_comment_score']\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_dataset, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    df_questions.at[index, 'Challenge_title'] = row['Question_title']\n",
    "    df_questions.at[index, 'Challenge_body'] = row['Question_body']\n",
    "    df_questions.at[index, 'Challenge_link'] = row['Question_link']\n",
    "    df_questions.at[index, 'Challenge_tag_count'] = row['Question_tag_count']\n",
    "    df_questions.at[index, 'Challenge_topic_count'] = row['Question_topic_count']\n",
    "    df_questions.at[index, 'Challenge_created_time'] = row['Question_created_time']\n",
    "    df_questions.at[index, 'Challenge_answer_count'] = row['Question_answer_count']\n",
    "    df_questions.at[index, 'Challenge_score_count'] = row['Question_score_count']\n",
    "    df_questions.at[index, 'Challenge_closed_time'] = row['Question_closed_time']\n",
    "    df_questions.at[index, 'Challenge_favorite_count'] = row['Question_favorite_count']\n",
    "    df_questions.at[index, 'Challenge_last_edit_time'] = row['Question_last_edit_time']\n",
    "    df_questions.at[index, 'Challenge_view_count'] = row['Question_view_count']\n",
    "    df_questions.at[index, 'Challenge_self_closed'] = row['Question_self_closed']\n",
    "    df_questions.at[index, 'Challenge_comment_count'] = row['Question_comment_count']\n",
    "    df_questions.at[index, 'Challenge_comment_body'] = row['Question_comment_body']\n",
    "    df_questions.at[index, 'Challenge_comment_score'] = row['Question_comment_score']\n",
    "\n",
    "    df_questions.at[index, 'Solution_body'] = row['Answer_body']\n",
    "    df_questions.at[index, 'Solution_score_count'] = row['Answer_score_count']\n",
    "    df_questions.at[index, 'Solution_comment_count'] = row['Answer_comment_count']\n",
    "    df_questions.at[index, 'Solution_comment_body'] = row['Answer_comment_body']\n",
    "    df_questions.at[index, 'Solution_comment_score'] = row['Answer_comment_score']\n",
    "    df_questions.at[index, 'Solution_last_edit_time'] = row['Answer_last_edit_time']\n",
    "\n",
    "df = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df = df[df.columns.drop(list(df.filter(regex=r'Issue|Question|Answer')))]\n",
    "df.to_json(os.path.join(path_dataset, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mState\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mChallenge_closed_time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pd\u001b[39m.\u001b[39misna(x) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m categories \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mPlatform\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTool\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mState\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m df_info \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mgroupby(categories)\u001b[39m.\u001b[39msize()\u001b[39m.\u001b[39mreset_index(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m labels \u001b[39m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m newDf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m~/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[1;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   8406\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[1;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[1;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[1;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[1;32m   8413\u001b[0m )\n",
      "File \u001b[0;32m~/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[1;32m    966\u001b[0m         obj,\n\u001b[1;32m    967\u001b[0m         keys,\n\u001b[1;32m    968\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    969\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m    970\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    971\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[1;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[1;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[1;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Tool'"
     ]
    }
   ],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_dataset, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/tmp/ipykernel_1673751/3007007082.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1673751/3007007082.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n",
      "/tmp/ipykernel_1673751/3007007082.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n",
      "/tmp/ipykernel_1673751/3007007082.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n",
      "/tmp/ipykernel_1673751/3007007082.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(content, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1 & 2\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'original.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    title_analyzer = split_content(row['Challenge_title'])\n",
    "    clean_title = preprocess_text(title_analyzer.text)\n",
    "    \n",
    "    challenge_analyzer = split_content(row['Challenge_title'] + row['Challenge_body'])\n",
    "    link_analyzer = analyze_links(challenge_analyzer.links)\n",
    "    text_analyzer = analyze_text(challenge_analyzer.text)\n",
    "    clean_text = preprocess_text(challenge_analyzer.text)\n",
    "    \n",
    "    df.at[index, 'Challenge_preprocessed_title'] = clean_title\n",
    "    df.at[index, 'Challenge_preprocessed_content'] = clean_text\n",
    "    df.at[index, 'Challenge_code_count'] = challenge_analyzer.code_line\n",
    "    df.at[index, 'Challenge_word_count'] = text_analyzer.word_count\n",
    "    df.at[index, 'Challenge_readability'] = text_analyzer.readability\n",
    "    df.at[index, 'Challenge_reading_time'] = text_analyzer.reading_time\n",
    "    df.at[index, 'Challenge_link_count_image'] = link_analyzer.image\n",
    "    df.at[index, 'Challenge_link_count_documentation'] = link_analyzer.documentation\n",
    "    df.at[index, 'Challenge_link_count_example'] = link_analyzer.example\n",
    "    df.at[index, 'Challenge_link_count_issue'] = link_analyzer.issue\n",
    "    df.at[index, 'Challenge_link_count_patch'] = link_analyzer.patch\n",
    "    df.at[index, 'Challenge_link_count_tool'] = link_analyzer.tool\n",
    "    df.at[index, 'Challenge_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Challenge_comment_body']):\n",
    "        comment_analyzer = split_content(row['Challenge_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Challenge_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Challenge_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Challenge_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Challenge_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Challenge_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Challenge_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Challenge_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Challenge_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Challenge_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Challenge_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Challenge_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "    if pd.notna(row['Solution_body']):\n",
    "        solution_analyzer = split_content(row['Solution_body'])\n",
    "        link_analyzer = analyze_links(solution_analyzer.links)\n",
    "        text_analyzer = analyze_text(solution_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_code_count'] = solution_analyzer.code_line\n",
    "        df.at[index, 'Solution_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_link_count_tutorial'] = link_analyzer.tutorial\n",
    "        \n",
    "    if pd.notna(row['Solution_comment_body']):\n",
    "        comment_analyzer = split_content(row['Solution_comment_body'])\n",
    "        link_analyzer = analyze_links(comment_analyzer.links)\n",
    "        text_analyzer = analyze_text(comment_analyzer.text)\n",
    "        \n",
    "        df.at[index, 'Solution_comment_code_count'] = comment_analyzer.code_line\n",
    "        df.at[index, 'Solution_comment_word_count'] = text_analyzer.word_count\n",
    "        df.at[index, 'Solution_comment_readability'] = text_analyzer.readability\n",
    "        df.at[index, 'Solution_comment_reading_time'] = text_analyzer.reading_time\n",
    "        df.at[index, 'Solution_comment_link_count_image'] = link_analyzer.image\n",
    "        df.at[index, 'Solution_comment_link_count_documentation'] = link_analyzer.documentation\n",
    "        df.at[index, 'Solution_comment_link_count_example'] = link_analyzer.example\n",
    "        df.at[index, 'Solution_comment_link_count_issue'] = link_analyzer.issue\n",
    "        df.at[index, 'Solution_comment_link_count_patch'] = link_analyzer.patch\n",
    "        df.at[index, 'Solution_comment_link_count_tool'] = link_analyzer.tool\n",
    "        df.at[index, 'Solution_comment_link_count_tutorial'] = link_analyzer.tutorial\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n",
      "/home/21zz42/Asset-Management-Topic-Modeling/.venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4895 tokens. Please reduce the length of the messages. on post https://github.com/awslabs/gluonts/issues/426\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 9084 tokens. Please reduce the length of the messages. on post https://github.com/huggingface/transformers/issues/13875\n",
      "persisting on post 199\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5022 tokens. Please reduce the length of the messages. on post https://github.com/rom1504/img2dataset/issues/219\n",
      "persisting on post 399\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4300 tokens. Please reduce the length of the messages. on post https://github.com/SeldonIO/seldon-core/issues/4497\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 17295 tokens. Please reduce the length of the messages. on post https://github.com/SeldonIO/seldon-core/issues/4014\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7432 tokens. Please reduce the length of the messages. on post https://github.com/SeldonIO/seldon-core/issues/3846\n",
      "This model's maximum context length is 4097 tokens. However, you requested 4105 tokens (4005 in the messages, 100 in the completion). Please reduce the length of the messages or completion. on post https://github.com/SeldonIO/seldon-core/issues/2874\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5893 tokens. Please reduce the length of the messages. on post https://github.com/SeldonIO/MLServer/issues/811\n",
      "persisting on post 599\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 13608 tokens. Please reduce the length of the messages. on post https://github.com/pycaret/pycaret/issues/3389\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7204 tokens. Please reduce the length of the messages. on post https://github.com/pycaret/pycaret/issues/3383\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4356 tokens. Please reduce the length of the messages. on post https://github.com/pycaret/pycaret/issues/2838\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6927 tokens. Please reduce the length of the messages. on post https://github.com/pycaret/pycaret/issues/931\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5094 tokens. Please reduce the length of the messages. on post https://github.com/prinz-nussknacker/prinz/issues/78\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5750 tokens. Please reduce the length of the messages. on post https://github.com/neptune-ai/kedro-neptune/issues/70\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4595 tokens. Please reduce the length of the messages. on post https://github.com/nv-morpheus/Morpheus/issues/512\n",
      "persisting on post 799\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 4630 tokens. Please reduce the length of the messages. on post https://github.com/aws/amazon-sagemaker-examples/issues/3521\n",
      "persisting on post 999\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 7513 tokens. Please reduce the length of the messages. on post https://github.com/aws/amazon-sagemaker-examples/issues/2044\n",
      "This model's maximum context length is 4097 tokens. However, you requested 4121 tokens (4021 in the messages, 100 in the completion). Please reduce the length of the messages or completion. on post https://github.com/aws/amazon-sagemaker-examples/issues/1316\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 6611 tokens. Please reduce the length of the messages. on post https://github.com/aws/amazon-sagemaker-examples/issues/702\n",
      "persisting on post 1199\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 10144 tokens. Please reduce the length of the messages. on post https://github.com/aws/deep-learning-containers/issues/2611\n",
      "This model's maximum context length is 4097 tokens. However, your messages resulted in 5710 tokens. Please reduce the length of the messages. on post https://github.com/aws/sagemaker-training-toolkit/issues/110\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 200 == 199:\n",
    "        print(f'persisting on post {index}')\n",
    "        df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n",
    "\n",
    "    # if pd.notna(row['Challenge_gpt_summary']):\n",
    "    #     continue\n",
    "    \n",
    "    try:\n",
    "        prompt = prompt_summary + 'Title: ' + row['Challenge_title'] + ' Body: ' + row['Challenge_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=100,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=50,\n",
    "            stream=False\n",
    "        )\n",
    "        df.at[index, 'Challenge_gpt_summary'] = response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f'{e} on post {row[\"Challenge_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df = pd.read_json(os.path.join(path_dataset, 'preprocessed.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clean_summary = preprocess_text(row['Challenge_gpt_summary'])\n",
    "    df.at[index, 'Challenge_preprocessed_summary'] = clean_summary\n",
    "\n",
    "df.to_json(os.path.join(path_dataset, 'preprocessed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from bertopic.backend import OpenAIBackend\n",
    "\n",
    "# # openai.api_key = MY_API_KEY\n",
    "# embedding_model = OpenAIBackend(delay_in_seconds=0.1, batch_size=10)\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "\n",
    "# docs = df[df['Challenge_summary'] != 'na']['Challenge_summary'].tolist() + df[df['Challenge_root_cause'] != 'na']['Challenge_root_cause'].tolist()\n",
    "\n",
    "# topic_model = BERTopic(embedding_model=embedding_model)\n",
    "# topics, probs = topic_model.fit_transform(docs)\n",
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def minimize_weighted_sum(df, sort_column):\n",
    "#     df_new = df.sort_values(sort_column, ascending=False)\n",
    "#     n = len(df)\n",
    "#     center_idx = (n - 1) // 2\n",
    "#     direction = -1\n",
    "#     distance = 0\n",
    "\n",
    "#     for _, row in df_new.iterrows():\n",
    "#         # Calculate the new index\n",
    "#         new_idx = center_idx + direction * distance\n",
    "        \n",
    "#         # Place the element from the sorted list into the new list\n",
    "#         df.iloc[new_idx] = row\n",
    "\n",
    "#         # If we've just moved to the left, increase the distance\n",
    "#         if direction == -1:\n",
    "#             distance += 1\n",
    "\n",
    "#         # Switch the direction\n",
    "#         direction *= -1\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Model Management - Handling and manipulation of models including training, saving, importing, and exporting.\n",
      "Topic 1: Data Pipelining - The process of managing and processing data through multiple pipelines.\n",
      "Topic 2: Package Installation - The process of installing, importing, and managing software packages using pip.\n",
      "Topic 3: Logging - The process of creating, tracking, and managing logs during model training.\n",
      "Topic 4: Docker Operations - Building, running, and managing Docker images and files.\n",
      "Topic 5: Access Management - Managing access permissions, roles, and tokens for secure operations.\n",
      "Topic 6: Data Labeling - The process of labeling data for training and object recognition.\n",
      "Topic 7: Git Operations - Managing data, files, and version control using Git.\n",
      "Topic 8: Bucket Operations - Managing files, data, and paths in storage buckets.\n",
      "Topic 9: Sweep Operations - Configuring, running, and managing multiple sweeps.\n",
      "Topic 10: Quota Management - Managing request quotas and handling limit exceptions.\n",
      "Topic 11: Remote Operations - Configuring, running, and connecting to remote files and executions.\n",
      "Topic 12: Batch Processing - Managing and processing data, files, and jobs in batches.\n",
      "Topic 13: Lambda Functions - Invoking and processing data using Lambda functions.\n",
      "Topic 14: Database Operations - Connecting, importing, and running operations on databases.\n",
      "Topic 15: Language Translation - Translating documents and languages using models.\n",
      "Topic 16: Panda Operations - Managing and converting files using Panda.\n",
      "Topic 17: Speech Processing - Handling audio files, generating speech, and transcribing services.\n",
      "Topic 18: Spark Operations - Configuring, implementing, and managing data using Spark.\n",
      "Topic 19: Instance Management - Creating, managing, and removing instances.\n",
      "Topic 20: Column Operations - Managing, cleaning, and visualizing data in columns.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description based on the stemmed words. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_rq1, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=3000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = '''Topic 0: Model Management - Handling and manipulation of models including training, saving, importing, and exporting.\n",
    "Topic 1: Data Pipelining - The process of managing and processing data through multiple pipelines.\n",
    "Topic 2: Package Installation - The process of installing, importing, and managing software packages using pip.\n",
    "Topic 3: Logging - The process of creating, tracking, and managing logs during model training.\n",
    "Topic 4: Docker Operations - Building, running, and managing Docker images and files.\n",
    "Topic 5: Access Management - Managing access permissions, roles, and tokens for secure operations.\n",
    "Topic 6: Data Labeling - The process of labeling data for training and object recognition.\n",
    "Topic 7: Git Operations - Managing data, files, and version control using Git.\n",
    "Topic 8: Bucket Operations - Managing files, data, and paths in storage buckets.\n",
    "Topic 9: Sweep Operations - Configuring, running, and managing multiple sweeps.\n",
    "Topic 10: Quota Management - Managing request quotas and handling limit exceptions.\n",
    "Topic 11: Remote Operations - Configuring, running, and connecting to remote files and executions.\n",
    "Topic 12: Batch Processing - Managing and processing data, files, and jobs in batches.\n",
    "Topic 13: Lambda Functions - Invoking and processing data using Lambda functions.\n",
    "Topic 14: Database Operations - Connecting, importing, and running operations on databases.\n",
    "Topic 15: Language Translation - Translating documents and languages using models.\n",
    "Topic 16: Panda Operations - Managing and converting files using Panda.\n",
    "Topic 17: Speech Processing - Handling audio files, generating speech, and transcribing services.\n",
    "Topic 18: Spark Operations - Configuring, implementing, and managing data using Spark.\n",
    "Topic 19: Instance Management - Creating, managing, and removing instances.\n",
    "Topic 20: Column Operations - Managing, cleaning, and visualizing data in columns.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "False\n",
      "{17, 15}\n"
     ]
    }
   ],
   "source": [
    "topic_list = [topic for topic in topics.split('\\n') if topic]\n",
    "macro_topic_mapping_inverse = {\n",
    "    '1: Observability Management': [3],\n",
    "    '2: Lifecycle Management': [1],\n",
    "    '3: Compute Management': [9, 10, 12, 13, 18],\n",
    "    '4: Environment Management': [2, 4, 19],\n",
    "    '5: Access Management': [5, 11],\n",
    "    '6: Model Management': [0],\n",
    "    '7: Data Management': [6, 8, 14, 16, 20],\n",
    "    '8: Code Management': [7],\n",
    "}\n",
    "        \n",
    "macro_topic_list = []\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_indexing = {}\n",
    "for macro_topic, sub_topics in macro_topic_mapping_inverse.items():\n",
    "    index, name = int(macro_topic.split(': ')[0]), macro_topic.split(': ')[1]\n",
    "    macro_topic_indexing[index] = name\n",
    "    macro_topic_list.extend(sub_topics)\n",
    "    for topic in sub_topics:\n",
    "        macro_topic_mapping[topic] = macro_topic\n",
    "\n",
    "print(find_duplicates(macro_topic_list))\n",
    "print(len(macro_topic_list) == len(topic_list))\n",
    "print(set(range(len(topic_list))).difference(set(macro_topic_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "# df = pd.read_json(os.path.join(path_special_output, 'labels.json'))\n",
    "# df['Challenge_topic_macro'] = -1\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Challenge_topic'] in macro_topic_mapping:\n",
    "#         df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "#     else:\n",
    "#         df.drop(index, inplace=True)\n",
    "\n",
    "# df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Topic & Percentage & Number \\\\\n",
      "\\midrule\n",
      "Model Management & 21.39 & 2378 \\\\\n",
      "Compute Management & 20.05 & 2229 \\\\\n",
      "Environment Management & 17.90 & 1990 \\\\\n",
      "Data Management & 13.13 & 1460 \\\\\n",
      "Lifecycle Management & 9.94 & 1105 \\\\\n",
      "Access Management & 7.84 & 872 \\\\\n",
      "Observability Management & 6.65 & 739 \\\\\n",
      "Code Management & 3.09 & 344 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'topics.json'))\n",
    "df['Challenge_topic_macro'] = -1\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Challenge_topic'] in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = int(macro_topic_mapping[row['Challenge_topic']].split(':')[0])\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_rq1, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "\n",
    "for name, group in df.groupby('Challenge_topic_macro'):\n",
    "    entry = {\n",
    "        'Topic': macro_topic_indexing[name],\n",
    "        'Percentage': round(len(group)/len(df)*100, 2),\n",
    "        'Number': len(group),\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "\n",
    "df_number = df_number.sort_values('Percentage', ascending=False)\n",
    "print(df_number.to_latex(float_format=\"%.2f\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_rq1, 'filtered.json'))\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "df['Challenge_topic_macro'] = df['Challenge_topic_macro'].apply(lambda x: macro_topic_indexing[x])\n",
    "categories = ['Challenge_type', 'Challenge_topic_macro', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_rq1, 'State type topic sankey.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
