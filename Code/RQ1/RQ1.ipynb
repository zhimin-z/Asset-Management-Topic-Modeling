{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import openai\n",
    "import textstat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The significance level indicates the probability of rejecting the null hypothesis when it is true.\n",
    "alpha = 0.05\n",
    "\n",
    "link_pattern = r'https?://[^\\s]+'\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY', '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')\n",
    "\n",
    "keywords_patch = {\n",
    "    'pull',\n",
    "}\n",
    "\n",
    "keywords_issue = {\n",
    "    'answers',\n",
    "    'discussions',\n",
    "    'forums',\n",
    "    'issues',\n",
    "    'questions',\n",
    "    'stackoverflow',\n",
    "}\n",
    "\n",
    "keywords_tool = {\n",
    "    'github',\n",
    "    'gitlab',\n",
    "    'pypi',\n",
    "}\n",
    "\n",
    "keywords_doc = {\n",
    "    'developers',\n",
    "    'docs',\n",
    "    'documentation',\n",
    "    'features',\n",
    "    'library',\n",
    "    'org',\n",
    "    'wiki',\n",
    "}\n",
    "\n",
    "keywords_tutorial = {\n",
    "    'guide',\n",
    "    'learn',\n",
    "    'tutorial',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_result = '../../Result'\n",
    "\n",
    "path_result_rq1 = os.path.join(path_result, 'RQ1')\n",
    "path_code_rq1 = os.path.join('..', 'RQ1')\n",
    "\n",
    "path_general_output = os.path.join(path_result_rq1, 'General Topic')\n",
    "path_special_output = os.path.join(path_result_rq1, 'Special Topic')\n",
    "\n",
    "path_general_topic = os.path.join(path_code_rq1, 'General Topic Modeling')\n",
    "path_special_topic = os.path.join(path_code_rq1, 'Special Topic Modeling')\n",
    "\n",
    "path_anomaly = os.path.join(path_special_topic, 'Anomaly')\n",
    "path_root_cause = os.path.join(path_special_topic, 'Root Cause')\n",
    "path_solution = os.path.join(path_special_topic, 'Solution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Data Pipelines - Processes and tools for building, running, and managing data pipelines.\n",
      "Topic 1: Docker - Techniques and commands for creating and managing Docker containers and environments.\n",
      "Topic 2: Role and Permissions - Management of roles and permissions in a software system, particularly in IAM.\n",
      "Topic 3: Version Control with Git - Use of Git for version control of files and repositories.\n",
      "Topic 4: TensorFlow - Building, running, and managing models using TensorFlow.\n",
      "Topic 5: Data Storage and Datasets - Handling of datasets and data storage, including data lakes.\n",
      "Topic 6: GPU Utilization - Monitoring and managing GPU utilization, particularly in PyTorch and CUDA.\n",
      "Topic 7: Artifacts - Management of artifacts in a software system, including storage and access.\n",
      "Topic 8: Libraries and Packages - Installation and management of software libraries and packages.\n",
      "Topic 9: Kubernetes - Deployment and management of services in a Kubernetes cluster.\n",
      "Topic 10: YAML and Pipelines - Use of YAML for configuring and structuring pipelines.\n",
      "Topic 11: Jupyter Notebooks - Running and managing Jupyter notebooks.\n",
      "Topic 12: Datasets - Manipulation and management of datasets, including splitting and combining.\n",
      "Topic 13: Logging - Configuration and management of logging in a software system.\n",
      "Topic 14: PySpark - Running and managing PySpark jobs and dataframes.\n",
      "Topic 15: Forecasting with AutoML - Use of AutoML for forecasting based on datasets.\n",
      "Topic 16: Models and Files - Handling of model files, including extraction and import.\n",
      "Topic 17: VPC and Connections - Configuration and management of VPC endpoints and connections.\n",
      "Topic 18: Model Training - Training and updating models, particularly with AutoML and sklearn.\n",
      "Topic 19: Plotting - Creation of various types of plots for data visualization.\n",
      "Topic 20: API Endpoints - Configuration and invocation of API endpoints.\n",
      "Topic 21: Pip Installation - Installation and management of pip, including version control.\n",
      "Topic 22: Regression - Use of regression models and evaluation of datasets.\n",
      "Topic 23: Predictions - Making predictions using API endpoints and data.\n",
      "Topic 24: Database Connections - Connecting to and managing databases, including SQL and MySQL.\n",
      "Topic 25: Model Deployment - Deployment and management of models in a machine learning project.\n",
      "Topic 26: Notebook Execution - Execution and management of notebooks.\n",
      "Topic 27: Buckets and Directories - Management of buckets and directories, including uploads and permissions.\n",
      "Topic 28: Hyperparameter Sweeping - Configuration and execution of hyperparameter sweeps.\n",
      "Topic 29: Training - Training models and handling training arguments.\n",
      "Topic 30: Multi-Model Endpoints - Management of endpoints that host multiple models.\n",
      "Topic 31: Workspace Connections - Connecting to and managing workspaces.\n",
      "Topic 32: PyTorch Logging - Logging in PyTorch, including loss tracking.\n",
      "Topic 33: Dependencies - Handling of dependencies and conflicts in software packages.\n",
      "Topic 34: Visual Studio Code - Use and management of Visual Studio Code.\n",
      "Topic 35: CSV and Pandas - Handling of CSV data with pandas, including reading and writing.\n",
      "Topic 36: Hyperparameter Tuning - Tuning of hyperparameters in a machine learning model.\n",
      "Topic 37: Vision APIs - Use of vision APIs for image recognition and processing.\n",
      "Topic 38: TensorBoard - Use and configuration of TensorBoard for visualizing machine learning models.\n",
      "Topic 39: Conda - Use and management of Conda environments.\n",
      "Topic 40: Clusters - Running and managing computational clusters.\n",
      "Topic 41: Research and Collaboration - Collaboration and contribution in research and development.\n",
      "Topic 42: Endpoint Scaling - Scaling and provisioning of API endpoints.\n",
      "Topic 43: Web Services - Deployment and management of web services.\n",
      "Topic 44: Speech Processing - Use of speech APIs for transcription and synthesis.\n",
      "Topic 45: Error Handling - Handling of common errors in model prediction.\n",
      "Topic 46: Dialogflow - Configuration and use of Dialogflow for conversational AI.\n",
      "Topic 47: Language Translation - Use of translation APIs for language translation.\n",
      "Topic 48: Documentation and Releases - Updating and releasing documentation for software versions.\n",
      "Topic 49: Directory Refactoring - Refactoring and management of directories and files.\n",
      "Topic 50: Execution Speed - Management of execution speed and time in software processes.\n",
      "Topic 51: Scheduling - Scheduling of jobs and executions in a software system.\n",
      "Topic 52: Logging and Metrics - Logging and fetching of metrics in a software system.\n",
      "Topic 53: Data Labeling - Processing and labeling of data for machine learning.\n",
      "Topic 54: CSV Processing - Processing and handling of CSV data.\n",
      "Topic 55: Convolutional Neural Networks - Training and use of convolutional neural networks (CNNs).\n",
      "Topic 56: PyTorch and CNNs - Use of PyTorch for training CNNs and GANs.\n",
      "Topic 57: Flask - Use and management of the Flask web framework.\n",
      "Topic 58: Data Transformation - Transformation and conversion of data for machine learning.\n",
      "Topic 59: File Uploads and Syncing - Uploading and syncing of files and artifacts.\n",
      "Topic 60: Document Processing - Processing and extraction of information from documents.\n",
      "Topic 61: Model Deployment and Schemas - Deployment of models and definition of schemas.\n",
      "Topic 62: Object Detection - Use of models for object detection in videos.\n",
      "Topic 63: PyTorch Environments - Management and use of PyTorch environments.\n",
      "Topic 64: Quotas - Management of quotas in a software system.\n",
      "Topic 65: Activity Panels and Computing - Management of activity panels and computing resources.\n",
      "Topic 66: Model Compilation - Compilation of models, particularly with Neo.\n",
      "Topic 67: Web Service Deployment - Deployment and management of web services.\n",
      "Topic 68: Disk Space - Management of disk space and quotas.\n",
      "Topic 69: Synchronization and Uploads - Synchronization and uploading of files and artifacts.\n",
      "Topic 70: Document Review - Review and processing of documents.\n",
      "Topic 71: Model Deployment and Services - Deployment of models and services.\n",
      "Topic 72: Object Detection - Detection and tracking of objects in videos.\n",
      "Topic 73: PyTorch Environments - Use and management of PyTorch environments.\n",
      "Topic 74: Quotas - Management of quotas in a software system.\n",
      "Topic 75: Activity Management - Management of activities and computing resources.\n",
      "Topic 76: Model Compilation - Compilation of models, particularly with Neo.\n",
      "Topic 77: Web Service Deployment - Deployment and management of web services.\n",
      "Topic 78: Disk Space - Management of disk space and quotas.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of stemmed words refering to specific software engineering topics. Please summarize each topic in terms and attach a one-liner description. Also, you must guarantee that the summaries are exclusive to one another.###\\n'''\n",
    "\n",
    "with open(os.path.join(path_general_topic, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "    topic_term_list = []\n",
    "    for index, topic in enumerate(topic_terms):\n",
    "        terms = ', '.join([term[0] for term in topic])\n",
    "        topic_term = f'Topic {index}: {terms}]'\n",
    "        topic_term_list.append(topic_term)\n",
    "\n",
    "    prompt = prompt_topic + '\\n'.join(topic_term_list) + '\\n###\\n'\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model='gpt-4',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        timeout=300,\n",
    "        stream=False)\n",
    "\n",
    "    topics = completion.choices[0].message.content\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 'NA',\n",
       " 0: 'Pipeline Management',\n",
       " 1: 'Docker Configuration',\n",
       " 2: 'Role Configuration',\n",
       " 3: 'Git Version Control',\n",
       " 4: 'TensorFlow Configuration',\n",
       " 5: 'Data Storage and Datasets',\n",
       " 6: 'GPU Utilization',\n",
       " 7: 'Artifact Management',\n",
       " 8: 'Package Management',\n",
       " 9: 'Kubernetes Service',\n",
       " 10: 'YAML Configuration',\n",
       " 11: 'Jupyter Configuration',\n",
       " 12: 'Dataset Management',\n",
       " 13: 'Log Management',\n",
       " 14: 'Spark Configuration',\n",
       " 15: 'AutoML Forecasting',\n",
       " 16: 'Model Storage and Models',\n",
       " 17: 'VPC and Connections',\n",
       " 18: 'Model Training',\n",
       " 19: 'Data Visualization',\n",
       " 20: 'Endpoint Invocation',\n",
       " 21: 'Pip Environment',\n",
       " 22: 'Regression',\n",
       " 23: 'Predictions and Inferences',\n",
       " 24: 'Database Management',\n",
       " 25: 'Model Deployment',\n",
       " 26: 'Notebook Operations',\n",
       " 27: 'Online Storage and Buckets',\n",
       " 28: 'Hyperparameter Sweeping',\n",
       " 29: 'Training Session',\n",
       " 30: 'Multi',\n",
       " 31: 'Workspace Configuration',\n",
       " 32: 'PyTorch and Logging',\n",
       " 33: 'Dependency Management',\n",
       " 34: 'VSCode Configuration',\n",
       " 35: 'Tabular Data Manipulation',\n",
       " 36: 'Hyperparameter Tuning',\n",
       " 37: 'Computer Vision',\n",
       " 38: 'TensorBoard Visualization',\n",
       " 39: 'Conda Environment',\n",
       " 40: 'Cluster Management',\n",
       " 41: 'Research and Collaboration',\n",
       " 42: 'Endpoint Request',\n",
       " 43: 'Web Services',\n",
       " 44: 'Speech Processing',\n",
       " 45: 'Error Handling',\n",
       " 46: 'Dialogflow Configuration',\n",
       " 47: 'Language Translation',\n",
       " 48: 'Documentation and Releases',\n",
       " 49: 'Directory Management',\n",
       " 50: 'Execution Speed',\n",
       " 51: 'Scheduling Management',\n",
       " 52: 'Logging and Metrics',\n",
       " 53: 'Data Labeling',\n",
       " 54: 'CSV Processing',\n",
       " 55: 'Convolutional Neural Networks',\n",
       " 56: 'Neural Network Design',\n",
       " 57: 'Flask Configuration',\n",
       " 58: 'Data Transformation',\n",
       " 59: 'File Syncing',\n",
       " 60: 'Document Processing',\n",
       " 61: 'Schema Configuration',\n",
       " 62: 'Object Detection',\n",
       " 63: 'PyTorch Configuration',\n",
       " 64: 'Quota Management',\n",
       " 65: 'Activity Panels and Computing',\n",
       " 66: 'Model Compilation',\n",
       " 67: 'Web Service Deployment',\n",
       " 68: 'Disk Management'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = '''Topic 0: Pipeline Management - Processes and tools for building, running, and managing data pipelines.\n",
    "Topic 1: Docker Configuration - Techniques and commands for creating and managing Docker containers and environments.\n",
    "Topic 2: Role Configuration - Management of roles and permissions in a software system, particularly in IAM.\n",
    "Topic 3: Git Version Control - Use of Git for version control of files and repositories.\n",
    "Topic 4: TensorFlow Configuration - Building, running, and managing models using TensorFlow.\n",
    "Topic 5: Data Storage and Datasets - Handling of datasets and data storage, including data lakes.\n",
    "Topic 6: GPU Utilization - Monitoring and managing GPU utilization, particularly in PyTorch and CUDA.\n",
    "Topic 7: Artifact Management - Management of artifacts in a software system, including storage and access.\n",
    "Topic 8: Package Management - Installation and management of software libraries and packages.\n",
    "Topic 9: Kubernetes Service - Deployment and management of services in a Kubernetes cluster.\n",
    "Topic 10: YAML Configuration - Use of YAML for configuring and structuring pipelines.\n",
    "Topic 11: Jupyter Configuration - Running and managing Jupyter notebooks.\n",
    "Topic 12: Dataset Management - Manipulation and management of datasets, including splitting and combining.\n",
    "Topic 13: Log Management - Configuration and management of logging in a software system.\n",
    "Topic 14: Spark Configuration - Running and managing PySpark jobs and dataframes.\n",
    "Topic 15: AutoML Forecasting - Use of AutoML for forecasting based on datasets.\n",
    "Topic 16: Model Storage and Models - Handling of model files, including extraction and import.\n",
    "Topic 17: VPC and Connections - Configuration and management of VPC endpoints and connections.\n",
    "Topic 18: Model Training - Training and updating models, particularly with AutoML and sklearn.\n",
    "Topic 19: Data Visualization - Creation of various types of plots for data visualization.\n",
    "Topic 20: Endpoint Invocation - Configuration and invocation of API endpoints.\n",
    "Topic 21: Pip Environment - Installation and management of pip, including version control.\n",
    "Topic 22: Regression - Use of regression models and evaluation of datasets.\n",
    "Topic 23: Predictions and Inferences - Making predictions using API endpoints and data.\n",
    "Topic 24: Database Management - Connecting to and managing databases, including SQL and MySQL.\n",
    "Topic 25: Model Deployment - Deployment and management of models in a machine learning project.\n",
    "Topic 26: Notebook Operations - Execution and management of notebooks.\n",
    "Topic 27: Online Storage and Buckets - Management of buckets and directories, including uploads and permissions.\n",
    "Topic 28: Hyperparameter Sweeping - Configuration and execution of hyperparameter sweeps.\n",
    "Topic 29: Training Session - Training models and handling training arguments.\n",
    "Topic 30: Multi-Model Endpoints - Management of endpoints that host multiple models.\n",
    "Topic 31: Workspace Configuration - Connecting to and managing workspaces.\n",
    "Topic 32: PyTorch and Logging - Logging in PyTorch, including loss tracking.\n",
    "Topic 33: Dependency Management - Handling of dependencies and conflicts in software packages.\n",
    "Topic 34: VSCode Configuration - Use and management of Visual Studio Code.\n",
    "Topic 35: Tabular Data Manipulation - Handling of CSV data with pandas, including reading and writing.\n",
    "Topic 36: Hyperparameter Tuning - Tuning of hyperparameters in a machine learning model.\n",
    "Topic 37: Computer Vision - Use of vision APIs for image recognition and processing.\n",
    "Topic 38: TensorBoard Visualization - Use and configuration of TensorBoard for visualizing machine learning models.\n",
    "Topic 39: Conda Environment - Use and management of Conda environments.\n",
    "Topic 40: Cluster Management - Running and managing computational clusters.\n",
    "Topic 41: Research and Collaboration - Collaboration and contribution in research and development.\n",
    "Topic 42: Endpoint Request - Scaling and provisioning of API endpoints.\n",
    "Topic 43: Web Services - Deployment and management of web services.\n",
    "Topic 44: Speech Processing - Use of speech APIs for transcription and synthesis.\n",
    "Topic 45: Error Handling - Handling of common errors in model prediction.\n",
    "Topic 46: Dialogflow Configuration - Configuration and use of Dialogflow for conversational AI.\n",
    "Topic 47: Language Translation - Use of translation APIs for language translation.\n",
    "Topic 48: Documentation and Releases - Updating and releasing documentation for software versions.\n",
    "Topic 49: Directory Management - Refactoring and management of directories and files.\n",
    "Topic 50: Execution Speed - Management of execution speed and time in software processes.\n",
    "Topic 51: Scheduling Management - Scheduling of jobs and executions in a software system.\n",
    "Topic 52: Logging and Metrics - Logging and fetching of metrics in a software system.\n",
    "Topic 53: Data Labeling - Processing and labeling of data for machine learning.\n",
    "Topic 54: CSV Processing - Processing and handling of CSV data.\n",
    "Topic 55: Convolutional Neural Networks - Training and use of convolutional neural networks (CNNs).\n",
    "Topic 56: Neural Network Design - Use of PyTorch for training CNNs and GANs.\n",
    "Topic 57: Flask Configuration - Use and management of the Flask web framework.\n",
    "Topic 58: Data Transformation - Transformation and conversion of data for machine learning.\n",
    "Topic 59: File Syncing - Uploading and syncing of files and artifacts.\n",
    "Topic 60: Document Processing - Processing and extraction of information from documents.\n",
    "Topic 61: Schema Configuration - Deployment of models and definition of schemas.\n",
    "Topic 62: Object Detection - Use of models for object detection in videos.\n",
    "Topic 63: PyTorch Configuration - Management and use of PyTorch environments.\n",
    "Topic 64: Quota Management - Management of quotas in a software system.\n",
    "Topic 65: Activity Panels and Computing - Management of activity panels and computing resources.\n",
    "Topic 66: Model Compilation - Compilation of models, particularly with Neo.\n",
    "Topic 67: Web Service Deployment - Deployment and management of web services.\n",
    "Topic 68: Disk Management - Management of disk space and quotas.'''\n",
    "\n",
    "index = 0\n",
    "topic_dict = {-1: 'NA'}\n",
    "\n",
    "for topic in topics.split('\\n'):\n",
    "    topic_dict[index] = topic.split('-')[0].split(':')[-1].strip()\n",
    "    index += 1\n",
    "    \n",
    "topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_topic_mapping_inverse = {\n",
    "    # These topics are all related to the management of parallel computing resources.\n",
    "    1: ('Compute Management', ['Cluster Computing', 'GPU Acceleration', 'Model Training', 'Spark Configuration', 'TensorFlow Configuration']),\n",
    "    # These topics are all related to the management of logs and metrics.\n",
    "    2: ('Performance Management', ['Hyperparameter Optimization', 'Quota Management', 'Logging', 'Tensorboard Logging']),\n",
    "    # These topics are all related to the management of services.\n",
    "    3: ('Service Management', ['Flask Configuration', 'Lambda Endpoint', 'Model Serving', 'Web Serving']),\n",
    "    # These topics are all related to the management of data and datasets.\n",
    "    4: ('Data Management', ['Artifact Management', 'Columnar Data', 'Data Labeling', 'Data Visualization', 'Database Connection', 'Datastore', 'File Management', 'JSON File', 'Pandas Dataframe']),\n",
    "    # These topics are all related to the management of machine learning models.\n",
    "    5: ('Model Management', ['Model Deployment', 'Model Development', 'Model Training', 'Model Troubleshooting', 'Model Versioning', 'ModelChimp Configuration', 'PyTorch Model Development', 'TensorFlow Model Development']),\n",
    "    # These topics are all related to the management of packages and distributions.\n",
    "    6: ('Infrastructure Management', ['Docker Deployment', 'Jupyter Configuration', 'Kubernetes Deployment', 'Notebook Lifecycle Management', 'Package Installation', 'Terraform Configuration', 'Workspace Management', 'YAML Configuration']),\n",
    "    # These topics are all related to the management of pipelines.\n",
    "    7: ('Lifecycle Management', ['Job Management', 'Pipeline Configuration']),\n",
    "    # These topics are all related to the management of permissions and connectivity.\n",
    "    8: ('Access Management', ['Account Management', 'Database Connection', 'Role Management', 'SSH Connection']),\n",
    "    # These topics are all related to the management of source code.\n",
    "    9: ('Code Management', ['Git Versioning']),\n",
    "}\n",
    "\n",
    "macro_topic_mapping = {}\n",
    "macro_topic_index_mapping = {}\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    macro_topic_index_mapping[macro_topic] = key\n",
    "    for topic in topics:\n",
    "        macro_topic_mapping[topic] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if exiting topics and chosen topics are the same\n",
    "\n",
    "# chosen_topics = set()\n",
    "# for topic in topic_mapping:\n",
    "#     chosen_topics.add(topic_mapping[topic][0])\n",
    "# existing_topics = set(macro_topic_mapping.keys())\n",
    "\n",
    "# print(len(topic_mapping) == len(macro_topic_mapping))\n",
    "# print(existing_topics.difference(chosen_topics))\n",
    "# print(chosen_topics.difference(existing_topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compute Management</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Performance Management</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Service Management</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model Management</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Infrastructure Management</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lifecycle Management</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Access Management</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Code Management</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Topic  Number\n",
       "0         Compute Management     566\n",
       "1     Performance Management     708\n",
       "2         Service Management    1060\n",
       "3            Data Management    1752\n",
       "4           Model Management    2268\n",
       "5  Infrastructure Management    1655\n",
       "6       Lifecycle Management     722\n",
       "7          Access Management     710\n",
       "8            Code Management     241"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general_output, 'topics.json'))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if topic_dict(row['Challenge_topic']) in macro_topic_mapping:\n",
    "        df.at[index, 'Challenge_topic_macro'] = macro_topic_mapping[topic_dict(row['Challenge_topic'])]\n",
    "    else:\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "df.to_json(os.path.join(path_general_output, 'filtered.json'), indent=4, orient='records')\n",
    "\n",
    "df_number = pd.DataFrame()\n",
    "for key, (macro_topic, topics) in macro_topic_mapping_inverse.items():\n",
    "    entry = {\n",
    "        'Topic': macro_topic,\n",
    "        'Number': len(df[df[\"Challenge_topic_macro\"] == macro_topic])\n",
    "    }\n",
    "    df_number = pd.concat([df_number, pd.DataFrame([entry])], ignore_index=True)\n",
    "df_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_general_output, 'filtered.json'))\n",
    "\n",
    "df['Challenge_type'] = 'na'\n",
    "df['Challenge_summary'] = 'na'\n",
    "df['Challenge_root_cause'] = 'na'\n",
    "df['Challenge_solution'] = 'na'\n",
    "\n",
    "df.to_json(os.path.join(path_special_output, 'labels.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw sankey diagram of tool and platform\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['State'] = df['Challenge_closed_time'].apply(lambda x: 'closed' if not pd.isna(x) else 'open')\n",
    "\n",
    "categories = ['Platform', 'Tool', 'State']\n",
    "df_info = df.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "labels = {}\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)):\n",
    "    labels.update(df[categories[i]].value_counts().to_dict())\n",
    "    if i == len(categories)-1:\n",
    "        break\n",
    "    tempDf = df_info[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "    \n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "source = newDf['source'].apply(lambda x: list(labels).index(x))\n",
    "target = newDf['target'].apply(lambda x: list(labels).index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "labels = [f'{k} ({v})' for k, v in labels.items()]\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=labels)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(width=1000, height=1000, font_size=20)\n",
    "fig.write_image(os.path.join(path_result_rq1, 'Tool platform state sankey.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add different metrics to each post\n",
    "\n",
    "df = pd.read_json(os.path.join(path_special_output, 'topics.json'))\n",
    "df = df[df['Challenge_type'] != 'na']\n",
    "\n",
    "df['Solution_word_count'] = np.nan\n",
    "df['Solution_sentence_count'] = np.nan\n",
    "df['Solution_readability'] = np.nan\n",
    "df['Solution_reading_time'] = np.nan\n",
    "df['Solution_link_count'] = np.nan\n",
    "\n",
    "df['Challenge_solved_time'] = np.nan\n",
    "df['Challenge_adjusted_solved_time'] = np.nan\n",
    "\n",
    "df['Solution_link_docs'] = np.nan\n",
    "df['Solution_link_issues'] = np.nan\n",
    "df['Solution_link_patches'] = np.nan\n",
    "df['Solution_link_tools'] = np.nan\n",
    "df['Solution_link_tutorials'] = np.nan\n",
    "df['Solution_link_examples'] = np.nan\n",
    "\n",
    "df['Challenge_created_time'] = pd.to_datetime(df['Challenge_created_time'])\n",
    "df['Challenge_closed_time'] = pd.to_datetime(df['Challenge_closed_time'])\n",
    "df['Challenge_last_edit_time'] = pd.to_datetime(df['Challenge_last_edit_time'])\n",
    "df['Solution_last_edit_time'] = pd.to_datetime(df['Solution_last_edit_time'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    challenge_content = row['Challenge_title'] + '.' + str(row['Challenge_body'])\n",
    "    df.at[index, 'Challenge_word_count'] = textstat.lexicon_count(challenge_content)\n",
    "    df.at[index, 'Challenge_sentence_count'] = textstat.sentence_count(challenge_content)\n",
    "    df.at[index, 'Challenge_readability'] = textstat.flesch_kincaid_grade(challenge_content)\n",
    "    df.at[index, 'Challenge_reading_time'] = textstat.reading_time(challenge_content)\n",
    "    \n",
    "    links = list(set(re.findall(link_pattern, challenge_content)))\n",
    "    df.at[index, 'Challenge_link_count'] = len(links)\n",
    "\n",
    "    solution_content = row['Solution_body']\n",
    "\n",
    "    if pd.notna(solution_content):\n",
    "        df.at[index, 'Solution_word_count'] = textstat.lexicon_count(solution_content)\n",
    "        df.at[index, 'Solution_sentence_count'] = textstat.sentence_count(solution_content)\n",
    "        df.at[index, 'Solution_readability'] = textstat.flesch_kincaid_grade(solution_content)\n",
    "        df.at[index, 'Solution_reading_time'] = textstat.reading_time(solution_content)\n",
    "        \n",
    "        links = list(set(re.findall(link_pattern, solution_content)))\n",
    "        df.at[index, 'Solution_link_count'] = len(links)\n",
    "        \n",
    "        link_docs = 0\n",
    "        link_tools = 0\n",
    "        link_issues = 0\n",
    "        link_patches = 0\n",
    "        link_tutorials = 0\n",
    "        link_examples = 0\n",
    "    \n",
    "        for link in links:\n",
    "            if any([patch in link for patch in keywords_patch]):\n",
    "                link_patches += 1\n",
    "            elif any([issue in link for issue in keywords_issue]):\n",
    "                link_issues += 1\n",
    "            elif any([tool in link for tool in keywords_tool]):\n",
    "                link_tools += 1\n",
    "            elif any([doc in link for doc in keywords_doc]):\n",
    "                link_docs += 1\n",
    "            elif any([tool in link for tool in keywords_tutorial]):\n",
    "                link_tutorials += 1\n",
    "            else:\n",
    "                link_examples += 1\n",
    "                \n",
    "        df.at[index, 'Solution_link_docs'] = link_docs\n",
    "        df.at[index, 'Solution_link_issues'] = link_issues\n",
    "        df.at[index, 'Solution_link_patches'] = link_patches\n",
    "        df.at[index, 'Solution_link_tools'] = link_tools\n",
    "        df.at[index, 'Solution_link_tutorials'] = link_tutorials\n",
    "        df.at[index, 'Solution_link_examples'] = link_examples\n",
    "        \n",
    "    creation_time = row['Challenge_created_time']\n",
    "    closed_time = row['Challenge_closed_time']\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "\n",
    "    creation_time = row['Challenge_last_edit_time'] if pd.notna(row['Challenge_last_edit_time']) else creation_time\n",
    "    closed_time = row['Solution_last_edit_time'] if pd.notna(row['Solution_last_edit_time']) else closed_time\n",
    "    if pd.notna(creation_time) and pd.notna(closed_time) and (closed_time > creation_time):\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = (closed_time - creation_time) / pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df.at[index, 'Challenge_adjusted_solved_time'] = df.at[index, 'Challenge_solved_time']\n",
    "\n",
    "df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "df['Challenge_answer_count'] = df['Challenge_answer_count'].fillna(0)\n",
    "df['Challenge_participation_count'] = df['Challenge_answer_count'] + df['Challenge_comment_count']\n",
    "\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_json(os.path.join(path_result_rq1, 'metrics.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Count'] = 1\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_topic_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge view count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_view_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_view_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge score count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_score_count'] = df_topics['Challenge_score_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_score_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_score_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge favorite count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_favorite_count'] = df_topics['Challenge_favorite_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_favorite_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_favorite_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge answer count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_answer_count'] = df_topics['Challenge_answer_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_answer_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_answer_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge comment count distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_comment_count'] = df_topics['Challenge_comment_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_comment_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_comment_count_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_result_rq1, 'metrics.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(\n",
    "    macro_topic_index_mapping)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map(\n",
    "    {True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_participation_count'].map(\n",
    "    lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Platform', 'Solved', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Tool', 'Platform', 'Solved'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    color_continuous_scale='RdBu',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel,\n",
    ")\n",
    "fig = fig.update_layout(\n",
    "    width=1500,\n",
    "    height=750,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(\n",
    "    path_result_rq1, 'Challenge_participation_count_distribution.png'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
