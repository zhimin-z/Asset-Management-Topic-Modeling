{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import shapiro\n",
    "from plotly.colors import n_colors\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "path_result = os.path.join(os.path.dirname(os.getcwd()), 'Result')\n",
    "if not os.path.exists(path_result):\n",
    "    os.makedirs(path_result)\n",
    "\n",
    "path_general = os.path.join(path_result, 'General')\n",
    "if not os.path.exists(path_general):\n",
    "    os.makedirs(path_general)\n",
    "\n",
    "path_challenge = os.path.join(path_result, 'Challenge')\n",
    "if not os.path.exists(path_challenge):\n",
    "    os.makedirs(path_challenge)\n",
    "\n",
    "path_solution = os.path.join(path_result, 'Solution')\n",
    "if not os.path.exists(path_solution):\n",
    "    os.makedirs(path_solution)\n",
    "\n",
    "path_challenge_git_qa = os.path.join(path_challenge, 'Git vs QA')\n",
    "if not os.path.exists(path_challenge_git_qa):\n",
    "    os.makedirs(path_challenge_git_qa)\n",
    "\n",
    "path_challenge_open_closed = os.path.join(path_challenge, 'Open vs Closed')\n",
    "if not os.path.exists(path_challenge_open_closed):\n",
    "    os.makedirs(path_challenge_open_closed)\n",
    "\n",
    "path_challenge_so_to = os.path.join(\n",
    "    path_challenge, 'Stack Overflow vs Tool-specific')\n",
    "if not os.path.exists(path_challenge_so_to):\n",
    "    os.makedirs(path_challenge_so_to)\n",
    "\n",
    "path_challenge_azureml_sagemaker = os.path.join(\n",
    "    path_challenge, 'AzureML vs SageMaker')\n",
    "if not os.path.exists(path_challenge_azureml_sagemaker):\n",
    "    os.makedirs(path_challenge_azureml_sagemaker)\n",
    "\n",
    "path_challenge_evolution = os.path.join(path_challenge, 'Evolution')\n",
    "if not os.path.exists(path_challenge_evolution):\n",
    "    os.makedirs(path_challenge_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The significance level is the probability of rejecting the null hypothesis when it is true.\n",
    "alpha = 0.01\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\",\n",
    "              None, 'display.max_colwidth', None)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Environment Setup - Setting up software environments for development and execution\n",
      "Topic 1: Pipeline Automation - Automating the execution of data processing pipelines\n",
      "Topic 2: Docker - Containerization platform for building, shipping, and running applications\n",
      "Topic 3: Hyperparameter Tuning - Optimizing model performance by tuning hyperparameters\n",
      "Topic 4: Git Version Control - Tracking changes to code and collaborating with others\n",
      "Topic 5: GPU Acceleration - Using graphics processing units to speed up machine learning tasks\n",
      "Topic 6: Artifact Management - Managing and storing artifacts such as models, datasets, and code\n",
      "Topic 7: Model Deployment - Deploying machine learning models for use in production environments\n",
      "Topic 8: Data Labeling - Assigning labels to data for use in supervised learning tasks\n",
      "Topic 9: Data Visualization - Creating visual representations of data for analysis and communication\n",
      "Topic 10: Logging Metrics - Recording and tracking performance metrics during model training and evaluation\n",
      "Topic 11: Account Management - Managing user accounts and access to resources\n",
      "Topic 12: Apache Spark - Open-source distributed computing system for big data processing\n",
      "Topic 13: TensorFlow - Open-source machine learning framework for building and training models\n",
      "Topic 14: Text Processing - Analyzing and manipulating text data\n",
      "Topic 15: Pandas DataFrames - Data structure for manipulating and analyzing tabular data\n",
      "Topic 16: Model Export - Saving and exporting trained machine learning models\n",
      "Topic 17: Role-Based Access Control - Controlling access to resources based on user roles and permissions\n",
      "Topic 18: Batch Processing - Processing large amounts of data in batches\n",
      "Topic 19: Model Registry - Managing and versioning machine learning models\n",
      "Topic 20: Database Connectivity - Connecting to and interacting with databases\n",
      "Topic 21: Resource Limitations - Setting and managing limits on resource usage\n",
      "Topic 22: API Invocation - Calling APIs to perform tasks or retrieve data\n",
      "Topic 23: AutoML Forecasting - Using automated machine learning to generate forecasts\n",
      "Topic 24: Column Manipulation - Working with and manipulating columns in datasets\n",
      "Topic 25: Computer Vision - Using machine learning to analyze and interpret visual data\n",
      "Topic 26: Web Service Deployment - Deploying machine learning models as web services\n",
      "Topic 27: Kubernetes - Open-source container orchestration platform for managing containerized applications\n",
      "Topic 28: Random Forest - Ensemble learning method for classification and regression tasks\n",
      "Topic 29: CSV Files - File format for storing and exchanging tabular data\n",
      "Topic 30: TensorBoard Logging - Visualizing and tracking model training and evaluation using TensorBoard\n",
      "Topic 31: Feature Roadmap - Planning and implementing new features for a platform or product\n",
      "Topic 32: Dataset Versioning - Managing and versioning datasets\n",
      "Topic 33: CloudWatch Logging - Monitoring and logging AWS resources and applications\n",
      "Topic 34: Speech-to-Text - Converting audio speech to text\n",
      "Topic 35: YAML Configuration - Using YAML files to configure applications and services\n",
      "Topic 36: Data Storage - Storing and accessing data in cloud-based storage solutions\n",
      "Topic 37: VPC Endpoints - Connecting to AWS services privately through a VPC\n",
      "Topic 38: Model Accuracy - Evaluating and improving the accuracy of machine learning models\n",
      "Topic 39: Model Input - Preparing and querying input data for machine learning models\n",
      "Topic 40: Bucket Access - Managing access to cloud-based storage buckets\n",
      "Topic 41: Run Management - Managing and monitoring the execution of jobs and tasks\n",
      "Topic 42: Model Prediction - Using trained machine learning models to make predictions\n",
      "Topic 43: Notebook Instances - Creating and managing cloud-based notebook instances for data analysis and experimentation\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of keywords for each topic, I want you to provide a description of each topic in a two-word phrase but guarantee that each description is exclusive to the other. Also, for each description, you need to attach short comments on what these keywords are talking about in general.\n",
    "###\\n'''\n",
    "\n",
    "with open(os.path.join(path_challenge, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "topic_term_list = []\n",
    "for index, topic in enumerate(topic_terms):\n",
    "    terms = ', '.join([term[0] for term in topic])\n",
    "    topic_term = f'Topic {index}: {terms}'\n",
    "    topic_term_list.append(topic_term)\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_topic +\n",
    "               '\\n'.join(topic_term_list) + '\\n###\\n'}],\n",
    "    temperature=0,\n",
    "    max_tokens=1500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    timeout=100,\n",
    "    stream=False)\n",
    "\n",
    "topic_challenge = completion.choices[0].message.content\n",
    "print(topic_challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping_challenge = {\n",
    "    0: ('Package Management', 'Installing and configuring software packages and dependencies'),\n",
    "    1: ('Pipeline Configuration', 'Automating the execution of data processing pipelines'),\n",
    "    2: ('Docker Configuration', 'Containerization platform for building, shipping, and running applications'),\n",
    "    3: ('Hyperparameter Tuning', 'Optimizing model performance by tuning hyperparameters'),\n",
    "    4: ('Code Versioning', 'Managing and tracking changes in a repository using Git'),\n",
    "    5: ('GPU Configuration', 'Using graphics processing units to speed up machine learning tasks'),\n",
    "    6: ('Artifact Management', 'Uploading, downloading, and storing artifacts'),\n",
    "    7: ('Endpoint Deployment', 'Deploying machine learning models for use in production environments'),\n",
    "    8: ('Data Labeling', 'Assigning labels to data for use in supervised learning tasks'),\n",
    "    9: ('Data Visualization', 'Creating visual representations of data for analysis and communication'),\n",
    "    10: ('Metrics Logging', 'Recording and tracking performance metrics during model training and evaluation'),\n",
    "    11: ('Account Management', 'Managing user accounts and access to resources'),\n",
    "    12: ('Apache Spark Configuration', 'Installing and configuring Apache Spark distributed computing system for big data processing'),\n",
    "    13: ('TensorFlow Configuration', 'Installing and configuring the TensorFlow machine learning framework'),\n",
    "    14: ('Text Processing', 'Analyzing and manipulating text data'),\n",
    "    15: ('Pandas Dataframe', 'Manipulating and analyzing tabular data using the Pandas library'),\n",
    "    16: ('Model Exporting', 'Saving and exporting trained machine learning models'),\n",
    "    17: ('Role-based Access Control', 'Controlling access to resources based on user roles and permissions'),\n",
    "    18: ('Batch Processing', 'Processing large amounts of data in batches'),\n",
    "    19: ('Model Registry', 'Registering, managing, and versioning models'),\n",
    "    20: ('Database Connectivity', 'Connecting to and interacting with databases'),\n",
    "    21: ('Resource Quota Control', 'Setting and managing limits on resource usage'),\n",
    "    22: ('API Invocation', 'Calling APIs to perform tasks or retrieve data'),\n",
    "    23: ('Forecasting', 'Using automated machine learning to generate forecasts'),\n",
    "    24: ('Columnar Manipulation', 'Working with and manipulating columns in datasets'),\n",
    "    25: ('Object Detection', 'Using machine learning to analyze and interpret visual data'),\n",
    "    26: ('Web Service', 'Deploying machine learning models as web services'),\n",
    "    27: ('Kubernetes Orchestration', 'Open-source container orchestration platform for managing containerized applications'),\n",
    "    28: ('Tree-based Model', 'Building, training, and cutting tree-like structure to make predictions'),\n",
    "    29: ('CSV Manipulation', 'Reading, writing, and manipulating CSV files'),\n",
    "    30: ('TensorBoard Logging', 'Visualizing and tracking model training and evaluation using TensorBoard'),\n",
    "    31: ('Feature Roadmap', 'Planning and implementing new features for a platform or product'),\n",
    "    32: ('Dataset Versioning', 'Managing and versioning datasets'),\n",
    "    33: ('CloudWatch Monitoring', 'Monitoring and logging AWS resources and applications'),\n",
    "    34: ('Speech-to-Text', 'Converting audio speech to text'),\n",
    "    35: ('YAML Configuration', 'Configuring and defining stages in a pipeline using YAML files'),\n",
    "    36: ('Data Storage', 'Storing and accessing data in cloud-based storage solutions'),\n",
    "    37: ('VPC Neworking', 'Connecting to AWS services privately through a VPC'),\n",
    "    38: ('Model Evaluation', 'Evaluating and improving the accuracy of machine learning models'),\n",
    "    39: ('Model Serving', 'Preparing and querying input data for machine learning models'),\n",
    "    40: ('Bucket Access Control', 'Managing access to cloud-based storage buckets'),\n",
    "    41: ('Run Management', 'Managing and monitoring the execution of jobs and tasks'),\n",
    "    42: ('Model Inference', 'Using trained machine learning models to make predictions'),\n",
    "    43: ('Jupyter Notebook', 'Creating and running interactive notebooks for data analysis and visualization'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Git Tracking - Managing and tracking changes in a repository using Git.\n",
      "Topic 1: Access Control - Managing user roles and permissions for accessing resources.\n",
      "Topic 2: Environment Setup - Installing and configuring software packages and dependencies.\n",
      "Topic 3: Logging Metrics - Capturing and analyzing data related to system performance and behavior.\n",
      "Topic 4: Dataset Management - Organizing and manipulating data for use in machine learning models.\n",
      "Topic 5: Docker Deployment - Packaging and deploying applications in containers using Docker.\n",
      "Topic 6: Parameter Configuration - Setting and adjusting parameters for software programs.\n",
      "Topic 7: YAML Configuration - Configuring and defining stages in a pipeline using YAML files.\n",
      "Topic 8: Endpoint Deployment - Deploying and managing endpoints for accessing APIs and web services.\n",
      "Topic 9: Jupyter Notebooks - Creating and running interactive notebooks for data analysis and visualization.\n",
      "Topic 10: Pandas Dataframes - Manipulating and analyzing tabular data using the Pandas library.\n",
      "Topic 11: TensorFlow Usage - Installing and using the TensorFlow library for machine learning.\n",
      "Topic 12: Artifact Management - Storing and managing files and artifacts for software projects.\n",
      "Topic 13: Model Deployment - Deploying and managing machine learning models in the cloud.\n",
      "Topic 14: Random Forest - Using the random forest algorithm for machine learning tasks.\n",
      "Topic 15: Pipeline Modeling - Building and deploying machine learning pipelines for data processing and analysis.\n",
      "Topic 16: JSON Payloads - Formatting and transmitting data using the JSON data interchange format.\n",
      "Topic 17: Remote Configuration - Configuring and managing remote resources and connections.\n",
      "Topic 18: Spark Computing - Using the Apache Spark framework for distributed computing and data processing.\n",
      "Topic 19: Python Model Implementation - Implementing machine learning models using the Python programming language.\n",
      "Topic 20: Data Upload/Download - Uploading and downloading data files and objects.\n",
      "Topic 21: Cluster Computing - Using clusters of computers for parallel processing and data analysis.\n",
      "Topic 22: Pipeline Data Input - Defining and managing input data for machine learning pipelines.\n",
      "Topic 23: CSV Formatting - Formatting and manipulating data using the CSV file format.\n",
      "Topic 24: Model Registration - Registering and managing machine learning models for use in applications.\n",
      "Topic 25: Memory Management - Managing memory usage and resources for large-scale data processing.\n",
      "Topic 26: Neural Networks - Building and training neural networks for machine learning tasks.\n",
      "Topic 27: SDK Versioning - Managing and updating software development kits for programming languages and frameworks.\n",
      "Topic 28: Lambda Invocation - Invoking and managing AWS Lambda functions for serverless computing and API Gateway.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of keywords for each topic, I want you to provide a description of each topic in a two-word phrase but guarantee that each description is exclusive to the other. Also, for each description, you need to attach short comments on what these keywords are talking about in general.\n",
    "###\\n'''\n",
    "\n",
    "with open(os.path.join(path_solution, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "topic_term_list = []\n",
    "for index, topic in enumerate(topic_terms):\n",
    "    terms = ', '.join([term[0] for term in topic])\n",
    "    topic_term = f'Topic {index}: {terms}'\n",
    "    topic_term_list.append(topic_term)\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_topic +\n",
    "               '\\n'.join(topic_term_list) + '\\n###\\n'}],\n",
    "    temperature=0,\n",
    "    max_tokens=1500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    timeout=100,\n",
    "    stream=False)\n",
    "\n",
    "topic_solution = completion.choices[0].message.content\n",
    "print(topic_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping_solution = {\n",
    "    0: ('Code Versioning', 'Managing and tracking changes in a repository using Git'),\n",
    "    1: ('Role-based Access Control', 'Controlling access to resources based on user roles and permissions'),\n",
    "    2: ('Package Management', 'Installing and configuring software packages and dependencies'),\n",
    "    3: ('Metrics Logging', 'Recording and tracking performance metrics during model training and evaluation'),\n",
    "    4: ('Columnar Manipulation', 'Working with and manipulating columns in datasets'),\n",
    "    5: ('Docker Configuration', 'Containerization platform for building, shipping, and running applications'),\n",
    "    6: ('Hyperparameter Tuning', 'Optimizing model performance by tuning hyperparameters'),\n",
    "    7: ('YAML Configuration', 'Configuring and defining stages in a pipeline using YAML files'),\n",
    "    8: ('Endpoint Serving', 'Creating, deploying, and managing endpoints for REST and web services'),\n",
    "    9: ('Jupyter Notebook', 'Creating and running interactive notebooks for data analysis and visualization'),\n",
    "    10: ('Pandas Dataframe', 'Manipulating and analyzing tabular data using the Pandas library'),\n",
    "    11: ('TensorFlow Configuration', 'Installing and configuring the TensorFlow machine learning framework'),\n",
    "    12: ('Artifact Management', 'Uploading, downloading, and storing artifacts'),\n",
    "    13: ('Endpoint Deployment', 'Deploying machine learning models for use in production environments'),\n",
    "    14: ('Tree-based Model', 'Building, training, and cutting tree-like structure to make predictions'),\n",
    "    15: ('Pipeline Configuration (Model)', 'Building, inputting, and parameterizing pipelines for API and object use'),\n",
    "    16: ('JSON Payload', 'Formatting, serializing, and loading data'),\n",
    "    17: ('Remote Configuration', 'Adding, modifying, and running remote URLs and resources'),\n",
    "    18: ('Apache Spark Configuration', 'Installing and configuring Apache Spark distributed computing system for big data processing'),\n",
    "    19: ('Model Wrapper', 'Using PyFunc and PythonModel interfaces, importing models, and loading models'),\n",
    "    20: ('Data Transfer', 'Transferring data between cloud-based storage solutions'),\n",
    "    21: ('Cluster Configuration', 'Running and managing distributed computing jobs'),\n",
    "    22: ('Pipeline Configuration (Data)', 'Creating and managing data pipelines'),\n",
    "    23: ('CSV Manipulation', 'Reading, writing, and manipulating CSV files'),\n",
    "    24: ('Model Registry', 'Registering, managing, and versioning models'),\n",
    "    25: ('Memory Management', 'Managing memory and distributing training for large datasets'),\n",
    "    26: ('Model Application', 'Using neural networks for machine learning'),\n",
    "    27: ('SDK Management', 'Managing SDK versions'),\n",
    "    28: ('Serverless Serving', 'Invoking endpoints and APIs using Lambda functions and API gateways'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ensemble = [\n",
    "    'Account Management',\n",
    "    'Apache Spark Configuration',\n",
    "    'API Invocation',\n",
    "    'Artifact Management',\n",
    "    'Batch Processing',\n",
    "    'Bucket Access Control',\n",
    "    'CloudWatch Monitoring',\n",
    "    'Cluster Configuration',\n",
    "    'Code Versioning',\n",
    "    'Columnar Manipulation',\n",
    "    'CSV Manipulation',\n",
    "    'Data Labeling',\n",
    "    'Data Storage',\n",
    "    'Data Transfer',\n",
    "    'Data Visualization',\n",
    "    'Database Connectivity',\n",
    "    'Dataset Versioning',\n",
    "    'Docker Configuration',\n",
    "    'GPU Configuration',\n",
    "    'Hyperparameter Tuning',\n",
    "    'JSON Payload',\n",
    "    'Jupyter Notebook',\n",
    "    'Kubernetes Orchestration',\n",
    "    'Memory Management',\n",
    "    'Metrics Logging',\n",
    "    'Model Evaluation',\n",
    "    'Model Exporting',\n",
    "    'Model Inference',\n",
    "    'Model Registry',\n",
    "    'Model Serving',\n",
    "    'Endpoint Serving',\n",
    "    'Endpoint Deployment',\n",
    "    'Serverless Serving',\n",
    "    'Pandas Dataframe',\n",
    "    'Pipeline Configuration',\n",
    "    'Pipeline Configuration (Data)',\n",
    "    'Pipeline Configuration (Model)',\n",
    "    'Package Management',\n",
    "    'Remote Configuration',\n",
    "    'Resource Quota Control',\n",
    "    'Role-based Access Control',\n",
    "    'Run Management',\n",
    "    'SDK Management',\n",
    "    'TensorBoard Logging',\n",
    "    'TensorFlow Configuration',\n",
    "    'VPC Networking',\n",
    "    'Web Service',\n",
    "    'YAML Configuration',\n",
    "]\n",
    "\n",
    "macro_topic_ensemble_inverse = [\n",
    "    # Code versioning refers to the practice of tracking changes to software code over time.\n",
    "    {'Code Management': ['Code Versioning']},\n",
    "    # These words are all related to data management and analysis. They refer to various tasks and techniques used to organize, manipulate, store, transfer, and analyze data.\n",
    "    {'Data Management': ['Artifact Management', 'Columnar Manipulation', 'CSV Manipulation', 'Data Labeling', 'Data Storage',\n",
    "                         'Data Transfer', 'Data Visualization', 'Database Connectivity', 'Dataset Versioning', 'Pandas Dataframe', 'Batch Processing']},\n",
    "    # All of these words are related to the deployment and management of machine learning models or web services.\n",
    "    {'Deployment Management': ['Endpoint Serving', 'Endpoint Deployment', 'Model Serving', 'Model Inference',\n",
    "                               'JSON Payload', 'Web Service', 'Serverless Serving', 'API Invocation']},\n",
    "    # All of these words relate to the configuration and management of infrastructure aspects of computer systems and networks. Specifically, they involve setting up and optimizing different components such as processing power, memory, network connections, and software to ensure that they work together efficiently and effectively.\n",
    "    {'Infrastructure Management': ['Apache Spark Configuration', 'Cluster Configuration', 'Docker Configuration', 'GPU Configuration', 'VPC Networking', 'Memory Management',\n",
    "                                   'Remote Configuration', 'Resource Quota Control', 'TensorFlow Configuration', 'Jupyter Notebook', 'Package Management', 'SDK Management', 'YAML Configuration']},\n",
    "    # These words are all related to the management and optimization of data pipelines in software development.\n",
    "    {'Lifecycle Management': ['Pipeline Configuration',\n",
    "                              'Pipeline Configuration (Data)', 'Pipeline Configuration (Model)', 'Run Management', 'Kubernetes Orchestration']},\n",
    "    # All of these words are related to the development and management of machine learning models.\n",
    "    {'Model Management': ['Hyperparameter Tuning',\n",
    "                          'Model Evaluation', 'Model Exporting', 'Model Registry']},\n",
    "    # All of these words are related to monitoring and logging data in various systems.\n",
    "    {'Report Management': ['CloudWatch Monitoring',\n",
    "                           'Metrics Logging', 'TensorBoard Logging', 'Metrics Logging']},\n",
    "    # All of these words are related to controlling access to information or resources in a system.\n",
    "    {'Security Management': ['Account Management',\n",
    "                             'Bucket Access Control', 'Role-based Access Control']},\n",
    "]\n",
    "\n",
    "macro_topic_ensemble = {}\n",
    "for dictionary in macro_topic_ensemble_inverse:\n",
    "    for key, values in dictionary.items():\n",
    "        for inner_value in values:\n",
    "            macro_topic_ensemble[inner_value] = key\n",
    "\n",
    "macro_topic_to_index = {}\n",
    "for index, topics in enumerate(macro_topic_ensemble_inverse):\n",
    "    for topic in topics:\n",
    "        macro_topic_to_index[topic] = index + 1\n",
    "\n",
    "colors = n_colors('rgb(5, 200, 200)', 'rgb(200, 10, 10)',\n",
    "                  len(macro_topic_ensemble_inverse), colortype='rgb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-08-08 14:04:22.160000'),\n",
       " Timestamp('2023-02-22 01:36:03.995000'))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_general, 'original.json'))\n",
    "# BigQuery Stack Overflow public dataset is updated until Nov 24, 2022, 1:39:22 PM UTC-5\n",
    "min(df['Challenge_creation_time']), max(df['Challenge_creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign human-readable & high-level topics to challenges & solutions\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'original.json'))\n",
    "# remove challenges that were created after the public dataset was released\n",
    "df_topics = df_topics[df_topics['Challenge_creation_time'] < '2022-11-24']\n",
    "\n",
    "for index, row in df_topics.iterrows():\n",
    "    if row['Challenge_topic'] in topic_mapping_challenge and topic_mapping_challenge[row['Challenge_topic']][0] in topic_ensemble:\n",
    "        topic = topic_mapping_challenge[row['Challenge_topic']][0]\n",
    "        df_topics.at[index, 'Challenge_topic'] = topic\n",
    "        df_topics.at[index, 'Challenge_topic_macro'] = macro_topic_ensemble[topic]\n",
    "        if pd.isna(row['Challenge_comment_count']):\n",
    "            df_topics.at[index, 'Challenge_comment_count'] = 0\n",
    "    else:\n",
    "        df_topics.drop(index, inplace=True)\n",
    "        continue\n",
    "\n",
    "    if row['Solution_topic'] in topic_mapping_solution and topic_mapping_solution[row['Solution_topic']][0] in topic_ensemble:\n",
    "        topic = topic_mapping_solution[row['Solution_topic']][0]\n",
    "        df_topics.at[index, 'Solution_topic'] = topic\n",
    "        df_topics.at[index, 'Solution_topic_macro'] = macro_topic_ensemble[topic]\n",
    "    else:\n",
    "        df_topics.at[index, 'Solution_topic'] = np.nan\n",
    "        df_topics.at[index, 'Solution_topic_macro'] = np.nan\n",
    "\n",
    "df_topics['Challenge_comment_count'] = df_topics['Challenge_comment_count'].fillna(0)\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_answer_count'] + df_topics['Challenge_comment_count']\n",
    "\n",
    "df_topics.to_json(os.path.join(path_general, 'filtered.json'),\n",
    "                  indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(macro_topic_to_index)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map({True: 'Closed', False: 'Open'})\n",
    "df_topics['Count'] = 1\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Count',\n",
    "    color='Challenge_topic_macro',\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.write_image(os.path.join(\n",
    "    path_challenge, 'Challenge_topic_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic participation distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "df_topics['Challenge_topic_macro'] = df_topics['Challenge_topic_macro'].map(macro_topic_to_index)\n",
    "df_topics['Solved'] = df_topics['Challenge_closed_time'].notna().map({True: 'Closed', False: 'Open'})\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_participation_count'].map(lambda x: 1e-07 if x == 0 else x)\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=[px.Constant('All'), 'Solved', 'Platform', 'Tool'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic_macro',\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.write_image(os.path.join(\n",
    "    path_challenge, 'Challenge_topic_participation_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "df_topics = df_topics[df_topics['Solution_topic_macro'].notna()]\n",
    "\n",
    "categories = ['Challenge_topic_macro', 'Solution_topic_macro']\n",
    "df_topics = df_topics.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "# we only visualize strong connection\n",
    "df_topics = df_topics[df_topics['value'] > 50]\n",
    "\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)-1):\n",
    "    tempDf = df_topics[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "\n",
    "label = list(np.unique(df_topics[categories].values))\n",
    "source = newDf['source'].apply(lambda x: label.index(x))\n",
    "target = newDf['target'].apply(lambda x: label.index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict()\n",
    "data = go.Sankey(\n",
    "    link=link,\n",
    "    node=dict(\n",
    "        label=label,\n",
    "        thickness=100,\n",
    "        pad=30,\n",
    "    ))\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(\n",
    "    height=2000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    ")\n",
    "fig.write_image(os.path.join(path_general,\n",
    "                'Challenge solution sankey.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log scale all numerical values for better visualization of long-tailed distributions\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "\n",
    "df['Challenge_answer_count'] = np.log(df['Challenge_answer_count'] + 1)\n",
    "df['Challenge_comment_count'] = np.log(df['Challenge_comment_count'] + 1)\n",
    "df['Challenge_participation_count'] = np.log(df['Challenge_participation_count'] + 1)\n",
    "df['Challenge_favorite_count'] = np.log(df['Challenge_favorite_count'] + 1)\n",
    "df['Challenge_link_count'] = np.log(df['Challenge_link_count'] + 1)\n",
    "df['Challenge_readability'] = np.log(df['Challenge_readability'] + 1)\n",
    "df['Challenge_score'] = np.log(df['Challenge_score'] + 1)\n",
    "df['Challenge_view_count'] = np.log(df['Challenge_view_count'] + 1)\n",
    "df['Challenge_information_entropy'] = np.log(df['Challenge_information_entropy'] + 1)\n",
    "df['Challenge_sentence_count'] = np.log(df['Challenge_sentence_count'] + 1)\n",
    "df['Challenge_unique_word_count'] = np.log(df['Challenge_unique_word_count'] + 1)\n",
    "df['Challenge_word_count'] = np.log(df['Challenge_word_count'] + 1)\n",
    "df['Solution_comment_count'] = np.log(df['Solution_comment_count'] + 1)\n",
    "df['Solution_comment_count'] = np.log(df['Solution_comment_count'] + 1)\n",
    "df['Solution_link_count'] = np.log(df['Solution_link_count'] + 1)\n",
    "df['Solution_readability'] = np.log(df['Solution_readability'] + 1)\n",
    "df['Solution_score'] = np.log(df['Solution_score'] + 1)\n",
    "df['Solution_sentence_count'] = np.log(df['Solution_sentence_count'] + 1)\n",
    "df['Solution_unique_word_count'] = np.log(df['Solution_unique_word_count'] + 1)\n",
    "df['Solution_word_count'] = np.log(df['Solution_word_count'] + 1)\n",
    "df['Challenge_solved_time'] = np.log(df['Challenge_solved_time'] + 1)\n",
    "df['Challenge_adjusted_solved_time'] = np.log(df['Challenge_adjusted_solved_time'] + 1)\n",
    "\n",
    "df.to_json(os.path.join(path_general, 'logscale.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.00, indicating challenge answer count: non-Gaussian\n",
      "p = 0.00, indicating challenge comment count: non-Gaussian\n",
      "p = 0.00, indicating challenge participation count: non-Gaussian\n",
      "p = 1.00, indicating challenge favorite count: Gaussian\n",
      "p = 0.00, indicating challenge link count: non-Gaussian\n",
      "p = 0.00, indicating challenge readability: non-Gaussian\n",
      "p = 1.00, indicating challenge score: Gaussian\n",
      "p = 1.00, indicating challenge view count: Gaussian\n",
      "p = 0.00, indicating challenge information entropy: non-Gaussian\n",
      "p = 0.00, indicating challenge sentence count: non-Gaussian\n",
      "p = 0.00, indicating challenge unique word count: non-Gaussian\n",
      "p = 0.00, indicating challenge word count: non-Gaussian\n",
      "p = 1.00, indicating solution comment count: Gaussian\n",
      "p = 1.00, indicating solution information entropy: Gaussian\n",
      "p = 1.00, indicating solution link count: Gaussian\n",
      "p = 1.00, indicating solution readability: Gaussian\n",
      "p = 1.00, indicating solution score: Gaussian\n",
      "p = 1.00, indicating solution sentence count: Gaussian\n",
      "p = 1.00, indicating solution unique word count: Gaussian\n",
      "p = 1.00, indicating solution word count: Gaussian\n",
      "p = 1.00, indicating challenge solved time: Gaussian\n",
      "p = 1.00, indicating challenge adjusted solved time: Gaussian\n"
     ]
    }
   ],
   "source": [
    "# perform the Shapiro-Wilk test for normality to check if various metrics are Gaussian distributed\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "\n",
    "_, p = shapiro(df['Challenge_answer_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge answer count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_comment_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge comment count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_participation_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge participation count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_favorite_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge favorite count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_link_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge link count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_readability'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge readability: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_score'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge score: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_view_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge view count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_information_entropy'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge information entropy: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_sentence_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge sentence count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_unique_word_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge unique word count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_word_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge word count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_comment_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution comment count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_comment_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution information entropy: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_link_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution link count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_readability'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution readability: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_score'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution score: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_sentence_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution sentence count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_unique_word_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution unique word count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Solution_word_count'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating solution word count: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_solved_time'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge solved time: {result}Gaussian')\n",
    "\n",
    "_, p = shapiro(df['Challenge_adjusted_solved_time'])\n",
    "result = 'non-' if p < alpha else ''\n",
    "print(f'p = {p:.2f}, indicating challenge adjusted solved time: {result}Gaussian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least the challenge answer count of one topic is significantly different from the others\n",
      "At least the challenge comment count of one topic is significantly different from the others\n",
      "At least the challenge participation count of one topic is significantly different from the others\n",
      "At least the challenge link count of one topic is significantly different from the others\n",
      "At least the challenge readability of one topic is significantly different from the others\n",
      "At least the challenge information entropy of one topic is significantly different from the others\n",
      "At least the challenge sentence count of one topic is significantly different from the others\n",
      "At least the challenge unique word count of one topic is significantly different from the others\n",
      "At least the challenge word count of one topic is significantly different from the others\n",
      "At least the solution information entropy of one topic is significantly different from the others\n",
      "At least the solution link count of one topic is significantly different from the others\n",
      "At least the solution readability of one topic is significantly different from the others\n",
      "At least the solution sentence count of one topic is significantly different from the others\n",
      "At least the solution unique word count of one topic is significantly different from the others\n",
      "At least the solution word count of one topic is significantly different from the others\n"
     ]
    }
   ],
   "source": [
    "# conduct Kruskal–Wallis test to check if various metrics differ significantly across topics\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "groups = df['Challenge_topic_macro'].unique()\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_answer_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge answer count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_comment_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge comment count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_participation_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge participation count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_favorite_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge favorite count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_link_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge link count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_readability'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge readability of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_score'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge score of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_information_entropy'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge information entropy of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_sentence_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge sentence count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_view_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge view count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_unique_word_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge unique word count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_word_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge word count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_solved_time'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge solved time of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Challenge_adjusted_solved_time'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the challenge adjusted solved time of one topic is significantly different from the others')\n",
    "\n",
    "df = df[df['Solution_topic_macro'].notna()]\n",
    "groups = df['Challenge_topic_macro'].unique()\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_comment_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution comment count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_information_entropy'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution information entropy of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_link_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution link count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_readability'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution readability of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_score'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution score of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_sentence_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution sentence count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_unique_word_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution unique word count of one topic is significantly different from the others')\n",
    "\n",
    "data = [df[df['Challenge_topic_macro'] == group]['Solution_word_count'] for group in groups]\n",
    "_, p = kruskal(*data)\n",
    "if p < alpha:\n",
    "    print('At least the solution word count of one topic is significantly different from the others')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Code Management in challenge score\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Code Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Code Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Data Management in challenge score\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Data Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Data Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Deployment Management in challenge readability\n",
      "p = 0.01, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Deployment Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Infrastructure Management in challenge score\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Infrastructure Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Infrastructure Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Infrastructure Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Infrastructure Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Lifecycle Management in challenge score\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Lifecycle Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Lifecycle Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Lifecycle Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Model Management in challenge score\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Model Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Model Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Model Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Report Management in challenge score\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Report Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Report Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Security Management in challenge score\n",
      "p = 0.01, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic Security Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos in higher level mean challenge solved time\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos in higher level median challenge solved time\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos in higher level adjusted mean challenge solved time\n",
      "p = 0.00, indicating different distribution of Q&A fora vs Git repos in higher level adjusted median challenge solved time\n"
     ]
    }
   ],
   "source": [
    "# Collect and compared Q&A forum and Git repo challenges across different topics\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'logscale.json'))\n",
    "\n",
    "df_qa = df[df['Platform'].isin(['Stack Overflow', 'Tool-specific'])]\n",
    "df_git = df[df['Platform'].isin(['Github', 'Gitlab'])]\n",
    "\n",
    "# Challenge topic count\n",
    "fig_challenge_count = go.Figure()\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_qa), 'Challenge topic count (higher level)'),\n",
    "        y=df_qa['Challenge_topic_macro'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_git), 'Challenge topic count (higher level)'),\n",
    "        y=df_git['Challenge_topic_macro'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_count.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge count.png'))\n",
    "\n",
    "# Challenge score\n",
    "fig_challenge_score = go.Figure()\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_score.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_score.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge score.png'))\n",
    "\n",
    "# Challenge favorite count\n",
    "fig_challenge_favorite_count = go.Figure()\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_favorite_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_favorite_count.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge favorite count.png'))\n",
    "\n",
    "# Challenge link count\n",
    "fig_challenge_link_count = go.Figure()\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_link_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_link_count.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge link count.png'))\n",
    "\n",
    "# Challenge readability\n",
    "fig_challenge_readability = go.Figure()\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_readability.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_readability.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge readability.png'))\n",
    "\n",
    "# Challenge view count\n",
    "fig_challenge_view_count = go.Figure()\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_view_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_view_count.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge view count.png'))\n",
    "\n",
    "# Challenge participation count\n",
    "fig_challenge_participation_count = go.Figure()\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_participation_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_participation_count.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge participation count.png'))\n",
    "\n",
    "# Challenge solved time\n",
    "fig_challenge_solved_time = go.Figure()\n",
    "fig_challenge_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_solved_time.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_solved_time.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge solved time.png'))\n",
    "\n",
    "# Challenge adjusted solved time\n",
    "fig_challenge_adjusted_solved_time = go.Figure()\n",
    "fig_challenge_adjusted_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_qa['Challenge_topic_macro'],\n",
    "        y=df_qa['Challenge_adjusted_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        name='QA',\n",
    "    ))\n",
    "fig_challenge_adjusted_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_git['Challenge_topic_macro'],\n",
    "        y=df_git['Challenge_adjusted_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        name='Git',\n",
    "    ))\n",
    "fig_challenge_adjusted_solved_time.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_adjusted_solved_time.write_image(os.path.join(\n",
    "    path_challenge_git_qa, 'Challenge adjusted solved time.png'))\n",
    "\n",
    "for name_group, color in zip(df.groupby('Challenge_topic_macro'), colors):\n",
    "    name, group = name_group\n",
    "    qa = group[group['Platform'].isin(['Stack Overflow', 'Tool-specific'])]\n",
    "    git = group[group['Platform'].isin(['Github', 'Gitlab'])]\n",
    "    \n",
    "    # Challenge score\n",
    "    challenge_score_qa = qa[qa['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    challenge_score_git = git[git['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    if len(challenge_score_qa) * len(challenge_score_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_score_qa, challenge_score_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge score')\n",
    "\n",
    "    # Challenge favorite count\n",
    "    challenge_favorite_count_qa = qa[qa['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    challenge_favorite_count_git = git[git['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    if len(challenge_favorite_count_qa) * len(challenge_favorite_count_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_favorite_count_qa,\n",
    "                            challenge_favorite_count_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge favorite count')\n",
    "\n",
    "    # Challenge link count\n",
    "    challenge_link_count_qa = qa[qa['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    challenge_link_count_git = git[git['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    if len(challenge_link_count_qa) * len(challenge_link_count_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_link_count_qa, challenge_link_count_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge link count')\n",
    "\n",
    "    # Challenge readability\n",
    "    challenge_readability_qa = qa[qa['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    challenge_readability_git = git[git['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    if len(challenge_readability_qa) * len(challenge_readability_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_readability_qa,\n",
    "                            challenge_readability_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge readability')\n",
    "\n",
    "    # Challenge view count\n",
    "    challenge_view_count_qa = qa[qa['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    challenge_view_count_git = git[git['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    if len(challenge_view_count_qa) * len(challenge_view_count_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_view_count_qa,\n",
    "                            challenge_view_count_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge answer count\n",
    "    challenge_answer_count_qa = qa['Challenge_answer_count']\n",
    "    challenge_answer_count_git = git['Challenge_answer_count']\n",
    "    if len(challenge_answer_count_qa) * len(challenge_answer_count_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_answer_count_qa,\n",
    "                            challenge_answer_count_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge comment count\n",
    "    challenge_comment_count_qa = qa['Challenge_comment_count']\n",
    "    challenge_comment_count_git = git['Challenge_comment_count']\n",
    "    if len(challenge_comment_count_qa) * len(challenge_comment_count_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_comment_count_qa,\n",
    "                            challenge_comment_count_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge comment count')\n",
    "\n",
    "    # Challenge participation count\n",
    "    challenge_participation_count_qa = qa['Challenge_participation_count']\n",
    "    challenge_participation_count_git = git['Challenge_participation_count']\n",
    "    if len(challenge_participation_count_qa) * len(challenge_participation_count_git) > 0:\n",
    "        _, p = mannwhitneyu(challenge_participation_count_qa,\n",
    "                            challenge_participation_count_git)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos challenge regarding higher level topic {name} in challenge participation count')\n",
    "\n",
    "# Challenge mean solved time\n",
    "challenge_mean_solved_time_qa = df_qa[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_solved_time']\n",
    "challenge_mean_solved_time_git = df_git[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_solved_time']\n",
    "_, p = mannwhitneyu(challenge_mean_solved_time_qa,\n",
    "                    challenge_mean_solved_time_git)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos in higher level mean challenge solved time')\n",
    "\n",
    "# Challenge median solved time\n",
    "challenge_median_solved_time_qa = df_qa[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_solved_time']\n",
    "challenge_median_solved_time_git = df_git[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_solved_time']\n",
    "_, p = mannwhitneyu(challenge_median_solved_time_qa,\n",
    "                    challenge_median_solved_time_git)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos in higher level median challenge solved time')\n",
    "\n",
    "# Challenge adjusted mean solved time\n",
    "challenge_adjusted_mean_solved_time_qa = df_qa[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_adjusted_solved_time']\n",
    "challenge_adjusted_mean_solved_time_git = df_git[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_adjusted_solved_time']\n",
    "_, p = mannwhitneyu(challenge_adjusted_mean_solved_time_qa,\n",
    "                    challenge_adjusted_mean_solved_time_git)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos in higher level adjusted mean challenge solved time')\n",
    "\n",
    "# Challenge adjusted median solved time\n",
    "challenge_adjusted_median_solved_time_qa = df_qa[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_adjusted_solved_time']\n",
    "challenge_adjusted_median_solved_time_git = df_git[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_adjusted_solved_time']\n",
    "_, p = mannwhitneyu(challenge_adjusted_median_solved_time_qa,\n",
    "                    challenge_adjusted_median_solved_time_git)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Q&A fora vs Git repos in higher level adjusted median challenge solved time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.01, indicating different distribution of open vs closed challenge regarding higher level topic Code Management in challenge favorite count\n",
      "p = 0.01, indicating different distribution of open vs closed challenge regarding higher level topic Code Management in challenge readability\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Code Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Code Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Data Management in challenge score\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Data Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Data Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Data Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Deployment Management in challenge score\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Deployment Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Deployment Management in challenge link count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Deployment Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Deployment Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Deployment Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Infrastructure Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Infrastructure Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Infrastructure Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Infrastructure Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Lifecycle Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Lifecycle Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Model Management in challenge score\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Model Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Model Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Model Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Report Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of open vs closed challenge regarding higher level topic Security Management in challenge answer count\n"
     ]
    }
   ],
   "source": [
    "# Collect and compared open vs closed challenges across different topics\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'logscale.json'))\n",
    "\n",
    "df_open = df[df['Challenge_closed_time'].isna()]\n",
    "df_closed = df[df['Challenge_closed_time'].notna()]\n",
    "\n",
    "# Challenge topic count\n",
    "fig_challenge_count = go.Figure()\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_open), 'Challenge topic count (higher level)'),\n",
    "        y=df_open['Challenge_topic_macro'],\n",
    "        opacity=0.5,\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_closed), 'Challenge topic count (higher level)'),\n",
    "        y=df_closed['Challenge_topic_macro'],\n",
    "        opacity=0.5,\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge count.png'))\n",
    "\n",
    "# Challenge score\n",
    "fig_challenge_score = go.Figure()\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_score.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_score.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge score.png'))\n",
    "\n",
    "# Challenge favorite count\n",
    "fig_challenge_favorite_count = go.Figure()\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_favorite_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_favorite_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge favorite count.png'))\n",
    "\n",
    "# Challenge link count\n",
    "fig_challenge_link_count = go.Figure()\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_link_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_link_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge link count.png'))\n",
    "\n",
    "# Challenge readability\n",
    "fig_challenge_readability = go.Figure()\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_readability.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_readability.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge readability.png'))\n",
    "\n",
    "# Challenge view count\n",
    "fig_challenge_view_count = go.Figure()\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_view_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_view_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge view count.png'))\n",
    "\n",
    "# Challenge answer count\n",
    "fig_challenge_answer_count = go.Figure()\n",
    "fig_challenge_answer_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_answer_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_answer_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_answer_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_answer_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_answer_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge answer count.png'))\n",
    "\n",
    "# Challenge comment count\n",
    "fig_challenge_comment_count = go.Figure()\n",
    "fig_challenge_comment_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_comment_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_comment_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_comment_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_comment_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_comment_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge comment count.png'))\n",
    "\n",
    "# Challenge participation count\n",
    "fig_challenge_participation_count = go.Figure()\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_open['Challenge_topic_macro'],\n",
    "        y=df_open['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Open',\n",
    "        scalegroup='Open',\n",
    "        name='Open',\n",
    "    ))\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_closed['Challenge_topic_macro'],\n",
    "        y=df_closed['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Closed',\n",
    "        scalegroup='Closed',\n",
    "        name='Closed',\n",
    "    ))\n",
    "fig_challenge_participation_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_participation_count.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge participation count.png'))\n",
    "\n",
    "fig_challenge_solved_time_closed = go.Figure()\n",
    "fig_challenge_adjusted_solved_time_closed = go.Figure()\n",
    "\n",
    "for name_group, color in zip(df.groupby('Challenge_topic_macro'), colors):\n",
    "    name, group = name_group\n",
    "    open = group[group['Challenge_closed_time'].isna()]\n",
    "    closed = group[group['Challenge_closed_time'].notna()]\n",
    "\n",
    "    fig_challenge_solved_time_closed.add_trace(go.Violin(\n",
    "        x=closed['Challenge_solved_time'], y=closed['Challenge_topic_macro'], line_color=color))\n",
    "    fig_challenge_adjusted_solved_time_closed.add_trace(go.Violin(\n",
    "        x=closed['Challenge_adjusted_solved_time'], y=closed['Challenge_topic_macro'], line_color=color))\n",
    "\n",
    "    # Challenge score\n",
    "    challenge_score_open = open[open['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    challenge_score_closed = closed[closed['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    if len(challenge_score_open) * len(challenge_score_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_score_open, challenge_score_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge score')\n",
    "\n",
    "    # Challenge favorite count\n",
    "    challenge_favorite_count_open = open[open['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    challenge_favorite_count_closed = closed[closed['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    if len(challenge_favorite_count_open) * len(challenge_favorite_count_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_favorite_count_open,\n",
    "                            challenge_favorite_count_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge favorite count')\n",
    "\n",
    "    # Challenge link count\n",
    "    challenge_link_count_open = open[open['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    challenge_link_count_closed = closed[closed['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    if len(challenge_link_count_open) * len(challenge_link_count_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_link_count_open,\n",
    "                            challenge_link_count_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge link count')\n",
    "\n",
    "    # Challenge readability\n",
    "    challenge_readability_open = open[open['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    challenge_readability_closed = closed[closed['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    if len(challenge_readability_open) * len(challenge_readability_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_readability_open,\n",
    "                            challenge_readability_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge readability')\n",
    "\n",
    "    # Challenge view count\n",
    "    challenge_view_count_open = open[open['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    challenge_view_count_closed = closed[closed['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    if len(challenge_view_count_open) * len(challenge_view_count_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_view_count_open,\n",
    "                            challenge_view_count_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge answer count\n",
    "    challenge_answer_count_open = open['Challenge_answer_count']\n",
    "    challenge_answer_count_closed = closed['Challenge_answer_count']\n",
    "    if len(challenge_answer_count_open) * len(challenge_answer_count_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_answer_count_open,\n",
    "                            challenge_answer_count_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge comment count\n",
    "    challenge_comment_count_open = open['Challenge_comment_count']\n",
    "    challenge_comment_count_closed = closed['Challenge_comment_count']\n",
    "    if len(challenge_comment_count_open) * len(challenge_comment_count_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_comment_count_open,\n",
    "                            challenge_comment_count_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge comment count')\n",
    "\n",
    "    # Challenge participation count\n",
    "    challenge_participation_count_open = open['Challenge_participation_count']\n",
    "    challenge_participation_count_closed = closed['Challenge_participation_count']\n",
    "    if len(challenge_participation_count_open) * len(challenge_participation_count_closed) > 0:\n",
    "        _, p = mannwhitneyu(challenge_participation_count_open,\n",
    "                            challenge_participation_count_closed)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of open vs closed challenge regarding higher level topic {name} in challenge participation count')\n",
    "\n",
    "fig_challenge_solved_time_closed.update_traces(\n",
    "    orientation='h', meanline_visible=True, side='positive', width=3, points=False)\n",
    "fig_challenge_solved_time_closed.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    xaxis_showgrid=False,\n",
    "    xaxis_zeroline=False,\n",
    "    showlegend=False,\n",
    "    xaxis_title='Challenge solved time (hours) - Closed',\n",
    ")\n",
    "fig_challenge_solved_time_closed.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge solved time (Closed).png'))\n",
    "\n",
    "fig_challenge_adjusted_solved_time_closed.update_traces(\n",
    "    orientation='h', meanline_visible=True, side='positive', width=3, points=False)\n",
    "fig_challenge_adjusted_solved_time_closed.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    xaxis_showgrid=False,\n",
    "    xaxis_zeroline=False,\n",
    "    showlegend=False,\n",
    "    xaxis_title='Challenge adjusted solved time (hours) - Closed',\n",
    ")\n",
    "fig_challenge_adjusted_solved_time_closed.write_image(os.path.join(\n",
    "    path_challenge_open_closed, 'Challenge adjusted solved time (Closed).png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Code Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Code Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Code Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Code Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Code Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge link count\n",
      "p = 0.01, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Data Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Deployment Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Deployment Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Deployment Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Deployment Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Deployment Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge readability\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Infrastructure Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Lifecycle Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Lifecycle Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Lifecycle Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Lifecycle Management in challenge answer count\n",
      "p = 0.01, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Lifecycle Management in challenge comment count\n",
      "p = 0.01, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Lifecycle Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Model Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge favorite count\n",
      "p = 0.01, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Report Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge score\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge link count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge answer count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic Security Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora in higher level mean challenge solved time\n",
      "p = 0.00, indicating different distribution of Stack Overflow vs tool-specific fora in higher level adjusted mean challenge solved time\n",
      "p = 0.01, indicating different distribution of Stack Overflow vs tool-specific fora in higher level adjusted median challenge solved time\n"
     ]
    }
   ],
   "source": [
    "# Collect and compared Stack Overflow vs tool-specific fora challenges across different topics\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "\n",
    "df_so = df[df['Platform'] == 'Stack Overflow']\n",
    "df_to = df[df['Platform'] == 'Tool-specific']\n",
    "\n",
    "# Challenge topic count\n",
    "fig_challenge_count = go.Figure()\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_so), 'Challenge topic count (higher level)'),\n",
    "        y=df_so['Challenge_topic_macro'],\n",
    "        opacity=0.5,\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_to), 'Challenge topic count (higher level)'),\n",
    "        y=df_to['Challenge_topic_macro'],\n",
    "        opacity=0.5,\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge count.png'))\n",
    "\n",
    "# Challenge score\n",
    "fig_challenge_score = go.Figure()\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_score.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_score.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge score.png'))\n",
    "\n",
    "# Challenge favorite count\n",
    "fig_challenge_favorite_count = go.Figure()\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_favorite_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_favorite_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge favorite count.png'))\n",
    "\n",
    "# Challenge link count\n",
    "fig_challenge_link_count = go.Figure()\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_link_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_link_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge link count.png'))\n",
    "\n",
    "# Challenge readability\n",
    "fig_challenge_readability = go.Figure()\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_readability.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_readability.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge readability.png'))\n",
    "\n",
    "# Challenge view count\n",
    "fig_challenge_view_count = go.Figure()\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_view_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_view_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge view count.png'))\n",
    "\n",
    "# Challenge answer count\n",
    "fig_challenge_answer_count = go.Figure()\n",
    "fig_challenge_answer_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_answer_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_answer_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_answer_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_answer_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_answer_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge answer count.png'))\n",
    "\n",
    "# Challenge comment count\n",
    "fig_challenge_comment_count = go.Figure()\n",
    "fig_challenge_comment_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_comment_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_comment_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_comment_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_comment_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_comment_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge comment count.png'))\n",
    "\n",
    "# Challenge participation count\n",
    "fig_challenge_participation_count = go.Figure()\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_participation_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_participation_count.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge participation count.png'))\n",
    "\n",
    "# Challenge solved time\n",
    "fig_challenge_solved_time = go.Figure()\n",
    "fig_challenge_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_solved_time.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_solved_time.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge solved time.png'))\n",
    "\n",
    "# Challenge adjusted solved time\n",
    "fig_challenge_adjusted_solved_time = go.Figure()\n",
    "fig_challenge_adjusted_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_so['Challenge_topic_macro'],\n",
    "        y=df_so['Challenge_adjusted_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Stack Overflow',\n",
    "        scalegroup='Stack Overflow',\n",
    "        name='Stack Overflow',\n",
    "    ))\n",
    "fig_challenge_adjusted_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_to['Challenge_topic_macro'],\n",
    "        y=df_to['Challenge_adjusted_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='Tool-specific',\n",
    "        scalegroup='Tool-specific',\n",
    "        name='Tool-specific',\n",
    "    ))\n",
    "fig_challenge_adjusted_solved_time.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_adjusted_solved_time.write_image(os.path.join(\n",
    "    path_challenge_so_to, 'Challenge adjusted solved time.png'))\n",
    "\n",
    "for name_group, color in zip(df.groupby('Challenge_topic_macro'), colors):\n",
    "    name, group = name_group\n",
    "    so = group[group['Platform'] == 'Stack Overflow']\n",
    "    to = group[group['Platform'] == 'Tool-specific']\n",
    "\n",
    "    # Challenge score\n",
    "    challenge_score_so = so[so['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    challenge_score_to = to[to['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    if len(challenge_score_so) * len(challenge_score_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_score_so, challenge_score_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge score')\n",
    "\n",
    "    # Challenge favorite count\n",
    "    challenge_favorite_count_so = so[so['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    challenge_favorite_count_to = to[to['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    if len(challenge_favorite_count_so) * len(challenge_favorite_count_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_favorite_count_so,\n",
    "                            challenge_favorite_count_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge favorite count')\n",
    "\n",
    "    # Challenge link count\n",
    "    challenge_link_count_so = so[so['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    challenge_link_count_to = to[to['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    if len(challenge_link_count_so) * len(challenge_link_count_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_link_count_so, challenge_link_count_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge link count')\n",
    "\n",
    "    # Challenge readability\n",
    "    challenge_readability_so = so[so['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    challenge_readability_to = to[to['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    if len(challenge_readability_so) * len(challenge_readability_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_readability_so,\n",
    "                            challenge_readability_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge readability')\n",
    "\n",
    "    # Challenge view count\n",
    "    challenge_view_count_so = so[so['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    challenge_view_count_to = to[to['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    if len(challenge_view_count_so) * len(challenge_view_count_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_view_count_so,\n",
    "                            challenge_view_count_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge answer count\n",
    "    challenge_answer_count_so = so['Challenge_answer_count']\n",
    "    challenge_answer_count_to = to['Challenge_answer_count']\n",
    "    if len(challenge_answer_count_so) * len(challenge_answer_count_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_answer_count_so,\n",
    "                            challenge_answer_count_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge comment count\n",
    "    challenge_comment_count_so = so['Challenge_comment_count']\n",
    "    challenge_comment_count_to = to['Challenge_comment_count']\n",
    "    if len(challenge_comment_count_so) * len(challenge_comment_count_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_comment_count_so,\n",
    "                            challenge_comment_count_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge comment count')\n",
    "\n",
    "    # Challenge participation count\n",
    "    challenge_participation_count_so = so['Challenge_participation_count']\n",
    "    challenge_participation_count_to = to['Challenge_participation_count']\n",
    "    if len(challenge_comment_count_so) * len(challenge_comment_count_to) > 0:\n",
    "        _, p = mannwhitneyu(challenge_comment_count_so,\n",
    "                            challenge_comment_count_to)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora challenge regarding higher level topic {name} in challenge participation count')\n",
    "\n",
    "# Challenge mean solved time\n",
    "challenge_mean_solved_time_so = df_so[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_solved_time']\n",
    "challenge_mean_solved_time_to = df_to[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_solved_time']\n",
    "_, p = mannwhitneyu(challenge_mean_solved_time_so,\n",
    "                    challenge_mean_solved_time_to)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora in higher level mean challenge solved time')\n",
    "\n",
    "# Challenge median solved time\n",
    "challenge_median_solved_time_so = df_so[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_solved_time']\n",
    "challenge_median_solved_time_to = df_to[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_solved_time']\n",
    "_, p = mannwhitneyu(challenge_median_solved_time_so,\n",
    "                    challenge_median_solved_time_to)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora in higher level median challenge solved time')\n",
    "\n",
    "# Challenge adjusted mean solved time\n",
    "challenge_adjusted_mean_solved_time_so = df_so[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_adjusted_solved_time']\n",
    "challenge_adjusted_mean_solved_time_to = df_to[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_adjusted_solved_time']\n",
    "_, p = mannwhitneyu(challenge_adjusted_mean_solved_time_so,\n",
    "                    challenge_adjusted_mean_solved_time_to)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora in higher level adjusted mean challenge solved time')\n",
    "\n",
    "# Challenge adjusted median solved time\n",
    "challenge_adjusted_median_solved_time_so = df_so[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_adjusted_solved_time']\n",
    "challenge_adjusted_median_solved_time_to = df_to[['Challenge_topic_macro', 'Challenge_adjusted_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_adjusted_solved_time']\n",
    "_, p = mannwhitneyu(challenge_adjusted_median_solved_time_so,\n",
    "                    challenge_adjusted_median_solved_time_to)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of Stack Overflow vs tool-specific fora in higher level adjusted median challenge solved time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Code Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Data Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Data Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Data Management in challenge participation count\n",
      "p = 0.01, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Deployment Management in challenge score\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Deployment Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Deployment Management in challenge readability\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Deployment Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Deployment Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Infrastructure Management in challenge score\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Infrastructure Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Infrastructure Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Infrastructure Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Lifecycle Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Lifecycle Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Lifecycle Management in challenge participation count\n",
      "p = 0.01, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Model Management in challenge score\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Model Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Model Management in challenge comment count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Model Management in challenge participation count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Report Management in challenge favorite count\n",
      "p = 0.00, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic Security Management in challenge favorite count\n"
     ]
    }
   ],
   "source": [
    "# Collect and compared SageMaker and AzureML challenges across different topics\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "\n",
    "df_sagemaker = df[df['Tool'] == 'Amazon SageMaker']\n",
    "df_azureml = df[df['Tool'] == 'Azure Machine Learning']\n",
    "\n",
    "# Challenge topic count\n",
    "fig_challenge_count = go.Figure()\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_azureml), 'Challenge topic count (higher level)'),\n",
    "        y=df_azureml['Challenge_topic_macro'],\n",
    "        opacity=0.5,\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=np.full(len(df_sagemaker), 'Challenge topic count (higher level)'),\n",
    "        y=df_sagemaker['Challenge_topic_macro'],\n",
    "        opacity=0.5,\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge count.png'))\n",
    "\n",
    "# Challenge score\n",
    "fig_challenge_score = go.Figure()\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_score.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_score'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_score.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_score.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge score.png'))\n",
    "\n",
    "# Challenge favorite count\n",
    "fig_challenge_favorite_count = go.Figure()\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_favorite_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_favorite_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_favorite_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_favorite_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge favorite count.png'))\n",
    "\n",
    "# Challenge link count\n",
    "fig_challenge_link_count = go.Figure()\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_link_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_link_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_link_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_link_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge link count.png'))\n",
    "\n",
    "# Challenge readability\n",
    "fig_challenge_readability = go.Figure()\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_readability.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_readability'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_readability.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_readability.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge readability.png'))\n",
    "\n",
    "# Challenge view count\n",
    "fig_challenge_view_count = go.Figure()\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_view_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_view_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_view_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_view_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge view count.png'))\n",
    "\n",
    "# Challenge answer count\n",
    "fig_challenge_answer_count = go.Figure()\n",
    "fig_challenge_answer_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_answer_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_answer_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_answer_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_answer_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_answer_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge answer count.png'))\n",
    "\n",
    "# Challenge comment count\n",
    "fig_challenge_comment_count = go.Figure()\n",
    "fig_challenge_comment_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_comment_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_comment_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_comment_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_comment_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_comment_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge comment count.png'))\n",
    "\n",
    "# Challenge participation count\n",
    "fig_challenge_participation_count = go.Figure()\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_topic_macro'],\n",
    "        y=df_azureml['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_participation_count.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_participation_count.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_participation_count.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge participation count.png'))\n",
    "\n",
    "# Challenge solved time\n",
    "fig_challenge_solved_time = go.Figure()\n",
    "fig_challenge_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_azureml['Challenge_solved_time'],\n",
    "        y=df_azureml['Challenge_participation_count'],\n",
    "        meanline_visible=True,\n",
    "        line_color='blue',\n",
    "        side='positive',\n",
    "        opacity=0.5,\n",
    "        legendgroup='SageMaker',\n",
    "        scalegroup='SageMaker',\n",
    "        name='SageMaker',\n",
    "    ))\n",
    "fig_challenge_solved_time.add_trace(\n",
    "    go.Violin(\n",
    "        x=df_sagemaker['Challenge_topic_macro'],\n",
    "        y=df_sagemaker['Challenge_solved_time'],\n",
    "        meanline_visible=True,\n",
    "        line_color='orange',\n",
    "        side='negative',\n",
    "        opacity=0.5,\n",
    "        legendgroup='AzureML',\n",
    "        scalegroup='AzureML',\n",
    "        name='AzureML',\n",
    "    ))\n",
    "fig_challenge_solved_time.update_layout(\n",
    "    height=1000,\n",
    "    width=2000,\n",
    "    font=dict(size=20),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "fig_challenge_solved_time.write_image(os.path.join(\n",
    "    path_challenge_azureml_sagemaker, 'Challenge solved time.png'))\n",
    "\n",
    "for name_group, color in zip(df.groupby('Challenge_topic_macro'), colors):\n",
    "    name, group = name_group\n",
    "    sagemaker = group[group['Tool'] == 'Amazon SageMaker']\n",
    "    azureml = group[group['Tool'] == 'Azure Machine Learning']\n",
    "\n",
    "    # Challenge score\n",
    "    challenge_score_azureml = azureml[azureml['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    challenge_score_sagemaker = sagemaker[sagemaker['Challenge_score'].notna(\n",
    "    )]['Challenge_score']\n",
    "    if len(challenge_score_azureml) * len(challenge_score_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_score_azureml, challenge_score_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge score')\n",
    "\n",
    "    # Challenge favorite count\n",
    "    challenge_favorite_count_azureml = azureml[azureml['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    challenge_favorite_count_sagemaker = sagemaker[sagemaker['Challenge_favorite_count'].notna(\n",
    "    )]['Challenge_favorite_count']\n",
    "    if len(challenge_favorite_count_azureml) * len(challenge_favorite_count_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_favorite_count_azureml,\n",
    "                            challenge_favorite_count_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge favorite count')\n",
    "\n",
    "    # Challenge link count\n",
    "    challenge_link_count_azureml = azureml[azureml['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    challenge_link_count_sagemaker = sagemaker[sagemaker['Challenge_link_count'].notna(\n",
    "    )]['Challenge_link_count']\n",
    "    if len(challenge_link_count_azureml) * len(challenge_link_count_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_link_count_azureml,\n",
    "                            challenge_link_count_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge link count')\n",
    "\n",
    "    # Challenge readability\n",
    "    challenge_readability_azureml = azureml[azureml['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    challenge_readability_sagemaker = sagemaker[sagemaker['Challenge_readability'].notna(\n",
    "    )]['Challenge_readability']\n",
    "    if len(challenge_readability_azureml) * len(challenge_readability_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_readability_azureml,\n",
    "                            challenge_readability_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge readability')\n",
    "\n",
    "    # Challenge view count\n",
    "    challenge_view_count_azureml = azureml[azureml['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    challenge_view_count_sagemaker = sagemaker[sagemaker['Challenge_view_count'].notna(\n",
    "    )]['Challenge_view_count']\n",
    "    if len(challenge_view_count_azureml) * len(challenge_view_count_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_view_count_azureml,\n",
    "                            challenge_view_count_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge answer count\n",
    "    challenge_answer_count_azureml = azureml['Challenge_answer_count']\n",
    "    challenge_answer_count_sagemaker = sagemaker['Challenge_answer_count']\n",
    "    if len(challenge_answer_count_azureml) * len(challenge_answer_count_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_answer_count_azureml,\n",
    "                            challenge_answer_count_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge answer count')\n",
    "\n",
    "    # Challenge comment count\n",
    "    challenge_comment_count_azureml = azureml['Challenge_comment_count']\n",
    "    challenge_comment_count_sagemaker = sagemaker['Challenge_comment_count']\n",
    "    if len(challenge_comment_count_azureml) * len(challenge_comment_count_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_comment_count_azureml,\n",
    "                            challenge_comment_count_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge comment count')\n",
    "\n",
    "    # Challenge participation count\n",
    "    challenge_participation_count_azureml = azureml['Challenge_participation_count']\n",
    "    challenge_participation_count_sagemaker = sagemaker['Challenge_participation_count']\n",
    "    if len(challenge_participation_count_azureml) * len(challenge_participation_count_sagemaker) > 0:\n",
    "        _, p = mannwhitneyu(challenge_participation_count_azureml,\n",
    "                            challenge_participation_count_sagemaker)\n",
    "        if p < alpha:\n",
    "            print(\n",
    "                f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML challenge regarding higher level topic {name} in challenge participation count')\n",
    "\n",
    "# Challenge mean solved time\n",
    "challenge_mean_solved_time_azureml = df_azureml[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_solved_time']\n",
    "challenge_mean_solved_time_sagemaker = df_sagemaker[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').mean()['Challenge_solved_time']\n",
    "_, p = mannwhitneyu(challenge_mean_solved_time_azureml,\n",
    "                    challenge_mean_solved_time_sagemaker)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML in higher level mean challenge solved time')\n",
    "\n",
    "# Challenge median solved time\n",
    "challenge_median_solved_time_azureml = df_azureml[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_solved_time']\n",
    "challenge_median_solved_time_sagemaker = df_sagemaker[['Challenge_topic_macro', 'Challenge_solved_time']].groupby(\n",
    "    'Challenge_topic_macro').median()['Challenge_solved_time']\n",
    "_, p = mannwhitneyu(challenge_median_solved_time_azureml,\n",
    "                    challenge_median_solved_time_sagemaker)\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating different distribution of SageMaker vs AzureML in higher level median challenge solved time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "\n",
    "df_topics = []\n",
    "\n",
    "for name, group in df.groupby('Challenge_topic_macro'):\n",
    "    Challenge_count = group['Challenge_topic_macro'].count()\n",
    "    Challenge_solved_ratio = group['Challenge_closed_time'].notna(\n",
    "    ).sum() / Challenge_count\n",
    "    Challenge_mean_score = group['Challenge_score'].mean()\n",
    "    Challenge_mean_favorite_count = group['Challenge_favorite_count'].mean()\n",
    "    Challenge_mean_link_count = group['Challenge_link_count'].mean()\n",
    "    Challenge_mean_information_entropy = group['Challenge_information_entropy'].mean(\n",
    "    )\n",
    "    Challenge_mean_readability = group['Challenge_readability'].mean()\n",
    "    Challenge_mean_sentence_count = group['Challenge_sentence_count'].mean()\n",
    "    Challenge_mean_word_count = group['Challenge_word_count'].mean()\n",
    "    Challenge_mean_unique_word_count = group['Challenge_unique_word_count'].mean(\n",
    "    )\n",
    "    Challenge_mean_view_count = group['Challenge_view_count'].mean()\n",
    "    Challenge_mean_answer_count = group['Challenge_answer_count'].mean()\n",
    "    Challenge_mean_comment_count = group['Challenge_comment_count'].mean()\n",
    "\n",
    "    Solution_mean_score = group['Solution_score'].mean()\n",
    "    Solution_mean_link_count = group['Solution_link_count'].mean()\n",
    "    Solution_mean_information_entropy = group['Solution_information_entropy'].mean(\n",
    "    )\n",
    "    Solution_mean_readability = group['Solution_readability'].mean()\n",
    "    Solution_mean_sentence_count = group['Solution_sentence_count'].mean()\n",
    "    Solution_mean_word_count = group['Solution_word_count'].mean()\n",
    "    Solution_mean_unique_word_count = group['Solution_unique_word_count'].mean(\n",
    "    )\n",
    "    Solution_mean_comment_count = group['Solution_comment_count'].mean()\n",
    "\n",
    "    Challenge_mean_solved_time = group['Challenge_solved_time'].mean()\n",
    "    Challenge_median_solved_time = group['Challenge_solved_time'].median()\n",
    "    Challenge_adjusted_mean_solved_time = group['Challenge_adjusted_solved_time'].mean(\n",
    "    )\n",
    "    Challenge_adjusted_meadian_solved_time = group['Challenge_adjusted_solved_time'].median(\n",
    "    )\n",
    "\n",
    "    topic_info = {\n",
    "        'Challenge topic': name,\n",
    "        'Challenge count': Challenge_count,\n",
    "        'Challenge solved ratio': Challenge_solved_ratio,\n",
    "        'Challenge mean score': Challenge_mean_score,\n",
    "        'Challenge mean favorite count': Challenge_mean_favorite_count,\n",
    "        'Challenge mean link count': Challenge_mean_link_count,\n",
    "        'Challenge mean information entropy': Challenge_mean_information_entropy,\n",
    "        'Challenge mean readability': Challenge_mean_readability,\n",
    "        'Challenge mean sentence count': Challenge_mean_sentence_count,\n",
    "        'Challenge mean word count': Challenge_mean_word_count,\n",
    "        'Challenge mean unique word count': Challenge_mean_unique_word_count,\n",
    "        'Challenge mean view count': Challenge_mean_view_count,\n",
    "        'Challenge mean answer count': Challenge_mean_answer_count,\n",
    "        'Challenge mean comment count': Challenge_mean_comment_count,\n",
    "\n",
    "        'Solution mean score': Solution_mean_score,\n",
    "        'Solution mean link count': Solution_mean_link_count,\n",
    "        'Solution mean information entropy': Solution_mean_information_entropy,\n",
    "        'Solution mean readability': Solution_mean_readability,\n",
    "        'Solution mean sentence count': Solution_mean_sentence_count,\n",
    "        'Solution mean word count': Solution_mean_word_count,\n",
    "        'Solution mean unique word count': Solution_mean_unique_word_count,\n",
    "        'Solution mean comment count': Solution_mean_comment_count,\n",
    "\n",
    "        'Challenge mean solved time': Challenge_mean_solved_time,\n",
    "        'Challenge median solved time': Challenge_median_solved_time,\n",
    "        'Challenge adjusted mean solved time': Challenge_adjusted_mean_solved_time,\n",
    "        'Challenge adjusted median solved time': Challenge_adjusted_meadian_solved_time,\n",
    "    }\n",
    "    df_topics.append(topic_info)\n",
    "\n",
    "df_topics = pd.DataFrame(df_topics)\n",
    "df_topics.to_json(os.path.join(path_challenge, 'general.json'),\n",
    "                  indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              OLS Regression Results                             \n",
      "=================================================================================\n",
      "Dep. Variable:     Challenge_solved_time   R-squared:                       0.035\n",
      "Model:                               OLS   Adj. R-squared:                  0.003\n",
      "Method:                    Least Squares   F-statistic:                     1.098\n",
      "Date:                   Fri, 14 Apr 2023   Prob (F-statistic):              0.362\n",
      "Time:                           12:04:50   Log-Likelihood:                -2941.6\n",
      "No. Observations:                    347   AIC:                             5907.\n",
      "Df Residuals:                        335   BIC:                             5953.\n",
      "Df Model:                             11                                         \n",
      "Covariance Type:               nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercept                         3.7135   1281.529      0.003      0.998   -2517.145    2524.572\n",
      "Challenge_link_count            -30.3836     39.799     -0.763      0.446    -108.670      47.903\n",
      "Challenge_information_entropy    35.3655    247.257      0.143      0.886    -451.006     521.737\n",
      "Challenge_readability            24.0970     14.691      1.640      0.102      -4.801      52.995\n",
      "Challenge_sentence_count         -0.4755      2.122     -0.224      0.823      -4.650       3.699\n",
      "Challenge_word_count              0.1282      0.628      0.204      0.838      -1.106       1.363\n",
      "Challenge_unique_word_count      -1.2063      3.139     -0.384      0.701      -7.380       4.968\n",
      "Challenge_score                  44.5149     24.543      1.814      0.071      -3.763      92.793\n",
      "Challenge_view_count             -0.0125      0.018     -0.705      0.482      -0.047       0.022\n",
      "Challenge_favorite_count       -140.9158     78.759     -1.789      0.074    -295.839      14.008\n",
      "Challenge_answer_count          126.4996     78.914      1.603      0.110     -28.730     281.729\n",
      "Challenge_comment_count           5.2985     41.431      0.128      0.898     -76.198      86.795\n",
      "==============================================================================\n",
      "Omnibus:                      412.338   Durbin-Watson:                   1.847\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            21536.308\n",
      "Skew:                           5.517   Prob(JB):                         0.00\n",
      "Kurtosis:                      39.984   Cond. No.                     1.28e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.28e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# perform Breusch-Pagan test for heteroskedasticity in a linear regression model\n",
    "\n",
    "df = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "# df = df[df['Challenge_solved_time'].notna()]\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imp = SimpleImputer(missing_values=np.nan, strategy='median', axis=0)\n",
    "# imp.fit(df.values)\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import numpy as np\n",
    "\n",
    "# # create a sample dataset\n",
    "# X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# y = np.array([4, 5, 6])\n",
    "\n",
    "# # create a linear regression object\n",
    "# model = LinearRegression().fit(X, y)\n",
    "\n",
    "# # fit the model on your data\n",
    "# # lr.fit(X, y)\n",
    "\n",
    "# # get feature importance using coef_ attribute\n",
    "# feature_importance = model.coef_\n",
    "\n",
    "# print(feature_importance)\n",
    "\n",
    "# df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "# df['Challenge_link_count'] = df['Challenge_link_count'].fillna(0)\n",
    "# df['Challenge_information_entropy'] = df['Challenge_information_entropy'].fillna(0)\n",
    "# df['Challenge_readability'] = df['Challenge_readability'].fillna(0)\n",
    "# df['Challenge_sentence_count'] = df['Challenge_sentence_count'].fillna(0)\n",
    "# df['Challenge_word_count'] = df['Challenge_word_count'].fillna(0)\n",
    "# df['Challenge_unique_word_count'] = df['Challenge_unique_word_count'].fillna(0)\n",
    "# df['Challenge_score'] = df['Challenge_score'].fillna(0)\n",
    "# df['Challenge_view_count'] = df['Challenge_view_count'].fillna(0)\n",
    "# df['Challenge_favorite_count'] = df['Challenge_favorite_count'].fillna(0)\n",
    "# df['Challenge_answer_count'] = df['Challenge_answer_count'].fillna(0)\n",
    "# df['Challenge_comment_count'] = df['Challenge_comment_count'].fillna(0)\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# X = df[['Challenge_link_count', 'Challenge_information_entropy', 'Challenge_readability', 'Challenge_sentence_count', 'Challenge_word_count', 'Challenge_unique_word_count', 'Challenge_score', 'Challenge_view_count', 'Challenge_favorite_count', 'Challenge_answer_count', 'Challenge_comment_count']]\n",
    "# y = df['Challenge_solved_time']\n",
    "# model = LogisticRegression().fit(X, y)\n",
    "\n",
    "model1 = sm.formula.ols('Challenge_solved_time ~ Challenge_link_count + Challenge_information_entropy + Challenge_readability + Challenge_sentence_count + Challenge_word_count + Challenge_unique_word_count + Challenge_score + Challenge_view_count + Challenge_favorite_count + Challenge_answer_count + Challenge_comment_count', data=df).fit()\n",
    "bp_test = het_breuschpagan(model1.resid, model1.model.exog)\n",
    "if bp_test[1] < alpha:\n",
    "    print(\n",
    "        f'p = {bp_test[1]}, indicating this regression model violates the homoscedasticity assumption')\n",
    "print(model1.summary())\n",
    "\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# Get the permutation importance of each predictor variable\n",
    "# result = permutation_importance(model, X, y, n_repeats=10)\n",
    "\n",
    "# Print the feature importance scores\n",
    "# print(result.importances_mean)\n",
    "\n",
    "# model2 = sm.formula.ols('Challenge_solved_time ~ Challenge_score + Challenge_view_count + Challenge_favorite_count + Challenge_answer_count + Challenge_comment_count', data=df).fit()\n",
    "# anova_results = sm.stats.anova_lm(model1, model2)\n",
    "# print(anova_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.0015991239950436504, indicating there is statistically significant correlation between challenge mean link count vs median solved time\n",
      "p = 0.0003168367306850429, indicating there is statistically significant correlation between challenge mean information entropy vs median solved time\n",
      "p = 0.00010628137079463929, indicating there is statistically significant correlation between challenge mean word count vs median solved time\n",
      "p = 4.687577349865672e-05, indicating there is statistically significant correlation between challenge mean sentence count vs median solved time\n",
      "p = 2.9987557433586082e-05, indicating there is statistically significant correlation between challenge mean unique word count vs median solved time\n",
      "p = 0.030424777563564508, indicating there is statistically significant correlation between challenge mean readability vs median solved time\n",
      "p = 0.0003859853481081621, indicating there is statistically significant correlation between challenge median vs mean solved time\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_challenge, 'general.json'))\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean score'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean score')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean favorite count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean favorite count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean follower count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean follower count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean link count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean link count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean information entropy'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean information entropy')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean readability'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean readability')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean sentence count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean sentence count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean word count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean word count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean unique word count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean unique word count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean view count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean view count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean answer count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean answer count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean comment count'],\n",
    "                df_topics['Challenge solved ratio'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs mean comment count')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge solved ratio'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge solved ratio vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean score'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean score vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean favorite count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean favorite count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean follower count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean follower count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean link count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean link count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean information entropy'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean information entropy vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean word count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean word count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean sentence count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean sentence count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean unique word count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean unique word count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean readability'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean readability vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean view count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean view count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean comment count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean comment count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean answer count'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge mean answer count vs median solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge mean solved time'],\n",
    "                df_topics['Challenge median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge median vs mean solved time')\n",
    "\n",
    "_, p = pearsonr(df_topics['Challenge adjusted mean solved time'],\n",
    "                df_topics['Challenge adjusted median solved time'])\n",
    "if p < alpha:\n",
    "    print(f'p = {p:.2f}, indicating there is statistically significant correlation between challenge adjusted median vs mean solved time')\n",
    "\n",
    "# Plot median solved time against mean solved time for each topic using count ratio as size\n",
    "fig = px.scatter(df_topics, y=\"Challenge median solved time\", x=\"Challenge mean solved time\",\n",
    "                 color=\"Challenge topic\", hover_name=\"Challenge topic\", size=\"Challenge count\", trendline=\"ols\")\n",
    "fig.update_layout(\n",
    "    width=2000,\n",
    "    height=1000,\n",
    "    margin=dict(l=0, r=0, t=0, b=0))\n",
    "fig.write_image(os.path.join(\n",
    "    path_challenge, 'Challenge median vs mean solved time.png'))\n",
    "\n",
    "# Plot adjusted median solved time against adjusted mean solved time for each topic using count ratio as size\n",
    "fig = px.scatter(df_topics, y=\"Challenge adjusted median solved time\", x=\"Challenge adjusted mean solved time\",\n",
    "                 color=\"Challenge topic\", hover_name=\"Challenge topic\", size=\"Challenge count\", trendline=\"ols\")\n",
    "fig.update_layout(\n",
    "    width=2000,\n",
    "    height=1000,\n",
    "    margin=dict(l=0, r=0, t=0, b=0))\n",
    "fig.write_image(os.path.join(\n",
    "    path_challenge, 'Challenge adjusted median vs mean solved time.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, y, xgrid, lowess_kw=None):\n",
    "    samples = np.random.choice(len(x), 50, replace=True)\n",
    "    y_s = y[samples]\n",
    "    x_s = x[samples]\n",
    "    y_sm = sm_lowess(y_s, x_s, **lowess_kw)\n",
    "    # regularly sample it onto the grid\n",
    "    y_grid = scipy.interpolate.interp1d(\n",
    "        x_s, y_sm, fill_value='extrapolate')(xgrid)\n",
    "    return y_grid\n",
    "\n",
    "\n",
    "def lowess_with_confidence_bounds(x, y, conf_interval=0.95, lowess_kw=None):\n",
    "    \"\"\"\n",
    "    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n",
    "    \"\"\"\n",
    "    xgrid = np.linspace(x.min(), x.max())\n",
    "\n",
    "    K = 100\n",
    "    smooths = np.stack([smooth(x, y, xgrid, lowess_kw) for _ in range(K)]).T\n",
    "\n",
    "    mean = np.nanmean(smooths, axis=1)\n",
    "    stderr = scipy.stats.sem(smooths, axis=1)\n",
    "\n",
    "    clower = np.nanpercentile(smooths, (1-conf_interval)*50, axis=1)\n",
    "    cupper = np.nanpercentile(smooths, (1+conf_interval)*50, axis=1)\n",
    "\n",
    "    return xgrid, mean, stderr, clower, cupper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-08-08 14:04:22.160000'),\n",
       " Timestamp('2023-02-22 01:36:03.995000'))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_challenge = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "# # BigQuery Stack Overflow public dataset is updated until Nov 24, 2022, 1:39:22 PM UTC-5\n",
    "# min(df_challenge['Challenge_creation_time']), max(\n",
    "#     df_challenge['Challenge_creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore challenge topics evolution\n",
    "\n",
    "df_challenge = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "# df_challenge = df_challenge[(df_challenge['Challenge_creation_time'] > '2014-09-14')\n",
    "#                             & (df_challenge['Challenge_creation_time'] < '2022-11-4')]\n",
    "\n",
    "for name, group in df_challenge.groupby('Challenge_topic_macro'):\n",
    "    group = group.groupby(pd.Grouper(key='Challenge_creation_time', freq='2W')).agg(\n",
    "        Count=('Challenge_topic_macro', 'count')).reset_index()\n",
    "    print(group)\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    x = np.array([i.astype('datetime64[D]').astype(int) for i in x])\n",
    "    y = group['Count'].values\n",
    "    xgrid, mean, stderr, clower, cupper = lowess_with_confidence_bounds(\n",
    "        x, y, conf_interval=1-alpha, lowess_kw={\"frac\": 0.5, \"it\": 5, \"return_sorted\": False})\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.plot(x, y, 'k.', label='Observations')\n",
    "    plt.plot(xgrid, mean, color='tomato', label='LOWESS')\n",
    "    plt.fill_between(xgrid, clower, cupper, alpha=0.3,\n",
    "                     label='LOWESS uncertainty')\n",
    "    plt.legend(loc='best')\n",
    "    # fig.savefig(os.path.join(path_challenge_evolution,\n",
    "    #             f'Topic_{name}'), bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore challenge topics evolution\n",
    "\n",
    "df_challenge = pd.read_json(os.path.join(path_general, 'filtered.json'))\n",
    "df_challenge = df_challenge[(df_challenge['Challenge_creation_time'] > '2014-09-14')\n",
    "                            & (df_challenge['Challenge_creation_time'] < '2022-11-21')]\n",
    "\n",
    "for name, group in df_challenge.groupby('Challenge_topic_macro'):\n",
    "    group = group.groupby(pd.Grouper(key='Challenge_creation_time', freq='2W')).agg(\n",
    "        Count=('Challenge_view_count', 'count')).reset_index()\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    x = np.array([i.astype('datetime64[D]').astype(int) for i in x])\n",
    "    y = group['Count'].values\n",
    "    xgrid, mean, stderr, clower, cupper = lowess_with_confidence_bounds(\n",
    "        x, y, conf_interval=1-alpha, lowess_kw={\"frac\": 0.5, \"it\": 5, \"return_sorted\": False})\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.plot(x, y, 'k.', label='Observations')\n",
    "    plt.plot(xgrid, mean, color='tomato', label='LOWESS')\n",
    "    plt.fill_between(xgrid, clower, cupper, alpha=0.3,\n",
    "                     label='LOWESS uncertainty')\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(os.path.join(path_challenge_evolution,\n",
    "                f'Topic_view_count_{name}'), bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
