{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\",\n",
    "              None, 'display.max_colwidth', None)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "path_result = os.path.join(os.path.dirname(os.getcwd()), 'Result')\n",
    "if not os.path.exists(path_result):\n",
    "    os.makedirs(path_result)\n",
    "\n",
    "path_general = os.path.join(path_result, 'General')\n",
    "if not os.path.exists(path_general):\n",
    "    os.makedirs(path_general)\n",
    "\n",
    "path_challenge = os.path.join(path_result, 'Challenge')\n",
    "if not os.path.exists(path_challenge):\n",
    "    os.makedirs(path_challenge)\n",
    "\n",
    "path_solution = os.path.join(path_result, 'Solution')\n",
    "if not os.path.exists(path_solution):\n",
    "    os.makedirs(path_solution)\n",
    "\n",
    "path_challenge_information = os.path.join(path_challenge, 'Information')\n",
    "if not os.path.exists(path_challenge_information):\n",
    "    os.makedirs(path_challenge_information)\n",
    "\n",
    "path_solution_information = os.path.join(path_solution, 'Information')\n",
    "if not os.path.exists(path_solution_information):\n",
    "    os.makedirs(path_solution_information)\n",
    "\n",
    "path_challenge_evolution = os.path.join(path_challenge, 'Evolution')\n",
    "if not os.path.exists(path_challenge_evolution):\n",
    "    os.makedirs(path_challenge_evolution)\n",
    "\n",
    "path_solution_evolution = os.path.join(path_solution, 'Evolution')\n",
    "if not os.path.exists(path_solution_evolution):\n",
    "    os.makedirs(path_solution_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Environment Setup - Setting up software environments for development and execution\n",
      "Topic 1: Pipeline Automation - Automating the execution of data processing pipelines\n",
      "Topic 2: Docker - Containerization platform for building, shipping, and running applications\n",
      "Topic 3: Hyperparameter Tuning - Optimizing model performance by tuning hyperparameters\n",
      "Topic 4: Git Version Control - Tracking changes to code and collaborating with others\n",
      "Topic 5: GPU Acceleration - Using graphics processing units to speed up machine learning tasks\n",
      "Topic 6: Artifact Management - Managing and storing artifacts such as models, datasets, and code\n",
      "Topic 7: Model Deployment - Deploying machine learning models for use in production environments\n",
      "Topic 8: Data Labeling - Assigning labels to data for use in supervised learning tasks\n",
      "Topic 9: Data Visualization - Creating visual representations of data for analysis and communication\n",
      "Topic 10: Logging Metrics - Recording and tracking performance metrics during model training and evaluation\n",
      "Topic 11: Account Management - Managing user accounts and access to resources\n",
      "Topic 12: Apache Spark - Open-source distributed computing system for big data processing\n",
      "Topic 13: TensorFlow - Open-source machine learning framework for building and training models\n",
      "Topic 14: Text Processing - Analyzing and manipulating text data\n",
      "Topic 15: Pandas DataFrames - Data structure for manipulating and analyzing tabular data\n",
      "Topic 16: Model Export - Saving and exporting trained machine learning models\n",
      "Topic 17: Role-Based Access Control - Controlling access to resources based on user roles and permissions\n",
      "Topic 18: Batch Processing - Processing large amounts of data in batches\n",
      "Topic 19: Model Registry - Managing and versioning machine learning models\n",
      "Topic 20: Database Connectivity - Connecting to and interacting with databases\n",
      "Topic 21: Resource Limitations - Setting and managing limits on resource usage\n",
      "Topic 22: API Invocation - Calling APIs to perform tasks or retrieve data\n",
      "Topic 23: AutoML Forecasting - Using automated machine learning to generate forecasts\n",
      "Topic 24: Column Manipulation - Working with and manipulating columns in datasets\n",
      "Topic 25: Computer Vision - Using machine learning to analyze and interpret visual data\n",
      "Topic 26: Web Service Deployment - Deploying machine learning models as web services\n",
      "Topic 27: Kubernetes - Open-source container orchestration platform for managing containerized applications\n",
      "Topic 28: Random Forest - Ensemble learning method for classification and regression tasks\n",
      "Topic 29: CSV Files - File format for storing and exchanging tabular data\n",
      "Topic 30: TensorBoard Logging - Visualizing and tracking model training and evaluation using TensorBoard\n",
      "Topic 31: Feature Roadmap - Planning and implementing new features for a platform or product\n",
      "Topic 32: Dataset Versioning - Managing and versioning datasets\n",
      "Topic 33: CloudWatch Logging - Monitoring and logging AWS resources and applications\n",
      "Topic 34: Speech-to-Text - Converting audio speech to text\n",
      "Topic 35: YAML Configuration - Using YAML files to configure applications and services\n",
      "Topic 36: Data Storage - Storing and accessing data in cloud-based storage solutions\n",
      "Topic 37: VPC Endpoints - Connecting to AWS services privately through a VPC\n",
      "Topic 38: Model Accuracy - Evaluating and improving the accuracy of machine learning models\n",
      "Topic 39: Model Input - Preparing and querying input data for machine learning models\n",
      "Topic 40: Bucket Access - Managing access to cloud-based storage buckets\n",
      "Topic 41: Run Management - Managing and monitoring the execution of jobs and tasks\n",
      "Topic 42: Model Prediction - Using trained machine learning models to make predictions\n",
      "Topic 43: Notebook Instances - Creating and managing cloud-based notebook instances for data analysis and experimentation\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of keywords for each topic, I want you to provide a description of each topic in a two-word phrase but guarantee that each description is exclusive to the other. Also, for each description, you need to attach short comments on what these keywords are talking about in general.'''\n",
    "\n",
    "with open(os.path.join(path_challenge, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "topic_term_list = []\n",
    "for index, topic in enumerate(topic_terms):\n",
    "    terms = ', '.join([term[0] for term in topic])\n",
    "    topic_term = f'Topic {index}: {terms}'\n",
    "    topic_term_list.append(topic_term)\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_topic +\n",
    "               '\\n###' + '\\n'.join(topic_term_list) + '###\\n'}],\n",
    "    temperature=0,\n",
    "    max_tokens=1500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    timeout=100,\n",
    "    stream=False)\n",
    "\n",
    "topic_challenge = completion.choices[0].message.content\n",
    "print(topic_challenge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping_challenge = {\n",
    "    -1: np.nan,\n",
    "    # Setting up software environments for development and execution\n",
    "    0: 'Environment Setup',\n",
    "    # Automating the execution of data processing pipelines\n",
    "    1: 'Pipeline Automation',\n",
    "    # Containerization platform for building, shipping, and running applications\n",
    "    2: 'Docker',\n",
    "    # Optimizing model performance by tuning hyperparameters\n",
    "    3: 'Hyperparameter Tuning',\n",
    "    # Tracking changes to code and collaborating with others\n",
    "    4: 'Git Version Control',\n",
    "    # Using graphics processing units to speed up machine learning tasks\n",
    "    5: 'GPU Acceleration',\n",
    "    # Managing and storing artifacts such as models, datasets, and code\n",
    "    6: 'Artifact Management',\n",
    "    # Deploying machine learning models for use in production environments\n",
    "    7: 'Model Deployment',\n",
    "    # Assigning labels to data for use in supervised learning tasks\n",
    "    8: 'Data Labeling',\n",
    "    # Creating visual representations of data for analysis and communication\n",
    "    9: 'Data Visualization',\n",
    "    # Recording and tracking performance metrics during model training and evaluation\n",
    "    10: 'Logging Metrics',\n",
    "    # Managing user accounts and access to resources\n",
    "    11: 'Account Management',\n",
    "    # Open-source distributed computing system for big data processing\n",
    "    12: 'Apache Spark',\n",
    "    # Open-source machine learning framework for building and training models\n",
    "    13: 'TensorFlow',\n",
    "    # Analyzing and manipulating text data\n",
    "    14: 'Text Processing',\n",
    "    # Data structure for manipulating and analyzing tabular data\n",
    "    15: 'Pandas DataFrames',\n",
    "    # Saving and exporting trained machine learning models\n",
    "    16: 'Model Export',\n",
    "    # Controlling access to resources based on user roles and permissions\n",
    "    17: 'Role-Based Access Control',\n",
    "    # Processing large amounts of data in batches\n",
    "    18: 'Batch Processing',\n",
    "    # Managing and versioning machine learning models\n",
    "    19: 'Model Registry',\n",
    "    # Connecting to and interacting with databases\n",
    "    20: 'Database Connectivity',\n",
    "    # Setting and managing limits on resource usage\n",
    "    21: 'Resource Quotas',\n",
    "    # Calling APIs to perform tasks or retrieve data\n",
    "    22: 'API Invocation',\n",
    "    # Using automated machine learning to generate forecasts\n",
    "    23: 'AutoML Forecasting',\n",
    "    # Working with and manipulating columns in datasets\n",
    "    24: 'Column Manipulation',\n",
    "    # Using machine learning to analyze and interpret visual data\n",
    "    25: 'Computer Vision',\n",
    "    # Deploying machine learning models as web services\n",
    "    26: 'Web Service Deployment',\n",
    "    # Open-source container orchestration platform for managing containerized applications\n",
    "    27: 'Kubernetes Management',\n",
    "    # Ensemble learning method for classification and regression tasks\n",
    "    28: 'Random Forest',\n",
    "    # File format for storing and exchanging tabular data\n",
    "    29: 'CSV Files',\n",
    "    # Visualizing and tracking model training and evaluation using TensorBoard\n",
    "    30: 'TensorBoard Logging',\n",
    "    # Planning and implementing new features for a platform or product\n",
    "    31: 'Feature Roadmap',\n",
    "    # Managing and versioning datasets\n",
    "    32: 'Dataset Versioning',\n",
    "    # Monitoring and logging AWS resources and applications\n",
    "    33: 'CloudWatch Logging',\n",
    "    # Converting audio speech to text\n",
    "    34: 'Speech-to-Text',\n",
    "    # Using YAML files to configure applications and services\n",
    "    35: 'YAML Configuration',\n",
    "    # Storing and accessing data in cloud-based storage solutions\n",
    "    36: 'Data Storage',\n",
    "    # Connecting to AWS services privately through a VPC\n",
    "    37: 'VPC Endpoints',\n",
    "    # Evaluating and improving the accuracy of machine learning models\n",
    "    38: 'Model Accuracy',\n",
    "    # Preparing and querying input data for machine learning models\n",
    "    39: 'Model Input',\n",
    "    # Managing access to cloud-based storage buckets\n",
    "    40: 'Bucket Access',\n",
    "    # Managing and monitoring the execution of jobs and tasks\n",
    "    41: 'Run Management',\n",
    "    # Using trained machine learning models to make predictions\n",
    "    42: 'Model Inference',\n",
    "    # Creating and managing cloud-based notebook instances for data analysis and experimentation\n",
    "    43: 'Notebook Instances',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Git Management - This topic is about managing files and repositories using Git, including tracking changes, pushing updates, and managing repositories.\n",
      "Topic 1: Role-Based Access Control - This topic is about controlling access to resources based on user roles and permissions, including creating, attaching, and executing roles.\n",
      "Topic 2: Package Management - This topic is about managing software packages and environments using tools like Conda and Pip, including installation, updating, and environment management.\n",
      "Topic 3: Logging and Metrics - This topic is about logging and tracking metrics in a pipeline, including logging data, creating tables, and using loggers.\n",
      "Topic 4: Dataset Management - This topic is about managing datasets and columns, including primary datasets, target columns, and scoring models.\n",
      "Topic 5: Docker Management - This topic is about managing Docker containers and images, including creating, running, and managing Docker environments.\n",
      "Topic 6: Parameter Management - This topic is about managing programmatic parameters, including setting and configuring hyperparameters, global parameters, and standard Python parameter types.\n",
      "Topic 7: YAML Configuration - This topic is about configuring pipelines and stages using YAML files, including specifying directories, paths, and output stages.\n",
      "Topic 8: Endpoint Management - This topic is about managing endpoints and APIs, including creating, deploying, and configuring endpoints for REST and web services.\n",
      "Topic 9: Jupyter Notebook - This topic is about running and managing Jupyter Notebooks, including opening files, restarting kernels, and managing directories.\n",
      "Topic 10: Pandas Dataframe - This topic is about managing tabular datasets using Pandas dataframes, including creating, scoring, and manipulating datasets.\n",
      "Topic 11: TensorFlow Management - This topic is about managing TensorFlow models, including installation, training, and logging.\n",
      "Topic 12: Artifact Management - This topic is about managing artifacts, including uploading, downloading, and storing files and paths.\n",
      "Topic 13: Model Deployment - This topic is about deploying models to endpoints, including creating, deploying, and managing endpoints for cloud and model services.\n",
      "Topic 14: Random Forest - This topic is about using the random forest algorithm for machine learning, including building, training, and cutting forests.\n",
      "Topic 15: Pipeline Modeling - This topic is about modeling pipelines, including building, inputting, and parameterizing pipelines for API and object use.\n",
      "Topic 16: JSON Payload - This topic is about managing JSON payloads, including formatting, serializing, and loading data.\n",
      "Topic 17: Remote Configuration - This topic is about configuring remote resources, including adding, modifying, and running remote URLs and resources.\n",
      "Topic 18: Spark Management - This topic is about managing Spark clusters and datasets, including running, testing, and using Spark for machine learning.\n",
      "Topic 19: Python Model - This topic is about implementing Python models, including using PyFunc and PythonModel interfaces, importing models, and loading models.\n",
      "Topic 20: Data Upload/Download - This topic is about uploading and downloading data and files, including saving CSV files and uploading file objects.\n",
      "Topic 21: Cluster Computing - This topic is about using parallel computing and clusters for machine learning, including exploring, creating, and running clusters.\n",
      "Topic 22: Pipeline Data - This topic is about managing pipeline data, including inputting datasets, creating pipeline classes, and using file inputs.\n",
      "Topic 23: CSV Formatting - This topic is about managing CSV files and formatting, including writing, converting, and importing CSV data.\n",
      "Topic 24: Model Registry - This topic is about registering and managing models, including registering models, managing metadata, and versioning models.\n",
      "Topic 25: Memory Management - This topic is about managing memory and distributing training for large datasets, including using CPUs and notebooks for training.\n",
      "Topic 26: Neural Networks - This topic is about using neural networks for machine learning, including NLP, vision, OCR, and speech-to-text analysis.\n",
      "Topic 27: SDK Versioning - This topic is about managing SDK versions, including updating, upgrading, and installing SDKs like PyTorch.\n",
      "Topic 28: Lambda Invocation - This topic is about invoking endpoints and APIs using Lambda functions and API gateways, including waiting for responses and using SDKs.\n"
     ]
    }
   ],
   "source": [
    "prompt_topic = '''You will be given a list of keywords for each topic, I want you to provide a description of each topic in a two-word phrase but guarantee that each description is exclusive to the other. Also, for each description, you need to attach short comments on what these keywords are talking about in general.'''\n",
    "\n",
    "with open(os.path.join(path_solution, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "topic_term_list = []\n",
    "for index, topic in enumerate(topic_terms):\n",
    "    terms = ', '.join([term[0] for term in topic])\n",
    "    topic_term = f'Topic {index}: {terms}'\n",
    "    topic_term_list.append(topic_term)\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_topic +\n",
    "               '\\n###' + '\\n'.join(topic_term_list) + '###\\n'}],\n",
    "    temperature=0,\n",
    "    max_tokens=1500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    timeout=100,\n",
    "    stream=False)\n",
    "\n",
    "topic_solution = completion.choices[0].message.content\n",
    "print(topic_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping_solution = {\n",
    "    -1: np.nan,\n",
    "    # This topic is about managing files and repositories using Git, including tracking changes, pushing updates, and managing repositories.\n",
    "    0: 'Git Version Control',\n",
    "    # This topic is about controlling access to resources based on user roles and permissions, including creating, attaching, and executing roles.\n",
    "    1: 'Role-Based Access Control',\n",
    "    # This topic is about managing software packages and environments using tools like Conda and Pip, including installation, updating, and environment management.\n",
    "    2: 'Package Management',\n",
    "    # This topic is about logging and tracking metrics in a pipeline, including logging data, creating tables, and using loggers.\n",
    "    3: 'Logging and Metrics',\n",
    "    # This topic is about managing datasets and columns, including primary datasets, target columns, and scoring models.\n",
    "    4: 'Dataset Management',\n",
    "    # This topic is about managing Docker containers and images, including creating, running, and managing Docker environments.\n",
    "    5: 'Docker',\n",
    "    # This topic is about managing programmatic parameters, including setting and configuring hyperparameters, global parameters, and standard Python parameter types.\n",
    "    6: 'Parameter Management',\n",
    "    # This topic is about configuring pipelines and stages using YAML files, including specifying directories, paths, and output stages.\n",
    "    7: 'YAML Configuration',\n",
    "    # This topic is about managing endpoints and APIs, including creating, deploying, and configuring endpoints for REST and web services.\n",
    "    8: 'Endpoint Management',\n",
    "    # This topic is about running and managing Jupyter Notebooks, including opening files, restarting kernels, and managing directories.\n",
    "    9: 'Jupyter Notebook',\n",
    "    # This topic is about managing tabular datasets using Pandas dataframes, including creating, scoring, and manipulating datasets.\n",
    "    10: 'Pandas Dataframe',\n",
    "    # This topic is about managing TensorFlow models, including installation, training, and logging.\n",
    "    11: 'TensorFlow',\n",
    "    # This topic is about managing artifacts, including uploading, downloading, and storing\n",
    "    12: 'Artifact Management',\n",
    "    # This topic is about deploying models to endpoints, including creating, deploying, and managing endpoints for cloud and model services.\n",
    "    13: 'Model Deployment',\n",
    "    # This topic is about using the random forest algorithm for machine learning, including building, training, and cutting forests.\n",
    "    14: 'Random Forest',\n",
    "    # This topic is about modeling pipelines, including building, inputting, and parameterizing pipelines for API and object use.\n",
    "    15: 'Pipeline Modeling',\n",
    "    # This topic is about managing JSON payloads, including formatting, serializing, and loading data.\n",
    "    16: 'JSON Payload',\n",
    "    # This topic is about configuring remote resources, including adding, modifying, and running remote URLs and resources.\n",
    "    17: 'Remote Configuration',\n",
    "    # This topic is about managing Spark clusters and datasets, including running, testing, and using Spark for machine learning.\n",
    "    18: 'Apache Spark',\n",
    "    # This topic is about implementing Python models, including using PyFunc and PythonModel interfaces, importing models, and loading models.\n",
    "    19: 'Python Model',\n",
    "    # This topic is about uploading and downloading data and files, including saving CSV files and uploading file objects.\n",
    "    20: 'Data Transfer',\n",
    "    # This topic is about using parallel computing and clusters for machine learning, including exploring, creating, and running clusters.\n",
    "    21: 'Cluster Computing',\n",
    "    # This topic is about managing pipeline data, including inputting datasets, creating pipeline classes, and using file inputs.\n",
    "    22: 'Pipeline Data',\n",
    "    # This topic is about managing CSV files and formatting, including writing, converting, and importing CSV data.\n",
    "    23: 'CSV Files',\n",
    "    # This topic is about registering and managing models, including registering models, managing metadata, and versioning models.\n",
    "    24: 'Model Registry',\n",
    "    # This topic is about managing memory and distributing training for large datasets, including using CPUs and notebooks for training.\n",
    "    25: 'Memory Management',\n",
    "    # This topic is about using neural networks for machine learning, including NLP, vision, OCR, and speech-to-text analysis.\n",
    "    26: 'Model Training',\n",
    "    # This topic is about managing SDK versions, including updating, upgrading, and installing SDKs like PyTorch.\n",
    "    27: 'SDK Versioning',\n",
    "    # This topic is about invoking endpoints and APIs using Lambda functions and API gateways, including waiting for responses and using SDKs.\n",
    "    28: 'API Invocation',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_json(os.path.join(path_general, 'topics.json'))\n",
    "df_topics = df_topics[df_topics['Solution_topic'] > -1]\n",
    "\n",
    "# as if we assign the topic id as the label\n",
    "label_challenge_original = df_topics['Challenge_topic'].unique().tolist()\n",
    "label_challenge_refined = [f'c_{label}' for label in label_challenge_original]\n",
    "label_challenge_map = dict(\n",
    "    zip(label_challenge_original, label_challenge_refined))\n",
    "\n",
    "label_solution_original = df_topics['Solution_topic'].unique().tolist()\n",
    "label_solution_refined = [f's_{label}' for label in label_solution_original]\n",
    "label_solution_map = dict(zip(label_solution_original, label_solution_refined))\n",
    "\n",
    "df_topics = df_topics.replace(\n",
    "    {'Challenge_topic': label_challenge_map, 'Solution_topic': label_solution_map})\n",
    "\n",
    "categories = ['Challenge_topic', 'Solution_topic']\n",
    "df_topics = df_topics.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "# we only visualize large topics\n",
    "df_topics = df_topics[df_topics['value'] > 20]\n",
    "\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)-1):\n",
    "    tempDf = df_topics[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "\n",
    "label = list(np.unique(df_topics[categories].values))\n",
    "source = newDf['source'].apply(lambda x: label.index(x))\n",
    "target = newDf['target'].apply(lambda x: label.index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=label)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(height=1000, width=1000, font=dict(size=30))\n",
    "fig.write_image(os.path.join(path_challenge_information,\n",
    "                'Challenge solution sankey.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_answer_count'] + \\\n",
    "    df_topics['Challenge_comment_count']\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=['Tool', 'Platform'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic',\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.write_image(os.path.join(path_challenge_information,\n",
    "                'Challenge_topic_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solution topic distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_topics = df_topics[df_topics['Solution_topic'] > -1]\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_answer_count'] + \\\n",
    "    df_topics['Challenge_comment_count']\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=['Tool', 'Platform'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Solution_topic',\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.write_image(os.path.join(path_solution_information,\n",
    "                'Solution_topic_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect challenge statistics information\n",
    "\n",
    "df_challenge = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_challenge = df_challenge[df_challenge['Challenge_topic'] > -1]\n",
    "\n",
    "df_challenge['Challenge_comment_count'] = df_challenge['Challenge_comment_count'].fillna(0)\n",
    "df_challenge['Challenge_solved_time'] = df_challenge['Challenge_closed_time'] - \\\n",
    "    df_challenge['Challenge_creation_time']\n",
    "df_challenge['Challenge_adjusted_solved_time'] = df_challenge['Solution_last_edit_time'] - \\\n",
    "    df_challenge['Challenge_last_edit_time']\n",
    "df_challenge['Challenge_participation_count'] = df_challenge['Challenge_answer_count'] + \\\n",
    "    df_challenge['Challenge_comment_count']\n",
    "\n",
    "total_count = df_challenge['Challenge_topic'].count()\n",
    "df_topics = []\n",
    "\n",
    "for name, group in df_challenge.groupby('Challenge_topic'):\n",
    "    Mean_score = group['Challenge_score'].mean()\n",
    "    Mean_favorite_count = group['Challenge_favorite_count'].mean()\n",
    "    Mean_follower_count = group['Challenge_follower_count'].mean()\n",
    "    Mean_link_count = group['Challenge_link_count'].mean()\n",
    "    Mean_information_entropy = group['Challenge_information_entropy'].mean()\n",
    "    Mean_readability = group['Challenge_readability'].mean()\n",
    "    Mean_sentence_count = group['Challenge_sentence_count'].mean()\n",
    "    Mean_word_count = group['Challenge_word_count'].mean()\n",
    "    Mean_unique_word_count = group['Challenge_unique_word_count'].mean()\n",
    "    Mean_view_count = group['Challenge_view_count'].mean()\n",
    "    Mean_answer_count = group['Challenge_answer_count'].mean()\n",
    "    Mean_comment_count = group['Challenge_comment_count'].mean()\n",
    "    Mean_participation_count = Mean_answer_count + Mean_comment_count\n",
    "    Score_participation_ratio = Mean_score / Mean_participation_count\n",
    "    Score_participation_weighted_product = (\n",
    "        group['Challenge_score'] * group['Challenge_participation_count']).mean()\n",
    "    Count = group['Challenge_topic'].count()\n",
    "    Count_ratio = Count / total_count * 100\n",
    "    Solved_ratio = group['Challenge_closed_time'].notna().sum() / Count\n",
    "    Mean_solved_time = group['Challenge_solved_time'].mean(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    Median_solved_time = group['Challenge_solved_time'].median(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    Mean_adjusted_solved_time = group['Challenge_adjusted_solved_time'].mean(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    Median_adjusted_solved_time = group['Challenge_adjusted_solved_time'].median(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    topic_info = {\n",
    "        'Challenge_topic': name,\n",
    "        'Mean_score': Mean_score,\n",
    "        'Mean_favorite_count': Mean_favorite_count,\n",
    "        'Mean_follower_count': Mean_follower_count,\n",
    "        'Mean_link_count': Mean_link_count,\n",
    "        'Mean_information_entropy': Mean_information_entropy,\n",
    "        'Mean_readability': Mean_readability,\n",
    "        'Mean_sentence_count': Mean_sentence_count,\n",
    "        'Mean_word_count': Mean_word_count,\n",
    "        'Mean_unique_word_count': Mean_unique_word_count,\n",
    "        'Mean_view_count': Mean_view_count,\n",
    "        'Mean_answer_count': Mean_answer_count,\n",
    "        'Mean_comment_count': Mean_comment_count,\n",
    "        'Score_participation_ratio': Score_participation_ratio,\n",
    "        'Score_participation_weighted_product': Score_participation_weighted_product,\n",
    "        'Count_ratio': Count_ratio,\n",
    "        'Solved_ratio': Solved_ratio,\n",
    "        'Mean_solved_time': Mean_solved_time,\n",
    "        'Median_solved_time': Median_solved_time,\n",
    "        'Mean_adjusted_solved_time': Mean_adjusted_solved_time,\n",
    "        'Median_adjusted_solved_time': Median_adjusted_solved_time,\n",
    "    }\n",
    "    df_topics.append(topic_info)\n",
    "\n",
    "df_topics = pd.DataFrame(df_topics)\n",
    "df_topics.to_json(os.path.join(path_challenge_information,\n",
    "                  'general.json'), indent=4, orient='records')\n",
    "df_topics = df_topics.set_index('Challenge_topic')\n",
    "\n",
    "fig = df_topics.sort_values('Mean_score', ascending=False)['Mean_score'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean score', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_score.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_favorite_count', ascending=False)['Mean_favorite_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean favorite count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_favorite_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_follower_count', ascending=False)['Mean_follower_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean follower count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_follower_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_link_count', ascending=False)['Mean_link_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean link count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_link_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_information_entropy', ascending=False)['Mean_information_entropy'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean info entropy', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_information_entropy.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_readability', ascending=False)['Mean_readability'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean readability', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_readability.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_sentence_count', ascending=False)['Mean_sentence_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean sentence count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_sentence_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_word_count', ascending=False)['Mean_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_unique_word_count', ascending=False)['Mean_unique_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean unique word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_unique_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_view_count', ascending=False)['Mean_view_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean view count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_view_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_answer_count', ascending=False)['Mean_answer_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean answer count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_answer_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_comment_count', ascending=False)['Mean_comment_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean comment count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_comment_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Score_participation_ratio', ascending=False)['Score_participation_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge score participation ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information,\n",
    "            'Score_participation_ratio.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Score_participation_weighted_product', ascending=False)['Score_participation_weighted_product'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge score participation weighted product', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information,\n",
    "            'Score_participation_weighted_product.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Count_ratio', ascending=False)['Count_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge count ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Count_ratio.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Solved_ratio')['Solved_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge Solved ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'solved_ratio.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_solved_time', ascending=False)['Mean_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge median solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'mean_solved_time.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Median_solved_time', ascending=False)['Median_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'median_solved_time.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_adjusted_solved_time', ascending=False)['Mean_adjusted_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean adjusted solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_adjusted_solved_time.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Median_adjusted_solved_time', ascending=False)['Median_adjusted_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge median adjusted solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Median_adjusted_solved_time.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect solution statistics information\n",
    "\n",
    "df_solution = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_solution = df_solution[df_solution['Solution_topic'] > -1]\n",
    "\n",
    "total_count = df_challenge['Solution_topic'].count()\n",
    "df_topics = []\n",
    "\n",
    "for name, group in df_challenge.groupby('Solution_topic'):\n",
    "    Mean_score = group['Solution_score'].mean()\n",
    "    Mean_link_count = group['Solution_link_count'].mean()\n",
    "    Mean_information_entropy = group['Solution_information_entropy'].mean()\n",
    "    Mean_readability = group['Solution_readability'].mean()\n",
    "    Mean_sentence_count = group['Solution_sentence_count'].mean()\n",
    "    Mean_word_count = group['Solution_word_count'].mean()\n",
    "    Mean_unique_word_count = group['Solution_unique_word_count'].mean()\n",
    "    Mean_comment_count = group['Solution_comment_count'].mean()\n",
    "    Count_ratio = group['Solution_topic'].count() / total_count * 100\n",
    "    topic_info = {\n",
    "        'Solution_topic': name,\n",
    "        'Mean_score': Mean_score,\n",
    "        'Mean_link_count': Mean_link_count,\n",
    "        'Mean_information_entropy': Mean_information_entropy,\n",
    "        'Mean_readability': Mean_readability,\n",
    "        'Mean_sentence_count': Mean_sentence_count,\n",
    "        'Mean_word_count': Mean_word_count,\n",
    "        'Mean_unique_word_count': Mean_unique_word_count,\n",
    "        'Mean_comment_count': Mean_comment_count,\n",
    "        'Count_ratio': Count_ratio,\n",
    "    }\n",
    "    df_topics.append(topic_info)\n",
    "\n",
    "df_topics = pd.DataFrame(df_topics)\n",
    "df_topics.to_json(os.path.join(path_solution_information,\n",
    "                  'general.json'), indent=4, orient='records')\n",
    "df_topics = df_topics.set_index('Solution_topic')\n",
    "\n",
    "fig = df_topics.sort_values('Mean_score', ascending=False)['Mean_score'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean score', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_score.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_link_count', ascending=False)['Mean_link_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean link count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_link_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_information_entropy', ascending=False)['Mean_information_entropy'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean info entropy', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_information_entropy.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_readability', ascending=False)['Mean_readability'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean readability', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_readability.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_sentence_count', ascending=False)['Mean_sentence_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean sentence count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_sentence_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_word_count', ascending=False)['Mean_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_unique_word_count', ascending=False)['Mean_unique_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean unique word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_unique_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_comment_count', ascending=False)['Mean_comment_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean comment count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_comment_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Count_ratio', ascending=False)['Count_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution count ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Count_ratio.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n",
    "\n",
    "\n",
    "def smooth(x, y, xgrid, lowess_kw=None):\n",
    "    samples = np.random.choice(len(x), 50, replace=True)\n",
    "    y_s = y[samples]\n",
    "    x_s = x[samples]\n",
    "    y_sm = sm_lowess(y_s, x_s, **lowess_kw)\n",
    "    # regularly sample it onto the grid\n",
    "    y_grid = scipy.interpolate.interp1d(\n",
    "        x_s, y_sm, fill_value='extrapolate')(xgrid)\n",
    "    return y_grid\n",
    "\n",
    "\n",
    "def lowess_with_confidence_bounds(x, y, conf_interval=0.95, lowess_kw=None):\n",
    "    \"\"\"\n",
    "    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n",
    "    \"\"\"\n",
    "    xgrid = np.linspace(x.min(), x.max())\n",
    "\n",
    "    K = 100\n",
    "    smooths = np.stack([smooth(x, y, xgrid, lowess_kw) for _ in range(K)]).T\n",
    "\n",
    "    mean = np.nanmean(smooths, axis=1)\n",
    "    stderr = scipy.stats.sem(smooths, axis=1)\n",
    "\n",
    "    clower = np.nanpercentile(smooths, (1-conf_interval)*50, axis=1)\n",
    "    cupper = np.nanpercentile(smooths, (1+conf_interval)*50, axis=1)\n",
    "\n",
    "    return xgrid, mean, stderr, clower, cupper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-08-08 14:04:22.160000'),\n",
       " Timestamp('2023-02-22 01:36:03.995000'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_challenge = df_all[df_all['Challenge_topic'] > -1]\n",
    "# BigQuery Stack Overflow public dataset is updated until Nov 24, 2022, 1:39:22 PM UTC-5\n",
    "min(df_challenge['Challenge_creation_time']), max(df_challenge['Challenge_creation_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore challenge topics evolution\n",
    "\n",
    "df_challenge = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_challenge = df_challenge[df_challenge['Challenge_topic'] > -1]\n",
    "df_challenge = df_challenge[(df_challenge['Challenge_creation_time'] > '2014-09-14')\n",
    "                            & (df_challenge['Challenge_creation_time'] < '2022-11-21')]\n",
    "\n",
    "for name, group in df_challenge.groupby('Challenge_topic'):\n",
    "    group = group.groupby(pd.Grouper(key='Challenge_creation_time', freq='2W')).agg(\n",
    "        Count=('Challenge_topic', 'count')).reset_index()\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    x = np.array([i.astype('datetime64[D]').astype(int) for i in x])\n",
    "    y = group['Count'].values\n",
    "    # 95% confidence interval\n",
    "    xgrid, mean, stderr, clower, cupper = lowess_with_confidence_bounds(\n",
    "        x, y, conf_interval=0.95, lowess_kw={\"frac\": 0.5, \"it\": 5, \"return_sorted\": False})\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.plot(x, y, 'k.', label='Observations')\n",
    "    plt.plot(xgrid, mean, color='tomato', label='LOWESS')\n",
    "    plt.fill_between(xgrid, clower, cupper, alpha=0.3,\n",
    "                     label='LOWESS uncertainty')\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(os.path.join(path_challenge_evolution,\n",
    "                f'Topic_{name}'), bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-09-14 22:12:24.493000'),\n",
       " Timestamp('2023-02-21 18:36:06.284000'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_solution = df_all[df_all['Solution_topic'] > -1]\n",
    "# BigQuery Stack Overflow public dataset is updated until Nov 24, 2022, 1:39:22 PM UTC-5\n",
    "min(df_solution['Challenge_creation_time']), max(\n",
    "    df_solution['Challenge_creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore solution topics evolution\n",
    "\n",
    "df_solution = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_solution = df_solution[df_solution['Solution_topic'] > -1]\n",
    "df_solution = df_solution[(df_solution['Challenge_creation_time'] > '2014-09-14')\n",
    "                          & (df_solution['Challenge_creation_time'] < '2022-11-21')]\n",
    "\n",
    "for name, group in df_solution.groupby('Solution_topic'):\n",
    "    group = group.groupby(pd.Grouper(key='Challenge_closed_time', freq='W')).agg(\n",
    "        Count=('Solution_topic', 'count')).reset_index()\n",
    "    x = pd.to_datetime(group['Challenge_closed_time']).values\n",
    "    x = np.array([i.astype('datetime64[D]').astype(int) for i in x])\n",
    "    y = group['Count'].values\n",
    "    # 95% confidence interval\n",
    "    xgrid, mean, stderr, clower, cupper = lowess_with_confidence_bounds(\n",
    "        x, y, conf_interval=0.95, lowess_kw={\"frac\": 0.5, \"it\": 5, \"return_sorted\": False})\n",
    "    x = pd.to_datetime(group['Challenge_closed_time']).values\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.plot(x, y, 'k.', label='Observations')\n",
    "    plt.plot(xgrid, mean, color='tomato', label='LOWESS')\n",
    "    plt.fill_between(xgrid, clower, cupper, alpha=0.3,\n",
    "                     label='LOWESS uncertainty')\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(os.path.join(path_solution_evolution,\n",
    "                f'Topic_{name}'), bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
