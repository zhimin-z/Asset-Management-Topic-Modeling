{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\",\n",
    "              None, 'display.max_colwidth', None)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n",
    "\n",
    "path_result = os.path.join(os.path.dirname(os.getcwd()), 'Result')\n",
    "if not os.path.exists(path_result):\n",
    "    os.makedirs(path_result)\n",
    "\n",
    "path_general = os.path.join(path_result, 'General')\n",
    "if not os.path.exists(path_general):\n",
    "    os.makedirs(path_general)\n",
    "\n",
    "path_challenge = os.path.join(path_result, 'Challenge')\n",
    "if not os.path.exists(path_challenge):\n",
    "    os.makedirs(path_challenge)\n",
    "\n",
    "path_solution = os.path.join(path_result, 'Solution')\n",
    "if not os.path.exists(path_solution):\n",
    "    os.makedirs(path_solution)\n",
    "\n",
    "path_challenge_information = os.path.join(path_challenge, 'Information')\n",
    "if not os.path.exists(path_challenge_information):\n",
    "    os.makedirs(path_challenge_information)\n",
    "\n",
    "path_solution_information = os.path.join(path_solution, 'Information')\n",
    "if not os.path.exists(path_solution_information):\n",
    "    os.makedirs(path_solution_information)\n",
    "\n",
    "path_challenge_evolution = os.path.join(path_challenge, 'Evolution')\n",
    "if not os.path.exists(path_challenge_evolution):\n",
    "    os.makedirs(path_challenge_evolution)\n",
    "\n",
    "path_solution_evolution = os.path.join(path_solution, 'Evolution')\n",
    "if not os.path.exists(path_solution_evolution):\n",
    "    os.makedirs(path_solution_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Topic 0': 'environ instal, instal conda, conda instal, environ conda, conda environ, instal jupyt, instal pip, pip instal, anaconda environ, environ notebook'},\n",
       " {'Topic 1': 'pipelin comput, pipelineparamet, pipelin automat, execut pipelin, sdk pipelin, pipelin execut, pipelin process, pipelin sdk, run pipelin, pipelin run'},\n",
       " {'Topic 2': 'dockerfil, run docker, docker file, docker run, dockerfil docker, docker imag, docker daemon, instal docker, docker instal, docker docker'},\n",
       " {'Topic 3': 'sweep hyperparamet, hyperparamet sweep, hyperparamet train, sweep configur, configur sweep, tune hyperparamet, hyperparamet optim, sweep config, sweep paramet, hyperparamet tune'},\n",
       " {'Topic 4': 'file git, file directori, track directori, directori path, git repo, run directori, gitignor file, git, file push, git repositori'},\n",
       " {'Topic 5': 'tensorflow gpu, docker gpu, gpu tensorflow, gpu pytorch, run gpu, gpu run, gpu train, gpu nvidia, gpu kera, train gpu'},\n",
       " {'Topic 6': 'artifact log, configur artifact, run artifact, artifact api, artifact run, artifact root, artifact path, artifact store, artifact folder, artifact access'},\n",
       " {'Topic 7': 'endpoint model, model endpoint, endpoint multimodel, multimodel endpoint, model deploy, endpoint deploi, deploy model, endpoint multi, model deploi, multi endpoint'},\n",
       " {'Topic 8': 'auto label, label job, label automl, job label, data label, label task, file label, dataset label, manual label, label dataset'},\n",
       " {'Topic 9': 'chart bar, chart log, plot dashboard, bar chart, line chart, chart, bar plot, data plot, plot visual, plot log'},\n",
       " {'Topic 10': 'log metric, metric log, epoch log, log train, log progress, log dataset, metric epoch, log run, updat metric, updat logger'},\n",
       " {'Topic 11': 'account extens, creat account, account invit, account associ, delet account, account usernam, account sign, account app, account ad, academ account'},\n",
       " {'Topic 12': 'spark pyspark, configur spark, load spark, spark scala, pyspark distribut, scala spark, spark session, sparkml, spark udf, spark model'},\n",
       " {'Topic 13': 'tensorflow model, model tensorflow, deploi tensorflow, train tensorflow, tensorflowmodel, endpoint tensorflow, tensorflow, predict tensorflow, tensorflow estim, save tensorflow'},\n",
       " {'Topic 14': 'text text, text announc, releas text, text, textract, improv text, updat text, text simpli, text detect, improv document'},\n",
       " {'Topic 15': 'datafram panda, panda datafram, dataset panda, panda data, data panda, datafram csv, tabl panda, convert panda, datafram dataset, data tabl'},\n",
       " {'Topic 16': 'model export, model file, model download, save model, model save, download model, model joblib, extract model, model devop, model artifact'},\n",
       " {'Topic 17': 'role permiss, iam role, role execut, execut role, attach role, role exist, role getobject, role, role creat, com role'},\n",
       " {'Topic 18': 'jsonl batch, batch job, transform batch, batch transform, format batch, job batchstrategi, batchstrategi multirecord, batch predict, json dump, batch'},\n",
       " {'Topic 19': 'model registri, model tag, model regist, model stage, tag model, model version, registri model, regist model, createmodel api, model adl'},\n",
       " {'Topic 20': 'databas connect, connect sql, sqlite databas, databas sqlalchemi, sql databas, databas backend, databas studio, mysql databas, databas servic, databas databas'},\n",
       " {'Topic 21': 'limit resourc, servic limit, resourc limit, limit quota, quota avail, resourc quota, servic quota, resourcelimitexceed, request limit, quota limit'},\n",
       " {'Topic 22': 'invok endpoint, endpoint invok, payload endpoint, api invok, api gatewai, endpoint api, endpoint lambda, gatewai api, lambda endpoint, payload evalu'},\n",
       " {'Topic 23': 'automl forecast, forecast automl, forecast data, data forecast, forecast forecast, autopilot forecast, forecast, creat forecast, seri forecast, forecast model'},\n",
       " {'Topic 24': 'column valu, column dataset, data type, valu column, column comput, column, column import, string column, categor data, column string'},\n",
       " {'Topic 25': 'vision api, object detect, cloud vision, video classif, train object, model detect, detect train, pixel accuraci, object track, detect object'},\n",
       " {'Topic 26': 'webservic deploi, deploi webservic, servic web, aciwebservic deploi, servic data, webservic packag, servic model, endpoint servic, web servic, servic endpoint'},\n",
       " {'Topic 27': 'model kubernet, deploi kubernet, node kubernet, kubernet servic, endpoint kubernet, kubernet cluster, kubernet environ, kubernet pod, kubernet, pod kubernet'},\n",
       " {'Topic 28': 'forest model, random forest, forest classifi, forest algorithm, tree model, tree predict, forest rcf, tree dataset, randomforest classifi, forest tree'},\n",
       " {'Topic 29': 'file csv, write csv, csv file, upload csv, exist csv, data csv, csv, import csv, read csv, load csv'},\n",
       " {'Topic 30': 'tensorboard log, log tensorboard, run tensorboard, import tensorboard, access tensorboard, tensorboard run, launch tensorboard, tensorboard tensorboard, start tensorboard, tensorflow tensorboard'},\n",
       " {'Topic 31': 'featur commun, implement roadmap, ideat featur, featur platform, roadmap, platform, featur websit, manufactur, hous platform, contribut'},\n",
       " {'Topic 32': 'version dataset, repositori data, datacatalog, version data, metadata dataset, dataset track, track dataset, larg dataset, extern dataset, dataset version'},\n",
       " {'Topic 33': 'cloudwatch log, log cloudwatch, metric cloudwatch, cloudwatchlog, cloudwatch metric, endpoint statu, cloudwatch advic, cloudwatch list, notebook cloudwatch, cloudwatch'},\n",
       " {'Topic 34': 'file speech, speech text, audio speech, text speech, manual transcript, audio file, convert speech, transcript improv, transcrib audio, speech api'},\n",
       " {'Topic 35': 'config yml, yml configur, paramet yml, file yml, yml file, paramet yaml, pass config, yaml file, config file, configur file'},\n",
       " {'Topic 36': 'creat datastor, mount dataset, datastor creat, access datastor, data folder, data directori, dataset datastor, dataset download, file dataset, creat dataset'},\n",
       " {'Topic 37': 'endpoint connect, vpc endpoint, configur secur, vpc interfac, vpc network, vpc subnet, instanc vpc, network configur, interfac vpc, subnet secur'},\n",
       " {'Topic 38': 'model accuraci, data autom, model imbalanc, model auc, imbalanc data, classif model, model qualiti, model automl, engin model, threshold valu'},\n",
       " {'Topic 39': 'input model, model queri, dtype input, input predict, transform model, predict endpoint, json serializ, endpoint predict, predict request, queri model'},\n",
       " {'Topic 40': 'bucket role, bucket access, access bucket, bucket account, role access, bucket permiss, role permiss, permiss bucket, ownership bucket, bucket startup'},\n",
       " {'Topic 41': 'search run, job cli, run successfulli, cancel run, run environ, run run, run match, runinfo field, stop job, stop comput'},\n",
       " {'Topic 42': 'column predict, predict model, fit predict, model predict, predict standard, built predict, predict perform, predict valu, predict, predict option'},\n",
       " {'Topic 43': 'notebook instanc, autostop, automat stop, time instanc, instanc stop, shut instanc, avail notebook, notebook run, auto shutdown, creat notebook'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(path_challenge, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "topic_term_list = []\n",
    "for index, topic in enumerate(topic_terms):\n",
    "    terms = ', '.join([term[0] for term in topic])\n",
    "    topic_term = {f'Topic {index}': terms}\n",
    "    topic_term_list.append(topic_term)\n",
    "topic_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Environment Setup - Setting up software environments for development and execution\n",
      "Topic 1: Pipeline Automation - Automating the execution of data processing pipelines\n",
      "Topic 2: Docker - Containerization platform for building, shipping, and running applications\n",
      "Topic 3: Hyperparameter Tuning - Optimizing model performance by tuning hyperparameters\n",
      "Topic 4: Git Version Control - Tracking changes to code and collaborating with others\n",
      "Topic 5: GPU Acceleration - Using graphics processing units to speed up machine learning tasks\n",
      "Topic 6: Artifact Management - Managing and storing artifacts such as models, datasets, and code\n",
      "Topic 7: Model Deployment - Deploying machine learning models for use in production environments\n",
      "Topic 8: Data Labeling - Assigning labels to data for use in supervised learning tasks\n",
      "Topic 9: Data Visualization - Creating visual representations of data for analysis and communication\n",
      "Topic 10: Logging Metrics - Recording and tracking performance metrics during model training and evaluation\n",
      "Topic 11: Account Management - Managing user accounts and access to resources\n",
      "Topic 12: Apache Spark - Open-source distributed computing system for big data processing\n",
      "Topic 13: TensorFlow - Open-source machine learning framework for building and training models\n",
      "Topic 14: Text Processing - Analyzing and manipulating text data\n",
      "Topic 15: Pandas DataFrames - Data structure for manipulating and analyzing tabular data\n",
      "Topic 16: Model Export - Saving and exporting trained machine learning models\n",
      "Topic 17: Role-Based Access Control - Controlling access to resources based on user roles and permissions\n",
      "Topic 18: Batch Processing - Processing large amounts of data in batches\n",
      "Topic 19: Model Registry - Managing and versioning machine learning models\n",
      "Topic 20: Database Connectivity - Connecting to and interacting with databases\n",
      "Topic 21: Resource Limitations - Setting and managing limits on resource usage\n",
      "Topic 22: API Invocation - Calling APIs to perform tasks or retrieve data\n",
      "Topic 23: AutoML Forecasting - Using automated machine learning to generate forecasts\n",
      "Topic 24: Column Manipulation - Working with and manipulating columns in datasets\n",
      "Topic 25: Computer Vision - Using machine learning to analyze and interpret visual data\n",
      "Topic 26: Web Service Deployment - Deploying machine learning models as web services\n",
      "Topic 27: Kubernetes - Open-source container orchestration platform for managing containerized applications\n",
      "Topic 28: Random Forest - Ensemble learning method for classification and regression tasks\n",
      "Topic 29: CSV Files - File format for storing and exchanging tabular data\n",
      "Topic 30: TensorBoard Logging - Visualizing and tracking model training and evaluation using TensorBoard\n",
      "Topic 31: Feature Roadmap - Planning and implementing new features for a platform or product\n",
      "Topic 32: Dataset Versioning - Managing and versioning datasets\n",
      "Topic 33: CloudWatch Logging - Monitoring and logging AWS resources and applications\n",
      "Topic 34: Speech-to-Text - Converting audio speech to text\n",
      "Topic 35: YAML Configuration - Using YAML files to configure applications and services\n",
      "Topic 36: Data Storage - Storing and accessing data in cloud-based storage solutions\n",
      "Topic 37: VPC Endpoints - Connecting to AWS services privately through a VPC\n",
      "Topic 38: Model Accuracy - Evaluating and improving the accuracy of machine learning models\n",
      "Topic 39: Model Input - Preparing and querying input data for machine learning models\n",
      "Topic 40: Bucket Access - Managing access to cloud-based storage buckets\n",
      "Topic 41: Run Management - Managing and monitoring the execution of jobs and tasks\n",
      "Topic 42: Model Prediction - Using trained machine learning models to make predictions\n",
      "Topic 43: Notebook Instances - Creating and managing cloud-based notebook instances for data analysis and experimentation\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"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\"\n",
    "\n",
    "prompt_topic = '''You will be given a list of keywords for each topic, I want you to provide a description of each topic in a two-word phrase but guarantee that each description is exclusive to the other. Also, for each description, you need to attach short comments on what these keywords are talking about in general.'''\n",
    "\n",
    "with open(os.path.join(path_challenge, 'Topic terms.pickle'), 'rb') as handle:\n",
    "    topic_terms = pickle.load(handle)\n",
    "\n",
    "topic_term_list = []\n",
    "for index, topic in enumerate(topic_terms):\n",
    "    terms = ', '.join([term[0] for term in topic])\n",
    "    topic_term = f'Topic {index}: {terms}'\n",
    "    topic_term_list.append(topic_term)\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_topic +\n",
    "               '\\n###' + '\\n'.join(topic_term_list) + '###\\n'}],\n",
    "    temperature=0,\n",
    "    max_tokens=1500,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    timeout=300,\n",
    "    stream=False)\n",
    "\n",
    "summary = completion.choices[0].message.content\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = {\n",
    "    -1: np.nan,\n",
    "    # Setting up software environments for development and execution\n",
    "    0: 'Environment Setup',\n",
    "    # Automating the execution of data processing pipelines\n",
    "    1: 'Pipeline Automation',\n",
    "    # Containerization platform for building, shipping, and running applications\n",
    "    2: 'Docker',\n",
    "    # Optimizing model performance by tuning hyperparameters\n",
    "    3: 'Hyperparameter Tuning',\n",
    "    # Tracking changes to code and collaborating with others\n",
    "    4: 'Git Version Control',\n",
    "    # Using graphics processing units to speed up machine learning tasks\n",
    "    5: 'GPU Acceleration',\n",
    "    # Managing and storing artifacts such as models, datasets, and code\n",
    "    6: 'Artifact Management',\n",
    "    # Deploying machine learning models for use in production environments\n",
    "    7: 'Model Deployment',\n",
    "    # Assigning labels to data for use in supervised learning tasks\n",
    "    8: 'Data Labeling',\n",
    "    # Creating visual representations of data for analysis and communication\n",
    "    9: 'Data Visualization',\n",
    "    # Recording and tracking performance metrics during model training and evaluation\n",
    "    10: 'Logging Metrics',\n",
    "    # Managing user accounts and access to resources\n",
    "    11: 'Account Management',\n",
    "    # Open-source distributed computing system for big data processing\n",
    "    12: 'Apache Spark',\n",
    "    # Open-source machine learning framework for building and training models\n",
    "    13: 'TensorFlow',\n",
    "    # Analyzing and manipulating text data\n",
    "    14: 'Text Processing',\n",
    "    # Data structure for manipulating and analyzing tabular data\n",
    "    15: 'Pandas DataFrames',\n",
    "    # Saving and exporting trained machine learning models\n",
    "    16: 'Model Export',\n",
    "    # Controlling access to resources based on user roles and permissions\n",
    "    17: 'Role-Based Access Control',\n",
    "    # Processing large amounts of data in batches\n",
    "    18: 'Batch Processing',\n",
    "    # Managing and versioning machine learning models\n",
    "    19: 'Model Registry',\n",
    "    # Connecting to and interacting with databases\n",
    "    20: 'Database Connectivity',\n",
    "    # Setting and managing limits on resource usage\n",
    "    21: 'Resource Quotas',\n",
    "    # Calling APIs to perform tasks or retrieve data\n",
    "    22: 'API Invocation',\n",
    "    # Using automated machine learning to generate forecasts\n",
    "    23: 'AutoML Forecasting',\n",
    "    # Working with and manipulating columns in datasets\n",
    "    24: 'Column Manipulation',\n",
    "    # Using machine learning to analyze and interpret visual data\n",
    "    25: 'Computer Vision',\n",
    "    # Deploying machine learning models as web services\n",
    "    26: 'Web Service Deployment',\n",
    "    # Open-source container orchestration platform for managing containerized applications\n",
    "    27: 'Kubernetes',\n",
    "    # Ensemble learning method for classification and regression tasks\n",
    "    28: 'Random Forest',\n",
    "    # File format for storing and exchanging tabular data\n",
    "    29: 'CSV Files',\n",
    "    # Visualizing and tracking model training and evaluation using TensorBoard\n",
    "    30: 'TensorBoard Logging',\n",
    "    # Planning and implementing new features for a platform or product\n",
    "    31: 'Feature Roadmap',\n",
    "    # Managing and versioning datasets\n",
    "    32: 'Dataset Versioning',\n",
    "    # Monitoring and logging AWS resources and applications\n",
    "    33: 'CloudWatch Logging',\n",
    "    # Converting audio speech to text\n",
    "    34: 'Speech-to-Text',\n",
    "    # Using YAML files to configure applications and services\n",
    "    35: 'YAML Configuration',\n",
    "    # Storing and accessing data in cloud-based storage solutions\n",
    "    36: 'Data Storage',\n",
    "    # Connecting to AWS services privately through a VPC\n",
    "    37: 'VPC Endpoints',\n",
    "    # Evaluating and improving the accuracy of machine learning models\n",
    "    38: 'Model Accuracy',\n",
    "    # Preparing and querying input data for machine learning models\n",
    "    39: 'Model Input',\n",
    "    # Managing access to cloud-based storage buckets\n",
    "    40: 'Bucket Access',\n",
    "    # Managing and monitoring the execution of jobs and tasks\n",
    "    41: 'Run Management',\n",
    "    # Using trained machine learning models to make predictions\n",
    "    42: 'Model Inference',\n",
    "    # Creating and managing cloud-based notebook instances for data analysis and experimentation\n",
    "    43: 'Notebook Instances',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = pd.read_json(os.path.join(path_general, 'topics.json'))\n",
    "df_topics = df_topics[df_topics['Solution_topic'] > -1]\n",
    "\n",
    "# as if we assign the topic id as the label\n",
    "label_challenge_original = df_topics['Challenge_topic'].unique().tolist()\n",
    "label_challenge_refined = [f'c_{label}' for label in label_challenge_original]\n",
    "label_challenge_map = dict(\n",
    "    zip(label_challenge_original, label_challenge_refined))\n",
    "\n",
    "label_solution_original = df_topics['Solution_topic'].unique().tolist()\n",
    "label_solution_refined = [f's_{label}' for label in label_solution_original]\n",
    "label_solution_map = dict(zip(label_solution_original, label_solution_refined))\n",
    "\n",
    "df_topics = df_topics.replace(\n",
    "    {'Challenge_topic': label_challenge_map, 'Solution_topic': label_solution_map})\n",
    "\n",
    "categories = ['Challenge_topic', 'Solution_topic']\n",
    "df_topics = df_topics.groupby(categories).size().reset_index(name='value')\n",
    "\n",
    "# we only visualize large topics\n",
    "df_topics = df_topics[df_topics['value'] > 20]\n",
    "\n",
    "newDf = pd.DataFrame()\n",
    "for i in range(len(categories)-1):\n",
    "    tempDf = df_topics[[categories[i], categories[i+1], 'value']]\n",
    "    tempDf.columns = ['source', 'target', 'value']\n",
    "    newDf = pd.concat([newDf, tempDf])\n",
    "newDf = newDf.groupby(['source', 'target']).agg({'value': 'sum'}).reset_index()\n",
    "\n",
    "label = list(np.unique(df_topics[categories].values))\n",
    "source = newDf['source'].apply(lambda x: label.index(x))\n",
    "target = newDf['target'].apply(lambda x: label.index(x))\n",
    "value = newDf['value']\n",
    "\n",
    "link = dict(source=source, target=target, value=value)\n",
    "node = dict(label=label)\n",
    "data = go.Sankey(link=link, node=node)\n",
    "\n",
    "fig = go.Figure(data)\n",
    "fig.update_layout(height=1000, width=1000, font=dict(size=30))\n",
    "fig.write_image(os.path.join(path_challenge_information,\n",
    "                'Challenge solution sankey.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create challenge topic distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_answer_count'] + \\\n",
    "    df_topics['Challenge_comment_count']\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=['Tool', 'Platform'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Challenge_topic',\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.write_image(os.path.join(path_challenge_information,\n",
    "                'Challenge_topic_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solution topic distribution tree map\n",
    "\n",
    "df_topics = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_topics = df_topics[df_topics['Solution_topic'] > -1]\n",
    "df_topics['Challenge_participation_count'] = df_topics['Challenge_answer_count'] + \\\n",
    "    df_topics['Challenge_comment_count']\n",
    "\n",
    "fig = px.treemap(\n",
    "    df_topics,\n",
    "    path=['Tool', 'Platform'],\n",
    "    values='Challenge_participation_count',\n",
    "    color='Solution_topic',\n",
    "    width=2000,\n",
    "    height=1000,\n",
    ")\n",
    "fig.write_image(os.path.join(path_solution_information,\n",
    "                'Solution_topic_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect challenge statistics information\n",
    "\n",
    "df_challenge = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_challenge = df_challenge[df_challenge['Challenge_topic'] > -1]\n",
    "\n",
    "df_challenge['Challenge_comment_count'] = df_challenge['Challenge_comment_count'].fillna(0)\n",
    "df_challenge['Challenge_solved_time'] = df_challenge['Challenge_closed_time'] - \\\n",
    "    df_challenge['Challenge_creation_time']\n",
    "df_challenge['Challenge_adjusted_solved_time'] = df_challenge['Solution_last_edit_time'] - \\\n",
    "    df_challenge['Challenge_last_edit_time']\n",
    "df_challenge['Challenge_participation_count'] = df_challenge['Challenge_answer_count'] + \\\n",
    "    df_challenge['Challenge_comment_count']\n",
    "\n",
    "total_count = df_challenge['Challenge_topic'].count()\n",
    "df_topics = []\n",
    "\n",
    "for name, group in df_challenge.groupby('Challenge_topic'):\n",
    "    Mean_score = group['Challenge_score'].mean()\n",
    "    Mean_favorite_count = group['Challenge_favorite_count'].mean()\n",
    "    Mean_follower_count = group['Challenge_follower_count'].mean()\n",
    "    Mean_link_count = group['Challenge_link_count'].mean()\n",
    "    Mean_information_entropy = group['Challenge_information_entropy'].mean()\n",
    "    Mean_readability = group['Challenge_readability'].mean()\n",
    "    Mean_sentence_count = group['Challenge_sentence_count'].mean()\n",
    "    Mean_word_count = group['Challenge_word_count'].mean()\n",
    "    Mean_unique_word_count = group['Challenge_unique_word_count'].mean()\n",
    "    Mean_view_count = group['Challenge_view_count'].mean()\n",
    "    Mean_answer_count = group['Challenge_answer_count'].mean()\n",
    "    Mean_comment_count = group['Challenge_comment_count'].mean()\n",
    "    Mean_participation_count = Mean_answer_count + Mean_comment_count\n",
    "    Score_participation_ratio = Mean_score / Mean_participation_count\n",
    "    Score_participation_weighted_product = (\n",
    "        group['Challenge_score'] * group['Challenge_participation_count']).mean()\n",
    "    Count = group['Challenge_topic'].count()\n",
    "    Count_ratio = Count / total_count * 100\n",
    "    Solved_ratio = group['Challenge_closed_time'].notna().sum() / Count\n",
    "    Mean_solved_time = group['Challenge_solved_time'].mean(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    Median_solved_time = group['Challenge_solved_time'].median(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    Mean_adjusted_solved_time = group['Challenge_adjusted_solved_time'].mean(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    Median_adjusted_solved_time = group['Challenge_adjusted_solved_time'].median(\n",
    "    ) / pd.Timedelta(hours=1)\n",
    "    topic_info = {\n",
    "        'Challenge_topic': name,\n",
    "        'Mean_score': Mean_score,\n",
    "        'Mean_favorite_count': Mean_favorite_count,\n",
    "        'Mean_follower_count': Mean_follower_count,\n",
    "        'Mean_link_count': Mean_link_count,\n",
    "        'Mean_information_entropy': Mean_information_entropy,\n",
    "        'Mean_readability': Mean_readability,\n",
    "        'Mean_sentence_count': Mean_sentence_count,\n",
    "        'Mean_word_count': Mean_word_count,\n",
    "        'Mean_unique_word_count': Mean_unique_word_count,\n",
    "        'Mean_view_count': Mean_view_count,\n",
    "        'Mean_answer_count': Mean_answer_count,\n",
    "        'Mean_comment_count': Mean_comment_count,\n",
    "        'Score_participation_ratio': Score_participation_ratio,\n",
    "        'Score_participation_weighted_product': Score_participation_weighted_product,\n",
    "        'Count_ratio': Count_ratio,\n",
    "        'Solved_ratio': Solved_ratio,\n",
    "        'Mean_solved_time': Mean_solved_time,\n",
    "        'Median_solved_time': Median_solved_time,\n",
    "        'Mean_adjusted_solved_time': Mean_adjusted_solved_time,\n",
    "        'Median_adjusted_solved_time': Median_adjusted_solved_time,\n",
    "    }\n",
    "    df_topics.append(topic_info)\n",
    "\n",
    "df_topics = pd.DataFrame(df_topics)\n",
    "df_topics.to_json(os.path.join(path_challenge_information,\n",
    "                  'general.json'), indent=4, orient='records')\n",
    "df_topics = df_topics.set_index('Challenge_topic')\n",
    "\n",
    "fig = df_topics.sort_values('Mean_score', ascending=False)['Mean_score'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean score', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_score.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_favorite_count', ascending=False)['Mean_favorite_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean favorite count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_favorite_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_follower_count', ascending=False)['Mean_follower_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean follower count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_follower_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_link_count', ascending=False)['Mean_link_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean link count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_link_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_information_entropy', ascending=False)['Mean_information_entropy'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean info entropy', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_information_entropy.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_readability', ascending=False)['Mean_readability'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean readability', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_readability.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_sentence_count', ascending=False)['Mean_sentence_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean sentence count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_sentence_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_word_count', ascending=False)['Mean_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_unique_word_count', ascending=False)['Mean_unique_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean unique word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_unique_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_view_count', ascending=False)['Mean_view_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean view count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_view_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_answer_count', ascending=False)['Mean_answer_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean answer count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_answer_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_comment_count', ascending=False)['Mean_comment_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean comment count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_comment_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Score_participation_ratio', ascending=False)['Score_participation_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge score participation ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information,\n",
    "            'Score_participation_ratio.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Score_participation_weighted_product', ascending=False)['Score_participation_weighted_product'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge score participation weighted product', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information,\n",
    "            'Score_participation_weighted_product.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Count_ratio', ascending=False)['Count_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge count ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Count_ratio.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Solved_ratio')['Solved_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge Solved ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'solved_ratio.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_solved_time', ascending=False)['Mean_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge median solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'mean_solved_time.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Median_solved_time', ascending=False)['Median_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'median_solved_time.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_adjusted_solved_time', ascending=False)['Mean_adjusted_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge mean adjusted solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Mean_adjusted_solved_time.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Median_adjusted_solved_time', ascending=False)['Median_adjusted_solved_time'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Challenge median adjusted solved time', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_challenge_information, 'Median_adjusted_solved_time.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect solution statistics information\n",
    "\n",
    "df_solution = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_solution = df_solution[df_solution['Solution_topic'] > -1]\n",
    "\n",
    "total_count = df_challenge['Solution_topic'].count()\n",
    "df_topics = []\n",
    "\n",
    "for name, group in df_challenge.groupby('Solution_topic'):\n",
    "    Mean_score = group['Solution_score'].mean()\n",
    "    Mean_link_count = group['Solution_link_count'].mean()\n",
    "    Mean_information_entropy = group['Solution_information_entropy'].mean()\n",
    "    Mean_readability = group['Solution_readability'].mean()\n",
    "    Mean_sentence_count = group['Solution_sentence_count'].mean()\n",
    "    Mean_word_count = group['Solution_word_count'].mean()\n",
    "    Mean_unique_word_count = group['Solution_unique_word_count'].mean()\n",
    "    Mean_comment_count = group['Solution_comment_count'].mean()\n",
    "    Count_ratio = group['Solution_topic'].count() / total_count * 100\n",
    "    topic_info = {\n",
    "        'Solution_topic': name,\n",
    "        'Mean_score': Mean_score,\n",
    "        'Mean_link_count': Mean_link_count,\n",
    "        'Mean_information_entropy': Mean_information_entropy,\n",
    "        'Mean_readability': Mean_readability,\n",
    "        'Mean_sentence_count': Mean_sentence_count,\n",
    "        'Mean_word_count': Mean_word_count,\n",
    "        'Mean_unique_word_count': Mean_unique_word_count,\n",
    "        'Mean_comment_count': Mean_comment_count,\n",
    "        'Count_ratio': Count_ratio,\n",
    "    }\n",
    "    df_topics.append(topic_info)\n",
    "\n",
    "df_topics = pd.DataFrame(df_topics)\n",
    "df_topics.to_json(os.path.join(path_solution_information,\n",
    "                  'general.json'), indent=4, orient='records')\n",
    "df_topics = df_topics.set_index('Solution_topic')\n",
    "\n",
    "fig = df_topics.sort_values('Mean_score', ascending=False)['Mean_score'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean score', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_score.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_link_count', ascending=False)['Mean_link_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean link count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_link_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_information_entropy', ascending=False)['Mean_information_entropy'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean info entropy', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_information_entropy.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_readability', ascending=False)['Mean_readability'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean readability', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_readability.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_sentence_count', ascending=False)['Mean_sentence_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean sentence count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_sentence_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_word_count', ascending=False)['Mean_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_unique_word_count', ascending=False)['Mean_unique_word_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean unique word count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_unique_word_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Mean_comment_count', ascending=False)['Mean_comment_count'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution mean comment count', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Mean_comment_count.png'))\n",
    "plt.close()\n",
    "\n",
    "fig = df_topics.sort_values('Count_ratio', ascending=False)['Count_ratio'].plot(\n",
    "    kind='bar', figsize=(15, 8), title='Solution count ratio', rot=15).get_figure()\n",
    "fig.savefig(os.path.join(path_solution_information, 'Count_ratio.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess as sm_lowess\n",
    "\n",
    "\n",
    "def smooth(x, y, xgrid, lowess_kw=None):\n",
    "    samples = np.random.choice(len(x), 50, replace=True)\n",
    "    y_s = y[samples]\n",
    "    x_s = x[samples]\n",
    "    y_sm = sm_lowess(y_s, x_s, **lowess_kw)\n",
    "    # regularly sample it onto the grid\n",
    "    y_grid = scipy.interpolate.interp1d(\n",
    "        x_s, y_sm, fill_value='extrapolate')(xgrid)\n",
    "    return y_grid\n",
    "\n",
    "\n",
    "def lowess_with_confidence_bounds(x, y, conf_interval=0.95, lowess_kw=None):\n",
    "    \"\"\"\n",
    "    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n",
    "    \"\"\"\n",
    "    xgrid = np.linspace(x.min(), x.max())\n",
    "\n",
    "    K = 100\n",
    "    smooths = np.stack([smooth(x, y, xgrid, lowess_kw) for _ in range(K)]).T\n",
    "\n",
    "    mean = np.nanmean(smooths, axis=1)\n",
    "    stderr = scipy.stats.sem(smooths, axis=1)\n",
    "\n",
    "    clower = np.nanpercentile(smooths, (1-conf_interval)*50, axis=1)\n",
    "    cupper = np.nanpercentile(smooths, (1+conf_interval)*50, axis=1)\n",
    "\n",
    "    return xgrid, mean, stderr, clower, cupper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-08-08 14:04:22.160000'),\n",
       " Timestamp('2023-02-22 01:36:03.995000'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_challenge = df_all[df_all['Challenge_topic'] > -1]\n",
    "# BigQuery Stack Overflow public dataset is updated until Nov 24, 2022, 1:39:22 PM UTC-5\n",
    "min(df_challenge['Challenge_creation_time']), max(df_challenge['Challenge_creation_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore challenge topics evolution\n",
    "\n",
    "df_challenge = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_challenge = df_challenge[df_challenge['Challenge_topic'] > -1]\n",
    "df_challenge = df_challenge[(df_challenge['Challenge_creation_time'] > '2014-09-14')\n",
    "                            & (df_challenge['Challenge_creation_time'] < '2022-11-21')]\n",
    "\n",
    "for name, group in df_challenge.groupby('Challenge_topic'):\n",
    "    group = group.groupby(pd.Grouper(key='Challenge_creation_time', freq='2W')).agg(\n",
    "        Count=('Challenge_topic', 'count')).reset_index()\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    x = np.array([i.astype('datetime64[D]').astype(int) for i in x])\n",
    "    y = group['Count'].values\n",
    "    # 95% confidence interval\n",
    "    xgrid, mean, stderr, clower, cupper = lowess_with_confidence_bounds(\n",
    "        x, y, conf_interval=0.95, lowess_kw={\"frac\": 0.5, \"it\": 5, \"return_sorted\": False})\n",
    "    x = pd.to_datetime(group['Challenge_creation_time']).values\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.plot(x, y, 'k.', label='Observations')\n",
    "    plt.plot(xgrid, mean, color='tomato', label='LOWESS')\n",
    "    plt.fill_between(xgrid, clower, cupper, alpha=0.3,\n",
    "                     label='LOWESS uncertainty')\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(os.path.join(path_challenge_evolution,\n",
    "                f'Topic_{name}'), bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-09-14 22:12:24.493000'),\n",
       " Timestamp('2023-02-21 18:36:06.284000'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_solution = df_all[df_all['Solution_topic'] > -1]\n",
    "# BigQuery Stack Overflow public dataset is updated until Nov 24, 2022, 1:39:22 PM UTC-5\n",
    "min(df_solution['Challenge_creation_time']), max(\n",
    "    df_solution['Challenge_creation_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore solution topics evolution\n",
    "\n",
    "df_solution = pd.read_json(os.path.join(path_general, 'Topics.json'))\n",
    "df_solution = df_solution[df_solution['Solution_topic'] > -1]\n",
    "df_solution = df_solution[(df_solution['Challenge_creation_time'] > '2014-09-14')\n",
    "                          & (df_solution['Challenge_creation_time'] < '2022-11-21')]\n",
    "\n",
    "for name, group in df_solution.groupby('Solution_topic'):\n",
    "    group = group.groupby(pd.Grouper(key='Challenge_closed_time', freq='W')).agg(\n",
    "        Count=('Solution_topic', 'count')).reset_index()\n",
    "    x = pd.to_datetime(group['Challenge_closed_time']).values\n",
    "    x = np.array([i.astype('datetime64[D]').astype(int) for i in x])\n",
    "    y = group['Count'].values\n",
    "    # 95% confidence interval\n",
    "    xgrid, mean, stderr, clower, cupper = lowess_with_confidence_bounds(\n",
    "        x, y, conf_interval=0.95, lowess_kw={\"frac\": 0.5, \"it\": 5, \"return_sorted\": False})\n",
    "    x = pd.to_datetime(group['Challenge_closed_time']).values\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.plot(x, y, 'k.', label='Observations')\n",
    "    plt.plot(xgrid, mean, color='tomato', label='LOWESS')\n",
    "    plt.fill_between(xgrid, clower, cupper, alpha=0.3,\n",
    "                     label='LOWESS uncertainty')\n",
    "    plt.legend(loc='best')\n",
    "    fig.savefig(os.path.join(path_solution_evolution,\n",
    "                f'Topic_{name}'), bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
