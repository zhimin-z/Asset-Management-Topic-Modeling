{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_dataset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Issue_original_content'] = df_issues['Issue_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('; Content:', ''))\n",
    "df_issues['Issue_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_issues['Issue_preprocessed_content'] = df_issues['Issue_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('; Content:', ''))\n",
    "\n",
    "df_questions['Question_original_content'] = df_questions['Question_original_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('; Content:', ''))\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('; Content:', ''))\n",
    "\n",
    "df_issues['Challenge_original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Challenge_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Challenge_preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "\n",
    "df_questions['Challenge_original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Challenge_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Challenge_preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "\n",
    "df_questions['Solution_original_content'] = df_questions['Answer_original_content']\n",
    "df_questions['Solution_original_content_gpt_summary'] = df_questions['Answer_original_content_gpt_summary']\n",
    "df_questions['Solution_preprocessed_content'] = df_questions['Answer_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_questions['Answer_original_content']\n",
    "del df_questions['Answer_original_content_gpt_summary']\n",
    "del df_questions['Answer_preprocessed_content']\n",
    "\n",
    "df_all = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_original.json'),\n",
    "               indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading\n",
      "fix import issue\n",
      "logger none\n",
      "i solved it, thank you.\n",
      "azurebug1 point\n",
      "you can try https://docs.microsoft.com/en-us/azure/machine-learning/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments\n",
      "this is now solved. thanks!\n",
      "try this in postman.\n",
      "answered on github\n",
      "yes that looks correct!\n",
      "thank you! all fixed.\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_original.json'))\n",
    "\n",
    "# remove issues with uninformed content\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(row['Challenge_original_content'].split()) < 4 or len(row['Challenge_original_content']) < 20:\n",
    "        print(row['Challenge_original_content'])\n",
    "        df_all.drop(index, inplace=True)\n",
    "    elif row['Solution_original_content'] and (len(row['Solution_original_content'].split()) < 6 or len(row['Solution_original_content']) < 30):\n",
    "        print(row['Solution_original_content'])\n",
    "        df_all.drop(index, inplace=True)\n",
    "\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_filtered.json'),\n",
    "               indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4870</td>\n",
       "      <td>-1_error attempting_error message_azure_endpoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>0_accessing s3_s3 bucket_access s3_s3 buckets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>297</td>\n",
       "      <td>1_git repository_git lfs_git repo_version control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>272</td>\n",
       "      <td>2_line plots_plot user_line chart_bar chart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>261</td>\n",
       "      <td>3_file batch_dataset csv_batch transform_csv f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>237</td>\n",
       "      <td>4_running pytorch_pytorch_model bin_deploy pyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>201</td>\n",
       "      <td>5_modulenotfounderror attempting_encountering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>192</td>\n",
       "      <td>6_jupyter notebooks_jupyter notebook_managed j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>182</td>\n",
       "      <td>7_annotation job_labelling job_labeling job_la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "      <td>8_dataset azure_azure file_azure blob_azure data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>9_rstudio application_rlang package_challenge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>147</td>\n",
       "      <td>10_deploying tensorflow_deploy tensorflow_depl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>11_endpoint failed_model endpoint_occurred mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>119</td>\n",
       "      <td>12_automl forecasting_forecasting automl_times...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>112</td>\n",
       "      <td>13_apache spark_sparkmagic pyspark_pysparkproc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>111</td>\n",
       "      <td>14_tree regression_decision tree_forest classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>108</td>\n",
       "      <td>15_aws lambda_lambda endpoint_lambda api_endpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>108</td>\n",
       "      <td>16_custom docker_model docker_docker environme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>99</td>\n",
       "      <td>17_xgboost model_model xgboost_xgboost load_mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>91</td>\n",
       "      <td>18_create pipeline_pipeline desired_data pipel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>81</td>\n",
       "      <td>19_translation api_api translate_translate api...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>77</td>\n",
       "      <td>20_guild file_guild yml_pythonpath guild_overw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>74</td>\n",
       "      <td>21_ai ml_ml platform_requesting organization_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>72</td>\n",
       "      <td>22_hyperparameter tuning_hyperparameter optimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "      <td>23_azure kubernetes_deploying kubernetes_kuber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>65</td>\n",
       "      <td>24_aws identity_amazonfullaccess policy_access...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>63</td>\n",
       "      <td>25_job stuck_job running_stop_training_job_cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>63</td>\n",
       "      <td>26_logging tensorboard_tensorboard logs_tensor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>27_pipeline run_running pipeline_pipeline succ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>53</td>\n",
       "      <td>28_parameters sweep_sweep agent_agent sweep_sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>29_azure automated_azure cloud_deploy azure_mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>49</td>\n",
       "      <td>30_workspace user_access workspace_workspace d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>48</td>\n",
       "      <td>31_model databricks_models databricks_databric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>32_gcp user_using gcp_gcp project_gcp endpoints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>33_delete experiments_deleting runs_run deleti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>37</td>\n",
       "      <td>34_deleted user_delete user_account deleted_us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>35_encountering memoryerror_memoryerror_memory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36_huggingface model_bert model_deploy hugging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>37_roles azure_workspace azure_account azure_a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   4870  -1_error attempting_error message_azure_endpoi...\n",
       "1       0    332      0_accessing s3_s3 bucket_access s3_s3 buckets\n",
       "2       1    297  1_git repository_git lfs_git repo_version control\n",
       "3       2    272        2_line plots_plot user_line chart_bar chart\n",
       "4       3    261  3_file batch_dataset csv_batch transform_csv f...\n",
       "5       4    237  4_running pytorch_pytorch_model bin_deploy pyt...\n",
       "6       5    201  5_modulenotfounderror attempting_encountering ...\n",
       "7       6    192  6_jupyter notebooks_jupyter notebook_managed j...\n",
       "8       7    182  7_annotation job_labelling job_labeling job_la...\n",
       "9       8    182   8_dataset azure_azure file_azure blob_azure data\n",
       "10      9    159  9_rstudio application_rlang package_challenge ...\n",
       "11     10    147  10_deploying tensorflow_deploy tensorflow_depl...\n",
       "12     11    121  11_endpoint failed_model endpoint_occurred mod...\n",
       "13     12    119  12_automl forecasting_forecasting automl_times...\n",
       "14     13    112  13_apache spark_sparkmagic pyspark_pysparkproc...\n",
       "15     14    111  14_tree regression_decision tree_forest classi...\n",
       "16     15    108  15_aws lambda_lambda endpoint_lambda api_endpo...\n",
       "17     16    108  16_custom docker_model docker_docker environme...\n",
       "18     17     99  17_xgboost model_model xgboost_xgboost load_mo...\n",
       "19     18     91  18_create pipeline_pipeline desired_data pipel...\n",
       "20     19     81  19_translation api_api translate_translate api...\n",
       "21     20     77  20_guild file_guild yml_pythonpath guild_overw...\n",
       "22     21     74  21_ai ml_ml platform_requesting organization_u...\n",
       "23     22     72  22_hyperparameter tuning_hyperparameter optimi...\n",
       "24     23     67  23_azure kubernetes_deploying kubernetes_kuber...\n",
       "25     24     65  24_aws identity_amazonfullaccess policy_access...\n",
       "26     25     63  25_job stuck_job running_stop_training_job_cha...\n",
       "27     26     63  26_logging tensorboard_tensorboard logs_tensor...\n",
       "28     27     55  27_pipeline run_running pipeline_pipeline succ...\n",
       "29     28     53  28_parameters sweep_sweep agent_agent sweep_sw...\n",
       "30     29     52  29_azure automated_azure cloud_deploy azure_mi...\n",
       "31     30     49  30_workspace user_access workspace_workspace d...\n",
       "32     31     48  31_model databricks_models databricks_databric...\n",
       "33     32     42    32_gcp user_using gcp_gcp project_gcp endpoints\n",
       "34     33     40  33_delete experiments_deleting runs_run deleti...\n",
       "35     34     37  34_deleted user_delete user_account deleted_us...\n",
       "36     35     36  35_encountering memoryerror_memoryerror_memory...\n",
       "37     36     36  36_huggingface model_bert model_deploy hugging...\n",
       "38     37     35  37_roles azure_workspace azure_account azure_a..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the best challenge topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_components=5, metric='manhattan',\n",
    "                  random_state=42, low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "min_samples = int(35 * 0.5)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=35, min_samples=min_samples, prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    calculate_probabilities=True\n",
    ")\n",
    "\n",
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_filtered.json'))\n",
    "docs = df_all['Challenge_original_content_gpt_summary'].tolist()\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "# This uses the soft-clustering as performed by HDBSCAN to find the best matching topic for each outlier document.\n",
    "new_topics_challenge = topic_model.reduce_outliers(\n",
    "    docs, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "\n",
    "# topic_model.save(os.path.join(path_dataset, 'Topic model'))\n",
    "\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Topic visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_barchart()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Term visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_heatmap()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Topic similarity visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_term_rank()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Term score decline visualization.html'))\n",
    "\n",
    "# hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "# fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "# fig.write_html(os.path.join(path_dataset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "# embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "# fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "# fig.write_html(os.path.join(path_dataset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_filtered.json'))\n",
    "df_all['Challenge_topic'] = ''\n",
    "\n",
    "for index, row in df_all.iterrows():\n",
    "  df_all.at[index, 'Challenge_topic'] = new_topics_challenge.pop(0)\n",
    "\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_topics.json'),\n",
    "               indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2076</td>\n",
       "      <td>-1_ai platform_lifecycle configuration_ml_aws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>0_bar chart_line chart_plot user_metrics run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>1_data git_git lfs_local git_version control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>2_accessing s3_access s3_s3 bucket_s3 user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>3_hyperparameter sweep_parameters sweep_hyperp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>4_azure blob_azure dataset_dataset azure_azure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>5_azure kubernetes_endpoint deployment_deployi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>6_launching jupyterlabs_jupyter notebooks_run ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>7_web services_web service_webservice_webservi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>8_azure automl_azure auto_automl forecasting_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>74</td>\n",
       "      <td>9_azure cloud_azure data_using azure_azure iot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>69</td>\n",
       "      <td>10_docker container_docker images_custom docke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>11_steps pipeline_creating pipeline_step pipel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>12_pytorch cuda_conda_pytorch_p36_conda_pytorc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>13_issue rstudio_rstudio_script azure_requests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>59</td>\n",
       "      <td>14_tensorflow aws_tensorflow serving_deploy te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>15_memoryerror attempting_encountering memorye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>16_pythonpath guild_guild yml_guild file_speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>17_forest classifier_random forest_forest algo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>47</td>\n",
       "      <td>18_model registry_registering model_registermo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>19_workspace azure_existing azure_azure accoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>20_xgboost container_deployed xgboost_run xgbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "      <td>21_gpu utilization_gpu memory_average gpu_gpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>22_speech api_google speech_text speech_speech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>23_submitting pipeline_pipeline failing_submit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>24_manually deleting_remote deleting_deleting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>42</td>\n",
       "      <td>25_text recognition_custom ocr_ocr_make ocr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>26_lambda endpoint_endpoint lambda_aws lambda_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>27_gcp api_endpoint gcp_gcp endpoints_pipeline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>28_logging tensorboard_data tensorboard_run te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>37</td>\n",
       "      <td>29_instance segmentation_labeling job_semantic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>30_quota ncast4_v3_lack quota_access compute_q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>31_process stuck_run stuck_running message_run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>32_translation api_translate api_google transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33_deleted user_delete user_account deleted_ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>34_ai ml_learning agent_learning artificial_in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>35_pyspark kernel_issue pyspark_parameters pys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>36_endpoints security_endpoint publicly_endpoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>37_json csv_json format_csv file_process json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>38_multimodel endpoint_model endpoint_containe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>26</td>\n",
       "      <td>39_log_model_log_model function_log kerasclass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>40_conda install_conda environment_conda env_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>41_azure databricks_databricks user_databricks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>25</td>\n",
       "      <td>42_issue pandas_dataset to_pandas_dataframe_to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>43_python dependency_import python_python modu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>44_deploy pickled_pickle error_runtimeerror lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>45</td>\n",
       "      <td>21</td>\n",
       "      <td>45_network error_encountering connecttimeout_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>46</td>\n",
       "      <td>21</td>\n",
       "      <td>46_pip packages_package python_install pip3_py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>47_understanding cost_cost data_pricing docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>48_parameters yaml_params yaml_yaml file_param...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>49</td>\n",
       "      <td>19</td>\n",
       "      <td>49_colab create_colab file_gcp colab_initialis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>50_learning automl_automl model_automl specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "      <td>51_dialogflow cx_dialogflow integrating_integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>52</td>\n",
       "      <td>17</td>\n",
       "      <td>52_encountering performance_processing time_pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>53</td>\n",
       "      <td>16</td>\n",
       "      <td>53_missing values_missing data_refactor datase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>54_azure powerbi_model powerbi_interfacing pow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   2076      -1_ai platform_lifecycle configuration_ml_aws\n",
       "1       0    207       0_bar chart_line chart_plot user_metrics run\n",
       "2       1    192       1_data git_git lfs_local git_version control\n",
       "3       2    160         2_accessing s3_access s3_s3 bucket_s3 user\n",
       "4       3    134  3_hyperparameter sweep_parameters sweep_hyperp...\n",
       "5       4     99  4_azure blob_azure dataset_dataset azure_azure...\n",
       "6       5     99  5_azure kubernetes_endpoint deployment_deployi...\n",
       "7       6     77  6_launching jupyterlabs_jupyter notebooks_run ...\n",
       "8       7     77  7_web services_web service_webservice_webservi...\n",
       "9       8     75  8_azure automl_azure auto_automl forecasting_a...\n",
       "10      9     74     9_azure cloud_azure data_using azure_azure iot\n",
       "11     10     69  10_docker container_docker images_custom docke...\n",
       "12     11     64  11_steps pipeline_creating pipeline_step pipel...\n",
       "13     12     60  12_pytorch cuda_conda_pytorch_p36_conda_pytorc...\n",
       "14     13     60  13_issue rstudio_rstudio_script azure_requests...\n",
       "15     14     59  14_tensorflow aws_tensorflow serving_deploy te...\n",
       "16     15     57  15_memoryerror attempting_encountering memorye...\n",
       "17     16     53  16_pythonpath guild_guild yml_guild file_speci...\n",
       "18     17     51  17_forest classifier_random forest_forest algo...\n",
       "19     18     47  18_model registry_registering model_registermo...\n",
       "20     19     45  19_workspace azure_existing azure_azure accoun...\n",
       "21     20     45  20_xgboost container_deployed xgboost_run xgbo...\n",
       "22     21     43     21_gpu utilization_gpu memory_average gpu_gpus\n",
       "23     22     43  22_speech api_google speech_text speech_speech...\n",
       "24     23     43  23_submitting pipeline_pipeline failing_submit...\n",
       "25     24     42  24_manually deleting_remote deleting_deleting ...\n",
       "26     25     42        25_text recognition_custom ocr_ocr_make ocr\n",
       "27     26     39  26_lambda endpoint_endpoint lambda_aws lambda_...\n",
       "28     27     39  27_gcp api_endpoint gcp_gcp endpoints_pipeline...\n",
       "29     28     38  28_logging tensorboard_data tensorboard_run te...\n",
       "30     29     37  29_instance segmentation_labeling job_semantic...\n",
       "31     30     35  30_quota ncast4_v3_lack quota_access compute_q...\n",
       "32     31     35  31_process stuck_run stuck_running message_run...\n",
       "33     32     34  32_translation api_translate api_google transl...\n",
       "34     33     33  33_deleted user_delete user_account deleted_ac...\n",
       "35     34     32  34_ai ml_learning agent_learning artificial_in...\n",
       "36     35     31  35_pyspark kernel_issue pyspark_parameters pys...\n",
       "37     36     31  36_endpoints security_endpoint publicly_endpoi...\n",
       "38     37     29      37_json csv_json format_csv file_process json\n",
       "39     38     29  38_multimodel endpoint_model endpoint_containe...\n",
       "40     39     26  39_log_model_log_model function_log kerasclass...\n",
       "41     40     26  40_conda install_conda environment_conda env_c...\n",
       "42     41     25  41_azure databricks_databricks user_databricks...\n",
       "43     42     25  42_issue pandas_dataset to_pandas_dataframe_to...\n",
       "44     43     24  43_python dependency_import python_python modu...\n",
       "45     44     22  44_deploy pickled_pickle error_runtimeerror lo...\n",
       "46     45     21  45_network error_encountering connecttimeout_u...\n",
       "47     46     21  46_pip packages_package python_install pip3_py...\n",
       "48     47     20  47_understanding cost_cost data_pricing docume...\n",
       "49     48     19  48_parameters yaml_params yaml_yaml file_param...\n",
       "50     49     19  49_colab create_colab file_gcp colab_initialis...\n",
       "51     50     18  50_learning automl_automl model_automl specifi...\n",
       "52     51     17  51_dialogflow cx_dialogflow integrating_integr...\n",
       "53     52     17  52_encountering performance_processing time_pe...\n",
       "54     53     16  53_missing values_missing data_refactor datase...\n",
       "55     54     15  54_azure powerbi_model powerbi_interfacing pow..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the best solution topic model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_components=5, metric='manhattan',\n",
    "                  random_state=42, low_memory=False)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "min_samples = int(15 * 0.5)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, min_samples=min_samples, prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    calculate_probabilities=True\n",
    ")\n",
    "\n",
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_filtered.json'))\n",
    "df_all = df_all[df_all['Solution_original_content'].isnull() == False]\n",
    "df_all = df_all[df_all['Solution_original_content'] != '']\n",
    "docs = df_all['Challenge_original_content_gpt_summary'].tolist()\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "# This uses the soft-clustering as performed by HDBSCAN to find the best matching topic for each outlier document.\n",
    "new_topics_solution = topic_model.reduce_outliers(\n",
    "    docs, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "\n",
    "# topic_model.save(os.path.join(path_dataset, 'Topic model'))\n",
    "\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Topic visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_barchart()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Term visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_heatmap()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Topic similarity visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_term_rank()\n",
    "# fig.write_html(os.path.join(path_dataset, 'Term score decline visualization.html'))\n",
    "\n",
    "# hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "# fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "# fig.write_html(os.path.join(path_dataset, 'Hierarchical clustering visualization.html'))\n",
    "\n",
    "# embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "# fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "# fig.write_html(os.path.join(path_dataset, 'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_topics.json'))\n",
    "df_all['Solution_topic'] = -1\n",
    "\n",
    "for index, row in df_all.iterrows():\n",
    "  if not row['Solution_original_content_gpt_summary']:\n",
    "    continue\n",
    "  df_all.at[index, 'Solution_topic'] = new_topics_solution.pop(0)\n",
    "\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_topics.json'),\n",
    "               indent=4, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
