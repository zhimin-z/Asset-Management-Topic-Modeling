{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\",\n",
    "              None, 'display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join(os.path.dirname(os.getcwd()), 'Dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine issues and questions\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(path_dataset, 'issues_original.json'))\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_dataset, 'questions_original.json'))\n",
    "\n",
    "df_issues['Challenge_link'] = df_issues['Issue_link']\n",
    "df_issues['Challenge_original_content'] = df_issues['Issue_original_content']\n",
    "df_issues['Challenge_original_content_gpt_summary'] = df_issues['Issue_original_content_gpt_summary']\n",
    "df_issues['Challenge_preprocessed_content'] = df_issues['Issue_preprocessed_content']\n",
    "df_issues['Challenge_creation_time'] = df_issues['Issue_creation_time']\n",
    "df_issues['Challenge_comment_count'] = df_issues['Issue_comment_count']\n",
    "df_issues['Challenge_score'] = df_issues['Issue_upvote_count'] - df_issues['Issue_downvote_count']\n",
    "\n",
    "df_questions['Challenge_link'] = df_questions['Question_link']\n",
    "df_questions['Challenge_original_content'] = df_questions['Question_original_content']\n",
    "df_questions['Challenge_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary']\n",
    "df_questions['Challenge_preprocessed_content'] = df_questions['Question_preprocessed_content']\n",
    "df_questions['Challenge_creation_time'] = df_questions['Question_creation_time']\n",
    "df_questions['Challenge_comment_count'] = df_questions['Question_comment_count']\n",
    "df_questions['Challenge_score'] = df_questions['Question_score']\n",
    "\n",
    "df_questions['Solution_original_content'] = df_questions['Answer_original_content']\n",
    "df_questions['Solution_original_content_gpt_summary'] = df_questions['Answer_original_content_gpt_summary']\n",
    "df_questions['Solution_preprocessed_content'] = df_questions['Answer_preprocessed_content']\n",
    "\n",
    "del df_issues['Issue_title']\n",
    "del df_issues['Issue_body']\n",
    "del df_issues['Issue_link']\n",
    "del df_issues['Issue_creation_time']\n",
    "del df_issues['Issue_comment_count']\n",
    "del df_issues['Issue_upvote_count']\n",
    "del df_issues['Issue_downvote_count']\n",
    "del df_issues['Issue_original_content']\n",
    "del df_issues['Issue_original_content_gpt_summary']\n",
    "del df_issues['Issue_preprocessed_content']\n",
    "\n",
    "del df_questions['Question_title']\n",
    "del df_questions['Question_body']\n",
    "del df_questions['Question_link']\n",
    "del df_questions['Question_creation_time']\n",
    "del df_questions['Question_comment_count']\n",
    "del df_questions['Question_score']\n",
    "del df_questions['Question_original_content']\n",
    "del df_questions['Question_original_content_gpt_summary']\n",
    "del df_questions['Question_preprocessed_content']\n",
    "\n",
    "del df_questions['Answer_original_content']\n",
    "del df_questions['Answer_original_content_gpt_summary']\n",
    "del df_questions['Answer_preprocessed_content']\n",
    "\n",
    "df_all = pd.concat([df_issues, df_questions], ignore_index=True)\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_original.json'),\n",
    "               indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess challenges and solutions\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS, remove_stopwords, preprocess_string\n",
    "\n",
    "stop_words_challenge = ['user', 'encount', 'attempt', 'unabl', 'try', 'seek', 'look', 'face', 'experi', 'struggl', 'us', 'challeng', 'difficulti', 'issu', 'error', 'problem', 'question']\n",
    "stop_words_solution = ['user', 'provid', 'need', 'includ', 'possibl', 'respond', 'suggest', 'resolv', 'differ', 'solut', 'answer', 'challeng', 'difficulti', 'issu', 'error', 'problem', 'question']\n",
    "\n",
    "stop_words_challenge = STOPWORDS.union(stop_words_challenge)\n",
    "stop_words_solution = STOPWORDS.union(stop_words_solution)\n",
    "\n",
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_original.json'))\n",
    "\n",
    "for index, row in df_all.iterrows():\n",
    "    df_all.at[index, 'Challenge_original_content'] = remove_stopwords(' '.join(preprocess_string(row['Challenge_original_content'].replace('Title: ', '').replace('; Content:', ''))), stopwords=stop_words_challenge)\n",
    "    df_all.at[index, 'Challenge_preprocessed_content'] = remove_stopwords(' '.join(preprocess_string(row['Challenge_preprocessed_content'].replace('Title: ', '').replace('; Content:', ''))), stopwords=stop_words_challenge)\n",
    "    df_all.at[index, 'Challenge_original_content_gpt_summary'] = remove_stopwords(' '.join(preprocess_string(row['Challenge_original_content_gpt_summary'])), stopwords=stop_words_challenge)\n",
    "    \n",
    "    if row['Solution_original_content']:\n",
    "        df_all.at[index, 'Solution_original_content'] = remove_stopwords(' '.join(preprocess_string(row['Solution_original_content'].replace('Title: ', '').replace('; Content:', ''))), stopwords=stop_words_solution)\n",
    "        df_all.at[index, 'Solution_preprocessed_content'] = remove_stopwords(' '.join(preprocess_string(row['Solution_preprocessed_content'].replace('Title: ', '').replace('; Content:', ''))), stopwords=stop_words_solution)\n",
    "        df_all.at[index, 'Solution_original_content_gpt_summary'] = remove_stopwords(' '.join(preprocess_string(row['Solution_original_content_gpt_summary'])), stopwords=stop_words_solution)\n",
    "    \n",
    "df_all.to_json(os.path.join(path_dataset, 'all_preprocessed.json'),\n",
    "               indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modulenotfounderror modul tensorboard\n",
      "combin param param work\n",
      "load\n",
      "deploy fail\n",
      "log val loss\n",
      "fix import\n",
      "tensorboard default logger option project\n",
      "logger\n",
      "let know happen\n",
      "file pin azurestor\n",
      "leav case come accross\n",
      "forc gpu help devic devic gpu\n",
      "tri work let file transient\n",
      "aw cli aw sdk exampl\n",
      "accord document maximum\n",
      "add run flush end script\n",
      "csv file folder recordio\n",
      "begin list\n",
      "convert dataset modul\n",
      "cloudform templat fix sure\n",
      "sure estim framework version version\n",
      "googl code work\n",
      "littl involv ye\n",
      "version correct\n",
      "situat chang git avail\n",
      "local comput pipelin articl\n",
      "think chang log stream servic\n",
      "current python confid interv\n",
      "fix updat core\n",
      "current bring data server\n",
      "support hive adl\n",
      "fit pass paramet job yourjobnam\n",
      "like templat run creat instanc\n",
      "bug got correct todai close\n",
      "pipelin moment\n",
      "vpc endpoint api api st\n",
      "delet creat endpoint fix microsoft\n",
      "file path ensur correct path\n",
      "upgrad sqlalchemi solv\n",
      "fix version sdk\n",
      "close solv wrong\n",
      "current metric publish\n",
      "network set\n",
      "chang bucket file work\n",
      "tri paramet registermodel step\n",
      "kernel run syntax highlight work\n",
      "multi attach support\n",
      "http stackoverflow com\n",
      "explain want insid lambda\n",
      "experienc updat tip\n",
      "updat forum updat run\n",
      "add set automl set auto\n",
      "shoaibakhtarshaikh thank column shown\n",
      "yolo azur support yolov import\n",
      "endpoint deploi webservic grei\n",
      "updat\n",
      "srin thank troubleshoot document\n",
      "solv thank\n",
      "disappear donâ€™t migrat design access\n",
      "bokkerslarlar dataset creat blob datastor\n",
      "rerun work fine\n",
      "believ subscript level owner contributor\n",
      "copi workspac duplic workspac debug\n",
      "kamilasoko got follow tutori\n",
      "updat base doc microsoft avail\n",
      "compat\n",
      "dont think ramr msft help\n",
      "studio durind deploi autom\n",
      "look like fix work normal\n",
      "solv thank\n",
      "data result point\n",
      "modifi web servic uri\n",
      "thank reach browser tri browser\n",
      "follow thread thank\n",
      "ad screenshot hope help regard\n",
      "like memori reduc train set\n",
      "better check excel blank row\n",
      "regard offici document\n",
      "anomali detect drop menu\n",
      "duplic review respons\n",
      "solv thank\n",
      "observ troubleshoot step help\n",
      "test fail custom speech\n",
      "nevermind work new workspac\n",
      "east region continu studi region\n",
      "frame datafram work\n",
      "abl select jupyt notebook\n",
      "try postman\n",
      "path parquet path field work\n",
      "github\n",
      "categori need help ask\n",
      "record discuss discord discord network\n",
      "workaround yaml anchor instead\n",
      "ye look correct\n",
      "thank fix\n",
      "version tell tri upgrad mlflw\n",
      "releas releas readi\n",
      "send slack alert failur slack\n",
      "support spark\n",
      "optim gener discuss board optim\n",
      "think ye share thought\n",
      "share encount\n",
      "noankmari dialogflow\n",
      "automl model invalid start letter\n",
      "reproduc reproduct step reproduc end\n",
      "research intern research intern offer\n",
      "contact support tell solv praneeth\n",
      "advanc nlu standard nlu\n",
      "export googl automl translat model\n",
      "gpu nvidia price butnvidia tesla\n",
      "contact cloud\n",
      "googl translat javascript api\n",
      "previous stack overflow post\n",
      "follow step follow codelab thank\n",
      "file tracker accord behavior shown\n",
      "right languag tab clickabl\n",
      "read documentationabout label vision\n",
      "look document appsheet offer\n",
      "text speech googl cloud python\n",
      "deploi pretrain fasttext model code\n",
      "integr wai integr articl resourc\n",
      "handov protocol facebook dialogflow\n",
      "cloud vision text coordin format\n",
      "think nlp hardest subset build\n",
      "wai download automl model train\n",
      "told support post transpar\n",
      "solv weekend\n",
      "hei lucasventura like\n",
      "attributeerror enumtypewrapp object attribut reslov\n",
      "slide fastai discord\n",
      "support help differ categori\n",
      "thank harveen convers continu\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_preprocessed.json'))\n",
    "\n",
    "# remove issues with uninformed content\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(row['Challenge_original_content'].split()) < 6 or len(row['Challenge_original_content']) < 30:\n",
    "        print(row['Challenge_original_content'])\n",
    "        df_all.drop(index, inplace=True)\n",
    "    elif row['Solution_original_content'] and (len(row['Solution_original_content'].split()) < 6 or len(row['Solution_original_content']) < 30):\n",
    "        print(row['Solution_original_content'])\n",
    "        df_all.drop(index, inplace=True)\n",
    "\n",
    "df_all.to_json(os.path.join(path_dataset, 'all_filtered.json'),\n",
    "               indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_json(os.path.join(path_dataset, 'all_topics.json'))\n",
    "df_all['Challenge_creation_date'] = ''\n",
    "# dates = []\n",
    "\n",
    "for index, row in df_all.iterrows():\n",
    "    if 'Git' in row['Platform']:\n",
    "        date = pd.to_datetime(row['Issue_creation_time'])\n",
    "    else:\n",
    "        date = pd.to_datetime(row['Question_creation_time'])\n",
    "    df_all.at[index, 'Challenge_creation_date'] = date.date()\n",
    "    # dates.append(date.date())\n",
    "\n",
    "# min(dates), max(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_challenge = df_all.groupby(\n",
    "    'Challenge_topic').size().reset_index(name='Count')\n",
    "df_challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
