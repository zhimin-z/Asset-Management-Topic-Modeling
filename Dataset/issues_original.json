[
    {
        "Issue_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/70",
        "Issue_title":"ResourceLimitExceeded for ml.m4.xlarge when running SageMaker studio demo in a new AWS account",
        "Issue_creation_time":1585108798000,
        "Issue_closed_time":1600123381000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"When walking through the SageMaker Studio tour :\r\n\r\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-end-to-end.html\r\n\r\nfor the first time in a new AWS account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model.\r\n\r\n`ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.`\r\n\r\nSuggestions:\r\n\r\n- The \"Prerequistes\" section could address this proactively, with a link to the service limit increase page, or...\r\n-  the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0`\r\n\r\nPlease LMK which is preferable and I will submit a PR\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: resourcelimitexceeded for ml.m4.xlarge when running studio demo in a new aws account; Content: when walking through the studio tour : https:\/\/docs.aws.amazon.com\/\/latest\/dg\/gs-studio-end-to-end.html for the first time in a new aws account, the usual service limit issue is hit when running code cell [17] to create an endpoint to host the model. `resourcelimitexceeded: an error occurred (resourcelimitexceeded) when calling the createendpoint operation: the account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 instances, with current utilization of 0 instances and a request delta of 1 instances. please contact aws support to request an increase for this limit.` suggestions: - the \"prerequistes\" section could address this proactively, with a link to the service limit increase page, or... - the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of `0` please lmk which is preferable and i will submit a pr",
        "Issue_original_content_gpt_summary":"The user encountered a resourcelimitexceeded error when running code cell [17] to create an endpoint to host the model in a new AWS account, due to the default service limit of 'ml.m4.xlarge for endpoint usage' being set to 0 instances.",
        "Issue_preprocessed_content":"Title: resourcelimitexceeded for when running studio demo in a new aws account; Content: when walking through the studio tour for the first time in a new aws account, the usual service limit issue is hit when running code cell to create an endpoint to host the model. suggestions the prerequistes section could address this proactively, with a link to the service limit increase page, the notebook could be changed to use an instance type for the endpoint that does not have a default service limit of please lmk which is preferable and i will submit a pr"
    },
    {
        "Issue_link":"https:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/issues\/69",
        "Issue_title":"sagemaker studio tour missing\/out of order steps",
        "Issue_creation_time":1585107785000,
        "Issue_closed_time":1593275392000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"regarding this section:\r\nhttps:\/\/github.com\/awsdocs\/amazon-sagemaker-developer-guide\/blob\/master\/doc_source\/gs-studio-end-to-end.md#keep-track-of-machine-learning-experiments\r\n\r\n- step 1 \"Run the following cell...\" refers to the 8th code cell in the notebook.  The previous 7 code cells need to be run first for the notebook to work, but are never referenced in the tour walkthrough doc.  The doc  goes from having the user clone the repo:\r\n\r\n`git clone https:\/\/github.com\/awslabs\/amazon-sagemaker-examples.git`\r\n\r\nstraight to having the user run the 8th code cell in the notebook, skipping the first 7 code cells.\r\n\r\n-  step 2 \"Create trials and associate....\" refers to code cell 11 in the notebook, but again jumps straight from cell 8 without ever running cells 9, or 10\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: studio tour missing\/out of order steps; Content: regarding this section: https:\/\/github.com\/awsdocs\/amazon--developer-guide\/blob\/master\/doc_source\/gs-studio-end-to-end.md#keep-track-of-machine-learning-experiments - step 1 \"run the following cell...\" refers to the 8th code cell in the notebook. the previous 7 code cells need to be run first for the notebook to work, but are never referenced in the tour walkthrough doc. the doc goes from having the user clone the repo: `git clone https:\/\/github.com\/awslabs\/amazon--examples.git` straight to having the user run the 8th code cell in the notebook, skipping the first 7 code cells. - step 2 \"create trials and associate....\" refers to code cell 11 in the notebook, but again jumps straight from cell 8 without ever running cells 9, or 10",
        "Issue_original_content_gpt_summary":"The user encountered challenges with the studio tour missing\/out of order steps, as the walkthrough doc skipped from having the user clone the repo to having the user run the 8th code cell in the notebook, without running the first 7 code cells, and then jumped straight from cell 8 to cell 11 without running cells 9 or 10.",
        "Issue_preprocessed_content":"Title: studio tour of order steps; Content: regarding this section step run the following refers to the th code cell in the notebook. the previous code cells need to be run first for the notebook to work, but are never referenced in the tour walkthrough doc. the doc goes from having the user clone the repo straight to having the user run the th code cell in the notebook, skipping the first code cells. step create trials and refers to code cell in the notebook, but again jumps straight from cell without ever running cells , or"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/170",
        "Issue_title":"[bug] : Model not loading while using existing container image to setup MME on sagemaker",
        "Issue_creation_time":1602135852000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [x] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [x] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nGetting this error, when invoking a MME on sagemaker setup using `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04` container image.\r\n\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=14448): Max retries exceeded with url: \/v1\/models\/d2295a7526f9df36354b8a2c4adc4f63 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f70966dba50>: Failed to establish a new connection: [Errno 111] Connection refused'))\r\nTraceback (most recent call last):\r\n  File \"\/sagemaker\/python_service.py\", line 157, in _handle_load_model_post\r\n    self._wait_for_model(model_name)\r\n  File \"\/sagemaker\/python_service.py\", line 247, in _wait_for_model\r\n    response = session.get(url)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 546, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send\r\n    raise ConnectionError(e, request=request)\r\n\r\n*DLC image\/dockerfile:*\r\n763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04\r\n*Current behavior:*\r\n\r\n*Expected behavior:*\r\nModel should load up and return prediction\r\n*Additional context:*\r\nI have setup a MME using the above mentioned container and invoking the endpoint using a lambda. The model files are in placed in S3 and are in the correct directory structure with a version number. ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] : model not loading while using existing container image to setup mme on ; Content: checklist - [x] i've prepended issue tag with type of change: [bug] - [ ] (if applicable) i've attached the script to reproduce the bug - [x] (if applicable) i've documented below the dlc image\/dockerfile this relates to - [ ] (if applicable) i've documented below the tests i've run on the dlc image - [x] i'm using an existing dlc image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html - [ ] i've built my own container based off dlc (and i've attached the code used to build my own image) *concise description:* getting this error, when invoking a mme on setup using `763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04` container image. urllib3.exceptions.maxretryerror: httpconnectionpool(host='localhost', port=14448): max retries exceeded with url: \/v1\/models\/d2295a7526f9df36354b8a2c4adc4f63 (caused by newconnectionerror(': failed to establish a new connection: [errno 111] connection refused')) traceback (most recent call last): file \"\/\/python_service.py\", line 157, in _handle_load_model_post self._wait_for_model(model_name) file \"\/\/python_service.py\", line 247, in _wait_for_model response = session.get(url) file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 546, in get return self.request('get', url, **kwargs) file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 533, in request resp = self.send(prep, **send_kwargs) file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/sessions.py\", line 646, in send r = adapter.send(request, **kwargs) file \"\/usr\/local\/lib\/python3.7\/site-packages\/requests\/adapters.py\", line 516, in send raise connectionerror(e, request=request) *dlc image\/dockerfile:* 763104351884.dkr.ecr.us-east-1.amazonaws.com\/tensorflow-inference:2.3.0-cpu-py37-ubuntu18.04 *current behavior:* *expected behavior:* model should load up and return prediction *additional context:* i have setup a mme using the above mentioned container and invoking the endpoint using a lambda. the model files are in placed in s3 and are in the correct directory structure with a version number.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with model not loading while using an existing container image to setup a MME on, resulting in a MaxRetryError and ConnectionError.",
        "Issue_preprocessed_content":"Title: model not loading while using existing container image to setup mme on ; Content: checklist i've prepended issue tag with type of change i've attached the script to reproduce the bug i've documented below the dlc this relates to i've documented below the tests i've run on the dlc image i'm using an existing dlc image listed here i've built my own container based off dlc concise description getting this error, when invoking a mme on setup using container image. httpconnectionpool max retries exceeded with url traceback file line , in file line , in response file line , in get return url, kwargs file line , in request resp file line , in send r kwargs file line , in send raise connectionerror dlc current behavior expected behavior model should load up and return prediction additional context i have setup a mme using the above mentioned container and invoking the endpoint using a lambda. the model files are in placed in s and are in the correct directory structure with a version number."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/73",
        "Issue_title":"Error in giving inputs to the tensorflow serving model on sagemaker. {'error': \"Missing 'inputs' or 'instances' key\"}",
        "Issue_creation_time":1567172491000,
        "Issue_closed_time":1571957138000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I have a custom model built-in TensorFlow. I am trying to deploy this model on amazon sagemaker for inference. The model takes three inputs and gives five outputs.\r\nThe name of the inputs are:\r\n1. `input_image` \r\n2. `input_image_meta` \r\n3. `input_anchors` \r\n\r\n\r\nand the name of outputs are:\r\n1.  `output_detections`\r\n2.  `output_mrcnn_class`\r\n3.  `output_mrcnn_bbox`\r\n4.  `output_mrcnn_mask`\r\n5.  `output_rois`\r\n\r\nI have successfully created the model endpoint on sagemaker and when I am trying to hit the request for the results, I am getting `{'error': \"Missing 'inputs' or 'instances' key\"}` in return.\r\n \r\nI have made a model.tar.gz file which has the following structure:\r\n\r\n    mymodel\r\n        |__1\r\n            |__variables\r\n            |__saved_model.pb\r\n\r\n    code\r\n        |__inference.py\r\n        |__requirements.txt\r\n\r\nAs specified in the documentation, inference.py has input_handler and output handler functions. From the client-side, I pass the S3 link of the image which then transforms to the three inputs for the model. \r\n\r\nThe structure of input_handler is as follows:\r\n\r\n```\r\ndef input_handler(data, context):\r\n     input_data = json.loads(data.read().decode('utf-8'))\r\n\r\n    obj = bucket.Object(input_data['img_link'])\r\n    tmp = tempfile.NamedTemporaryFile()\r\n    \r\n    # download image from AWS S3\r\n    with open(tmp.name, 'wb') as f:\r\n        obj.download_fileobj(f)\r\n        image=mpimg.imread(tmp.name)\r\n    \r\n    # make preprocessing\r\n    image = Image.fromarray(image)\r\n     \r\n     ...... # some more transformations \r\n     return = {\"input_image\": Python list for image,\r\n                    \"input_image_meta: Python list for input image meta,\r\n                    \"input_anchors\": Python list for input anchors}\r\n\r\n```\r\nThe deifinition of output_handler is as follows:\r\n\r\n```\r\ndef output_handler(data, context):\r\n      output_string = data.content.decode('unicode-escape')\r\n      return output_string, context.accept_header\r\n```\r\n\r\nThe sagemaker endpoint gets created and the tensorflow server also starts(as shown in CloudWatch logs).\r\nOn the client side, I call the predictor using follwoing code:\r\n\r\n```\r\nrequest = {}\r\nrequest[\"img_link\"] = \"image.jpg\"\r\nresult = predictor.predict(request)\r\n```\r\n\r\nBut when I print the result the following gets printed out, `{'error': \"Missing 'inputs' or 'instances' key\"}`\r\nAll the bucket connections for loading the image are in inference.py\r\n      ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error in giving inputs to the tensorflow serving model on . {'error': \"missing 'inputs' or 'instances' key\"}; Content: i have a custom model built-in tensorflow. i am trying to deploy this model on for inference. the model takes three inputs and gives five outputs. the name of the inputs are: 1. `input_image` 2. `input_image_meta` 3. `input_anchors` and the name of outputs are: 1. `output_detections` 2. `output_mrcnn_class` 3. `output_mrcnn_bbox` 4. `output_mrcnn_mask` 5. `output_rois` i have successfully created the model endpoint on and when i am trying to hit the request for the results, i am getting `{'error': \"missing 'inputs' or 'instances' key\"}` in return. i have made a model.tar.gz file which has the following structure: mymodel |__1 |__variables |__saved_model.pb code |__inference.py |__requirements.txt as specified in the documentation, inference.py has input_handler and output handler functions. from the client-side, i pass the s3 link of the image which then transforms to the three inputs for the model. the structure of input_handler is as follows: ``` def input_handler(data, context): input_data = json.loads(data.read().decode('utf-8')) obj = bucket.object(input_data['img_link']) tmp = tempfile.namedtemporaryfile() # download image from aws s3 with open(tmp.name, 'wb') as f: obj.download_fileobj(f) image=mpimg.imread(tmp.name) # make preprocessing image = image.fromarray(image) ...... # some more transformations return = {\"input_image\": python list for image, \"input_image_meta: python list for input image meta, \"input_anchors\": python list for input anchors} ``` the deifinition of output_handler is as follows: ``` def output_handler(data, context): output_string = data.content.decode('unicode-escape') return output_string, context.accept_header ``` the endpoint gets created and the tensorflow server also starts(as shown in cloudwatch logs). on the client side, i call the predictor using follwoing code: ``` request = {} request[\"img_link\"] = \"image.jpg\" result = predictor.predict(request) ``` but when i print the result the following gets printed out, `{'error': \"missing 'inputs' or 'instances' key\"}` all the bucket connections for loading the image are in inference.py",
        "Issue_original_content_gpt_summary":"The user is encountering an error when attempting to give inputs to a TensorFlow Serving model on AWS, receiving an error message of \"missing 'inputs' or 'instances' key\".",
        "Issue_preprocessed_content":"Title: error in giving inputs to the tensorflow serving model on . ; Content: i have a custom model tensorflow. i am trying to deploy this model on for inference. the model takes three inputs and gives five outputs. the name of the inputs are . . . and the name of outputs are . . . . . i have successfully created the model endpoint on and when i am trying to hit the request for the results, i am getting in return. i have made a file which has the following structure mymodel code as specified in the documentation, has and output handler functions. from the i pass the s link of the image which then transforms to the three inputs for the model. the structure of is as follows the deifinition of is as follows the endpoint gets created and the tensorflow server also starts . on the client side, i call the predictor using follwoing code but when i print the result the following gets printed out, all the bucket connections for loading the image are in"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/175",
        "Issue_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Issue_creation_time":1615292895000,
        "Issue_closed_time":1615314058000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error: no kind \"trainingjob\" is registered for version \".aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"; Content:",
        "Issue_original_content_gpt_summary":"The user encountered an error with the \"trainingjob\" not being registered for the version \".aws.amazon.com\/v1\" in the \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\" scheme.",
        "Issue_preprocessed_content":"Title: error no kind trainingjob is registered for version in scheme ; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/174",
        "Issue_title":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Issue_creation_time":1615292808000,
        "Issue_closed_time":1632465008000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"error: no kind \"TrainingJob\" is registered for version \"sagemaker.aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error: no kind \"trainingjob\" is registered for version \".aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"; Content: error: no kind \"trainingjob\" is registered for version \".aws.amazon.com\/v1\" in scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\"",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to register a \"trainingjob\" with the version \".aws.amazon.com\/v1\" in the scheme \"k8s.io\/kubectl\/pkg\/scheme\/scheme.go:28\".",
        "Issue_preprocessed_content":"Title: error no kind trainingjob is registered for version in scheme ; Content: error no kind trainingjob is registered for version in scheme"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/125",
        "Issue_title":"SageMaker Operator Types fails KubeBuilder Pattern validation check",
        "Issue_creation_time":1595360580000,
        "Issue_closed_time":1596827786000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nSageMaker Operator Types, when included as part of KubeBuilder V2 custom CRD definition fail due to validation errors of unescaped regex patterns. \r\n\r\n```\r\n\/go\/bin\/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\".\/...\" output:crd:artifacts:config=config\/crd\/bases\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n\/go\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/sagemaker_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at <input>:1:12)\r\n```\r\n\r\n**What you expected to happen**:\r\nKubeBuilder should generate CRD specification which includes AWS SageMaker Operator Types\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n```\r\nimport (\r\n\tcommonv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\"\r\n\tmetav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\"\r\n)\r\n\r\n\/\/ GuestbookSpec defines the desired state of Guestbook\r\ntype GuestbookSpec struct {\r\n\t\/\/ INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\r\n\t\/\/ Important: Run \"make\" to regenerate code after modifying this file\r\n\r\n\tAlgorithmSpecification *commonv1.AlgorithmSpecification `json:\"algorithmSpecification\"`\r\n\r\n\tEnableInterContainerTrafficEncryption *bool `json:\"enableInterContainerTrafficEncryption,omitempty\"`\r\n\r\n\tEnableNetworkIsolation *bool `json:\"enableNetworkIsolation,omitempty\"`\r\n...\r\n\/\/Run make install with above  types in custom operator\r\nmake install \r\n```\r\n\r\n**Anything else we need to know?**:\r\nTried copying the above types and escaped the regex pattern with quotes (``\/\/ +kubebuilder:validation:Pattern='^(https|s3):\/\/([^\/]+)\/?(.*)$'``) and everything worked\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`):Version: version.Version{KubeBuilderVersion:\"2.3.1\", KubernetesVendor:\"1.16.4\", GitCommit:\"8b53abeb4280186e494b726edf8f54ca7aa64a49\", BuildDate:\"2020-03-26T16:42:00Z\", GoOs:\"unknown\", GoArch:\"unknown\"}\r\n- Operator version (controller image tag):\tgithub.com\/aws\/amazon-sagemaker-operator-for-k8s v1.0.1-0.20200410212604-780c48ecb21a\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: operator types fails kubebuilder pattern validation check; Content: **what happened**: operator types, when included as part of kubebuilder v2 custom crd definition fail due to validation errors of unescaped regex patterns. ``` \/go\/bin\/controller-gen \"crd:trivialversions=true\" rbac:rolename=manager-role webhook paths=\".\/...\" output:crd:artifacts:config=config\/crd\/bases \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:500:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:450:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:82:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:466:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:103:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:488:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:515:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) \/go\/pkg\/mod\/github.com\/aws\/amazon--operator-for-k8s@v1.0.1-0.20200410212604-780c48ecb21a\/api\/v1\/common\/_api.go:110:2: extra arguments provided: \":\/\/([^\/]+)\/?(.*)$\" (at :1:12) ``` **what you expected to happen**: kubebuilder should generate crd specification which includes operator types **how to reproduce it (as minimally and precisely as possible)**: ``` import ( commonv1 \"github.com\/aws\/amazon--operator-for-k8s\/api\/v1\/common\" metav1 \"k8s.io\/apimachinery\/pkg\/apis\/meta\/v1\" ) \/\/ guestbookspec defines the desired state of guestbook type guestbookspec struct { \/\/ insert additional spec fields - desired state of cluster \/\/ important: run \"make\" to regenerate code after modifying this file algorithmspecification *commonv1.algorithmspecification `json:\"algorithmspecification\"` enableintercontainertrafficencryption *bool `json:\"enableintercontainertrafficencryption,omitempty\"` enablenetworkisolation *bool `json:\"enablenetworkisolation,omitempty\"` ... \/\/run make install with above types in custom operator make install ``` **anything else we need to know?**: tried copying the above types and escaped the regex pattern with quotes (``\/\/ +kubebuilder:validation:pattern='^(https|s3):\/\/([^\/]+)\/?(.*)$'``) and everything worked **environment**: - kubernetes version (use `kubectl version`):version: version.version{kubebuilderversion:\"2.3.1\", kubernetesvendor:\"1.16.4\", gitcommit:\"8b53abeb4280186e494b726edf8f54ca7aa64a49\", builddate:\"2020-03-26t16:42:00z\", goos:\"unknown\", goarch:\"unknown\"} - operator version (controller image tag): github.com\/aws\/amazon--operator-for-k8s v1.0.1-0.20200410212604-780c48ecb21a - os (e.g: `cat \/etc\/os-release`): - kernel (e.g. `uname -a`): - installation method: - others:",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where operator types, when included as part of kubebuilder v2 custom crd definition, failed due to validation errors of unescaped regex patterns.",
        "Issue_preprocessed_content":"Title: operator types fails kubebuilder pattern validation check; Content: what happened operator types, when included as part of kubebuilder v custom crd definition fail due to validation errors of unescaped regex patterns. what you expected to happen kubebuilder should generate crd specification which includes operator types how to reproduce it anything else we need to know? tried copying the above types and escaped the regex pattern with quotes and everything worked environment kubernetes version version gitcommit b abeb e b edf f ca aa a , goos unknown , goarch unknown operator version os kernel installation method others"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/122",
        "Issue_title":"Error Building SageMaker Types due to missing types in common\/manual_deepcopy",
        "Issue_creation_time":1592335014000,
        "Issue_closed_time":1599678360000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!\r\n\r\nIf you would like to report a vulnerability or have a security concern regarding AWS cloud services, please email aws-security@amazon.com\r\n-->\r\n\r\n\r\n**What happened**:\r\nError Building SageMaker Types due to missing types in common\/manual_deepcopy\r\n(base) afccd2:example nj$ make all\r\ngo: creating new go.mod: module tmp\r\ngo: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5\r\n\/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerFile=\"hack\/boilerplate.go.txt\" paths=\".\/...\"\r\ngo fmt .\/...\r\ncontrollers\/guestbook_controller.go\r\ngo vet .\/...\r\ngithub.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/common\r\n..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon-sagemaker-operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.DeepCopy undefined (type Tag has no field or method DeepCopy)\r\nmake: *** [vet] Error 2**\r\n\r\n**What you expected to happen**:\r\nPackaged types refer to types in zz_generated_deepcopy which are missing\r\n\r\n**How to reproduce it (as minimally and precisely as possible)**:\r\n\r\n\r\nImport of sagemaker types in Go Client fails build\r\n\r\nimport (\r\n\ttrainingjobv1 \"github.com\/aws\/amazon-sagemaker-operator-for-k8s\/api\/v1\/trainingjob\"\r\n)\r\n\r\n\r\n**Anything else we need to know?**:\r\n\r\n**Environment**:\r\n- Kubernetes version (use `kubectl version`): \r\n- Operator version (controller image tag): v1.1.0\r\n- OS (e.g: `cat \/etc\/os-release`):\r\n- Kernel (e.g. `uname -a`):\r\n- Installation method:\r\n- Others:\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error building types due to missing types in common\/manual_deepcopy; Content: **what happened**: error building types due to missing types in common\/manual_deepcopy (base) afccd2:example nj$ make all go: creating new go.mod: module tmp go: found sigs.k8s.io\/controller-tools\/cmd\/controller-gen in sigs.k8s.io\/controller-tools v0.2.5 \/devel\/projects\/go_tutorial\/bin\/controller-gen object:headerfile=\"hack\/boilerplate.go.txt\" paths=\".\/...\" go fmt .\/... controllers\/guestbook_controller.go go vet .\/... github.com\/aws\/amazon--operator-for-k8s\/api\/v1\/common ..\/..\/..\/go_tutorial\/pkg\/mod\/github.com\/aws\/amazon--operator-for-**k8s@v1.1.0\/api\/v1\/common\/manual_deepcopy.go:28:19: tag.deepcopy undefined (type tag has no field or method deepcopy) make: *** [vet] error 2** **what you expected to happen**: packaged types refer to types in zz_generated_deepcopy which are missing **how to reproduce it (as minimally and precisely as possible)**: import of types in go client fails build import ( trainingjobv1 \"github.com\/aws\/amazon--operator-for-k8s\/api\/v1\/trainingjob\" ) **anything else we need to know?**: **environment**: - kubernetes version (use `kubectl version`): - operator version (controller image tag): v1.1.0 - os (e.g: `cat \/etc\/os-release`): - kernel (e.g. `uname -a`): - installation method: - others:",
        "Issue_original_content_gpt_summary":"The user encountered a challenge building types due to missing types in common\/manual_deepcopy.",
        "Issue_preprocessed_content":"Title: error building types due to missing types in ; Content: what happened error building types due to missing types in base afccd example nj$ make all go creating new module tmp go found in go fmt go vet undefined make error what you expected to happen packaged types refer to types in which are missing how to reproduce it import of types in go client fails build import anything else we need to know? environment kubernetes version operator version os kernel installation method others"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/amazon-sagemaker-operator-for-k8s\/issues\/99",
        "Issue_title":"unable to kick off the sagemaker job",
        "Issue_creation_time":1583773133000,
        "Issue_closed_time":1599677796000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":15.0,
        "Issue_body":"\r\nDeployed the sample mnist training job but seems its not getting invoked on the SageMaker\r\n\r\n```\r\nkubectl describe TrainingJob            \r\nName:         xgboost-mnist\r\nNamespace:    default\r\nLabels:       <none>\r\nAnnotations:  kubectl.kubernetes.io\/last-applied-configuration:\r\n                {\"apiVersion\":\"sagemaker.aws.amazon.com\/v1\",\"kind\":\"TrainingJob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"...\r\nAPI Version:  sagemaker.aws.amazon.com\/v1\r\nKind:         TrainingJob\r\nMetadata:\r\n  Creation Timestamp:  2020-03-09T06:58:17Z\r\n  Generation:          1\r\n  Resource Version:    117181\r\n  Self Link:           \/apis\/sagemaker.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist\r\n  UID:                 5a907178-61d3-11ea-b461-02efd6507006\r\nSpec:\r\n  Algorithm Specification:\r\n    Training Image:       825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest\r\n    Training Input Mode:  File\r\n  Hyper Parameters:\r\n    Name:   max_depth\r\n    Value:  5\r\n    Name:   eta\r\n    Value:  0.2\r\n    Name:   gamma\r\n    Value:  4\r\n    Name:   min_child_weight\r\n    Value:  6\r\n    Name:   silent\r\n    Value:  0\r\n    Name:   objective\r\n    Value:  multi:softmax\r\n    Name:   num_class\r\n    Value:  10\r\n    Name:   num_round\r\n    Value:  10\r\n  Input Data Config:\r\n    Channel Name:      train\r\n    Compression Type:  None\r\n    Content Type:      text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/train\/\r\n    Channel Name:                    validation\r\n    Compression Type:                None\r\n    Content Type:                    text\/csv\r\n    Data Source:\r\n      S 3 Data Source:\r\n        S 3 Data Distribution Type:  FullyReplicated\r\n        S 3 Data Type:               S3Prefix\r\n        S 3 Uri:                     s3:\/\/<MY-BUCKET>\/xgboost-mnist\/validation\/\r\n  Output Data Config:\r\n    S 3 Output Path:  s3:\/\/<MY-BUCKET>\/xgboost-mnist\/models\/\r\n  Region:             us-east-2\r\n  Resource Config:\r\n    Instance Count:     1\r\n    Instance Type:      ml.m4.xlarge\r\n    Volume Size In GB:  5\r\n  Role Arn:             arn:aws:iam::<ACCOUNT>:role\/sagemaker_execution_role\r\n  Stopping Condition:\r\n    Max Runtime In Seconds:  86400```\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to kick off the job; Content: deployed the sample mnist training job but seems its not getting invoked on the ``` kubectl describe trainingjob name: xgboost-mnist namespace: default labels: annotations: kubectl.kubernetes.io\/last-applied-configuration: {\"apiversion\":\".aws.amazon.com\/v1\",\"kind\":\"trainingjob\",\"metadata\":{\"annotations\":{},\"name\":\"xgboost-mnist\",\"namespace\":\"default\"... api version: .aws.amazon.com\/v1 kind: trainingjob metadata: creation timestamp: 2020-03-09t06:58:17z generation: 1 resource version: 117181 self link: \/apis\/.aws.amazon.com\/v1\/namespaces\/default\/trainingjobs\/xgboost-mnist uid: 5a907178-61d3-11ea-b461-02efd6507006 spec: algorithm specification: training image: 825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest training input mode: file hyper parameters: name: max_depth value: 5 name: eta value: 0.2 name: gamma value: 4 name: min_child_weight value: 6 name: silent value: 0 name: objective value: multi:softmax name: num_class value: 10 name: num_round value: 10 input data config: channel name: train compression type: none content type: text\/csv data source: s 3 data source: s 3 data distribution type: fullyreplicated s 3 data type: s3prefix s 3 uri: s3:\/\/\/xgboost-mnist\/train\/ channel name: validation compression type: none content type: text\/csv data source: s 3 data source: s 3 data distribution type: fullyreplicated s 3 data type: s3prefix s 3 uri: s3:\/\/\/xgboost-mnist\/validation\/ output data config: s 3 output path: s3:\/\/\/xgboost-mnist\/models\/ region: us-east-2 resource config: instance count: 1 instance type: ml.m4.xlarge volume size in gb: 5 role arn: arn:aws:iam:::role\/_execution_role stopping condition: max runtime in seconds: 86400```",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to kick off a sample mnist training job, as the job was not being invoked on the kubectl describe trainingjob.",
        "Issue_preprocessed_content":"Title: unable to kick off the job; Content: deployed the sample mnist training job but seems its not getting invoked on the"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/188",
        "Issue_title":"pip install stepfunctions fails in SageMaker Studio Notebook",
        "Issue_creation_time":1651588352000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### What did you do?\r\n\r\n<!--\r\n-->pip install stepfunctions fails in SageMaker Studio Notebook\r\n\r\nNotebook is using the Python3 (Data Science) kernel.\r\n\r\n\r\n\r\n\r\n### Reproduction Steps\r\n\r\n<!--\r\n--> pip install stepfunctions\r\n\r\n### What did you expect to happen?\r\n\r\n<!--\r\n-->I expected to be able to install AWS stepfunctions.\r\n\r\n### What actually happened?\r\n\r\n<!--\r\n-->\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\r\n  from cryptography.utils import int_from_bytes\r\nCollecting stepfunctions\r\n  Using cached stepfunctions-2.3.0.tar.gz (67 kB)\r\n  Preparing metadata (setup.py) ... error\r\n  error: subprocess-exited-with-error\r\n  \r\n  \u00d7 python setup.py egg_info did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [22 lines of output]\r\n      \/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py:760: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\r\n        % (opt, underscore_opt)\r\n      Traceback (most recent call last):\r\n        File \"<string>\", line 36, in <module>\r\n        File \"<pip-setuptools-caller>\", line 34, in <module>\r\n        File \"\/tmp\/pip-install-a9sl8pu9\/stepfunctions_fec8ededb6d5452993a38c0c5620f20d\/setup.py\", line 70, in <module>\r\n          \"IPython\",\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup\r\n          return distutils.core.setup(**attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup\r\n          _setup_distribution = dist = klass(attrs)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__\r\n          for k, v in attrs.items()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__\r\n          self.finalize_options()\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options\r\n          for ep in sorted(loaded, key=by_order):\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in <lambda>\r\n          loaded = map(lambda e: e.load(), filtered)\r\n        File \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load\r\n          return functools.reduce(getattr, attrs, module)\r\n      AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\r\n      [end of output]\r\n  \r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: metadata-generation-failed\r\n\r\n\u00d7 Encountered error while generating package metadata.\r\n\u2570\u2500> See above for output.\r\n\r\nnote: This is an issue with the package mentioned above, not pip.\r\nhint: See above for details.\r\n\r\n\r\n### Environment\r\n\r\n  - **AWS Step Functions Data Science Python SDK version  : 2.3.0\r\n  - **Python Version:** <!-- Version of Python (run the command `python3 --version`) --> 3.7\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stack-traces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, slack, etc -->\r\n\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: pip install stepfunctions fails in studio notebook; Content: ### what did you do? pip install stepfunctions fails in studio notebook notebook is using the python3 (data science) kernel. ### reproduction steps pip install stepfunctions ### what did you expect to happen? i expected to be able to install aws stepfunctions. ### what actually happened? \/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/dhcrypto.py:16: cryptographydeprecationwarning: int_from_bytes is deprecated, use int.from_bytes instead from cryptography.utils import int_from_bytes \/opt\/conda\/lib\/python3.7\/site-packages\/secretstorage\/util.py:25: cryptographydeprecationwarning: int_from_bytes is deprecated, use int.from_bytes instead from cryptography.utils import int_from_bytes collecting stepfunctions using cached stepfunctions-2.3.0.tar.gz (67 kb) preparing metadata (setup.py) ... error error: subprocess-exited-with-error python setup.py egg_info did not run successfully. exit code: 1 > [22 lines of output] \/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py:760: userwarning: usage of dash-separated 'description-file' will not be supported in future versions. please use the underscore name 'description_file' instead % (opt, underscore_opt) traceback (most recent call last): file \"\", line 36, in file \"\", line 34, in file \"\/tmp\/pip-install-a9sl8pu9\/stepfunctions_fec8ededb6d5452993a38c0c5620f20d\/setup.py\", line 70, in \"ipython\", file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/__init__.py\", line 87, in setup return distutils.core.setup(**attrs) file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/core.py\", line 109, in setup _setup_distribution = dist = klass(attrs) file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 466, in __init__ for k, v in attrs.items() file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_distutils\/dist.py\", line 293, in __init__ self.finalize_options() file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 885, in finalize_options for ep in sorted(loaded, key=by_order): file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/dist.py\", line 884, in loaded = map(lambda e: e.load(), filtered) file \"\/opt\/conda\/lib\/python3.7\/site-packages\/setuptools\/_vendor\/importlib_metadata\/__init__.py\", line 196, in load return functools.reduce(getattr, attrs, module) attributeerror: type object 'distribution' has no attribute '_finalize_feature_opts' [end of output] note: this error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed encountered error while generating package metadata. > see above for output. note: this is an issue with the package mentioned above, not pip. hint: see above for details. ### environment - **aws step functions data science python sdk version : 2.3.0 - **python version:** 3.7 ### other --- this is :bug: bug report",
        "Issue_original_content_gpt_summary":"The user encountered an error while attempting to install the AWS Step Functions Data Science Python SDK in a Studio notebook using the Python3 (Data Science) kernel.",
        "Issue_preprocessed_content":"Title: pip install stepfunctions fails in studio notebook; Content: what did you do? pip install stepfunctions fails in studio notebook notebook is using the python kernel. reproduction steps pip install stepfunctions what did you expect to happen? i expected to be able to install aws stepfunctions. what actually happened? cryptographydeprecationwarning is deprecated, use instead from import cryptographydeprecationwarning is deprecated, use instead from import collecting stepfunctions using cached preparing metadata error error python did not run successfully. exit code userwarning usage of will not be supported in future versions. please use the underscore name instead % traceback file string , line , in file line , in file line , in ipython , file line , in setup return file line , in setup dist klass file line , in for k, v in file line , in file line , in for ep in sorted file line , in loaded map , filtered file line , in load return attrs, module attributeerror type object 'distribution' has no attribute note this error originates from a subprocess, and is likely not a problem with pip. error encountered error while generating package metadata. see above for output. note this is an issue with the package mentioned above, not pip. hint see above for details. environment aws step functions data science python sdk version python version other detailed explanation, related issues, suggestions on how to fix, links for us to have context, eg. associated stackoverflow, slack, etc this is bug bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/aws-step-functions-data-science-sdk-python\/issues\/17",
        "Issue_title":"Support Tensorflow with the new sagemaker.tensorflow.serving.Model",
        "Issue_creation_time":1578691549000,
        "Issue_closed_time":1579559963000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"TrainingPipeline needs to be updated to accommodate the `sagemaker.tensorflow.serving.Model` from Tensorflow package.\r\n\r\nRelated Thread: https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1201",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: support tensorflow with the new .tensorflow.serving.model; Content: trainingpipeline needs to be updated to accommodate the `.tensorflow.serving.model` from tensorflow package. related thread: https:\/\/github.com\/aws\/-python-sdk\/issues\/1201",
        "Issue_original_content_gpt_summary":"The user encountered a challenge in updating the training pipeline to support the new `.tensorflow.serving.model` from the TensorFlow package.",
        "Issue_preprocessed_content":"Title: support tensorflow with the new ; Content: trainingpipeline needs to be updated to accommodate the from tensorflow package. related thread"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/167",
        "Issue_title":"inability to reimage SageMaker Studio Lab instance to get the space back",
        "Issue_creation_time":1668737794000,
        "Issue_closed_time":1668738306000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Hello,\r\n\r\nAfter I tried to build a Conda environment using mlu-tab.yml I was ran out of space with no environment created. After I deleted all files from my home folder I still had 95% of my space used. There is no way to \"reimage\" my Studio Lab instance and get back the initial 30Gb of space.\r\n\r\nI followed the AWS Machine Learning University course and cloned the examples for Tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)]\r\n\r\nAfter that I was stupid enough to try creating the Conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed.\r\nCurrently I have 95% space usage of my \/home\/studio-lab-user folder with no files in it.\r\n\r\nHow can I reimage SageMaker Studio Lab instance to get the space back or uninstall all libraries installed by creating the Conda environment?\r\n\r\nOS: Windows 10\r\nBrowser: Chrome 107.0.5304.107\r\n\r\n![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png)\r\n![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: inability to reimage studio lab instance to get the space back; Content: hello, after i tried to build a conda environment using mlu-tab.yml i was ran out of space with no environment created. after i deleted all files from my home folder i still had 95% of my space used. there is no way to \"reimage\" my studio lab instance and get back the initial 30gb of space. i followed the aws machine learning university course and cloned the examples for tabular data course: [(https:\/\/github.com\/aws-samples\/aws-machine-learning-university-accelerated-tab)] after that i was stupid enough to try creating the conda environment using the mlu-tab.yml file. the environment creation ate all my space available and creation was failed. currently i have 95% space usage of my \/home\/studio-lab-user folder with no files in it. how can i reimage studio lab instance to get the space back or uninstall all libraries installed by creating the conda environment? os: windows 10 browser: chrome 107.0.5304.107 ![space issue1](https:\/\/user-images.githubusercontent.com\/12427856\/202601233-b7378b40-17d6-4ea3-8e8c-c96bebde0010.png) ![space issue2](https:\/\/user-images.githubusercontent.com\/12427856\/202601236-c6fe41d5-0171-4539-8d82-3eaf0577f427.png)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with their AWS Machine Learning University course, where they were unable to reimage their studio lab instance to get back the initial 30GB of space after running out of space while trying to build a conda environment.",
        "Issue_preprocessed_content":"Title: inability to reimage studio lab instance to get the space back; Content: hello, after i tried to build a conda environment using i was ran out of space with no environment created. after i deleted all files from my home folder i still had % of my space used. there is no way to reimage my studio lab instance and get back the initial gb of space. i followed the aws machine learning university course and cloned the examples for tabular data course after that i was stupid enough to try creating the conda environment using the file. the environment creation ate all my space available and creation was failed. currently i have % space usage of my folder with no files in it. how can i reimage studio lab instance to get the space back or uninstall all libraries installed by creating the conda environment? os windows browser chrome"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/166",
        "Issue_title":"Starting 16th Nov 2022 04:00 PM PST, we are experiencing elevated error starting runtimes. The SageMaker Studio Lab team is working to restore the service. We apologize for any inconvenience.",
        "Issue_creation_time":1668650514000,
        "Issue_closed_time":1668731619000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nThe banner message is shown on the top page of Studio Lab.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Studio Lab Top Page\r\n2. The message is shown.\r\n\r\n**Expected behavior**\r\nWe can use the Studio Lab as usual.\r\n\r\nI confirmed the following error.\r\n\r\n* We can start runtime but when clicking \"Open Project\", `ERR_EMPTY_RESPONSE` occurs in the browser.\r\n* When we click the start runtime, \"There was a problem when loading your project. This should be resolved shortly. Please try again later.\" occurred.\r\n\r\n**Screenshots**\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/544269\/202335625-de4d1505-97a7-4748-93d5-f0d6b0f5c597.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: Windows\r\n - Browser Chrome\r\n\r\n**Additional context**\r\n\r\nAs the message suggests, we are working to restore the service. We apologize for any inconvenience.\r\nI'll announce after the service is back. \r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: starting 16th nov 2022 04:00 pm pst, we are experiencing elevated error starting runtimes. the studio lab team is working to restore the service. we apologize for any inconvenience.; Content: **describe the bug** the banner message is shown on the top page of studio lab. **to reproduce** steps to reproduce the behavior: 1. go to studio lab top page 2. the message is shown. **expected behavior** we can use the studio lab as usual. i confirmed the following error. * we can start runtime but when clicking \"open project\", `err_empty_response` occurs in the browser. * when we click the start runtime, \"there was a problem when loading your project. this should be resolved shortly. please try again later.\" occurred. **screenshots** ![image](https:\/\/user-images.githubusercontent.com\/544269\/202335625-de4d1505-97a7-4748-93d5-f0d6b0f5c597.png) **desktop (please complete the following information):** - os: windows - browser chrome **additional context** as the message suggests, we are working to restore the service. we apologize for any inconvenience. i'll announce after the service is back.",
        "Issue_original_content_gpt_summary":"The user is experiencing elevated error starting runtimes on the Studio Lab top page, resulting in `err_empty_response` and \"there was a problem when loading your project\" messages.",
        "Issue_preprocessed_content":"Title: starting th nov pm pst, we are experiencing elevated error starting runtimes. the studio lab team is working to restore the service. we apologize for any inconvenience.; Content: describe the bug the banner message is shown on the top page of studio lab. to reproduce steps to reproduce the behavior . go to studio lab top page . the message is shown. expected behavior we can use the studio lab as usual. i confirmed the following error. we can start runtime but when clicking open project , occurs in the browser. when we click the start runtime, there was a problem when loading your project. this should be resolved shortly. please try again occurred. screenshots desktop os windows browser chrome additional context as the message suggests, we are working to restore the service. we apologize for any inconvenience. i'll announce after the service is back."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/155",
        "Issue_title":"we are experiencing elevated fault rate in start runtime API. The SageMaker Studio Lab team is working to restore the service.",
        "Issue_creation_time":1667438077000,
        "Issue_closed_time":1667627477000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in sagemaker\r\nhow long is this going to even take man",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: we are experiencing elevated fault rate in start runtime api. the studio lab team is working to restore the service.; Content: its been more than 3 days and im still getting this issue, i cant run cpu or even gpu runtimes in how long is this going to even take man",
        "Issue_original_content_gpt_summary":"The user is experiencing an elevated fault rate in the Start Runtime API, and has been unable to run CPU or GPU runtimes for more than three days, raising questions about how long the issue will take to resolve.",
        "Issue_preprocessed_content":"Title: we are experiencing elevated fault rate in start runtime api. the studio lab team is working to restore the service.; Content: its been more than days and im still getting this issue, i cant run cpu or even gpu runtimes in how long is this going to even take man"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/118",
        "Issue_title":"How to get root access in SageMaker Studio Lab",
        "Issue_creation_time":1654778335000,
        "Issue_closed_time":1655933766000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Hi, I am trying to install some libraries in Studio Lab which requires root privileges. \r\n\r\nBelow I have run `whoami` to check if I am root user. (I am not as it should print 'root' in case of root user)\r\n![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png)\r\n\r\nBelow you can see the error on running sudo: ->  `bash: sudo: command not found`\r\n![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png)\r\n\r\nI followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. \r\nOn running `su -` , It asks for the password, but we don't have any password for Studio Lab. \r\n![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png)\r\n\r\nCan anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). \r\nPlease let me know if my query is not clear. ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: how to get root access in studio lab; Content: hi, i am trying to install some libraries in studio lab which requires root privileges. below i have run `whoami` to check if i am root user. (i am not as it should print 'root' in case of root user) ![whoami_image](https:\/\/user-images.githubusercontent.com\/91401599\/172846069-ae664262-ae25-4cf0-9a60-ed5bf657029f.png) below you can see the error on running sudo: -> `bash: sudo: command not found` ![sudo_cmd](https:\/\/user-images.githubusercontent.com\/91401599\/172847142-57fb5a9f-720b-41af-989a-93740c29805c.png) i followed [this ](https:\/\/stackoverflow.com\/questions\/44443228\/sudo-command-not-found-when-i-ssh-into-server)link to install sudo. on running `su -` , it asks for the password, but we don't have any password for studio lab. ![password](https:\/\/user-images.githubusercontent.com\/91401599\/172847894-34da1cd8-f59c-4f65-9500-c870b50095c6.png) can anyone tell how to get root access or a way to install libraries which require root access\/(or packages which installs using sudo). please let me know if my query is not clear.",
        "Issue_original_content_gpt_summary":"The user is encountering challenges in trying to gain root access in studio lab in order to install libraries that require root privileges.",
        "Issue_preprocessed_content":"Title: how to get root access in studio lab; Content: hi, i am trying to install some libraries in studio lab which requires root privileges. below i have run to check if i am root user. below you can see the error on running sudo i followed link to install sudo. on running , it asks for the password, but we don't have any password for studio lab. can anyone tell how to get root access or a way to install libraries which require root packages which installs using sudo . please let me know if my query is not clear."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/94",
        "Issue_title":"Unable to open database file, Unexpected error while saving file: d2l-pytorch-sagemaker-studio-lab\/dash\/Untitled.ipynb unable to open database file",
        "Issue_creation_time":1647978773000,
        "Issue_closed_time":1655626980000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"**Describe the bug**\r\n![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png)\r\n\r\n**To Reproduce**\r\nI've deleted some of the unwanted notebooks from studio lab's files and now I am getting this error. \r\ncannot install libraries with pip, cannot create new files, cannot even start kernel ",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to open database file, unexpected error while saving file: d2l-pytorch--studio-lab\/dash\/untitled.ipynb unable to open database file; Content: **describe the bug** ![image](https:\/\/user-images.githubusercontent.com\/42097653\/159563812-a9471c23-ad6a-4354-9e30-ef001df04352.png) **to reproduce** i've deleted some of the unwanted notebooks from studio lab's files and now i am getting this error. cannot install libraries with pip, cannot create new files, cannot even start kernel",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected error while saving a file in d2l-pytorch--studio-lab\/dash\/untitled.ipynb, unable to open database file, and was unable to install libraries with pip, create new files, or start the kernel.",
        "Issue_preprocessed_content":"Title: unable to open database file, unexpected error while saving file unable to open database file; Content: describe the bug to reproduce i've deleted some of the unwanted notebooks from studio lab's files and now i am getting this error. cannot install libraries with pip, cannot create new files, cannot even start kernel"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/72",
        "Issue_title":"Can't open project on amazon sagemaker",
        "Issue_creation_time":1645584371000,
        "Issue_closed_time":1668499115000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Hello, I can't open my project on amazon sagemaker. When I am clicking the 'open project' button, it is loading indefinitely, and I can't do anything with the files. I have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from GPU to CPU but nothing did work. Can you please take a look into my account and resolve the issue? A screenshot is attached here to understand better. Thanks!\r\n<img width=\"1363\" alt=\"Screen Shot 2022-02-22 at 9 45 35 PM\" src=\"https:\/\/user-images.githubusercontent.com\/12325889\/155253679-bc27e42d-0a34-4e8d-8a08-7c1ad5fde9a8.png\">\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: can't open project on ; Content: hello, i can't open my project on . when i am clicking the 'open project' button, it is loading indefinitely, and i can't do anything with the files. i have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from gpu to cpu but nothing did work. can you please take a look into my account and resolve the issue? a screenshot is attached here to understand better. thanks!",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with opening a project on , where the 'open project' button is loading indefinitely and none of the attempted solutions have worked.",
        "Issue_preprocessed_content":"Title: can't open project on ; Content: hello, i can't open my project on . when i am clicking the 'open project' button, it is loading indefinitely, and i can't do anything with the files. i have restarted my project, browser, laptop, cleared cache, tried from other browsers, changed the env from gpu to cpu but nothing did work. can you please take a look into my account and resolve the issue? a screenshot is attached here to understand better. thanks! img width alt screen shot at pm"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/54",
        "Issue_title":"[BUG] \"Open In in Sagemaker Studio Lab\" button process fails when attempting to \"Copy Notebooks Only\"",
        "Issue_creation_time":1643305784000,
        "Issue_closed_time":1643319424000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nCloning a single notebook using the \"Open In in Sagemaker Studio Lab\" fails.  Cloning the whole repo works.  \r\n\r\nUsing sagemaker's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, I get this error:\r\n```\r\nUnable to copy notebook to project.\r\nThe link to this notebook is broken or blocked. If this is a private GitHub notebook, sign in to GitHub before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb\r\n```\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create MD cell with `[![Open in SageMaker Studio Lab](https:\/\/studiolab.sagemaker.aws\/studiolab.svg)](https:\/\/studiolab.sagemaker.aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/NLP_Disaster_Recovery_Translation.ipynb)`  and run it\r\n2. Click on the button that appears once you run the cell.  Will open new tab in browser\r\n3. In the new pop up tab, click \"Copy to Project\".  Will open new tab in browser\r\n4. In the new pop up tab's modal, select \"Copy Notebook Only\"\r\n5. Error will now appear\r\n\r\n**Expected behavior**\r\nMy notebook will open and appear, just as it would with cloning a directory\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png)\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [Windows 11]\r\n - Browser [Chrome]\r\n - Version [97.0.4692.71]\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] \"open in in studio lab\" button process fails when attempting to \"copy notebooks only\"; Content: **describe the bug** cloning a single notebook using the \"open in in studio lab\" fails. cloning the whole repo works. using 's sample, https:\/\/github.com\/aws\/studio-lab-examples\/tree\/main\/open-in-studio-lab, i get this error: ``` unable to copy notebook to project. the link to this notebook is broken or blocked. if this is a private github notebook, sign in to github before copying the notebook.aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/nlp_disaster_recovery_translation.ipynb ``` **to reproduce** steps to reproduce the behavior: 1. create md cell with `[![open in studio lab](https:\/\/studiolab..aws\/studiolab.svg)](https:\/\/studiolab..aws\/import\/github\/aws\/studio-lab-examples\/blob\/main\/natural-language-processing\/nlp_disaster_recovery_translation.ipynb)` and run it 2. click on the button that appears once you run the cell. will open new tab in browser 3. in the new pop up tab, click \"copy to project\". will open new tab in browser 4. in the new pop up tab's modal, select \"copy notebook only\" 5. error will now appear **expected behavior** my notebook will open and appear, just as it would with cloning a directory **screenshots** ![image](https:\/\/user-images.githubusercontent.com\/46935140\/151415892-7d033f97-f98c-4ac8-9c50-99223253b1ee.png) **desktop (please complete the following information):** - os: [windows 11] - browser [chrome] - version [97.0.4692.71]",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to clone a single notebook using the \"open in in studio lab\" button, where the process fails and an error message appears instead of the expected behavior.",
        "Issue_preprocessed_content":"Title: open in in studio lab button process fails when attempting to copy notebooks only ; Content: describe the bug cloning a single notebook using the open in in studio lab fails. cloning the whole repo works. using 's sample, i get this error to reproduce steps to reproduce the behavior . create md cell with and run it . click on the button that appears once you run the cell. will open new tab in browser . in the new pop up tab, click copy to project . will open new tab in browser . in the new pop up tab's modal, select copy notebook only . error will now appear expected behavior my notebook will open and appear, just as it would with cloning a directory screenshots desktop os browser version"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/52",
        "Issue_title":"couldn't open Project on amazon sagemaker studio lab",
        "Issue_creation_time":1642548428000,
        "Issue_closed_time":1643050102000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"**Describe the bug**\r\nAmazon Sagemaker studio lab is not opening jupyter notebook. It is loading indefinitely at Preparing project run time after that i am getting `There was a problem when starting the project runtime. This should be resolved shortly.` Please try again later. It's been almost a week and it still hasn't been resolved. Even though I tried shifting the runtime from CPU to GPU but issue still persists. Any help would be appreciated.\r\n\r\n**The error i am getting is**\r\n![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: couldn't open project on studio lab; Content: **describe the bug** studio lab is not opening jupyter notebook. it is loading indefinitely at preparing project run time after that i am getting `there was a problem when starting the project runtime. this should be resolved shortly.` please try again later. it's been almost a week and it still hasn't been resolved. even though i tried shifting the runtime from cpu to gpu but issue still persists. any help would be appreciated. **the error i am getting is** ![image](https:\/\/user-images.githubusercontent.com\/81302966\/150034710-378eabbc-13fa-4820-adea-f2ebc8d66431.png)",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with studio lab not opening jupyter notebook, and is receiving an error message after attempting to shift the runtime from CPU to GPU.",
        "Issue_preprocessed_content":"Title: couldn't open project on studio lab; Content: describe the bug studio lab is not opening jupyter notebook. it is loading indefinitely at preparing project run time after that i am getting please try again later. it's been almost a week and it still hasn't been resolved. even though i tried shifting the runtime from cpu to gpu but issue still persists. any help would be appreciated. the error i am getting is"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/38",
        "Issue_title":"I just wonder if i can initialize my sagemaker studio lab? ",
        "Issue_creation_time":1641220236000,
        "Issue_closed_time":1641229646000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"as the title suggests\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: i just wonder if i can initialize my studio lab? ; Content: as the title suggests",
        "Issue_original_content_gpt_summary":"The user is wondering if they can initialize their studio lab.",
        "Issue_preprocessed_content":"Title: i just wonder if i can initialize my studio lab? ; Content: as the title suggests"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/studio-lab-examples\/issues\/30",
        "Issue_title":"Can't configure profile with AWS CLI for using AWS Built-in sagemaker algorithms ",
        "Issue_creation_time":1639662702000,
        "Issue_closed_time":1639666969000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"Hi everybody,\r\n\r\nI am trying to use AWS built-in algorithms in Sagemaker Studio Lab. For that I need an execution role and region etc. \r\nWhen I try to run my code it outputs\r\n\r\nValueError: Must setup local AWS configuration with a region supported by SageMaker.\r\n\r\nIs it even possible to link access AWS resources in Studiolab?\r\n\r\nMany thanks in advance!\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: can't configure profile with aws cli for using aws built-in algorithms ; Content: hi everybody, i am trying to use aws built-in algorithms in studio lab. for that i need an execution role and region etc. when i try to run my code it outputs valueerror: must setup local aws configuration with a region supported by . is it even possible to link access aws resources in studiolab? many thanks in advance!",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge in configuring their profile with AWS CLI for using AWS built-in algorithms in Studio Lab.",
        "Issue_preprocessed_content":"Title: can't configure profile with aws cli for using aws algorithms ; Content: hi everybody, i am trying to use aws algorithms in studio lab. for that i need an execution role and region etc. when i try to run my code it outputs valueerror must setup local aws configuration with a region supported by . is it even possible to link access aws resources in studiolab? many thanks in advance!"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/issues\/37",
        "Issue_title":"Pytorch Sagemaker Container STDERR output",
        "Issue_creation_time":1571735353000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"In Pytorch images all the prints in stderr are not catched and are ignored:\r\n\r\n\r\n### Describe the problem\r\n\r\n### Minimal repro \/ logs\r\nEntrypoint.py:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    import sys\r\n    sys.stderr.write('Coucou stderr')\r\n    sys.stdout.write('Coucou stdout')\r\n```\r\n\r\n```\r\nfrom sagemaker.pytorch import PyTorch\r\nestimator = PyTorch(entry_point='entrypoint.py',\r\n                    role=role,\r\n                    framework_version='1.1.0',\r\n                    train_instance_count=1,\r\n                    train_instance_type='local',\r\n                )\r\nestimator.fit({'config': 's3:\/\/sagemaker-eu-*************\/config\/test_sagemaker_1.json'})\r\n```\r\n\r\n<details><summary>LOGS<\/summary>\r\n<p>\r\n\r\nCreating tmpqp7i_4w3_algo-1-8gd7b_1 ... \r\nAttaching to tmpqp7i_4w3_algo-1-8gd7b_12mdone\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,345 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,349 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,363 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,365 sagemaker_pytorch_container.training INFO     Invoking user training script.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Module entrypoint does not provide a setup.py. \r\nalgo-1-8gd7b_1  | Generating setup.py\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating setup.cfg\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,489 sagemaker-containers INFO     Generating MANIFEST.in\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:21,490 sagemaker-containers INFO     Installing module with the following command:\r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m pip install . \r\nalgo-1-8gd7b_1  | Processing \/opt\/ml\/code\r\nalgo-1-8gd7b_1  | Building wheels for collected packages: entrypoint\r\nalgo-1-8gd7b_1  |   Running setup.py bdist_wheel for entrypoint ... done\r\nalgo-1-8gd7b_1  |   Stored in directory: \/tmp\/pip-ephem-wheel-cache-44kbrxy0\/wheels\/35\/24\/16\/37574d11bf9bde50616c******356bc7164af8ca3\r\nalgo-1-8gd7b_1  | Successfully built entrypoint\r\nalgo-1-8gd7b_1  | Installing collected packages: entrypoint\r\nalgo-1-8gd7b_1  | Successfully installed entrypoint-1.0.0\r\nalgo-1-8gd7b_1  | You are using pip version 18.1, however version 19.3.1 is available.\r\nalgo-1-8gd7b_1  | You should consider upgrading via the 'pip install --upgrade pip' command.\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,054 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\nalgo-1-8gd7b_1  | 2019-10-22 09:06:23,069 sagemaker-containers INFO     Invoking user script\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Training Env:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | {\r\nalgo-1-8gd7b_1  |     \"additional_framework_parameters\": {},\r\nalgo-1-8gd7b_1  |     \"channel_input_dirs\": {\r\nalgo-1-8gd7b_1  |         \"config\": \"\/opt\/ml\/input\/data\/config\"\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\r\nalgo-1-8gd7b_1  |     \"hosts\": [\r\nalgo-1-8gd7b_1  |         \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |     ],\r\nalgo-1-8gd7b_1  |     \"hyperparameters\": {},\r\nalgo-1-8gd7b_1  |     \"input_config_dir\": \"\/opt\/ml\/input\/config\",\r\nalgo-1-8gd7b_1  |     \"input_data_config\": {\r\nalgo-1-8gd7b_1  |         \"config\": {\r\nalgo-1-8gd7b_1  |             \"TrainingInputMode\": \"File\"\r\nalgo-1-8gd7b_1  |         }\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"input_dir\": \"\/opt\/ml\/input\",\r\nalgo-1-8gd7b_1  |     \"is_master\": true,\r\nalgo-1-8gd7b_1  |     \"job_name\": \"sagemaker-pytorch-2019-10-22-09-06-18-353\",\r\nalgo-1-8gd7b_1  |     \"log_level\": 20,\r\nalgo-1-8gd7b_1  |     \"master_hostname\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |     \"model_dir\": \"\/opt\/ml\/model\",\r\nalgo-1-8gd7b_1  |     \"module_dir\": \"s3:\/\/sagemaker-eu-west-1-*********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\r\nalgo-1-8gd7b_1  |     \"module_name\": \"entrypoint\",\r\nalgo-1-8gd7b_1  |     \"network_interface_name\": \"eth0\",\r\nalgo-1-8gd7b_1  |     \"num_cpus\": 2,\r\nalgo-1-8gd7b_1  |     \"num_gpus\": 0,\r\nalgo-1-8gd7b_1  |     \"output_data_dir\": \"\/opt\/ml\/output\/data\",\r\nalgo-1-8gd7b_1  |     \"output_dir\": \"\/opt\/ml\/output\",\r\nalgo-1-8gd7b_1  |     \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\",\r\nalgo-1-8gd7b_1  |     \"resource_config\": {\r\nalgo-1-8gd7b_1  |         \"current_host\": \"algo-1-8gd7b\",\r\nalgo-1-8gd7b_1  |         \"hosts\": [\r\nalgo-1-8gd7b_1  |             \"algo-1-8gd7b\"\r\nalgo-1-8gd7b_1  |         ]\r\nalgo-1-8gd7b_1  |     },\r\nalgo-1-8gd7b_1  |     \"user_entry_point\": \"entrypoint.py\"\r\nalgo-1-8gd7b_1  | }\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Environment variables:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | SM_HOSTS=[\"algo-1-8gd7b\"]\r\nalgo-1-8gd7b_1  | SM_NETWORK_INTERFACE_NAME=eth0\r\nalgo-1-8gd7b_1  | SM_HPS={}\r\nalgo-1-8gd7b_1  | SM_USER_ENTRY_POINT=entrypoint.py\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_PARAMS={}\r\nalgo-1-8gd7b_1  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]}\r\nalgo-1-8gd7b_1  | SM_INPUT_DATA_CONFIG={\"config\":{\"TrainingInputMode\":\"File\"}}\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\r\nalgo-1-8gd7b_1  | SM_CHANNELS=[\"config\"]\r\nalgo-1-8gd7b_1  | SM_CURRENT_HOST=algo-1-8gd7b\r\nalgo-1-8gd7b_1  | SM_MODULE_NAME=entrypoint\r\nalgo-1-8gd7b_1  | SM_LOG_LEVEL=20\r\nalgo-1-8gd7b_1  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\r\nalgo-1-8gd7b_1  | SM_INPUT_DIR=\/opt\/ml\/input\r\nalgo-1-8gd7b_1  | SM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\r\nalgo-1-8gd7b_1  | SM_OUTPUT_DIR=\/opt\/ml\/output\r\nalgo-1-8gd7b_1  | SM_NUM_CPUS=2\r\nalgo-1-8gd7b_1  | SM_NUM_GPUS=0\r\nalgo-1-8gd7b_1  | SM_MODEL_DIR=\/opt\/ml\/model\r\nalgo-1-8gd7b_1  | SM_MODULE_DIR=s3:\/\/sagemaker-eu-west-1-***********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\r\nalgo-1-8gd7b_1  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"\/opt\/ml\/input\/data\/config\"},\"current_host\":\"algo-1-8gd7b\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-8gd7b\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"config\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-10-22-09-06-18-353\",\"log_level\":20,\"master_hostname\":\"algo-1-8gd7b\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/sagemaker-eu-west-1-**********\/sagemaker-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]},\"user_entry_point\":\"entrypoint.py\"}\r\nalgo-1-8gd7b_1  | SM_USER_ARGS=[]\r\nalgo-1-8gd7b_1  | SM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\r\nalgo-1-8gd7b_1  | SM_CHANNEL_CONFIG=\/opt\/ml\/input\/data\/config\r\nalgo-1-8gd7b_1  | PYTHONPATH=\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Invoking script with the following command:\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \/usr\/bin\/python -m entrypoint\r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | \r\nalgo-1-8gd7b_1  | Coucou stdout2019-10-22 09:06:23,102 sagemaker-containers INFO     Reporting training SUCCESS\r\ntmpqp7i_4w3_algo-1-8gd7b_1 exited with code 0\r\nAborting on container exit...\r\n===== Job Complete =====\r\n<\/p>\r\n<\/details>\r\n\r\nAs you see the coucou stdout has been printed, stderr has been ignored. In distant mode same result.\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: pytorch container stderr output; Content: in pytorch images all the prints in stderr are not catched and are ignored: ### describe the problem ### minimal repro \/ logs entrypoint.py: ``` if __name__ == '__main__': import sys sys.stderr.write('coucou stderr') sys.stdout.write('coucou stdout') ``` ``` from .pytorch import pytorch estimator = pytorch(entry_point='entrypoint.py', role=role, framework_version='1.1.0', train_instance_count=1, train_instance_type='local', ) estimator.fit({'config': 's3:\/\/-eu-*************\/config\/test__1.json'}) ``` logs creating tmpqp7i_4w3_algo-1-8gd7b_1 ... attaching to tmpqp7i_4w3_algo-1-8gd7b_12mdone algo-1-8gd7b_1 | 2019-10-22 09:06:21,345 -containers info imported framework _pytorch_container.training algo-1-8gd7b_1 | 2019-10-22 09:06:21,349 -containers info no gpus detected (normal if no gpus installed) algo-1-8gd7b_1 | 2019-10-22 09:06:21,363 _pytorch_container.training info block until all host dns lookups succeed. algo-1-8gd7b_1 | 2019-10-22 09:06:21,365 _pytorch_container.training info invoking user training script. algo-1-8gd7b_1 | 2019-10-22 09:06:21,489 -containers info module entrypoint does not provide a setup.py. algo-1-8gd7b_1 | generating setup.py algo-1-8gd7b_1 | 2019-10-22 09:06:21,489 -containers info generating setup.cfg algo-1-8gd7b_1 | 2019-10-22 09:06:21,489 -containers info generating manifest.in algo-1-8gd7b_1 | 2019-10-22 09:06:21,490 -containers info installing module with the following command: algo-1-8gd7b_1 | \/usr\/bin\/python -m pip install . algo-1-8gd7b_1 | processing \/opt\/ml\/code algo-1-8gd7b_1 | building wheels for collected packages: entrypoint algo-1-8gd7b_1 | running setup.py bdist_wheel for entrypoint ... done algo-1-8gd7b_1 | stored in directory: \/tmp\/pip-ephem-wheel-cache-44kbrxy0\/wheels\/35\/24\/16\/37574d11bf9bde50616c******356bc7164af8ca3 algo-1-8gd7b_1 | successfully built entrypoint algo-1-8gd7b_1 | installing collected packages: entrypoint algo-1-8gd7b_1 | successfully installed entrypoint-1.0.0 algo-1-8gd7b_1 | you are using pip version 18.1, however version 19.3.1 is available. algo-1-8gd7b_1 | you should consider upgrading via the 'pip install --upgrade pip' command. algo-1-8gd7b_1 | 2019-10-22 09:06:23,054 -containers info no gpus detected (normal if no gpus installed) algo-1-8gd7b_1 | 2019-10-22 09:06:23,069 -containers info invoking user script algo-1-8gd7b_1 | algo-1-8gd7b_1 | training env: algo-1-8gd7b_1 | algo-1-8gd7b_1 | { algo-1-8gd7b_1 | \"additional_framework_parameters\": {}, algo-1-8gd7b_1 | \"channel_input_dirs\": { algo-1-8gd7b_1 | \"config\": \"\/opt\/ml\/input\/data\/config\" algo-1-8gd7b_1 | }, algo-1-8gd7b_1 | \"current_host\": \"algo-1-8gd7b\", algo-1-8gd7b_1 | \"framework_module\": \"_pytorch_container.training:main\", algo-1-8gd7b_1 | \"hosts\": [ algo-1-8gd7b_1 | \"algo-1-8gd7b\" algo-1-8gd7b_1 | ], algo-1-8gd7b_1 | \"hyperparameters\": {}, algo-1-8gd7b_1 | \"input_config_dir\": \"\/opt\/ml\/input\/config\", algo-1-8gd7b_1 | \"input_data_config\": { algo-1-8gd7b_1 | \"config\": { algo-1-8gd7b_1 | \"traininginputmode\": \"file\" algo-1-8gd7b_1 | } algo-1-8gd7b_1 | }, algo-1-8gd7b_1 | \"input_dir\": \"\/opt\/ml\/input\", algo-1-8gd7b_1 | \"is_master\": true, algo-1-8gd7b_1 | \"job_name\": \"-pytorch-2019-10-22-09-06-18-353\", algo-1-8gd7b_1 | \"log_level\": 20, algo-1-8gd7b_1 | \"master_hostname\": \"algo-1-8gd7b\", algo-1-8gd7b_1 | \"model_dir\": \"\/opt\/ml\/model\", algo-1-8gd7b_1 | \"module_dir\": \"s3:\/\/-eu-west-1-*********\/-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\", algo-1-8gd7b_1 | \"module_name\": \"entrypoint\", algo-1-8gd7b_1 | \"network_interface_name\": \"eth0\", algo-1-8gd7b_1 | \"num_cpus\": 2, algo-1-8gd7b_1 | \"num_gpus\": 0, algo-1-8gd7b_1 | \"output_data_dir\": \"\/opt\/ml\/output\/data\", algo-1-8gd7b_1 | \"output_dir\": \"\/opt\/ml\/output\", algo-1-8gd7b_1 | \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\", algo-1-8gd7b_1 | \"resource_config\": { algo-1-8gd7b_1 | \"current_host\": \"algo-1-8gd7b\", algo-1-8gd7b_1 | \"hosts\": [ algo-1-8gd7b_1 | \"algo-1-8gd7b\" algo-1-8gd7b_1 | ] algo-1-8gd7b_1 | }, algo-1-8gd7b_1 | \"user_entry_point\": \"entrypoint.py\" algo-1-8gd7b_1 | } algo-1-8gd7b_1 | algo-1-8gd7b_1 | environment variables: algo-1-8gd7b_1 | algo-1-8gd7b_1 | sm_hosts=[\"algo-1-8gd7b\"] algo-1-8gd7b_1 | sm_network_interface_name=eth0 algo-1-8gd7b_1 | sm_hps={} algo-1-8gd7b_1 | sm_user_entry_point=entrypoint.py algo-1-8gd7b_1 | sm_framework_params={} algo-1-8gd7b_1 | sm_resource_config={\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]} algo-1-8gd7b_1 | sm_input_data_config={\"config\":{\"traininginputmode\":\"file\"}} algo-1-8gd7b_1 | sm_output_data_dir=\/opt\/ml\/output\/data algo-1-8gd7b_1 | sm_channels=[\"config\"] algo-1-8gd7b_1 | sm_current_host=algo-1-8gd7b algo-1-8gd7b_1 | sm_module_name=entrypoint algo-1-8gd7b_1 | sm_log_level=20 algo-1-8gd7b_1 | sm_framework_module=_pytorch_container.training:main algo-1-8gd7b_1 | sm_input_dir=\/opt\/ml\/input algo-1-8gd7b_1 | sm_input_config_dir=\/opt\/ml\/input\/config algo-1-8gd7b_1 | sm_output_dir=\/opt\/ml\/output algo-1-8gd7b_1 | sm_num_cpus=2 algo-1-8gd7b_1 | sm_num_gpus=0 algo-1-8gd7b_1 | sm_model_dir=\/opt\/ml\/model algo-1-8gd7b_1 | sm_module_dir=s3:\/\/-eu-west-1-***********\/-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz algo-1-8gd7b_1 | sm_training_env={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"config\":\"\/opt\/ml\/input\/data\/config\"},\"current_host\":\"algo-1-8gd7b\",\"framework_module\":\"_pytorch_container.training:main\",\"hosts\":[\"algo-1-8gd7b\"],\"hyperparameters\":{},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"config\":{\"traininginputmode\":\"file\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"-pytorch-2019-10-22-09-06-18-353\",\"log_level\":20,\"master_hostname\":\"algo-1-8gd7b\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/-eu-west-1-**********\/-pytorch-2019-10-22-09-06-18-353\/source\/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-8gd7b\",\"hosts\":[\"algo-1-8gd7b\"]},\"user_entry_point\":\"entrypoint.py\"} algo-1-8gd7b_1 | sm_user_args=[] algo-1-8gd7b_1 | sm_output_intermediate_dir=\/opt\/ml\/output\/intermediate algo-1-8gd7b_1 | sm_channel_config=\/opt\/ml\/input\/data\/config algo-1-8gd7b_1 | pythonpath=\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages algo-1-8gd7b_1 | algo-1-8gd7b_1 | invoking script with the following command: algo-1-8gd7b_1 | algo-1-8gd7b_1 | \/usr\/bin\/python -m entrypoint algo-1-8gd7b_1 | algo-1-8gd7b_1 | algo-1-8gd7b_1 | coucou stdout2019-10-22 09:06:23,102 -containers info reporting training success tmpqp7i_4w3_algo-1-8gd7b_1 exited with code 0 aborting on container exit... ===== job complete ===== as you see the coucou stdout has been printed, stderr has been ignored. in distant mode same result.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where prints in stderr were not caught and ignored when using pytorch images.",
        "Issue_preprocessed_content":"Title: pytorch container stderr output; Content: in pytorch images all the prints in stderr are not catched and are ignored describe the problem minimal repro \/ logs p creating attaching to , info imported framework , info no gpus detected , info block until all host dns lookups succeed. , info invoking user training script. , info module entrypoint does not provide a generating , info generating , info generating , info installing module with the following command m pip install . processing building wheels for collected packages entrypoint running for entrypoint done stored in directory successfully built entrypoint installing collected packages entrypoint successfully installed you are using pip version however version is available. you should consider upgrading via the 'pip install pip' command. , info no gpus detected , info invoking user script training env , , hosts , hyperparameters , , true, , entrypoint , eth , , , , environment variables invoking script with the following command m entrypoint coucou , info reporting training success exited with code aborting on container job complete as you see the coucou stdout has been printed, stderr has been ignored. in distant mode same result."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/88",
        "Issue_title":"renaming of mxnet-model-server in sagemaker-inference package 1.5.3 causing entrypoint with command `serve` to fail",
        "Issue_creation_time":1607044885000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n`sagemaker-inference` recently (10\/15) released v1.5.3, which included [this commit](https:\/\/github.com\/aws\/sagemaker-inference-toolkit\/commit\/8efb1672798d747cd623e5dd2eb7919af87a1b80) updating the name of the model server artifact and command from `mxnet-model-server` to `multi-model-server`.\r\n\r\nall containers defined in this repository install `sagemaker-inference` as a dependency of this repo itself, on lines\r\n\r\n```dockerfile\r\nRUN pip install --no-cache-dir \"sagemaker-pytorch-inference<2\"\r\n```\r\n\r\nand this repo's `setup.py` has an `install_requires` which includes `sagemaker-inference>=1.3.1`. as a result, `sagemaker-inference=1.5.3` installed.\r\n\r\nso while the `Dockerfile`'s `CMD` value (which calls `mxnet-model-server` directly) will succeed, attempts to use the `ENTRYPOINT` with `serve` as a build arg will fail with message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 22, in <module>\r\n    serving.main()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 39, in main\r\n    _start_model_server()\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 49, in wrapped_f\r\n    return Retrying(*dargs, **dkw).call(f, *args, **kw)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 206, in call\r\n    return attempt.get(self._wrap_exception)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 247, in get\r\n    six.reraise(self.value[0], self.value[1], self.value[2])\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 200, in call\r\n    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_pytorch_serving_container\/serving.py\", line 35, in _start_model_server\r\n    model_server.start_model_server(handler_service=HANDLER_SERVICE)\r\n  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/model_server.py\", line 94, in start_model_server\r\n    subprocess.Popen(multi_model_server_cmd)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 709, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 1344, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'multi-model-server': 'multi-model-server'\r\n\r\n```\r\n\r\n**To reproduce**\r\n1. build any container\r\n1. mount a model and `inference.py` (e.g. `half_plus_three`) into `\/opt\/ml\/model`\r\n1. `docker run [tag name] serve`\r\n\r\n**Expected behavior**\r\ntensorflow serving serves the mounted model \/ `inference.py`\r\n\r\n**System information**\r\nA description of your system. Please provide:\r\n- **Toolkit version**: 2.0.5, but should apply to all versions\r\n- **Framework version**: 1.4, but should apply to all versions\r\n- **Python version**: 3.7\r\n- **CPU or GPU**: cpu, but should apply to both\r\n- **Custom Docker image (Y\/N)**: N",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: renaming of mxnet-model-server in -inference package 1.5.3 causing entrypoint with command `serve` to fail; Content: **describe the bug** `-inference` recently (10\/15) released v1.5.3, which included [this commit](https:\/\/github.com\/aws\/-inference-toolkit\/commit\/8efb1672798d747cd623e5dd2eb7919af87a1b80) updating the name of the model server artifact and command from `mxnet-model-server` to `multi-model-server`. all containers defined in this repository install `-inference` as a dependency of this repo itself, on lines ```dockerfile run pip install --no-cache-dir \"-pytorch-inference<2\" ``` and this repo's `setup.py` has an `install_requires` which includes `-inference>=1.3.1`. as a result, `-inference=1.5.3` installed. so while the `dockerfile`'s `cmd` value (which calls `mxnet-model-server` directly) will succeed, attempts to use the `entrypoint` with `serve` as a build arg will fail with message: ``` traceback (most recent call last): file \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 22, in serving.main() file \"\/opt\/conda\/lib\/python3.6\/site-packages\/_pytorch_serving_container\/serving.py\", line 39, in main _start_model_server() file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 49, in wrapped_f return retrying(*dargs, **dkw).call(f, *args, **kw) file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) file \"\/opt\/conda\/lib\/python3.6\/site-packages\/six.py\", line 703, in reraise raise value file \"\/opt\/conda\/lib\/python3.6\/site-packages\/retrying.py\", line 200, in call attempt = attempt(fn(*args, **kwargs), attempt_number, false) file \"\/opt\/conda\/lib\/python3.6\/site-packages\/_pytorch_serving_container\/serving.py\", line 35, in _start_model_server model_server.start_model_server(handler_service=handler_service) file \"\/opt\/conda\/lib\/python3.6\/site-packages\/_inference\/model_server.py\", line 94, in start_model_server subprocess.popen(multi_model_server_cmd) file \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 709, in __init__ restore_signals, start_new_session) file \"\/opt\/conda\/lib\/python3.6\/subprocess.py\", line 1344, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename) filenotfounderror: [errno 2] no such file or directory: 'multi-model-server': 'multi-model-server' ``` **to reproduce** 1. build any container 1. mount a model and `inference.py` (e.g. `half_plus_three`) into `\/opt\/ml\/model` 1. `docker run [tag name] serve` **expected behavior** tensorflow serving serves the mounted model \/ `inference.py` **system information** a description of your system. please provide: - **toolkit version**: 2.0.5, but should apply to all versions - **framework version**: 1.4, but should apply to all versions - **python version**: 3.7 - **cpu or gpu**: cpu, but should apply to both - **custom docker image (y\/n)**: n",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with renaming of mxnet-model-server in -inference package 1.5.3 causing entrypoint with command `serve` to fail.",
        "Issue_preprocessed_content":"Title: renaming of in package causing entrypoint with command to fail; Content: describe the bug recently released which included updating the name of the model server artifact and command from to . all containers defined in this repository install as a dependency of this repo itself, on lines and this repo's has an which includes . as a result, installed. so while the 's value will succeed, attempts to use the with as a build arg will fail with message to reproduce . build any container . mount a model and into . expected behavior tensorflow serving serves the mounted model \/ system information a description of your system. please provide toolkit version but should apply to all versions framework version but should apply to all versions python version cpu or gpu cpu, but should apply to both custom docker image n"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/70",
        "Issue_title":"[Bug] highlight incorrect field in screenshot of importing Sagemaker workspace",
        "Issue_creation_time":1659604135000,
        "Issue_closed_time":1662449543000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nshould highlight `instance type` field\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png)\r\n\r\nthe field `AutoStopIdleTimeInMinutes` also is required.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] highlight incorrect field in screenshot of importing workspace; Content: **describe the bug** a clear and concise description of what the bug is. should highlight `instance type` field ![image](https:\/\/user-images.githubusercontent.com\/843303\/182809305-2d25c565-18f8-4da0-ad9e-847c28cc62b0.png) the field `autostopidletimeinminutes` also is required. **to reproduce** steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error **expected behavior** a clear and concise description of what you expected to happen. **screenshots** if applicable, add screenshots to help explain your problem. **versions (please complete the following information):** - release version installed [e.g. v1.0.3] **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the incorrect field was highlighted in a screenshot of importing a workspace, and the field 'autostopidletimeinminutes' was also required.",
        "Issue_preprocessed_content":"Title: highlight incorrect field in screenshot of importing workspace; Content: describe the bug a clear and concise description of what the bug is. should highlight field the field also is required. to reproduce steps to reproduce the behavior . go to . click on . scroll down to . see error expected behavior a clear and concise description of what you expected to happen. screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/45",
        "Issue_title":"[Bug] In HongKong region, After user stop sagemaker workspace manually, web console show \"UNKNOWN\" status",
        "Issue_creation_time":1657604472000,
        "Issue_closed_time":1659958133000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Deploy SWB in hongkong reigon\r\n2. Create a Sagemaker workspace\r\n3. Click \"Stop\" button.\r\n5. workspace status show \"UNKNOWN\"\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] in hongkong region, after user stop workspace manually, web console show \"unknown\" status; Content: **describe the bug** a clear and concise description of what the bug is. **to reproduce** steps to reproduce the behavior: 1. deploy swb in hongkong reigon 2. create a workspace 3. click \"stop\" button. 5. workspace status show \"unknown\"",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the Hong Kong region where, after manually stopping a workspace, the web console showed an \"unknown\" status.",
        "Issue_preprocessed_content":"Title: in hongkong region, after user stop workspace manually, web console show unknown status; Content: describe the bug a clear and concise description of what the bug is. to reproduce steps to reproduce the behavior . deploy swb in hongkong reigon . create a workspace . click stop button. . workspace status show unknown"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws-cn\/issues\/44",
        "Issue_title":"[Bug] Sagemaker template, after auto stoped, workspace env status is not updated",
        "Issue_creation_time":1657451336000,
        "Issue_closed_time":1657702977000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nAfter Sagemaker workspace stopped automatically, workspace env status is not updated.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Set sagemaker workspace config's AutoStopIdleTimeInMinutes as 10 minutes\r\n2. Create sagemaker workspace and wait for more than 10 minutes,\r\n3. Check sagemaker notebook instances to confirm the instance status is Stopped\r\n4. Check Service Workbench workspace status, it is still \"AVAILABLE\"\r\n\r\n**Expected behavior**\r\n1. Above step 4, workspace status should be \"STOPPED\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] template, after auto stoped, workspace env status is not updated; Content: **describe the bug** after workspace stopped automatically, workspace env status is not updated. **to reproduce** steps to reproduce the behavior: 1. set workspace config's autostopidletimeinminutes as 10 minutes 2. create workspace and wait for more than 10 minutes, 3. check notebook instances to confirm the instance status is stopped 4. check service workbench workspace status, it is still \"available\" **expected behavior** 1. above step 4, workspace status should be \"stopped\" **screenshots** if applicable, add screenshots to help explain your problem. **versions (please complete the following information):** - release version installed [e.g. v1.0.3] **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the workspace environment status was not updated after the workspace was automatically stopped.",
        "Issue_preprocessed_content":"Title: template, after auto stoped, workspace env status is not updated; Content: describe the bug after workspace stopped automatically, workspace env status is not updated. to reproduce steps to reproduce the behavior . set workspace config's autostopidletimeinminutes as minutes . create workspace and wait for more than minutes, . check notebook instances to confirm the instance status is stopped . check service workbench workspace status, it is still available expected behavior . above step , workspace status should be stopped screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/36",
        "Issue_title":"Sagemaker dependencies",
        "Issue_creation_time":1563873801000,
        "Issue_closed_time":1564216116000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Hi,\r\n\r\nGood day.\r\n\r\nCould you add to the Sagemaker section?:\r\n```\r\npip install urllib3==1.24.3\r\npip install PyYAML==3.13\r\npip install ipython\r\n```\r\nFrom https:\/\/medium.com\/@jonathantse\/train-deepracer-model-locally-with-gpu-support-29cce0bdb0f9. \r\n\r\nKeep up the good work!",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: dependencies; Content: hi, good day. could you add to the section?: ``` pip install urllib3==1.24.3 pip install pyyaml==3.13 pip install ipython ``` from https:\/\/medium.com\/@jonathantse\/train-deepracer-model-locally-with-gpu-support-29cce0bdb0f9. keep up the good work!",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to install additional dependencies in order to train a DeepRacer model locally with GPU support.",
        "Issue_preprocessed_content":"Title: dependencies; Content: hi, good day. could you add to the section? from keep up the good work!"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/21",
        "Issue_title":"When pretrained model is not found, sagemaker falls into an infinite silent loop",
        "Issue_creation_time":1561357678000,
        "Issue_closed_time":1562359564000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"I mistakenly put my model in pretrained folder but outside the model subfolder. In such case an exception is caught silently and then a sleep is called only to retry the exact behaviour.\r\nWhile I did not fix the issue, I added logging to make it verbose. I will try to upload a patch.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: when pretrained model is not found, falls into an infinite silent loop; Content: i mistakenly put my model in pretrained folder but outside the model subfolder. in such case an exception is caught silently and then a sleep is called only to retry the exact behaviour. while i did not fix the issue, i added logging to make it verbose. i will try to upload a patch.",
        "Issue_original_content_gpt_summary":"The user encountered an infinite silent loop when a pretrained model was not found, and added logging to make the issue more verbose.",
        "Issue_preprocessed_content":"Title: when pretrained model is not found, falls into an infinite silent loop; Content: i mistakenly put my model in pretrained folder but outside the model subfolder. in such case an exception is caught silently and then a sleep is called only to retry the exact behaviour. while i did not fix the issue, i added logging to make it verbose. i will try to upload a patch."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-deepracer-community\/deepracer-core\/issues\/18",
        "Issue_title":"No tensorflow reported when trying to run nvidia image for sagemaker",
        "Issue_creation_time":1560810377000,
        "Issue_closed_time":1564215404000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Steps to reproduce:\r\nI followed instructions in the readme, but instead of `docker pull nabcrr\/sagemaker-rl-tensorflow:console` I did `docker pull nabcrr\/sagemaker-rl-tensorflow:nvidia` and then tagged it as instructed. Before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` I went to that file and commented out the line that Lonon mentioned in #17 \r\n\r\nExpected result:\r\nWhen running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins\r\n\r\nActual result:\r\n```\r\nalgo-1-vrm2i_1  | ERROR: ld.so: object '\/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\r\nalgo-1-vrm2i_1  | Reporting training FAILURE\r\nalgo-1-vrm2i_1  | framework error:\r\nalgo-1-vrm2i_1  | Traceback (most recent call last):\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_containers\/_trainer.py\", line 60, in train\r\nalgo-1-vrm2i_1  |     framework = importlib.import_module(framework_name)\r\nalgo-1-vrm2i_1  |   File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\r\nalgo-1-vrm2i_1  |     return _bootstrap._gcd_import(name[level:], package, level)\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\nalgo-1-vrm2i_1  |   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nalgo-1-vrm2i_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/sagemaker_tensorflow_container\/training.py\", line 24, in <module>\r\nalgo-1-vrm2i_1  |     import tensorflow as tf\r\nalgo-1-vrm2i_1  | ModuleNotFoundError: No module named 'tensorflow'\r\nalgo-1-vrm2i_1  |\r\nalgo-1-vrm2i_1  | No module named 'tensorflow'\r\n```\r\n\r\nSystem info:\r\nUbuntu 18.04.2 LTS\r\n\r\n```\r\n$ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi\r\nMon Jun 17 22:24:56 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 660M    Off  | 00000000:01:00.0 N\/A |                  N\/A |\r\n| N\/A   46C    P8    N\/A \/  N\/A |    266MiB \/  1999MiB |     N\/A      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0                    Not Supported                                       |\r\n+-----------------------------------------------------------------------------+\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: no tensorflow reported when trying to run nvidia image for ; steps to reproduce: i followed instructions in the readme, but instead of `docker pull nabcrr\/-rl-tensorflow:console` i did `docker pull nabcrr\/-rl-tensorflow:nvidia` and then tagged it as instructed. before running `(cd rl_coach; ipython rl_deepracer_coach_robomaker.py)` i went to that file and commented out the line that lonon mentioned in #17 expected result: when running `(cd rl_coach; Content: ipython rl_deepracer_coach_robomaker.py)` my gpu is detected and training begins actual result: ``` algo-1-vrm2i_1 | error: ld.so: object '\/libchangehostname.so' from ld_preload cannot be preloaded (cannot open shared object file): ignored. algo-1-vrm2i_1 | reporting training failure algo-1-vrm2i_1 | framework error: algo-1-vrm2i_1 | traceback (most recent call last): algo-1-vrm2i_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/_containers\/_trainer.py\", line 60, in train algo-1-vrm2i_1 | framework = importlib.import_module(framework_name) algo-1-vrm2i_1 | file \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module algo-1-vrm2i_1 | return _bootstrap._gcd_import(name[level:], package, level) algo-1-vrm2i_1 | file \"\", line 994, in _gcd_import algo-1-vrm2i_1 | file \"\", line 971, in _find_and_load algo-1-vrm2i_1 | file \"\", line 955, in _find_and_load_unlocked algo-1-vrm2i_1 | file \"\", line 665, in _load_unlocked algo-1-vrm2i_1 | file \"\", line 678, in exec_module algo-1-vrm2i_1 | file \"\", line 219, in _call_with_frames_removed algo-1-vrm2i_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/_tensorflow_container\/training.py\", line 24, in algo-1-vrm2i_1 | import tensorflow as tf algo-1-vrm2i_1 | modulenotfounderror: no module named 'tensorflow' algo-1-vrm2i_1 | algo-1-vrm2i_1 | no module named 'tensorflow' ``` system info: ubuntu 18.04.2 lts ``` $ docker run --runtime=nvidia --rm nvidia\/cuda:10.1-base nvidia-smi mon jun 17 22:24:56 2019 +-----------------------------------------------------------------------------+ | nvidia-smi 418.56 driver version: 418.56 cuda version: 10.1 | |-------------------------------+----------------------+----------------------+ | gpu name persistence-m| bus-id disp.a | volatile uncorr. ecc | | fan temp perf pwr:usage\/cap| memory-usage | gpu-util compute m. | |===============================+======================+======================| | 0 geforce gtx 660m off | 00000000:01:00.0 n\/a | n\/a | | n\/a 46c p8 n\/a \/ n\/a | 266mib \/ 1999mib | n\/a default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | processes: gpu memory | | gpu pid type process name usage | |=============================================================================| | 0 not supported | +-----------------------------------------------------------------------------+ ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to run the NVIDIA image for TensorFlow, where the expected result of the GPU being detected and training beginning was not achieved due to an error with the TensorFlow module.",
        "Issue_preprocessed_content":"Title: no tensorflow reported when trying to run nvidia image for ; Content: steps to reproduce i followed instructions in the readme, but instead of i did and then tagged it as instructed. before running i went to that file and commented out the line that lonon mentioned in expected result when running my gpu is detected and training begins actual result system info ubuntu lts"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-samples\/eks-kubeflow-workshop\/issues\/1",
        "Issue_title":"Can not compile SageMaker examples",
        "Issue_creation_time":1571075742000,
        "Issue_closed_time":1586730089000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"Trying our your Kubeflow\/SageMaker notebook in your workshop and received a pipeline compile error.  \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: can not compile examples; Content: trying our your kubeflow\/ notebook in your workshop and received a pipeline compile error. ![image](https:\/\/user-images.githubusercontent.com\/4739316\/66772250-1e628900-ee71-11e9-92f0-afceb992313a.png)",
        "Issue_original_content_gpt_summary":"The user encountered a pipeline compile error when trying out the Kubeflow\/notebook in their workshop.",
        "Issue_preprocessed_content":"Title: can not compile examples; Content: trying our your notebook in your workshop and received a pipeline compile error."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/issues\/32",
        "Issue_title":"Error in PyTorch MNIST Notebook with SageMaker Studio ",
        "Issue_creation_time":1590122594000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/blob\/master\/hpo_pytorch_mnist\/hpo_pytorch_mnist.ipynb\r\n\r\nNeed to add \r\n```\r\n!pip install ipywidgets\r\n!jupyter nbextension enable --py widgetsnbextension\r\n```\r\n\r\nWith the command below, SageMaker local mode still show some errors: \r\n```\r\n!pip install docker-compose\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error in pytorch mnist notebook with studio ; Content: https:\/\/github.com\/aws-samples\/amazon--examples-jp\/blob\/master\/hpo_pytorch_mnist\/hpo_pytorch_mnist.ipynb need to add ``` !pip install ipywidgets !jupyter nbextension enable --py widgetsnbextension ``` with the command below, local mode still show some errors: ``` !pip install docker-compose ```",
        "Issue_original_content_gpt_summary":"The user encountered an error in the pytorch mnist notebook with AWS Studio, and needed to add additional commands to the notebook in order to resolve the issue.",
        "Issue_preprocessed_content":"Title: error in pytorch mnist notebook with studio ; Content: need to add with the command below, local mode still show some errors"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/issues\/31",
        "Issue_title":"Error in Autopilot Notebook with SageMaker Studio ",
        "Issue_creation_time":1590122435000,
        "Issue_closed_time":null,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"https:\/\/github.com\/aws-samples\/amazon-sagemaker-examples-jp\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb\r\n\r\nResolve by adding `!apt-get install unzip -y`\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error in autopilot notebook with studio ; Content: https:\/\/github.com\/aws-samples\/amazon--examples-jp\/blob\/master\/autopilot\/autopilot_customer_churn.ipynb resolve by adding `!apt-get install unzip -y`",
        "Issue_original_content_gpt_summary":"The user encountered an error in the autopilot notebook with studio, which was resolved by adding `!apt-get install unzip -y`.",
        "Issue_preprocessed_content":"Title: error in autopilot notebook with studio ; Content: resolve by adding"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws-solutions\/mlops-workload-orchestrator\/issues\/6",
        "Issue_title":"Error with sagemaker_layer in lambda \"create_sagemakermodel\"",
        "Issue_creation_time":1620397656000,
        "Issue_closed_time":1620839615000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nWhen making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createModel\" when I run the pipeline from scratch. The error is:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png)\r\n\r\n**To Reproduce**\r\nThe only steps I took was to unfold it as it naturally comes. This bug prevented me from creating a sagemaker model for both the batch and realtime pipelines.\r\n\r\n**Expected behavior**\r\nThe ideal and expected behavior is that this error does not occur and you can create the model.\r\n\r\n**Solution to that moment**\r\nTry to fix the numpy versions issue by re-creating the `sagemaker_layer` layer, via pip installation of the libraries. However, there were conflicts with other modified libraries at the time of `pip install numpy`. For this reason, I had to choose to use the default AWS library that comes with numpy \"AWSLambda-Python38-SciPy1x-v29\". For this, I had to modify the code as follows:\r\n\r\nin deploy_actions.py \/ create_sagemaker_model - I add the layer:\r\n\"arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python38-SciPy1x:29\u201d\r\n\r\nWith this, I stop throwing that error at me. I think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. Please check if it still exists in the new versions.\r\n\r\n**Please complete the following information about the solution:**\r\n- [ ] Version: [e.g. v1.1.0]\r\n\r\n\r\nTo get the version of the solution, you can look at the description of the created CloudFormation stack. For example, \"(SO0136) - AWS MLOps Framework. Version v1.1.0\".\r\n\r\n- [ ] Region: [e.g. us-east-1]\r\n- [ ] Was the solution modified from the version published on this repository? No\r\n- [ ] If the answer to the previous question was yes, are the changes available on GitHub? -\r\n- [ ] Have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses?\r\n- [ ] Were there any errors in the CloudWatch Logs? Yes\r\n\r\n**Additional context**\r\nI am a Solution Architect of an advanced AWS partner company, and we are running a proof of concept with a real client.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: error with _layer in lambda \"create_model\"; Content: **describe the bug** when making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of \"createmodel\" when i run the pipeline from scratch. the error is: ![image](https:\/\/user-images.githubusercontent.com\/21212412\/117463159-5e8ad000-af1d-11eb-9568-90380ee83ef3.png) **to reproduce** the only steps i took was to unfold it as it naturally comes. this bug prevented me from creating a model for both the batch and realtime pipelines. **expected behavior** the ideal and expected behavior is that this error does not occur and you can create the model. **solution to that moment** try to fix the numpy versions issue by re-creating the `_layer` layer, via pip installation of the libraries. however, there were conflicts with other modified libraries at the time of `pip install numpy`. for this reason, i had to choose to use the default aws library that comes with numpy \"awslambda-python38-scipy1x-v29\". for this, i had to modify the code as follows: in deploy_actions.py \/ create__model - i add the layer: \"arn:aws:lambda:us-east-1:668099181075:layer:awslambda-python38-scipy1x:29 with this, i stop throwing that error at me. i think it is likely that due to library or version incompatibility issues, this error is by default in the mlops-framework solution. please check if it still exists in the new versions. **please complete the following information about the solution:** - [ ] version: [e.g. v1.1.0] to get the version of the solution, you can look at the description of the created cloudformation stack. for example, \"(so0136) - aws mlops framework. version v1.1.0\". - [ ] region: [e.g. us-east-1] - [ ] was the solution modified from the version published on this repository? no - [ ] if the answer to the previous question was yes, are the changes available on github? - - [ ] have you checked your [service quotas](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws_service_limits.html) for the sevices this solution uses? - [ ] were there any errors in the cloudwatch logs? yes **additional context** i am a solution architect of an advanced aws partner company, and we are running a proof of concept with a real client.",
        "Issue_original_content_gpt_summary":"The user encountered an error related to numpy in the lambda of \"createmodel\" when running the pipeline from scratch, which was solved by re-creating the `_layer` layer and using the default aws library that comes with numpy \"awslambda-python38-scipy1x-v29\".",
        "Issue_preprocessed_content":"Title: error with in lambda ; Content: describe the bug when making the natural deployment of the framework, and deploying the framework, there is an error related to numpy in the lambda of createmodel when i run the pipeline from scratch. the error is to reproduce the only steps i took was to unfold it as it naturally comes. this bug prevented me from creating a model for both the batch and realtime pipelines. expected behavior the ideal and expected behavior is that this error does not occur and you can create the model. solution to that moment try to fix the numpy versions issue by the layer, via pip installation of the libraries. however, there were conflicts with other modified libraries at the time of . for this reason, i had to choose to use the default aws library that comes with numpy for this, i had to modify the code as follows in \/ i add the layer with this, i stop throwing that error at me. i think it is likely that due to library or version incompatibility issues, this error is by default in the solution. please check if it still exists in the new versions. please complete the following information about the solution version to get the version of the solution, you can look at the description of the created cloudformation stack. for example, so aws mlops framework. version region was the solution modified from the version published on this repository? no if the answer to the previous question was yes, are the changes available on github? have you checked your for the sevices this solution uses? were there any errors in the cloudwatch logs? yes additional context i am a solution architect of an advanced aws partner company, and we are running a proof of concept with a real client."
    },
    {
        "Issue_link":"https:\/\/github.com\/udacity\/ML_SageMaker_Studies\/issues\/15",
        "Issue_title":"With \"sagemaker 2.31.1\", \"sagemaker.pytorch.PyTorch\" needs to specify both \"framework_version\" and \"py_version\"",
        "Issue_creation_time":1619388470000,
        "Issue_closed_time":1623053107000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"In **Moon_Classification_Solution.ipynb**, the original code below would cause an error `ValueError: framework_version or py_version was None, yet image_uri was also None. Either specify both framework_version and py_version, or specify image_uri.` So I specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. Or I guess just add `!pip install sagemaker==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/sagemaker-deployment\/blob\/master\/Mini-Projects\/IMDB%20Sentiment%20Analysis%20-%20XGBoost%20(Batch%20Transform)%20-%20Solution.ipynb) would also solve the issue.\r\n\r\n```\r\n# import a PyTorch wrapper\r\nfrom sagemaker.pytorch import PyTorch\r\n\r\n# specify an output path\r\n# prefix is specified above\r\noutput_path = 's3:\/\/{}\/{}'.format(bucket, prefix)\r\n\r\n# instantiate a pytorch estimator\r\nestimator = PyTorch(entry_point='train.py',\r\n                    source_dir='source_solution', # this should be just \"source\" for your code\r\n                    role=role,\r\n                    framework_version='1.0',\r\n                    py_version='py3', ### <------------------------ added a line here\r\n                    train_instance_count=1,\r\n                    train_instance_type='ml.c4.xlarge',\r\n                    output_path=output_path,\r\n                    sagemaker_session=sagemaker_session,\r\n                    hyperparameters={\r\n                        'input_dim': 2,  # num of features\r\n                        'hidden_dim': 20,\r\n                        'output_dim': 1,\r\n                        'epochs': 80 # could change to higher\r\n                    })\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: with \" 2.31.1\", \".pytorch.pytorch\" needs to specify both \"framework_version\" and \"py_version\"; Content: in **moon_classification_solution.ipynb**, the original code below would cause an error `valueerror: framework_version or py_version was none, yet image_uri was also none. either specify both framework_version and py_version, or specify image_uri.` so i specified `py_version='py3'`, cause the framework version only supports `py2` and `py3`, which fixed the problem. or i guess just add `!pip install ==1.72.0` like notebooks in another [**repo**](https:\/\/github.com\/udacity\/-deployment\/blob\/master\/mini-projects\/imdb%20sentiment%20analysis%20-%20xgboost%20(batch%20transform)%20-%20solution.ipynb) would also solve the issue. ``` # import a pytorch wrapper from .pytorch import pytorch # specify an output path # prefix is specified above output_path = 's3:\/\/{}\/{}'.format(bucket, prefix) # instantiate a pytorch estimator estimator = pytorch(entry_point='train.py', source_dir='source_solution', # this should be just \"source\" for your code role=role, framework_version='1.0', py_version='py3', ### <------------------------ added a line here train_instance_count=1, train_instance_type='ml.c4.xlarge', output_path=output_path, _session=_session, hyperparameters={ 'input_dim': 2, # num of features 'hidden_dim': 20, 'output_dim': 1, 'epochs': 80 # could change to higher }) ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they needed to specify both \"framework_version\" and \"py_version\" when using \".pytorch.pytorch\" in order to avoid an error.",
        "Issue_preprocessed_content":"Title: with needs to specify both and ; Content: in the original code below would cause an error so i specified , cause the framework version only supports and , which fixed the problem. or i guess just add like notebooks in another would also solve the issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/7040",
        "Issue_title":"[bug] Idempotency in kubeflow pipeline sagemaker component. ",
        "Issue_creation_time":1639174458000,
        "Issue_closed_time":null,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### What steps did you take\r\n\r\nIf node scales\/up down, the sagemaker component tries to create the same job which fails. Since sagemaker does not let create the same name job. Component controller should be able to detect this and resume the job from existing state. \r\n\r\n### What happened:\r\nthe job hangs\/fail \r\n\r\n### What did you expect to happen:\r\nI expect the job to resume from previous state. \r\n\r\n### Environment:\r\nkfp-1.6\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] idempotency in kubeflow pipeline component. ; Content: ### what steps did you take if node scales\/up down, the component tries to create the same job which fails. since does not let create the same name job. component controller should be able to detect this and resume the job from existing state. ### what happened: the job hangs\/fail ### what did you expect to happen: i expect the job to resume from previous state. ### environment: kfp-1.6 impacted by this bug? give it a . we prioritise the issues with the most .",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with idempotency in a Kubeflow Pipeline component, where the job hangs\/fails if the node scales\/up down and the component tries to create the same job which fails due to not being able to create the same name job.",
        "Issue_preprocessed_content":"Title: idempotency in kubeflow pipeline component. ; Content: what steps did you take if node down, the component tries to create the same job which fails. since does not let create the same name job. component controller should be able to detect this and resume the job from existing state. what happened the job what did you expect to happen i expect the job to resume from previous state. environment don't delete message below to encourage users to support your issue! impacted by this bug? give it a . we prioritise the issues with the most ."
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/6465",
        "Issue_title":"[bug] Unhandled SageMaker training job status 'stopped' causing infinite loop",
        "Issue_creation_time":1630204381000,
        "Issue_closed_time":null,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### What steps did you take\r\n\r\nCode gets stuck in infinite loop is SageMaker training job gets stopped (unhandled use case)\r\n\r\n### What happened:\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/sagemaker\/train\/src\/sagemaker_training_component.py#L57-L66\r\n\r\nAbove code only caters for training job status `Completed` or `Failed`, so if the training job status is marked as `Stopped`, it causes an infinite loop in below code\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/sagemaker\/common\/sagemaker_component.py#L197-L201\r\n\r\n### What did you expect to happen:\r\n\r\nTraining job status `stopped` to be catered for\r\n\r\n### Environment:\r\n\r\n### Anything else you would like to add:\r\n\r\n\r\n### Labels\r\n<!-- Please include labels below by uncommenting them to help us better triage issues -->\r\n\r\n<!-- \/area frontend -->\r\n<!-- \/area backend -->\r\n<!-- \/area sdk -->\r\n<!-- \/area testing -->\r\n<!-- \/area samples -->\r\n<!-- \/area components -->\r\n\r\n\r\n---\r\n\r\n<!-- Don't delete message below to encourage users to support your issue! -->\r\nImpacted by this bug? Give it a \ud83d\udc4d. We prioritise the issues with the most \ud83d\udc4d.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] unhandled training job status 'stopped' causing infinite loop; Content: ### what steps did you take code gets stuck in infinite loop is training job gets stopped (unhandled use case) ### what happened: https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/components\/aws\/\/train\/src\/_training_component.py#l57-l66 above code only caters for training job status `completed` or `failed`, so if the training job status is marked as `stopped`, it causes an infinite loop in below code https:\/\/github.com\/kubeflow\/pipelines\/blob\/d9c019641ef9ebd78db60cdb78ea29b0d9933008\/components\/aws\/\/common\/_component.py#l197-l201 ### what did you expect to happen: training job status `stopped` to be catered for ### environment: ### anything else you would like to add: ### labels --- impacted by this bug? give it a . we prioritise the issues with the most .",
        "Issue_original_content_gpt_summary":"The user encountered an infinite loop caused by an unhandled training job status of 'stopped' in the Kubeflow Pipelines code.",
        "Issue_preprocessed_content":"Title: unhandled training job status 'stopped' causing infinite loop; Content: what steps did you take code gets stuck in infinite loop is training job gets stopped what happened above code only caters for training job status or , so if the training job status is marked as , it causes an infinite loop in below code what did you expect to happen training job status to be catered for environment anything else you would like to add labels please include labels below by uncommenting them to help us better triage issues frontend backend sdk testing samples components don't delete message below to encourage users to support your issue! impacted by this bug? give it a . we prioritise the issues with the most ."
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4888",
        "Issue_title":"TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'",
        "Issue_creation_time":1607683045000,
        "Issue_closed_time":1611093472000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"### What steps did you take:\r\n[A clear and concise description of what the bug is.]\r\n\r\nI am use the re usable Sagemaker Components for building kubeflow pipelines.\r\n\r\nsagemaker_train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/train\/component.yaml')\r\nsagemaker_model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/model\/component.yaml')\r\nsagemaker_deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/sagemaker\/deploy\/component.yaml')\r\n\r\nWhen i am trying to update the endpoint that already exists \r\n\r\npiece of code i used to update the endpoint.\r\n\r\n**#deploy the pipeline\r\nprediction = sagemaker_deploy_op(\r\n        region=aws_region,\r\n        endpoint_name='Endpoint-price-prediction-model',\r\n        endpoint_config_name='EndpointConfig-price-prediction-model',\r\n        update_endpoint=True,\r\n        model_name_1 = create_model.output,\r\n        instance_type_1='ml.m5.large'\r\n    )\r\n# compiling the pipeline\r\nkfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')**\r\n\r\n\r\n### What happened:\r\nI am getting this error \r\nTypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\nI think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this\r\n\r\n\r\nTraceback (most recent call last):\r\n--\r\n414 | File \"pipeline.py\", line 94, in <module>\r\n415 | kfp.compiler.Compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')\r\n416 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile\r\n417 | self._create_and_write_workflow(\r\n418 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow\r\n419 | workflow = self._create_workflow(\r\n420 | File \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow\r\n421 | pipeline_func(*args_list)\r\n422 | File \"pipeline.py\", line 85, in car_price_prediction\r\n423 | prediction = sagemaker_deploy_op(\r\n424 | TypeError: Sagemaker - Deploy Model() got an unexpected keyword argument 'update_endpoint'\r\n\r\n\r\n\r\n### What did you expect to happen:\r\nto update the endpoint without any issue\r\n### Environment:\r\n<!-- Please fill in those that seem relevant. -->\r\nusing kfp 1.1.2\r\nsagemaker 2.1.0\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\n<!-- If you are not sure, here's [an introduction of all options](https:\/\/www.kubeflow.org\/docs\/pipelines\/installation\/overview\/). -->\r\n\r\nKFP version: <!-- If you are not sure, build commit shows on bottom of KFP UI left sidenav. -->\r\n\r\nKFP SDK version: <!-- Please attach the output of this shell command: $pip list | grep kfp -->\r\nkfp-1.1.2.tar.gz \r\n\r\n### Anything else you would like to add:\r\n[Miscellaneous information that will assist in solving the issue.]\r\n\r\nPlease help me out \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\r\n\/\/ \/area frontend\r\n\/\/ \/area backend\r\n\/\/ \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: typeerror: - deploy model() got an unexpected keyword argument 'update_endpoint'; Content: ### what steps did you take: [a clear and concise description of what the bug is.] i am use the re usable components for building kubeflow pipelines. _train_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/\/train\/component.yaml') _model_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/\/model\/component.yaml') _deploy_op = components.load_component_from_url('https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/cb36f87b727df0578f4c1e3fe9c24a30bb59e5a2\/components\/aws\/\/deploy\/component.yaml') when i am trying to update the endpoint that already exists piece of code i used to update the endpoint. **#deploy the pipeline prediction = _deploy_op( region=aws_region, endpoint_name='endpoint-price-prediction-model', endpoint_config_name='endpointconfig-price-prediction-model', update_endpoint=true, model_name_1 = create_model.output, instance_type_1='ml.m5.large' ) # compiling the pipeline kfp.compiler.compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip')** ### what happened: i am getting this error typeerror: - deploy model() got an unexpected keyword argument 'update_endpoint' i think while compile the pipeline kfp is throwing this error.can you suggest me or help me out in this traceback (most recent call last): -- 414 | file \"pipeline.py\", line 94, in 415 | kfp.compiler.compiler().compile(car_price_prediction,'car-price-pred-pipeline.zip') 416 | file \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 920, in compile 417 | self._create_and_write_workflow( 418 | file \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 972, in _create_and_write_workflow 419 | workflow = self._create_workflow( 420 | file \"\/root\/.pyenv\/versions\/3.8.3\/lib\/python3.8\/site-packages\/kfp\/compiler\/compiler.py\", line 813, in _create_workflow 421 | pipeline_func(*args_list) 422 | file \"pipeline.py\", line 85, in car_price_prediction 423 | prediction = _deploy_op( 424 | typeerror: - deploy model() got an unexpected keyword argument 'update_endpoint' ### what did you expect to happen: to update the endpoint without any issue ### environment: using kfp 1.1.2 2.1.0 how did you deploy kubeflow pipelines (kfp)? kfp version: kfp sdk version: kfp-1.1.2.tar.gz ### anything else you would like to add: [miscellaneous information that will assist in solving the issue.] please help me out \/kind bug",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when attempting to update an existing endpoint using the Kubeflow Pipelines SDK, resulting in an unexpected keyword argument 'update_endpoint'.",
        "Issue_preprocessed_content":"Title: typeerror deploy model got an unexpected keyword argument ; Content: what steps did you take a clear and concise description of what the bug i am use the re usable components for building kubeflow pipelines. when i am trying to update the endpoint that already exists piece of code i used to update the endpoint. deploy the pipeline prediction compiling the pipeline what happened i am getting this error typeerror deploy model got an unexpected keyword argument i think while compile the pipeline kfp is throwing this you suggest me or help me out in this traceback file line , in file line , in compile file line , in workflow file line , in file line , in prediction typeerror deploy model got an unexpected keyword argument what did you expect to happen to update the endpoint without any issue environment please fill in those that seem relevant. using kfp how did you deploy kubeflow pipelines ? if you are not sure, here's . kfp version kfp sdk version anything else you would like to add miscellaneous information that will assist in solving the please help me out bug please include labels by uncommenting them to help us better triage issues, choose from the following frontend backend sdk testing engprod"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/4352",
        "Issue_title":"Want to create ANY model and do batch transform in without Training in Amazon SageMaker ",
        "Issue_creation_time":1597092939000,
        "Issue_closed_time":1632453568000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"### What steps did you take: Removed the HPO and Training Jobs only creating the model and batch transforming in SageMaker \r\n[Automatically taking the HPO and Training on SageMaker facing some issue while kfp compile]\r\n\r\n### What happened: Getting output properly in Kubefow. But I want to to see custom  model (ANY) output without HPO and Model training in Sagemaker\r\n\r\n### What did you expect to happen: Without HPO and batch job in SageMaker\r\n\r\n\r\n\r\n\r\n### Anything else you would like to add:\r\nAny open source loan data model using KF would be appriciated \r\n\r\n\/kind bug\r\n<!-- Please include labels by uncommenting them to help us better triage issues, choose from the following -->\r\n<!--\/\/compile(kfp.compile ) \r\n\/\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: want to create any model and do batch transform in without training in ; Content: ### what steps did you take: removed the hpo and training jobs only creating the model and batch transforming in [automatically taking the hpo and training on facing some issue while kfp compile] ### what happened: getting output properly in kubefow. but i want to to see custom model (any) output without hpo and model training in ### what did you expect to happen: without hpo and batch job in ### anything else you would like to add: any open source loan data model using kf would be appriciated \/kind bug <!--\/\/compile(kfp.compile ) \/",
        "Issue_original_content_gpt_summary":"The user encountered challenges while trying to create a custom model and do batch transform in Kubeflow without training, and was unable to get the expected output.",
        "Issue_preprocessed_content":"Title: want to create any model and do batch transform in without training in ; Content: what steps did you take removed the hpo and training jobs only creating the model and batch transforming in automatically taking the hpo and training on facing some issue while kfp compile what happened getting output properly in kubefow. but i want to to see custom model output without hpo and model training in what did you expect to happen without hpo and batch job in anything else you would like to add any open source loan data model using kf would be appriciated bug please include labels by uncommenting them to help us better triage issues, choose from the following \/"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3728",
        "Issue_title":"Sagemaker Training Operator throws an error if custom image is not hosted on ECR",
        "Issue_creation_time":1588992332000,
        "Issue_closed_time":1589522860000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### What steps did you take:\r\nAttempted to run the Sagemaker training operator using a custom image that is not hosted on ECR\r\n\r\n### What happened:\r\nI got the following error:\r\n```\r\nException: Invalid training image. Please provide a valid Amazon Elastic Container Registry path of the Docker image to run.\r\n```\r\n\r\n### What did you expect to happen:\r\nOur CI\/CD pipeline is set up to push images to our own personal registry that is not hosted on ECR - ideally, I would want to run Sagemaker training jobs using images hosted from our personal registry instead of having to also push our images to ECR (much more error-prone + having to maintain two container registries ...)\r\n\r\n\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nDeployed kubeflow piplines as part of kubeflow deployment on AWS EKS",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: training operator throws an error if custom image is not hosted on ecr; Content: ### what steps did you take: attempted to run the training operator using a custom image that is not hosted on ecr ### what happened: i got the following error: ``` exception: invalid training image. please provide a valid amazon elastic container registry path of the docker image to run. ``` ### what did you expect to happen: our ci\/cd pipeline is set up to push images to our own personal registry that is not hosted on ecr - ideally, i would want to run training jobs using images hosted from our personal registry instead of having to also push our images to ecr (much more error-prone + having to maintain two container registries ...) how did you deploy kubeflow pipelines (kfp)? deployed kubeflow piplines as part of kubeflow deployment on aws eks",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to run the training operator using a custom image that is not hosted on Amazon Elastic Container Registry (ECR), and was hoping to use an image hosted from their own personal registry instead.",
        "Issue_preprocessed_content":"Title: training operator throws an error if custom image is not hosted on ecr; Content: what steps did you take attempted to run the training operator using a custom image that is not hosted on ecr what happened i got the following error what did you expect to happen our pipeline is set up to push images to our own personal registry that is not hosted on ecr ideally, i would want to run training jobs using images hosted from our personal registry instead of having to also push our images to ecr how did you deploy kubeflow pipelines ? deployed kubeflow piplines as part of kubeflow deployment on aws eks"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/3670",
        "Issue_title":"Sagemaker Custom Training Job Error: Unable to locate botocore.credentials",
        "Issue_creation_time":1588293591000,
        "Issue_closed_time":1589303399000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### What steps did you take:\r\nI run a custom image using the sagemaker training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/sagemaker\/train\/component.yaml) and it ran fine. I am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path.\r\n\r\nThe problem arises however if inside the custom script I use boto3 to manually download an object from s3 - then I get an error: Unable to locate credentials ...  \r\n\r\n### What happened:\r\nBelow is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image \r\n\r\n```\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\nINFO:root:Submitting Training Job to SageMaker...\r\nINFO:root:Created Training Job with name: TrainingJob-20200430232331-LPHY\r\nINFO:root:Training job in SageMaker: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/sagemaker\/home?region=us-west-2#\/jobs\/TrainingJob-20200430232331-LPHY\r\nINFO:root:CloudWatch logs: \r\nhttps:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logStream:group=\/aws\/sagemaker\/TrainingJobs;prefix=TrainingJob-20200430232331-LPHY;streamFilter=typeLogStreamPrefix\r\nINFO:root:Job request submitted. Waiting for completion...\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training job is still in status: InProgress\r\nINFO:root:Training failed with the following error: AlgorithmError: Exception during training: Unable to locate credentials\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 174, in main\r\n    preprocessor_path = get_local_path(params[\"preprocessor_path\"])\r\n  File \"main.py\", line 86, in get_local_path\r\n    for s3_object in s3_bucket.objects.all():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__\r\n    for page in self.pages():\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages\r\n    for page in pages:\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__\r\n    response = self._make_request(current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request\r\n    return self._method(**current_kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/conda\/lib\/python3.7\/site-packag\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 81, in <module>\r\n    main()\r\n  File \"train.py\", line 64, in main\r\n    _utils.wait_for_training_job(client, job_name)\r\n  File \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job\r\n    raise Exception('Training job failed')\r\nException: Training job failed\r\n```\r\n\r\n### What did you expect to happen:\r\nI would have expected the credentials to be passed to the image that the training operator is running but it is not the case ...\r\n\r\n### Environment:\r\nHow did you deploy Kubeflow Pipelines (KFP)?\r\nI deployed kubeflow pipelines as part of my kubeflow deployment on AWS EKS:\r\n\r\nKFP version: \r\nBuild commit: 743746b\r\n\r\nKFP SDK version:\r\n0.5.0\r\n\r\n\/kind bug\r\n<!--\r\n\/\/ \/area frontend\r\n \/area backend\r\n \/area sdk\r\n\/\/ \/area testing\r\n\/\/ \/area engprod\r\n-->\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: custom training job error: unable to locate botocore.credentials; Content: ### what steps did you take: i run a custom image using the training operator (https:\/\/raw.githubusercontent.com\/kubeflow\/pipelines\/master\/components\/aws\/\/train\/component.yaml) and it ran fine. i am using `kfp.aws.use_aws_secret` and the objects from s3 are being correctly copied over to the specified local channel path. the problem arises however if inside the custom script i use boto3 to manually download an object from s3 - then i get an error: unable to locate credentials ... ### what happened: below is a copy of the component's logs - notice the very first log statement says that the boto credentials are found in environment variables ... but somehow they never make their way to the boto3 client that is instantiated inside the custom image ``` info:botocore.credentials:found credentials in environment variables. info:root:submitting training job to ... info:root:created training job with name: trainingjob-20200430232331-lphy info:root:training job in : https:\/\/us-west-2.console.aws.amazon.com\/\/home?region=us-west-2#\/jobs\/trainingjob-20200430232331-lphy info:root:cloudwatch logs: https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logstream:group=\/aws\/\/trainingjobs;prefix=trainingjob-20200430232331-lphy;streamfilter=typelogstreamprefix info:root:job request submitted. waiting for completion... info:root:training job is still in status: inprogress info:root:training job is still in status: inprogress info:root:training job is still in status: inprogress info:root:training job is still in status: inprogress info:root:training job is still in status: inprogress info:root:training job is still in status: inprogress info:root:training job is still in status: inprogress info:root:training failed with the following error: algorithmerror: exception during training: unable to locate credentials traceback (most recent call last): file \"main.py\", line 174, in main preprocessor_path = get_local_path(params[\"preprocessor_path\"]) file \"main.py\", line 86, in get_local_path for s3_object in s3_bucket.objects.all(): file \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 83, in __iter__ for page in self.pages(): file \"\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/resources\/collection.py\", line 166, in pages for page in pages: file \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 255, in __iter__ response = self._make_request(current_kwargs) file \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/paginate.py\", line 332, in _make_request return self._method(**current_kwargs) file \"\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call return self._make_api_call(operation_name, kwargs) file \"\/opt\/conda\/lib\/python3.7\/site-packag traceback (most recent call last): file \"train.py\", line 81, in main() file \"train.py\", line 64, in main _utils.wait_for_training_job(client, job_name) file \"\/app\/common\/_utils.py\", line 185, in wait_for_training_job raise exception('training job failed') exception: training job failed ``` ### what did you expect to happen: i would have expected the credentials to be passed to the image that the training operator is running but it is not the case ... ### environment: how did you deploy kubeflow pipelines (kfp)? i deployed kubeflow pipelines as part of my kubeflow deployment on aws eks: kfp version: build commit: 743746b kfp sdk version: 0.5.0 \/kind bug",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to use boto3 to manually download an object from s3 while running a custom image using the training operator in Kubeflow Pipelines on AWS EKS, despite the credentials being found in environment variables.",
        "Issue_preprocessed_content":"Title: custom training job error unable to locate ; Content: what steps did you take i run a custom image using the training operator and it ran fine. i am using and the objects from s are being correctly copied over to the specified local channel path. the problem arises however if inside the custom script i use boto to manually download an object from s then i get an error unable to locate credentials what happened below is a copy of the component's logs notice the very first log statement says that the boto credentials are found in environment variables but somehow they never make their way to the boto client that is instantiated inside the custom image what did you expect to happen i would have expected the credentials to be passed to the image that the training operator is running but it is not the case environment how did you deploy kubeflow pipelines ? i deployed kubeflow pipelines as part of my kubeflow deployment on aws eks kfp version build commit b kfp sdk version bug frontend backend sdk testing engprod"
    },
    {
        "Issue_link":"https:\/\/github.com\/kubeflow\/pipelines\/issues\/1370",
        "Issue_title":"Kubeflow-pipeline running with aws sagemaker throws an error passing K-Mean and feature_dim parameters",
        "Issue_creation_time":1558527534000,
        "Issue_closed_time":1563269566000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":13.0,
        "Issue_body":"Hi, \r\n\r\nI have copied the git code for aws sagemaker to execute through the Kubeflow pipeline\r\n\r\nhttps:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-sagemaker\/mnist-classification-pipeline.py\r\n\r\nWhile executing the kubeflow pipeline, I am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define.\r\n\r\nerror:\r\n\r\nTraining failed with the following error: ClientError: No value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by ValidationError)\r\n\r\npipeline parameters are:\r\n\r\n@dsl.pipeline(\r\n    name='MNIST Classification pipeline',\r\n    description='MNIST Classification using KMEANS in SageMaker'\r\n)\r\ndef mnist_classification(region='us-east-1',\r\n    image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1',\r\n    dataset_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/data',\r\n    instance_type='ml.c4.8xlarge',\r\n    instance_count='2',\r\n    volume_size='50',\r\n    model_output_path='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/model',\r\n    batch_transform_input='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/input',\r\n    batch_transform_ouput='s3:\/\/s3-sagemaker-us-east-1\/mnist_kmeans_example\/output',\r\n    role_arn=''\r\n    ):\r\n\r\nPlease let me know why this error is appeared and how should it get resolved ?\r\n\r\nRegards,\r\nVarun\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: kubeflow-pipeline running with throws an error passing k-mean and feature_dim parameters; Content: hi, i have copied the git code for to execute through the kubeflow pipeline https:\/\/github.com\/kubeflow\/pipelines\/blob\/master\/samples\/aws-samples\/mnist-kmeans-\/mnist-classification-pipeline.py while executing the kubeflow pipeline, i am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define. error: training failed with the following error: clienterror: no value(s) were specified for 'k', 'feature_dim' which are required hyperparameter(s) (caused by validationerror) pipeline parameters are: @dsl.pipeline( name='mnist classification pipeline', description='mnist classification using kmeans in ' ) def mnist_classification(region='us-east-1', image='174872318107.dkr.ecr.us-west-2.amazonaws.com\/kmeans:1', dataset_path='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/data', instance_type='ml.c4.8xlarge', instance_count='2', volume_size='50', model_output_path='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/model', batch_transform_input='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/input', batch_transform_ouput='s3:\/\/s3--us-east-1\/mnist_kmeans_example\/output', role_arn='' ): please let me know why this error is appeared and how should it get resolved ? regards, varun",
        "Issue_original_content_gpt_summary":"The user encountered an error while running a kubeflow pipeline, which was caused by the lack of specified values for the required hyperparameters 'k' and 'feature_dim'.",
        "Issue_preprocessed_content":"Title: running with throws an error passing and parameters; Content: hi, i have copied the git code for to execute through the kubeflow pipeline while executing the kubeflow pipeline, i am getting the error of assigning the hyperparameters, although in pipeline parameters there are no such parameters define. error training failed with the following error clienterror no value were specified for 'k', which are required hyperparameter pipeline parameters are name 'mnist classification pipeline', description 'mnist classification using kmeans in ' def please let me know why this error is appeared and how should it get resolved ? regards, varun"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/589",
        "Issue_title":"[bug] Sagemaker Remote Test reporting issues",
        "Issue_creation_time":1600043448000,
        "Issue_closed_time":1626207887000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Checklist\r\n- [x] I've prepended issue tag with type of change: [bug]\r\n- [ ] (If applicable) I've attached the script to reproduce the bug\r\n- [ ] (If applicable) I've documented below the DLC image\/dockerfile this relates to\r\n- [ ] (If applicable) I've documented below the tests I've run on the DLC image\r\n- [ ] I'm using an existing DLC image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\r\n- [ ] I've built my own container based off DLC (and I've attached the code used to build my own image)\r\n\r\n*Concise Description:*\r\nSM Remote Test log doesn't get reported correctly.\r\n\r\nObserved in 2 commits of the PR: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\r\n\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8\r\n- https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730\r\n\r\n*DLC image\/dockerfile:*\r\nMX 1.6 DLC\r\n\r\n*Current behavior:*\r\nGithub shows \"pending\" status.\r\nCodeBuild logs show \"Failed\" status.\r\nHowever, actual codebuild logs doesn't bear Failure log. It terminates abruptly.\r\n\r\n```\r\n\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1\r\nrootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests\r\nplugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2\r\ngw0 I \/ gw1 I \/ gw2 I \/ gw3 I \/ gw4 I \/ gw5 I \/ gw6 I \/ gw7 I\r\ngw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3]\r\n```\r\n\r\nSM-Cloudwatch log\r\nNavigating to the appropriate SM training log shows that the job ran for 2 hours and ended successfully. It says: \r\n`mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900`\r\n```\r\n2020-09-11 23:31:37,755 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\n```\r\n\r\n*Expected behavior:*\r\n\r\n1. PR commit status should say Failed if CodeBuild log says Failed\r\n2. CodeBuild log should not abruptly hang. It should print out the error. Currently it just terminates after printing some logs post session start.\r\n\r\n*Additional context:*\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] remote test reporting issues; Content: checklist - [x] i've prepended issue tag with type of change: [bug] - [ ] (if applicable) i've attached the script to reproduce the bug - [ ] (if applicable) i've documented below the dlc image\/dockerfile this relates to - [ ] (if applicable) i've documented below the tests i've run on the dlc image - [ ] i'm using an existing dlc image listed here: https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html - [ ] i've built my own container based off dlc (and i've attached the code used to build my own image) *concise description:* sm remote test log doesn't get reported correctly. observed in 2 commits of the pr: https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444 - https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/5dd2de96fb6f88707a030fca111ca6585534dbb8 - https:\/\/github.com\/aws\/deep-learning-containers\/pull\/444\/commits\/867d3946aabd6e30accde84337e1f76c40211730 *dlc image\/dockerfile:* mx 1.6 dlc *current behavior:* github shows \"pending\" status. codebuild logs show \"failed\" status. however, actual codebuild logs doesn't bear failure log. it terminates abruptly. ``` ============================= test session starts ============================== platform linux -- python 3.8.0, pytest-5.3.5, py-1.9.0, pluggy-0.13.1 rootdir: \/codebuild\/output\/src687836801\/src\/github.com\/aws\/deep-learning-containers\/test\/dlc_tests plugins: rerunfailures-9.0, forked-1.3.0, xdist-1.31.0, timeout-1.4.2 gw0 i \/ gw1 i \/ gw2 i \/ gw3 i \/ gw4 i \/ gw5 i \/ gw6 i \/ gw7 i gw0 [3] \/ gw1 [3] \/ gw2 [3] \/ gw3 [3] \/ gw4 [3] \/ gw5 [3] \/ gw6 [3] \/ gw7 [3] ``` sm-cloudwatch log navigating to the appropriate sm training log shows that the job ran for 2 hours and ended successfully. it says: `mx-tr-bench-gpu-4-node-py3-867d394-2020-09-11-21-28-30\/algo-1-1599859900` ``` 2020-09-11 23:31:37,755 -training-toolkit info reporting training success ``` *expected behavior:* 1. pr commit status should say failed if codebuild log says failed 2. codebuild log should not abruptly hang. it should print out the error. currently it just terminates after printing some logs post session start. *additional context:*",
        "Issue_original_content_gpt_summary":"The user encountered challenges with remote test reporting issues, where the GitHub PR commit status said \"failed\" while the CodeBuild log said \"failed\" but did not provide any additional information, and the SM-Cloudwatch log showed that the job had run for two hours and ended successfully.",
        "Issue_preprocessed_content":"Title: remote test reporting issues; Content: checklist i've prepended issue tag with type of change i've attached the script to reproduce the bug i've documented below the dlc this relates to i've documented below the tests i've run on the dlc image i'm using an existing dlc image listed here i've built my own container based off dlc concise description sm remote test log doesn't get reported correctly. observed in commits of the pr dlc mx dlc current behavior github shows pending status. codebuild logs show failed status. however, actual codebuild logs doesn't bear failure log. it terminates abruptly. log navigating to the appropriate sm training log shows that the job ran for hours and ended successfully. it says expected behavior . pr commit status should say failed if codebuild log says failed . codebuild log should not abruptly hang. it should print out the error. currently it just terminates after printing some logs post session start. additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/deep-learning-containers\/issues\/517",
        "Issue_title":"[bug] apt-get failure in sagemaker-local-test builds",
        "Issue_creation_time":1597888000000,
        "Issue_closed_time":1598551848000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"\r\n*Description:*\r\n\r\nAn apt-get error is seen in `sagemaker-local-test` builds as below. This is because `apt-get` process is already running and in active state.\r\n\r\n```\r\nE: Could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: Resource temporarily unavailable)\r\n--\r\n294 | E: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it?\r\n```\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] apt-get failure in -local-test builds; Content: *description:* an apt-get error is seen in `-local-test` builds as below. this is because `apt-get` process is already running and in active state. ``` e: could not get lock \/var\/lib\/dpkg\/lock-frontend - open (11: resource temporarily unavailable) -- 294 | e: unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), is another process using it? ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with apt-get failing in -local-test builds due to an active process already using the dpkg frontend lock.",
        "Issue_preprocessed_content":"Title: failure in builds; Content: description an error is seen in builds as below. this is because process is already running and in active state."
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1650",
        "Issue_title":"[BUG] Unable to train on multiple GPUs in Sagemaker Notebook Terminal",
        "Issue_creation_time":1649258575000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"- [ X] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\nAutogluon 0.4.0 TextPredictor training on p3.8xl 4-GPU instance in Sagemaker Notebook terminal, with `env.num_gpus: 4` setting.  I get an error in spawning multiprocessing.  When I train with everything the same, but only on a single GPU within the same instance and setup, it trains without a problem.\r\n\r\n**Expected behavior**\r\nTrain across all 4 GPUs in the p3.8xl instance with no errors.\r\n\r\n**To Reproduce**\r\n* SageMaker Notebook p3.8xl instance\r\n* python 3.7.12.  \r\n* pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0\r\n* python train.py\r\n\r\nCode:\r\nin `train.py` file\r\n```from argparse import Namespace\r\n\r\nimport pandas as pd\r\nimport awswrangler as wr\r\nfrom autogluon.text import TextPredictor\r\n\r\nargs = Namespace(\r\n    train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\",\r\n)\r\n\r\nmodel_config = {\r\n    \"eval_metric\": \"accuracy\",\r\n    \"time_limit\": 60*60*3,\r\n    \"features\": ['label', 'item_name_orig']\r\n}\r\n\r\nhyperparameters = {\r\n    'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base',\r\n    'optimization.top_k': 1,\r\n    'optimization.lr_decay': 0.9,\r\n    'optimization.learning_rate': 1e-4,\r\n    'env.precision': 32,\r\n    'env.per_gpu_batch_size': 4,\r\n    'env.num_gpus': 4\r\n}\r\n\r\ntrain_df = wr.s3.read_parquet(args.full_train_filename)\r\nprint(train_df.info())\r\n\r\npredictor = TextPredictor(\r\n    label='label',\r\n    eval_metric=model_config['eval_metric']\r\n)\r\n\r\npredictor.fit(\r\n    train_data=train_df[model_config['features']],\r\n    hyperparameters=hyperparameters,\r\n    time_limit=model_config['time_limit']\r\n)\r\n\r\n```\r\n\r\n**Screenshots**\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main\r\n    exitcode = _main(fd)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main\r\n    prepare(preparation_data)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path\r\n    run_name=\"__mp_main__\")\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/ec2-user\/SageMaker\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in <module>\r\n    time_limit=model_config['time_limit']\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit\r\n    seed=seed,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit\r\n    enable_progress_bar=self._enable_progress_bar,\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit\r\n    ckpt_path=self._ckpt_path,  # this is to resume training that was broken accidentally\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl\r\n    self._run(model, ckpt_path=ckpt_path)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run\r\n    self._dispatch()\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch\r\n    self.training_type_plugin.start_training(self)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training\r\n    self.spawn(self.new_process, trainer, self.mp_queue, return_result=False)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn\r\n    mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes)\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn\r\n    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n  File \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes\r\n    process.start()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main\r\n    is not going to be frozen to produce an executable.''')\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\nIf you are using 0.4.0 and newer, please run the following code snippet:\r\n<details>\r\n\r\n```python\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-06\r\ntime                 : 15:22:16.975165\r\npython               : 3.7.12.final.0\r\nOS                   : Linux\r\nOS-release           : 4.14.252-131.483.amzn1.x86_64\r\nVersion              : #1 SMP Mon Nov 1 20:48:11 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 32\r\ncpu_ram_mb           : 245845\r\ncuda version         : 11.450.142.00\r\nnum_gpus             : 4\r\ngpu_ram_mb           : [8404, 8476, 16160, 16160]\r\navail_disk_size_mb   : 11391\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.34\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nmatplotlib           : 3.5.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.21.5\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : None\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorchmetrics         : 0.7.3\r\ntqdm                 : 4.64.0\r\ntransformers         : 4.16.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] unable to train on multiple gpus in notebook terminal; Content: - [ x] i have checked that this bug exists on the latest stable version of autogluon - [ ] and\/or i have checked that this bug exists on the latest mainline of autogluon via source installation **describe the bug** autogluon 0.4.0 textpredictor training on p3.8xl 4-gpu instance in notebook terminal, with `env.num_gpus: 4` setting. i get an error in spawning multiprocessing. when i train with everything the same, but only on a single gpu within the same instance and setup, it trains without a problem. **expected behavior** train across all 4 gpus in the p3.8xl instance with no errors. **to reproduce** * notebook p3.8xl instance * python 3.7.12. * pip install torch==1.10.0 autogluon.text==0.4.0 awswrangler pandas autofluon.features==0.4.0 * python train.py code: in `train.py` file ```from argparse import namespace import pandas as pd import awswrangler as wr from autogluon.text import textpredictor args = namespace( train_filename = \"s3:\/\/ccds-asin-drc\/eu\/modeling-data\/mf2_no_emb\/train\/0\/train.parquet\", ) model_config = { \"eval_metric\": \"accuracy\", \"time_limit\": 60*60*3, \"features\": ['label', 'item_name_orig'] } hyperparameters = { 'model.hf_text.checkpoint_name': 'microsoft\/mdeberta-v3-base', 'optimization.top_k': 1, 'optimization.lr_decay': 0.9, 'optimization.learning_rate': 1e-4, 'env.precision': 32, 'env.per_gpu_batch_size': 4, 'env.num_gpus': 4 } train_df = wr.s3.read_parquet(args.full_train_filename) print(train_df.info()) predictor = textpredictor( label='label', eval_metric=model_config['eval_metric'] ) predictor.fit( train_data=train_df[model_config['features']], hyperparameters=hyperparameters, time_limit=model_config['time_limit'] ) ``` **screenshots** error: ``` traceback (most recent call last): file \"\", line 1, in file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 105, in spawn_main exitcode = _main(fd) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 114, in _main prepare(preparation_data) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 225, in prepare _fixup_main_from_path(data['init_main_from_path']) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 277, in _fixup_main_from_path run_name=\"__mp_main__\") file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/runpy.py\", line 263, in run_path pkg_name=pkg_name, script_name=fname) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code mod_name, mod_spec, pkg_name, script_name) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/runpy.py\", line 85, in _run_code exec(code, run_globals) file \"\/home\/ec2-user\/\/rubinome_labs\/lab\/202203_drc_multilingual\/train_textonly.py\", line 46, in time_limit=model_config['time_limit'] file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/text_prediction\/predictor.py\", line 248, in fit seed=seed, file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 410, in fit enable_progress_bar=self._enable_progress_bar, file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/autogluon\/text\/automm\/predictor.py\", line 561, in _fit ckpt_path=self._ckpt_path, # this is to resume training that was broken accidentally file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 741, in fit self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 685, in _call_and_handle_interrupt return trainer_fn(*args, **kwargs) file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 777, in _fit_impl self._run(model, ckpt_path=ckpt_path) file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1199, in _run self._dispatch() file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1279, in _dispatch self.training_type_plugin.start_training(self) file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 173, in start_training self.spawn(self.new_process, trainer, self.mp_queue, return_result=false) file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/pytorch_lightning\/plugins\/training_type\/ddp_spawn.py\", line 201, in spawn mp.spawn(self._wrapped_function, args=(function, args, kwargs, return_queue), nprocs=self.num_processes) file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 230, in spawn return start_processes(fn, args, nprocs, join, daemon, start_method='spawn') file \"\/home\/ec2-user\/myagenv\/lib\/python3.7\/site-packages\/torch\/multiprocessing\/spawn.py\", line 179, in start_processes process.start() file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/process.py\", line 112, in start self._popen = self._popen(self) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/context.py\", line 284, in _popen return popen(process_obj) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__ super().__init__(process_obj) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/popen_fork.py\", line 20, in __init__ self._launch(process_obj) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch prep_data = spawn.get_preparation_data(process_obj._name) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 143, in get_preparation_data _check_not_importing_main() file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.7\/multiprocessing\/spawn.py\", line 136, in _check_not_importing_main is not going to be frozen to produce an executable.''') runtimeerror: an attempt has been made to start a new process before the current process has finished its bootstrapping phase. this probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module: if __name__ == '__main__': freeze_support() ... the \"freeze_support()\" line can be omitted if the program is not going to be frozen to produce an executable. ``` **installed versions** which version of autogluon are you are using? if you are using 0.4.0 and newer, please run the following code snippet: ```python installed versions ------------------ date : 2022-04-06 time : 15:22:16.975165 python : 3.7.12.final.0 os : linux os-release : 4.14.252-131.483.amzn1.x86_64 version : #1 smp mon nov 1 20:48:11 utc 2021 machine : x86_64 processor : x86_64 num_cores : 32 cpu_ram_mb : 245845 cuda version : 11.450.142.00 num_gpus : 4 gpu_ram_mb : [8404, 8476, 16160, 16160] avail_disk_size_mb : 11391 autogluon.common : 0.4.0 autogluon.core : 0.4.0 autogluon.features : 0.4.0 autogluon.text : 0.4.0 autogluon_contrib_nlp: none boto3 : 1.21.34 dask : 2021.11.2 distributed : 2021.11.2 fairscale : 0.4.6 matplotlib : 3.5.1 nptyping : 1.4.4 numpy : 1.21.5 omegaconf : 2.1.1 pandas : 1.3.5 pil : 9.0.1 psutil : 5.8.0 pytorch_lightning : 1.5.10 ray : none requests : 2.27.1 scipy : 1.7.3 sentencepiece : none skimage : 0.19.2 sklearn : 1.0.2 smart_open : 5.2.1 timm : 0.5.4 torchmetrics : 0.7.3 tqdm : 4.64.0 transformers : 4.16.2 ``` **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to train Autogluon 0.4.0 TextPredictor on a p3.8xl 4-GPU instance in a notebook terminal, resulting in an error in spawning multiprocessing.",
        "Issue_preprocessed_content":"Title: unable to train on multiple gpus in notebook terminal; Content: i have checked that this bug exists on the latest stable version of autogluon i have checked that this bug exists on the latest mainline of autogluon via source installation describe the bug autogluon textpredictor training on instance in notebook terminal, with setting. i get an error in spawning multiprocessing. when i train with everything the same, but only on a single gpu within the same instance and setup, it trains without a problem. expected behavior train across all gpus in the instance with no errors. to reproduce notebook instance python pip install awswrangler pandas python code in file screenshots error installed versions which version of autogluon are you are using? if you are using and newer, please run the following code snippet details additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1644",
        "Issue_title":"[BUG] SageMaker endpoint appears unable to load model file \/ use image paths as features",
        "Issue_creation_time":1648949150000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"- [x] I have checked that this bug exists on the latest stable version of AutoGluon\r\n- [ ] and\/or I have checked that this bug exists on the latest mainline of AutoGluon via source installation\r\n\r\n**Describe the bug**\r\n```python\r\n...\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\nIt appears that the SageMaker endpoint isn't able to find \/ open the model file. I was able to use the example code in the tutorial [Deploying AutoGluon Models with AWS SageMaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html) and managed to deploy an endpoint to SageMaker. But I get this error when I go to make predictions with test data. I wonder if this might be related to transition from MXNet to Pytorch and how their artifacts are typically stored? I'm using `v0.4.0` but the predictor object is `sagemaker.mxnet.model.MXNetPredictor`. This discrepancy in framework seems supported be a related error that I found in a GitHub issue [here](https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1238).\r\n\r\nNote also that I am attempting to adapt the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset), because I'm ultimately try to deploy a multi-modal model and figure out how to pass image_paths to the SageMaker endpoint. \r\n\r\nHere's the full traceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nModelError                                Traceback (most recent call last)\r\n<ipython-input-51-abf97eb84e0a> in <module>\r\n----> 1 predictions = predictor.predict(test_data[:1].values)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id)\r\n    159             data, initial_args, target_model, target_variant, inference_id\r\n    160         )\r\n--> 161         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\r\n    162         return self._handle_response(response)\r\n    163 \r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\r\n    389                     \"%s() only accepts keyword arguments.\" % py_operation_name)\r\n    390             # The \"self\" in this scope is referring to the BaseClient.\r\n--> 391             return self._make_api_call(operation_name, kwargs)\r\n    392 \r\n    393         _api_call.__name__ = str(py_operation_name)\r\n\r\n~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\r\n    717             error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\r\n    718             error_class = self.exceptions.from_code(error_code)\r\n--> 719             raise error_class(parsed_response, operation_name)\r\n    720         else:\r\n    721             return parsed_response\r\n\r\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message \"[Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 110, in transform\r\n    self.validate_and_initialize(model_dir=model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/sagemaker_inference\/transformer.py\", line 158, in validate_and_initialize\r\n    self._model = self._model_fn(model_dir)\r\n  File \"\/opt\/ml\/model\/code\/tabular_serve.py\", line 11, in model_fn\r\n    model = TabularPredictor.load(model_dir)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2816, in load\r\n    predictor = cls._load(path=path)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2772, in _load\r\n    predictor: TabularPredictor = load_pkl.load(path=path + cls.predictor_file_name)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/common\/loaders\/load_pkl.py\", line 37, in load\r\n    with compression_fn_map[compression_fn]['open'](validated_path, 'rb', **compression_fn_kwargs) as fin:\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/.sagemaker\/mms\/models\/model\/predictor.pkl'\r\n```\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**To Reproduce**\r\n\r\n1. Train a multi modal model, using code adapted from [Multimodal Data Tables: Tabular, Text, and Image Tutorial](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html): [petfinder_train.py](https:\/\/gist.github.com\/ijmiller2\/f5837977077674fe741fee031d2bad2a)\r\n2. Deploy the pet finder model: [petfinder_deploy.py](https:\/\/gist.github.com\/ijmiller2\/f6b21c2b0b40211161d1fb0252542189)\r\n\r\n**Screenshots**\r\nNA\r\n\r\n**Installed Versions**\r\nWhich version of AutoGluon are you are using?  \r\n`0.4.0`\r\n<details>\r\n\r\n```python\r\n# Replace this code with the output of the following:\r\nINSTALLED VERSIONS\r\n------------------\r\ndate                 : 2022-04-02\r\ntime                 : 19:45:52.692253\r\npython               : 3.9.7.final.0\r\nOS                   : Linux\r\nOS-release           : 5.4.0-66-generic\r\nVersion              : #74~18.04.2-Ubuntu SMP Fri Feb 5 11:17:31 UTC 2021\r\nmachine              : x86_64\r\nprocessor            : x86_64\r\nnum_cores            : 12\r\ncpu_ram_mb           : 64324\r\ncuda version         : None\r\nnum_gpus             : 0\r\ngpu_ram_mb           : []\r\navail_disk_size_mb   : 289302\r\n\r\nautogluon.common     : 0.4.0\r\nautogluon.core       : 0.4.0\r\nautogluon.features   : 0.4.0\r\nautogluon.tabular    : 0.4.0\r\nautogluon.text       : 0.4.0\r\nautogluon.vision     : 0.4.0\r\nautogluon_contrib_nlp: None\r\nboto3                : 1.21.21\r\ncatboost             : 1.0.4\r\ndask                 : 2021.11.2\r\ndistributed          : 2021.11.2\r\nfairscale            : 0.4.6\r\nfastai               : 2.5.3\r\ngluoncv              : 0.11.0\r\nlightgbm             : 3.3.2\r\nmatplotlib           : 3.5.1\r\nnetworkx             : 2.7.1\r\nnptyping             : 1.4.4\r\nnumpy                : 1.22.3\r\nomegaconf            : 2.1.1\r\npandas               : 1.3.5\r\nPIL                  : 9.0.1\r\npsutil               : 5.8.0\r\npytorch_lightning    : 1.5.10\r\nray                  : 1.8.0\r\nrequests             : 2.27.1\r\nscipy                : 1.7.3\r\nsentencepiece        : None\r\nskimage              : 0.19.2\r\nsklearn              : 1.0.2\r\nsmart_open           : 5.2.1\r\ntimm                 : 0.5.4\r\ntorch                : 1.10.1+cpu\r\ntorchmetrics         : 0.7.2\r\ntqdm                 : 4.63.0\r\ntransformers         : 4.16.2\r\nxgboost              : 1.4.2\r\n```\r\n\r\n<\/details>\r\n\r\n**Additional context**\r\n\r\nI am attempting to follow the [tutorial to deploy a model via Sagemaker](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws-sagemaker-deployment.html), however, adapting to use the example model trained in the [Multi-Modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the PetFinder dataset).\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] endpoint appears unable to load model file \/ use image paths as features; Content: - [x] i have checked that this bug exists on the latest stable version of autogluon - [ ] and\/or i have checked that this bug exists on the latest mainline of autogluon via source installation **describe the bug** ```python ... modelerror: an error occurred (modelerror) when calling the invokeendpoint operation: received server error (500) from primary with message \"[errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl' ... filenotfounderror: [errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl' ``` it appears that the endpoint isn't able to find \/ open the model file. i was able to use the example code in the tutorial [deploying autogluon models with ](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws--deployment.html) and managed to deploy an endpoint to . but i get this error when i go to make predictions with test data. i wonder if this might be related to transition from mxnet to pytorch and how their artifacts are typically stored? i'm using `v0.4.0` but the predictor object is `.mxnet.model.mxnetpredictor`. this discrepancy in framework seems supported be a related error that i found in a github issue [here](https:\/\/github.com\/aws\/amazon--examples\/issues\/1238). note also that i am attempting to adapt the example model trained in the [multi-modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the petfinder dataset), because i'm ultimately try to deploy a multi-modal model and figure out how to pass image_paths to the endpoint. here's the full traceback: ``` --------------------------------------------------------------------------- modelerror traceback (most recent call last) in ----> 1 predictions = predictor.predict(test_data[:1].values) ~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/\/predictor.py in predict(self, data, initial_args, target_model, target_variant, inference_id) 159 data, initial_args, target_model, target_variant, inference_id 160 ) --> 161 response = self._session._runtime_client.invoke_endpoint(**request_args) 162 return self._handle_response(response) 163 ~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs) 389 \"%s() only accepts keyword arguments.\" % py_operation_name) 390 # the \"self\" in this scope is referring to the baseclient. --> 391 return self._make_api_call(operation_name, kwargs) 392 393 _api_call.__name__ = str(py_operation_name) ~\/anaconda3\/envs\/betterbin\/lib\/python3.8\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params) 717 error_code = parsed_response.get(\"error\", {}).get(\"code\") 718 error_class = self.exceptions.from_code(error_code) --> 719 raise error_class(parsed_response, operation_name) 720 else: 721 return parsed_response modelerror: an error occurred (modelerror) when calling the invokeendpoint operation: received server error (500) from primary with message \"[errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl' traceback (most recent call last): file \"\/usr\/local\/lib\/python3.8\/dist-packages\/_inference\/transformer.py\", line 110, in transform self.validate_and_initialize(model_dir=model_dir) file \"\/usr\/local\/lib\/python3.8\/dist-packages\/_inference\/transformer.py\", line 158, in validate_and_initialize self._model = self._model_fn(model_dir) file \"\/opt\/ml\/model\/code\/tabular_serve.py\", line 11, in model_fn model = tabularpredictor.load(model_dir) file \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2816, in load predictor = cls._load(path=path) file \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/tabular\/predictor\/predictor.py\", line 2772, in _load predictor: tabularpredictor = load_pkl.load(path=path + cls.predictor_file_name) file \"\/usr\/local\/lib\/python3.8\/dist-packages\/autogluon\/common\/loaders\/load_pkl.py\", line 37, in load with compression_fn_map[compression_fn]['open'](validated_path, 'rb', **compression_fn_kwargs) as fin: filenotfounderror: [errno 2] no such file or directory: '\/.\/mms\/models\/model\/predictor.pkl' ``` **expected behavior** a clear and concise description of what you expected to happen. **to reproduce** 1. train a multi modal model, using code adapted from [multimodal data tables: tabular, text, and image tutorial](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html): [petfinder_train.py](https:\/\/gist.github.com\/ijmiller2\/f5837977077674fe741fee031d2bad2a) 2. deploy the pet finder model: [petfinder_deploy.py](https:\/\/gist.github.com\/ijmiller2\/f6b21c2b0b40211161d1fb0252542189) **screenshots** na **installed versions** which version of autogluon are you are using? `0.4.0` ```python # replace this code with the output of the following: installed versions ------------------ date : 2022-04-02 time : 19:45:52.692253 python : 3.9.7.final.0 os : linux os-release : 5.4.0-66-generic version : #74~18.04.2-ubuntu smp fri feb 5 11:17:31 utc 2021 machine : x86_64 processor : x86_64 num_cores : 12 cpu_ram_mb : 64324 cuda version : none num_gpus : 0 gpu_ram_mb : [] avail_disk_size_mb : 289302 autogluon.common : 0.4.0 autogluon.core : 0.4.0 autogluon.features : 0.4.0 autogluon.tabular : 0.4.0 autogluon.text : 0.4.0 autogluon.vision : 0.4.0 autogluon_contrib_nlp: none boto3 : 1.21.21 catboost : 1.0.4 dask : 2021.11.2 distributed : 2021.11.2 fairscale : 0.4.6 fastai : 2.5.3 gluoncv : 0.11.0 lightgbm : 3.3.2 matplotlib : 3.5.1 networkx : 2.7.1 nptyping : 1.4.4 numpy : 1.22.3 omegaconf : 2.1.1 pandas : 1.3.5 pil : 9.0.1 psutil : 5.8.0 pytorch_lightning : 1.5.10 ray : 1.8.0 requests : 2.27.1 scipy : 1.7.3 sentencepiece : none skimage : 0.19.2 sklearn : 1.0.2 smart_open : 5.2.1 timm : 0.5.4 torch : 1.10.1+cpu torchmetrics : 0.7.2 tqdm : 4.63.0 transformers : 4.16.2 xgboost : 1.4.2 ``` **additional context** i am attempting to follow the [tutorial to deploy a model via ](https:\/\/auto.gluon.ai\/stable\/tutorials\/cloud_fit_deploy\/cloud-aws--deployment.html), however, adapting to use the example model trained in the [multi-modal documentation](https:\/\/auto.gluon.ai\/stable\/tutorials\/tabular_prediction\/tabular-multimodal.html) (i.e., using the petfinder dataset).",
        "Issue_original_content_gpt_summary":"The user encountered a bug when attempting to deploy a multi-modal model to , where the endpoint was unable to find\/open the model file, potentially due to a discrepancy in framework between MXNet and PyTorch.",
        "Issue_preprocessed_content":"Title: endpoint appears unable to load model file \/ use image paths as features; Content: i have checked that this bug exists on the latest stable version of autogluon i have checked that this bug exists on the latest mainline of autogluon via source installation describe the bug it appears that the endpoint isn't able to find \/ open the model file. i was able to use the example code in the tutorial and managed to deploy an endpoint to . but i get this error when i go to make predictions with test data. i wonder if this might be related to transition from mxnet to pytorch and how their artifacts are typically stored? i'm using but the predictor object is . this discrepancy in framework seems supported be a related error that i found in a github issue . note also that i am attempting to adapt the example model trained in the , because i'm ultimately try to deploy a model and figure out how to pass to the endpoint. here's the full traceback expected behavior a clear and concise description of what you expected to happen. to reproduce . train a multi modal model, using code adapted from . deploy the pet finder model screenshots na installed versions which version of autogluon are you are using? details additional context i am attempting to follow the , however, adapting to use the example model trained in the ."
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/1551",
        "Issue_title":"How to make plot_ensemble_model() work in sagemaker (or any jupyter based env)",
        "Issue_creation_time":1645398079000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Has anyone figured out an easy way to make plot_ensemble_model() work in jupyter based environments? I'm having a lot of difficulty installing pygraphviz (think it might be related to pygraphviz not able to see where graphviz is being installed? but not sure)\r\n\r\nI've tried the following code without success: \r\n%pip install python3-dev\r\n%pip install graphviz\r\n%pip install libgraphviz-dev\r\n%pip install pkg-config\r\n\r\n%pip install pygraphviz\r\n\r\n\r\nThanks for the help!",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: how to make plot_ensemble_model() work in (or any jupyter based env); Content: has anyone figured out an easy way to make plot_ensemble_model() work in jupyter based environments? i'm having a lot of difficulty installing pygraphviz (think it might be related to pygraphviz not able to see where graphviz is being installed? but not sure) i've tried the following code without success: %pip install python3-dev %pip install graphviz %pip install libgraphviz-dev %pip install pkg-config %pip install pygraphviz thanks for the help!",
        "Issue_original_content_gpt_summary":"The user is encountering difficulty installing pygraphviz in order to make plot_ensemble_model() work in a jupyter based environment.",
        "Issue_preprocessed_content":"Title: how to make work in ; Content: has anyone figured out an easy way to make work in jupyter based environments? i'm having a lot of difficulty installing pygraphviz i've tried the following code without success %pip install %pip install graphviz %pip install %pip install %pip install pygraphviz thanks for the help!"
    },
    {
        "Issue_link":"https:\/\/github.com\/autogluon\/autogluon\/issues\/268",
        "Issue_title":"ImportError for TabularPrediction in SageMaker Notebook Instance",
        "Issue_creation_time":1580947939000,
        "Issue_closed_time":1613263024000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I got **ImportError** when trying to use AutoGluon in a SageMaker instance (ml.c5d.4xlarge), with kernel being **conda_python3**.\r\n\r\nThe error I got is:\r\n```\r\nfrom autogluon import TabularPrediction as task\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-6f7d1b4fed2f> in <module>()\r\n----> 1 from autogluon import TabularPrediction as task\r\n\r\nImportError: cannot import name 'TabularPrediction'\r\n```\r\n\r\nIf I try\r\n```\r\nimport autogluon as ag\r\nag.TabularPrediction.Dataset(file_path='data\/nbc_golf_model_1_training.csv')\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-a8e4ec84df4b> in <module>()\r\n----> 1 ag.TabularPrediction.Dataset(file_path='nbc_golf_model_1_training.csv')\r\n\r\nAttributeError: module 'autogluon' has no attribute 'TabularPrediction'\r\n```\r\n\r\nFor your reference:\r\n\r\nI installed AutoGluon by using Version PIP in the notebook as usual.\r\n```\r\n!pip install --upgrade mxnet\r\n!pip install autogluon\r\n```\r\n\r\n```\r\nCollecting mxnet\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/6c\/c6e5562f8face683cec73f5d4d74a58f8572c0595d54f1fed9d923020bbd\/mxnet-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (25.4MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25.4MB 1.9MB\/s eta 0:00:01\r\nRequirement not upgraded as not directly required: requests<3,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (2.20.0)\r\nRequirement not upgraded as not directly required: graphviz<0.9.0,>=0.8.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (0.8.4)\r\nRequirement not upgraded as not directly required: numpy<2.0.0,>1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from mxnet) (1.16.4)\r\nRequirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\r\nRequirement not upgraded as not directly required: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2.6)\r\nRequirement not upgraded as not directly required: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (2019.9.11)\r\nRequirement not upgraded as not directly required: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests<3,>=2.20.0->mxnet) (1.23)\r\nInstalling collected packages: mxnet\r\nSuccessfully installed mxnet-1.5.1.post0\r\n```\r\nThere are 2 errors in the second installation step:\r\n\r\n**ERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.**\r\n```\r\nCollecting autogluon\r\n  Downloading autogluon-0.0.5-py3-none-any.whl (328 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 328 kB 18.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: tornado>=5.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.0.2)\r\nRequirement already satisfied: cryptography>=2.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.8)\r\nCollecting lightgbm==2.3.0\r\n  Downloading lightgbm-2.3.0-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 33.4 MB\/s eta 0:00:01\r\nRequirement already satisfied: paramiko>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.6.0)\r\nCollecting scipy>=1.3.3\r\n  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 32.8 MB\/s eta 0:00:01\r\nCollecting boto3==1.9.187\r\n  Downloading boto3-1.9.187-py2.py3-none-any.whl (128 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 36.5 MB\/s eta 0:00:01\r\nRequirement already satisfied: cython in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.28.2)\r\nCollecting scikit-optimize\r\n  Downloading scikit_optimize-0.7.1-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 10.7 MB\/s eta 0:00:01\r\nRequirement already satisfied: Pillow<=6.2.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.2.0)\r\nCollecting catboost\r\n  Downloading catboost-0.21-cp36-none-manylinux1_x86_64.whl (64.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64.0 MB 36.8 MB\/s eta 0:00:01\r\nCollecting gluonnlp==0.8.1\r\n  Downloading gluonnlp-0.8.1.tar.gz (236 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 236 kB 63.1 MB\/s eta 0:00:01\r\nRequirement already satisfied: psutil>=5.0.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (5.6.3)\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.24.2)\r\nRequirement already satisfied: graphviz in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (0.8.4)\r\nCollecting dask==2.6.0\r\n  Downloading dask-2.6.0-py3-none-any.whl (760 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 760 kB 66.0 MB\/s eta 0:00:01\r\nRequirement already satisfied: requests in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (2.20.0)\r\nCollecting scikit-learn==0.21.2\r\n  Downloading scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.7 MB 32.2 MB\/s eta 0:00:01\r\nCollecting distributed==2.6.0\r\n  Downloading distributed-2.6.0-py3-none-any.whl (560 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560 kB 70.9 MB\/s eta 0:00:01\r\nRequirement already satisfied: matplotlib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (3.0.3)\r\nCollecting ConfigSpace<=0.4.10\r\n  Downloading ConfigSpace-0.4.10.tar.gz (882 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 882 kB 72.3 MB\/s eta 0:00:01\r\nCollecting tqdm>=4.38.0\r\n  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.6 MB\/s eta 0:00:01\r\nCollecting gluoncv>=0.5.0\r\n  Downloading gluoncv-0.6.0-py2.py3-none-any.whl (693 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 693 kB 69.3 MB\/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.16.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from autogluon) (1.16.4)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.5)\r\nRequirement already satisfied: six>=1.4.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cryptography>=2.8->autogluon) (1.11.0)\r\nRequirement already satisfied: bcrypt>=3.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (3.1.7)\r\nRequirement already satisfied: pynacl>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from paramiko>=2.5.0->autogluon) (1.3.0)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.9.4)\r\nRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from boto3==1.9.187->autogluon) (0.2.1)\r\nCollecting botocore<1.13.0,>=1.12.187\r\n  Downloading botocore-1.12.253-py2.py3-none-any.whl (5.7 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.7 MB 47.6 MB\/s eta 0:00:01\r\nCollecting pyaml\r\n  Downloading pyaml-19.12.0-py2.py3-none-any.whl (17 kB)\r\nCollecting joblib\r\n  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 294 kB 55.6 MB\/s eta 0:00:01\r\nRequirement already satisfied: plotly in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from catboost->autogluon) (4.2.1)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2018.4)\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->autogluon) (2.7.3)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2.6)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (1.23)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from requests->autogluon) (2019.9.11)\r\nRequirement already satisfied: tblib in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.3.2)\r\nRequirement already satisfied: pyyaml in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (3.12)\r\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (1.5.10)\r\nRequirement already satisfied: msgpack in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.6.0)\r\nRequirement already satisfied: zict>=0.1.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.1.3)\r\nRequirement already satisfied: toolz>=0.7.4 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.9.0)\r\nRequirement already satisfied: cloudpickle>=0.2.2 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (0.5.3)\r\nRequirement already satisfied: click>=6.6 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from distributed==2.6.0->autogluon) (6.7)\r\nRequirement already satisfied: cycler>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (1.0.1)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from matplotlib->autogluon) (2.2.0)\r\nRequirement already satisfied: typing in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from ConfigSpace<=0.4.10->autogluon) (3.6.4)\r\nCollecting portalocker\r\n  Downloading portalocker-1.5.2-py2.py3-none-any.whl (14 kB)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->autogluon) (2.18)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from botocore<1.13.0,>=1.12.187->boto3==1.9.187->autogluon) (0.14)\r\nRequirement already satisfied: retrying>=1.3.3 in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from plotly->catboost->autogluon) (1.3.3)\r\nRequirement already satisfied: heapdict in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from zict>=0.1.3->distributed==2.6.0->autogluon) (1.0.0)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages (from kiwisolver>=1.0.1->matplotlib->autogluon) (39.1.0)\r\nBuilding wheels for collected packages: gluonnlp, ConfigSpace\r\n  Building wheel for gluonnlp (setup.py) ... done\r\n  Created wheel for gluonnlp: filename=gluonnlp-0.8.1-py3-none-any.whl size=289392 sha256=3eba5a08b1bdd7719e9e6d869c3029e8aae5eb848f58c3f30ad5d42fe0969b9f\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/cb\/1c\/e6fb5e5eefcd5fe8ee2163f27c79a63c96d9a956e8d93fb496\r\n  Building wheel for ConfigSpace (setup.py) ... done\r\n  Created wheel for ConfigSpace: filename=ConfigSpace-0.4.10-cp36-cp36m-linux_x86_64.whl size=3000873 sha256=35ce111cf113601a2e6543690fb721b2449622e0c010e0b6bc094a498890edc4\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/70\/71\/a2\/00ca7cb0f71294d73e8791d6fe5cd0c7401066ec3b7e1026db\r\nSuccessfully built gluonnlp ConfigSpace\r\nERROR: sagemaker 1.43.4.post1 has requirement boto3>=1.9.213, but you'll have boto3 1.9.187 which is incompatible.\r\nERROR: awscli 1.16.283 has requirement botocore==1.13.19, but you'll have botocore 1.12.253 which is incompatible.\r\nInstalling collected packages: scipy, joblib, scikit-learn, lightgbm, botocore, boto3, pyaml, scikit-optimize, catboost, gluonnlp, dask, distributed, ConfigSpace, tqdm, portalocker, gluoncv, autogluon\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.2.1\r\n    Uninstalling scipy-1.2.1:\r\n      Successfully uninstalled scipy-1.2.1\r\n  Attempting uninstall: scikit-learn\r\n    Found existing installation: scikit-learn 0.20.3\r\n    Uninstalling scikit-learn-0.20.3:\r\n      Successfully uninstalled scikit-learn-0.20.3\r\n  Attempting uninstall: botocore\r\n    Found existing installation: botocore 1.13.19\r\n    Uninstalling botocore-1.13.19:\r\n      Successfully uninstalled botocore-1.13.19\r\n  Attempting uninstall: boto3\r\n    Found existing installation: boto3 1.10.19\r\n    Uninstalling boto3-1.10.19:\r\n      Successfully uninstalled boto3-1.10.19\r\n  Attempting uninstall: dask\r\n    Found existing installation: dask 0.17.5\r\n    Uninstalling dask-0.17.5:\r\n      Successfully uninstalled dask-0.17.5\r\n  Attempting uninstall: distributed\r\n    Found existing installation: distributed 1.21.8\r\n    Uninstalling distributed-1.21.8:\r\n      Successfully uninstalled distributed-1.21.8\r\nSuccessfully installed ConfigSpace-0.4.10 autogluon-0.0.5 boto3-1.9.187 botocore-1.12.253 catboost-0.21 dask-2.6.0 distributed-2.6.0 gluoncv-0.6.0 gluonnlp-0.8.1 joblib-0.14.1 lightgbm-2.3.0 portalocker-1.5.2 pyaml-19.12.0 scikit-learn-0.21.2 scikit-optimize-0.7.1 scipy-1.4.1 tqdm-4.42.1\r\n```\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: importerror for tabularprediction in notebook instance; Content: i got **importerror** when trying to use autogluon in a instance (ml.c5d.4xlarge), with kernel being **conda_python3**. the error i got is: ``` from autogluon import tabularprediction as task ``` ``` --------------------------------------------------------------------------- importerror traceback (most recent call last) in () ----> 1 from autogluon import tabularprediction as task importerror: cannot import name 'tabularprediction' ``` if i try ``` import autogluon as ag ag.tabularprediction.dataset(file_path='data\/nbc_golf_model_1_training.csv') ``` ``` --------------------------------------------------------------------------- attributeerror traceback (most recent call last) in () ----> 1 ag.tabularprediction.dataset(file_path='nbc_golf_model_1_training.csv') attributeerror: module 'autogluon' has no attribute 'tabularprediction' ```",
        "Issue_original_content_gpt_summary":"The user encountered an ImportError and AttributeError when trying to use AutoGluon's tabularprediction module in a notebook instance with a Conda Python 3 kernel.",
        "Issue_preprocessed_content":"Title: importerror for tabularprediction in notebook instance; Content: i got importerror when trying to use autogluon in a instance , with kernel being the error i got is if i try for your reference i installed autogluon by using version pip in the notebook as usual. there are errors in the second installation step error has requirement but you'll have boto which is incompatible. error awscli has requirement but you'll have botocore which is"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/1039",
        "Issue_title":"Issue with installing GlounTS on Sagemaker notebook instance from Github",
        "Issue_creation_time":1600235220000,
        "Issue_closed_time":1600271697000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\nI am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/GluonTS_SageMaker_SDK_Tutorial.ipynb but at first step when I ran `!pip install --upgrade mxnet==1.6  git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` I got the following error,\r\n\r\n## Error message or code output\r\n```Obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]\r\n  Updating .\/src\/gluonts clone\r\n  Running command git fetch -q --tags\r\n  Running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel Check the logs for full command output.\r\n\r\n```\r\n\r\n\r\n## Environment\r\nNote: Previously, I installed Gluon-TS (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if I do `! pip list` I can see the package is installed but when I ran `!pip uninstall glounts` it says `WARNING: Skipping glounts as it is not installed.`\r\n\r\n- Operating system: Sagemaker notebook instance with conda_mxnet_p36 kernel.\r\n- Python version: 3.6\r\n- GluonTS version: 0.5.2 is already installed.\r\n- MXNet version:1.6",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: issue with installing glounts on notebook instance from github; Content: ## description i am following the instructions on https:\/\/github.com\/awslabs\/gluon-ts\/blob\/acfd7e14c4ef6eaa62fea6d6233a9e336f6366e4\/examples\/gluonts__sdk_tutorial.ipynb but at first step when i ran `!pip install --upgrade mxnet==1.6 git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev]` i got the following error, ## error message or code output ```obtaining gluonts[dev] from git+https:\/\/github.com\/awslabs\/gluon-ts.git#egg=gluonts[dev] updating .\/src\/gluonts clone running command git fetch -q --tags running command git reset --hard -q fc203f51f01036e854ce6a0da1a43b562074e187 installing build dependencies ... error error: command errored out with exit status 1: command: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel cwd: none complete output (14 lines): traceback (most recent call last): file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main \"__main__\", mod_spec) file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/runpy.py\", line 85, in _run_code exec(code, run_globals) file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/__main__.py\", line 16, in from pip._internal.cli.main import main as _main # isort:skip # noqa file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip\/_internal\/cli\/main.py\", line 5, in import locale file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/locale.py\", line 16, in import re file \"\/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/re.py\", line 142, in class regexflag(enum.intflag): attributeerror: module 'enum' has no attribute 'intflag' ---------------------------------------- error: command errored out with exit status 1: \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/bin\/python \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pip install --ignore-installed --no-user --prefix \/tmp\/pip-build-env-_u9w80jg\/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https:\/\/pypi.org\/simple -- 'setuptools>=40.8.0' wheel check the logs for full command output. ``` ## environment note: previously, i installed gluon-ts (0.5.2) using `! pip install --upgrade mxnet==1.6 gluonts` and if i do `! pip list` i can see the package is installed but when i ran `!pip uninstall glounts` it says `warning: skipping glounts as it is not installed.` - operating system: notebook instance with conda_mxnet_p36 kernel. - python version: 3.6 - gluonts version: 0.5.2 is already installed. - mxnet version:1.6",
        "Issue_original_content_gpt_summary":"The user encountered an issue with installing glounts on a notebook instance from github, resulting in an error message due to an attributeerror with the 'enum' module.",
        "Issue_preprocessed_content":"Title: issue with installing glounts on notebook instance from github; Content: description i am following the instructions on but at first step when i ran i got the following error, error message or code output environment note previously, i installed using and if i do i can see the package is installed but when i ran it says operating system notebook instance with kernel. python version gluonts version is already installed. mxnet"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/gluonts\/issues\/426",
        "Issue_title":"Problems using GPU with Amazon SageMaker on AWS-instance \"ml.p2.xlarge\".",
        "Issue_creation_time":1573166280000,
        "Issue_closed_time":1573208758000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## Description\r\nUsing Amazon SageMaker on an AWS GPU-instance \"ml.p2.xlarge\", I was not able to run the example `benchmark_m4.py` script (copy\/pasted in SageMaker) on GPU. \r\n\r\n## To Reproduce\r\nAfter starting the instance: \r\n```\r\n!pip install gluonts\r\n```\r\n\r\nNext cell: paste the slightly modified script `benchmark_m4.py` with a little modification:\r\n \r\n```python\r\nestimators = [\r\n    partial(\r\n        DeepAREstimator,\r\n        trainer=Trainer(\r\n            epochs=epochs, \r\n            num_batches_per_epoch=num_batches_per_epoch,\r\n            ctx=\"gpu\"\r\n        ),\r\n    ),\r\n]\r\n```\r\n(without specifying the context this works fine, but is only running on CPU)\r\n\r\n## Error Message\r\n\r\n```\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_quarterly.\r\nINFO:root:Start model training\r\nINFO:root:using dataset already processed in path \/home\/ec2-user\/.mxnet\/gluon-ts\/datasets\/m4_yearly.\r\nINFO:root:Start model training\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[24000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"3M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=8, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='3M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='24000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=8>, train=<gluonts.dataset.common.FileDataset object at 0x7f9377c9e748>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\nevaluating gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[23000], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.student_t.StudentTOutput(), dropout_rate=0.1, embedding_dimension=20, freq=\"12M\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=6, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=mxnet.context.Context(\"gpu\", 0), epochs=100, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=200, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True) on TrainDatasets(metadata=<MetaData freq='12M' target=None feat_static_cat=[<CategoricalFeatureInfo name='feat_static_cat' cardinality='23000'>] feat_static_real=[] feat_dynamic_real=[] feat_dynamic_cat=[] prediction_length=6>, train=<gluonts.dataset.common.FileDataset object at 0x7f937812ce48>, test=<gluonts.dataset.common.FileDataset object at 0x7f9377c53208>)\r\n[22:17:01] src\/ndarray\/ndarray.cc:1279: GPU is not enabled\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::CopyFromTo(mxnet::NDArray const&, mxnet::NDArray const&, int, bool)+0x723) [0x7f9397cf7623]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::imperative::PushFComputeEx(std::function<void (nnvm::NodeAttrs const&, mxnet::OpContext const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::NDArray, std::allocator<mxnet::NDArray> > const&)> const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&)+0x47e) [0x7f9397bad59e]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x839) [0x7f9397bb28f9]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n\r\n\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-15-b3fbc3bdf424> in <module>()\r\n     88             \"MASE\",\r\n     89             \"sMAPE\",\r\n---> 90             \"MSIS\",\r\n     91         ]\r\n     92     ]\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key)\r\n   2999             if is_iterator(key):\r\n   3000                 key = list(key)\r\n-> 3001             indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\r\n   3002 \r\n   3003         # take() does not accept boolean indexers\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing)\r\n   1283                 # When setting, missing keys are not allowed, even with .loc:\r\n   1284                 kwargs = {\"raise_missing\": True if is_setter else raise_missing}\r\n-> 1285                 return self._get_listlike_indexer(obj, axis, **kwargs)[1]\r\n   1286         else:\r\n   1287             try:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing)\r\n   1090 \r\n   1091         self._validate_read_indexer(\r\n-> 1092             keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\r\n   1093         )\r\n   1094         return keyarr, indexer\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing)\r\n   1175                 raise KeyError(\r\n   1176                     \"None of [{key}] are in the [{axis}]\".format(\r\n-> 1177                         key=key, axis=self.obj._get_axis_name(axis)\r\n   1178                     )\r\n   1179                 )\r\n\r\nKeyError: \"None of [Index(['dataset', 'estimator', 'RMSE', 'mean_wQuantileLoss', 'MASE', 'sMAPE',\\n       'MSIS'],\\n      dtype='object')] are in the [columns]\"\r\n```\r\n\r\n## Other\r\nIn addition, before installing gluonts (from https:\/\/beta.mxnet.io\/guide\/crash-course\/6-use_gpus.html): \r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n[[1. 1. 1. 1.]\r\n [1. 1. 1. 1.]\r\n [1. 1. 1. 1.]]\r\n<NDArray 3x4 @gpu(0)>\r\n```\r\n\r\nAfter installing gluonts: \r\n\r\n```python\r\nx = nd.ones((3,4), ctx=gpu())\r\nx\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nMXNetError                                Traceback (most recent call last)\r\n<ipython-input-16-749bd657d613> in <module>()\r\n      5 \r\n      6 \r\n----> 7 x = nd.ones((3,4), ctx=gpu())\r\n      8 x\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/ndarray.py in ones(shape, ctx, dtype, **kwargs)\r\n   2419     dtype = mx_real_t if dtype is None else dtype\r\n   2420     # pylint: disable= no-member, protected-access\r\n-> 2421     return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)\r\n   2422     # pylint: enable= no-member, protected-access\r\n   2423 \r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/ndarray\/register.py in _ones(shape, ctx, dtype, out, name, **kwargs)\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/_ctypes\/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/base.py in check_call(ret)\r\n    250     \"\"\"\r\n    251     if ret != 0:\r\n--> 252         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    253 \r\n    254 \r\n\r\nMXNetError: [22:29:51] src\/imperative\/imperative.cc:79: Operator _ones is not implemented for GPU.\r\n\r\nStack trace returned 10 entries:\r\n[bt] (0) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23d55a) [0x7f93951c155a]\r\n[bt] (1) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x23dbc1) [0x7f93951c1bc1]\r\n[bt] (2) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::InvokeOp(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode, mxnet::OpStatePtr)+0x9fb) [0x7f9397bb2abb]\r\n[bt] (3) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(mxnet::Imperative::Invoke(mxnet::Context const&, nnvm::NodeAttrs const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&)+0x38c) [0x7f9397bb317c]\r\n[bt] (4) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(+0x2b34989) [0x7f9397ab8989]\r\n[bt] (5) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/mxnet\/libmxnet.so(MXImperativeInvokeEx+0x6f) [0x7f9397ab8f7f]\r\n[bt] (6) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call_unix64+0x4c) [0x7f93d58efec0]\r\n[bt] (7) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/..\/..\/libffi.so.6(ffi_call+0x22d) [0x7f93d58ef87d]\r\n[bt] (8) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7f93d5b04e2e]\r\n[bt] (9) \/home\/ec2-user\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/lib-dynload\/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x12865) [0x7f93d5b05865]\r\n```\r\n\r\n## Environment\r\n\r\n- Amazon SageMaker, running on AWS instance \"ml.p2.xlarge\". \r\n- GluonTS version: 0.3.3 installed using pip.\r\n- Kernel: conda_mxnet_p36 \r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: problems using gpu with on aws-instance \"ml.p2.xlarge\".; Content: ## description using on an aws gpu-instance \"ml.p2.xlarge\", i was not able to run the example `benchmark_m4.py` script (copy\/pasted in ) on gpu. ## to reproduce after starting the instance: ``` !pip install gluonts ``` next cell: paste the slightly modified script `benchmark_m4.py` with a little modification: ```python estimators = [ partial( deeparestimator, trainer=trainer( epochs=epochs, num_batches_per_epoch=num_batches_per_epoch, ctx=\"gpu\" ), ), ] ``` (without specifying the context this works fine, but is only running on cpu) ## error message ```keyerror traceback (most recent call last) in () 88 \"mase\", 89 \"smape\", ---> 90 \"msis\", 91 ] 92 ] ~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/frame.py in __getitem__(self, key) 2999 if is_iterator(key): 3000 key = list(key) -> 3001 indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=true) 3002 3003 # take() does not accept boolean indexers ~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _convert_to_indexer(self, obj, axis, is_setter, raise_missing) 1283 # when setting, missing keys are not allowed, even with .loc: 1284 kwargs = {\"raise_missing\": true if is_setter else raise_missing} -> 1285 return self._get_listlike_indexer(obj, axis, **kwargs)[1] 1286 else: 1287 try: ~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _get_listlike_indexer(self, key, axis, raise_missing) 1090 1091 self._validate_read_indexer( -> 1092 keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing 1093 ) 1094 return keyarr, indexer ~\/anaconda3\/envs\/mxnet_p36\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing) 1175 raise keyerror( 1176 \"none of [{key}] are in the [{axis}]\".format( -> 1177 key=key, axis=self.obj._get_axis_name(axis) 1178 ) 1179 ) keyerror: \"none of [index(['dataset', 'estimator', 'rmse', 'mean_wquantileloss', 'mase', 'smape',\\n 'msis'],\\n dtype='object')] are in the [columns]\" ```",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to run the example script 'benchmark_m4.py' on a GPU-instance \"ml.p2.xlarge\" on AWS, resulting in a KeyError.",
        "Issue_preprocessed_content":"Title: problems using gpu with on ; Content: description using on an aws i was not able to run the example script on gpu. to reproduce after starting the instance next cell paste the slightly modified script with a little modification without specifying the context this works fine, but is only running on cpu error message other in addition, before installing gluonts after installing gluonts environment , running on aws instance gluonts version installed using pip. kernel"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro\/issues\/308",
        "Issue_title":"Sagemaker notebooks raise error for `pandas.CSVDataSet`",
        "Issue_creation_time":1585713762000,
        "Issue_closed_time":1585791347000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## Description\r\nThe conda environment for python3.6 in notebooks cannot find `pandas.CSVDataSet`\r\n\r\n## Context\r\nI'm wanting to use sagemaker as my development environment. However, I cannot get kedro to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines).\r\n\r\n## Steps to Reproduce\r\n\r\n0. Startup a Sagemaker instance with defaults\r\n\r\nTerminal success:\r\n\r\n1. `pip install kedro` in the terminal\r\n2. `kedro new`\r\n2a. `testing` for name\r\n2b. `y` for example project\r\n3. `cd testing; kedro run` => Success!\r\n\r\nNotebook fail:\r\n1. Create a new `conda_python3` notebook in `testing\/notebooks\/`\r\n2. `!pip install kedro` in a notebook \r\n> The environments for the terminal and notebooks are separate by design in Sagemaker\r\n2. Load the kedro context as described [here](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run-kedro-jupyter-notebook) \r\n> Note that I've started to use the code below; Without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `Path.cwd()` to point to the root dir not `notebook\/`, as intended.\r\n```\r\nif \"current_dir\" not in locals():\r\n    # Check it exists first. For some reason this is not an idempotent operation?\r\n    current_dir = Path.cwd()  # this points to 'notebooks\/' folder\r\nproj_path = current_dir.parent  # point back to the root of the project\r\ncontext = load_context(proj_path)\r\n```\r\n3. Run `context.catalog.list()`\r\n\r\n## Expected Result\r\nThe notebook should print:\r\n```\r\n['example_iris_data',\r\n 'parameters',\r\n 'params:example_test_data_ratio',\r\n 'params:example_num_train_iter',\r\n 'params:example_learning_rate']\r\n```\r\n\r\n## Actual Result\r\n```\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\nFull trace.\r\n```\r\n---------------------------------------------------------------------------\r\nStopIteration                             Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    416         try:\r\n--> 417             class_obj = next(obj for obj in trials if obj is not None)\r\n    418         except StopIteration:\r\n\r\nStopIteration: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    148             class_obj, config = parse_dataset_definition(\r\n--> 149                 config, load_version, save_version\r\n    150             )\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in parse_dataset_definition(config, load_version, save_version)\r\n    418         except StopIteration:\r\n--> 419             raise DataSetError(\"Class `{}` not found.\".format(class_obj))\r\n    420 \r\n\r\nDataSetError: Class `pandas.CSVDataSet` not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataSetError                              Traceback (most recent call last)\r\n<ipython-input-4-5848382c8bb9> in <module>()\r\n----> 1 context.catalog.list()\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in catalog(self)\r\n    206 \r\n    207         \"\"\"\r\n--> 208         return self._get_catalog()\r\n    209 \r\n    210     @property\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _get_catalog(self, save_version, journal, load_versions)\r\n    243         conf_creds = self._get_config_credentials()\r\n    244         catalog = self._create_catalog(\r\n--> 245             conf_catalog, conf_creds, save_version, journal, load_versions\r\n    246         )\r\n    247         catalog.add_feed_dict(self._get_feed_dict())\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions)\r\n    267             save_version=save_version,\r\n    268             journal=journal,\r\n--> 269             load_versions=load_versions,\r\n    270         )\r\n    271 \r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal)\r\n    298             ds_config = _resolve_credentials(ds_config, credentials)\r\n    299             data_sets[ds_name] = AbstractDataSet.from_config(\r\n--> 300                 ds_name, ds_config, load_versions.get(ds_name), save_version\r\n    301             )\r\n    302         return cls(data_sets=data_sets, journal=journal)\r\n\r\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/kedro\/io\/core.py in from_config(cls, name, config, load_version, save_version)\r\n    152             raise DataSetError(\r\n    153                 \"An exception occurred when parsing config \"\r\n--> 154                 \"for DataSet `{}`:\\n{}\".format(name, str(ex))\r\n    155             )\r\n    156 \r\n\r\nDataSetError: An exception occurred when parsing config for DataSet `example_iris_data`:\r\nClass `pandas.CSVDataSet` not found.\r\n```\r\n\r\n## Investigations so far\r\n\r\n### `CSVLocalDataSet`\r\nUpon changing the yaml type for iris.csv from `pandas.CSVDataSet` to `CSVLocalDataSet`, we get success on both the terminal and the notebook. However, this is not my desired outcome; The transition to using `pandas.CSVDataSet` makes it easier, for me at least, to use both S3 and local datasets.\r\n\r\n### `pip install kedro` output from notebook\r\n```\r\nCollecting kedro\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/67\/6f\/4faaa0e58728a318aeabc490271a636f87f6b9165245ce1d3adc764240cf\/kedro-0.15.8-py3-none-any.whl (12.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.5MB 4.1MB\/s eta 0:00:01\r\nRequirement already satisfied: xlsxwriter<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.0.4)\r\nCollecting azure-storage-file<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c9\/33\/6c611563412ffc409b2413ac50e3a063133ea235b86c137759774c77f3ad\/azure_storage_file-1.4.0-py2.py3-none-any.whl\r\nCollecting fsspec<1.0,>=0.5.1 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6e\/2b\/63420d49d5e5f885451429e9e0f40ad1787eed0d32b1aedd6b10f9c2719a\/fsspec-0.7.1-py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 33.5MB\/s ta 0:00:01\r\nRequirement already satisfied: pandas<1.0,>=0.24.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (0.24.2)\r\nCollecting s3fs<1.0,>=0.3.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b8\/e4\/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675\/s3fs-0.4.2-py3-none-any.whl\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nCollecting tables<3.6,>=3.4.4 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/f7\/bb0ec32a3f3dd74143a3108fbf737e6dcfd47f0ffd61b52af7106ab7a38a\/tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3MB 10.2MB\/s ta 0:00:01\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (2.20.0)\r\nCollecting toposort<2.0,>=1.5 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e9\/8a\/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4\/toposort-1.5-py2.py3-none-any.whl\r\nRequirement already satisfied: click<8.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (6.7)\r\nCollecting azure-storage-queue<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/72\/94\/4db044f1c155b40c5ebc037bfd9d1c24562845692c06798fbe869fe160e6\/azure_storage_queue-1.4.0-py2.py3-none-any.whl\r\nCollecting cookiecutter<2.0,>=1.6.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/86\/c9\/7184edfb0e89abedc37211743d1420810f6b49ae4fa695dfc443c273470d\/cookiecutter-1.7.0-py2.py3-none-any.whl (40kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40kB 24.6MB\/s ta 0:00:01\r\nCollecting pandas-gbq<1.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/c3\/74\/126408f6bdb7b2cb1dcb8c6e4bd69a511a7f85792d686d1237d9825e6194\/pandas_gbq-0.13.1-py3-none-any.whl\r\nCollecting pip-tools<5.0.0,>=4.0.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/94\/8f\/59495d651f3ced9b06b69545756a27296861a6edd6c5709fbe1265ed9032\/pip_tools-4.5.1-py2.py3-none-any.whl (41kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 27.5MB\/s ta 0:00:01\r\nCollecting azure-storage-blob<2.0,>=1.1.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/25\/f4\/a307ed89014e9abb5c5cfc8ca7f8f797d12f619f17a6059a6fd4b153b5d0\/azure_storage_blob-1.5.0-py2.py3-none-any.whl (75kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 35.2MB\/s ta 0:00:01\r\nCollecting pyarrow<1.0.0,>=0.12.0 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/ba\/10\/93fad5849418eade4a4cd581f8cd27be1bbe51e18968ba1492140c887f3f\/pyarrow-0.16.0-cp36-cp36m-manylinux1_x86_64.whl (62.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62.9MB 779kB\/s eta 0:00:01    40% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 25.7MB 56.1MB\/s eta 0:00:01\r\nRequirement already satisfied: SQLAlchemy<2.0,>=1.2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.2.11)\r\nRequirement already satisfied: xlrd<2.0,>=1.0.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from kedro) (1.1.0)\r\nCollecting python-json-logger<1.0,>=0.1.9 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/80\/9d\/1c3393a6067716e04e6fcef95104c8426d262b4adaf18d7aa2470eab028d\/python-json-logger-0.1.11.tar.gz\r\nCollecting anyconfig<1.0,>=0.9.7 (from kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/4c\/00\/cc525eb0240b6ef196b98300d505114339bbb7ddd68e3155483f1eb32050\/anyconfig-0.9.10.tar.gz (103kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112kB 34.4MB\/s ta 0:00:01\r\nCollecting azure-storage-common~=1.4 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/6c\/b2285bf3687768dbf61b6bc085b0c1be2893b6e2757a9d023263764177f3\/azure_storage_common-1.4.2-py2.py3-none-any.whl (47kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 25.9MB\/s ta 0:00:01\r\nCollecting azure-common>=1.1.5 (from azure-storage-file<2.0,>=1.1.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/e5\/4d\/d000fc3c5af601d00d55750b71da5c231fcb128f42ac95b208ed1091c2c1\/azure_common-1.1.25-py2.py3-none-any.whl\r\nRequirement already satisfied: python-dateutil>=2.5.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.7.3)\r\nRequirement already satisfied: numpy>=1.12.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.14.3)\r\nRequirement already satisfied: pytz>=2011k in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2018.4)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (4.0.1)\r\nRequirement already satisfied: numexpr>=2.6.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (2.6.5)\r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.11.0)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.23)\r\nRequirement already satisfied: idna<2.8,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.6)\r\nCollecting whichcraft>=0.4.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/b5\/a2\/81887a0dae2e4d2adc70d9a3557fdda969f863ced51cd3c47b587d25bce5\/whichcraft-0.6.1-py2.py3-none-any.whl\r\nCollecting future>=0.15.2 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/45\/0b\/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9\/future-0.18.2.tar.gz (829kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 829kB 27.8MB\/s ta 0:00:01\r\nCollecting poyo>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/42\/50\/0b0820601bde2eda403f47b9a4a1f270098ed0dd4c00c443d883164bdccc\/poyo-0.5.0-py2.py3-none-any.whl\r\nCollecting binaryornot>=0.2.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/24\/7e\/f7b6f453e6481d1e233540262ccbfcf89adcd43606f44a028d7f5fae5eb2\/binaryornot-0.4.4-py2.py3-none-any.whl\r\nCollecting jinja2-time>=0.1.0 (from cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/6a\/a1\/d44fa38306ffa34a7e1af09632b158e13ec89670ce491f8a15af3ebcb4e4\/jinja2_time-0.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.10)\r\nCollecting google-auth-oauthlib (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/7b\/b8\/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b\/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\r\nCollecting google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/b0\/cc391ebf8ebf7855cdcfe0a9a4cdc8dcd90287c90e1ac22651d104ac6481\/google_auth-1.12.0-py2.py3-none-any.whl (83kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 35.5MB\/s ta 0:00:01\r\nCollecting pydata-google-auth (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/87\/ed\/9c9f410c032645632de787b8c285a78496bd89590c777385b921eb89433d\/pydata_google_auth-0.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (39.1.0)\r\nCollecting google-cloud-bigquery>=1.11.1 (from pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/8f\/f7\/b6f55e144da37f38a79552a06103f2df4a9569e2dfc6d741a7e2a63d3592\/google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 39.2MB\/s ta 0:00:01\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.14)\r\nCollecting arrow (from jinja2-time>=0.1.0->cookiecutter<2.0,>=1.6.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/92\/fa\/f84896dede5decf284e6922134bf03fd26c90870bbf8015f4e8ee2a07bcc\/arrow-0.15.5-py2.py3-none-any.whl (46kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 26.3MB\/s ta 0:00:01\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.0)\r\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/a3\/12\/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379\/requests_oauthlib-1.3.0-py2.py3-none-any.whl\r\nCollecting pyasn1-modules>=0.2.1 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/95\/de\/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d\/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 163kB 32.5MB\/s ta 0:00:01\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0 (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/08\/6a\/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425\/cachetools-4.0.0-py3-none-any.whl\r\nCollecting google-api-core<2.0dev,>=1.15.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/63\/7e\/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77\/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 29.9MB\/s ta 0:00:01\r\nCollecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/89\/3c\/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97\/google_cloud_core-1.3.0-py2.py3-none-any.whl\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.6.1)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/35\/9e\/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098\/google_resumable_media-0.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.11.5)\r\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/57\/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704\/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 153kB 42.0MB\/s ta 0:00:01\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro)\r\n  Downloading https:\/\/files.pythonhosted.org\/packages\/05\/46\/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf\/googleapis-common-protos-1.51.0.tar.gz\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.18)\r\nBuilding wheels for collected packages: python-json-logger, anyconfig, future, googleapis-common-protos\r\n  Running setup.py bdist_wheel for python-json-logger ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\r\n  Running setup.py bdist_wheel for anyconfig ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\r\n  Running setup.py bdist_wheel for future ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\r\n  Running setup.py bdist_wheel for googleapis-common-protos ... done\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\r\nSuccessfully built python-json-logger anyconfig future googleapis-common-protos\r\ncookiecutter 1.7.0 has requirement click>=7.0, but you'll have click 6.7 which is incompatible.\r\ngoogle-auth 1.12.0 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\r\ngoogle-cloud-bigquery 1.24.0 has requirement six<2.0.0dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\r\npip-tools 4.5.1 has requirement click>=7, but you'll have click 6.7 which is incompatible.\r\nInstalling collected packages: azure-common, azure-storage-common, azure-storage-file, fsspec, s3fs, tables, toposort, azure-storage-queue, whichcraft, future, poyo, binaryornot, arrow, jinja2-time, cookiecutter, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, pandas-gbq, pip-tools, azure-storage-blob, pyarrow, python-json-logger, anyconfig, kedro\r\n  Found existing installation: s3fs 0.1.5\r\n    Uninstalling s3fs-0.1.5:\r\n      Successfully uninstalled s3fs-0.1.5\r\n  Found existing installation: tables 3.4.3\r\n    Uninstalling tables-3.4.3:\r\n      Successfully uninstalled tables-3.4.3\r\nSuccessfully installed anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 cookiecutter-1.7.0 fsspec-0.7.1 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 oauthlib-3.1.0 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 s3fs-0.4.2 tables-3.5.2 toposort-1.5 whichcraft-0.6.1\r\n```\r\n\r\n### `pip install kedro` output from terminal\r\n```\r\nCollecting kedro\r\n  Using cached kedro-0.15.8-py3-none-any.whl (12.5 MB)\r\nCollecting pandas<1.0,>=0.24.0\r\n  Downloading pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.4 MB 9.6 MB\/s \r\nCollecting azure-storage-file<2.0,>=1.1.0\r\n  Using cached azure_storage_file-1.4.0-py2.py3-none-any.whl (30 kB)\r\nCollecting click<8.0\r\n  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 82 kB 1.7 MB\/s \r\nCollecting cookiecutter<2.0,>=1.6.0\r\n  Using cached cookiecutter-1.7.0-py2.py3-none-any.whl (40 kB)\r\nCollecting SQLAlchemy<2.0,>=1.2.0\r\n  Downloading SQLAlchemy-1.3.15.tar.gz (6.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.1 MB 49.2 MB\/s \r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... done\r\n    Preparing wheel metadata ... done\r\nCollecting tables<3.6,>=3.4.4\r\n  Using cached tables-3.5.2-cp36-cp36m-manylinux1_x86_64.whl (4.3 MB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/97\/f7\/a1\/752e22bb30c1cfe38194ea0070a5c66e76ef4d06ad0c7dc401\/python_json_logger-0.1.11-py2.py3-none-any.whl\r\nCollecting azure-storage-blob<2.0,>=1.1.0\r\n  Using cached azure_storage_blob-1.5.0-py2.py3-none-any.whl (75 kB)\r\nCollecting pandas-gbq<1.0,>=0.12.0\r\n  Using cached pandas_gbq-0.13.1-py3-none-any.whl (23 kB)\r\nRequirement already satisfied: fsspec<1.0,>=0.5.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.6.3)\r\nCollecting xlsxwriter<2.0,>=1.0.0\r\n  Downloading XlsxWriter-1.2.8-py2.py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 65.9 MB\/s \r\nCollecting pip-tools<5.0.0,>=4.0.0\r\n  Using cached pip_tools-4.5.1-py2.py3-none-any.whl (41 kB)\r\nCollecting pyarrow<1.0.0,>=0.12.0\r\n  Downloading pyarrow-0.16.0-cp36-cp36m-manylinux2014_x86_64.whl (63.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 63.1 MB 25 kB\/s \r\nCollecting xlrd<2.0,>=1.0.0\r\n  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 103 kB 66.5 MB\/s \r\nRequirement already satisfied: s3fs<1.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (0.4.0)\r\nCollecting azure-storage-queue<2.0,>=1.1.0\r\n  Using cached azure_storage_queue-1.4.0-py2.py3-none-any.whl (23 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/5a\/82\/0d\/e374b7c77f4e4aa846a9bc2057e1d108c7f8e6b97a383befc9\/anyconfig-0.9.10-py2.py3-none-any.whl\r\nCollecting toposort<2.0,>=1.5\r\n  Using cached toposort-1.5-py2.py3-none-any.whl (7.6 kB)\r\nRequirement already satisfied: PyYAML<6.0,>=4.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (5.3.1)\r\nRequirement already satisfied: requests<3.0,>=2.20.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from kedro) (2.23.0)\r\nRequirement already satisfied: pytz>=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2019.3)\r\nRequirement already satisfied: numpy>=1.13.3 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (1.18.1)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas<1.0,>=0.24.0->kedro) (2.8.1)\r\nCollecting azure-common>=1.1.5\r\n  Using cached azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\r\nCollecting azure-storage-common~=1.4\r\n  Using cached azure_storage_common-1.4.2-py2.py3-none-any.whl (47 kB)\r\nCollecting poyo>=0.1.0\r\n  Using cached poyo-0.5.0-py2.py3-none-any.whl (10 kB)\r\nCollecting jinja2-time>=0.1.0\r\n  Using cached jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\r\nCollecting whichcraft>=0.4.0\r\n  Using cached whichcraft-0.6.1-py2.py3-none-any.whl (5.2 kB)\r\nCollecting binaryornot>=0.2.0\r\n  Using cached binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\r\nRequirement already satisfied: jinja2>=2.7 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cookiecutter<2.0,>=1.6.0->kedro) (2.11.1)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/8b\/99\/a0\/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\/future-0.18.2-cp36-none-any.whl\r\nRequirement already satisfied: mock>=2.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (3.0.5)\r\nCollecting numexpr>=2.6.2\r\n  Downloading numexpr-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (162 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 162 kB 66.7 MB\/s \r\nRequirement already satisfied: six>=1.9.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from tables<3.6,>=3.4.4->kedro) (1.14.0)\r\nCollecting pydata-google-auth\r\n  Using cached pydata_google_auth-0.3.0-py2.py3-none-any.whl (12 kB)\r\nCollecting google-auth-oauthlib\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting google-cloud-bigquery>=1.11.1\r\n  Using cached google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\r\nCollecting google-auth\r\n  Using cached google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\r\nRequirement already satisfied: setuptools in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pandas-gbq<1.0,>=0.12.0->kedro) (46.1.1.post20200323)\r\nRequirement already satisfied: boto3>=1.9.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.12.27)\r\nRequirement already satisfied: botocore>=1.12.91 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from s3fs<1.0,>=0.3.0->kedro) (1.15.27)\r\nRequirement already satisfied: idna<3,>=2.5 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from requests<3.0,>=2.20.0->kedro) (2019.11.28)\r\nRequirement already satisfied: cryptography in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.8)\r\nCollecting arrow\r\n  Using cached arrow-0.15.5-py2.py3-none-any.whl (46 kB)\r\nRequirement already satisfied: MarkupSafe>=0.23 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from jinja2>=2.7->cookiecutter<2.0,>=1.6.0->kedro) (1.1.1)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting google-resumable-media<0.6dev,>=0.5.0\r\n  Using cached google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\r\nCollecting google-cloud-core<2.0dev,>=1.1.0\r\n  Using cached google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\r\nRequirement already satisfied: protobuf>=3.6.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq<1.0,>=0.12.0->kedro) (3.11.3)\r\nCollecting google-api-core<2.0dev,>=1.15.0\r\n  Using cached google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nRequirement already satisfied: rsa<4.1,>=3.1.4 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (3.4.2)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\r\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.3.3)\r\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from boto3>=1.9.91->s3fs<1.0,>=0.3.0->kedro) (0.9.4)\r\nRequirement already satisfied: docutils<0.16,>=0.10 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from botocore>=1.12.91->s3fs<1.0,>=0.3.0->kedro) (0.15.2)\r\nRequirement already satisfied: cffi!=1.11.3,>=1.8 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (1.14.0)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nProcessing \/home\/ec2-user\/.cache\/pip\/wheels\/2c\/f9\/7f\/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\/googleapis_common_protos-1.51.0-cp36-none-any.whl\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from pyasn1-modules>=0.2.1->google-auth->pandas-gbq<1.0,>=0.12.0->kedro) (0.4.8)\r\nRequirement already satisfied: pycparser in \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages (from cffi!=1.11.3,>=1.8->cryptography->azure-storage-common~=1.4->azure-storage-file<2.0,>=1.1.0->kedro) (2.20)\r\nBuilding wheels for collected packages: SQLAlchemy\r\n  Building wheel for SQLAlchemy (PEP 517) ... done\r\n  Created wheel for SQLAlchemy: filename=SQLAlchemy-1.3.15-cp36-cp36m-linux_x86_64.whl size=1215829 sha256=112167e02a19acada7f367d8aca55bbd1e0c655de9edfabebae5e9d055d9a9a6\r\n  Stored in directory: \/home\/ec2-user\/.cache\/pip\/wheels\/4a\/1b\/3a\/c73044d7be48baeb47cbee343334f7803726ca1e9ba7b29095\r\nSuccessfully built SQLAlchemy\r\nInstalling collected packages: pandas, azure-common, azure-storage-common, azure-storage-file, click, poyo, arrow, jinja2-time, whichcraft, binaryornot, future, cookiecutter, SQLAlchemy, numexpr, tables, python-json-logger, azure-storage-blob, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, pydata-google-auth, google-resumable-media, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery, pandas-gbq, xlsxwriter, pip-tools, pyarrow, xlrd, azure-storage-queue, anyconfig, toposort, kedro\r\n  Attempting uninstall: pandas\r\n    Found existing installation: pandas 0.22.0\r\n    Uninstalling pandas-0.22.0:\r\n      Successfully uninstalled pandas-0.22.0\r\nSuccessfully installed SQLAlchemy-1.3.15 anyconfig-0.9.10 arrow-0.15.5 azure-common-1.1.25 azure-storage-blob-1.5.0 azure-storage-common-1.4.2 azure-storage-file-1.4.0 azure-storage-queue-1.4.0 binaryornot-0.4.4 cachetools-4.0.0 click-7.1.1 cookiecutter-1.7.0 future-0.18.2 google-api-core-1.16.0 google-auth-1.12.0 google-auth-oauthlib-0.4.1 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jinja2-time-0.2.0 kedro-0.15.8 numexpr-2.7.1 oauthlib-3.1.0 pandas-0.25.3 pandas-gbq-0.13.1 pip-tools-4.5.1 poyo-0.5.0 pyarrow-0.16.0 pyasn1-modules-0.2.8 pydata-google-auth-0.3.0 python-json-logger-0.1.11 requests-oauthlib-1.3.0 tables-3.5.2 toposort-1.5 whichcraft-0.6.1 xlrd-1.2.0 xlsxwriter-1.2.8\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n|environment | terminal | notebook|\r\n|----|----|----|\r\n|`kedro -V` | kedro, version 0.15.8 | kedro, version 0.15.8|\r\n|`python -V` | Python 3.6.10 :: Anaconda, Inc. | Python 3.6.5 :: Anaconda, Inc.|\r\n|os |  `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"` | `PRETTY_NAME=\"Amazon Linux AMI 2018.03\"\"` `ID_LIKE=\"rhel fedora\"`|\r\n|`pip freeze` | anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==1.3.0<br>attrs==19.3.0<br>autovizwidget==0.12.9<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>backcall==0.1.0<br>bcrypt==3.1.7<br>binaryornot==0.4.4<br>bleach==3.1.0<br>boto3==1.12.27<br>botocore==1.15.27<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.14.0<br>chardet==3.0.4<br>click==7.1.1<br>colorama==0.4.3<br>cookiecutter==1.7.0<br>cryptography==2.8<br>decorator==4.4.2<br>defusedxml==0.6.0<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.15.2<br>entrypoints==0.3<br>environment-kernels==1.1.1<br>fsspec==0.6.3<br>future==0.18.2<br>gitdb==4.0.2<br>GitPython==3.1.0<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>hdijupyterutils==0.12.9<br>idna==2.9<br>importlib-metadata==1.5.0<br>ipykernel==5.1.4<br>ipython==7.13.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.5.1<br>jedi==0.16.0<br>Jinja2==2.11.1<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>json5==0.9.3<br>jsonschema==3.2.0<br>jupyter==1.0.0<br>jupyter-client==6.0.0<br>jupyter-console==6.1.0<br>jupyter-core==4.6.1<br>jupyterlab==1.2.7<br>jupyterlab-git==0.9.0<br>jupyterlab-server==1.0.7<br>kedro==0.15.8<br>MarkupSafe==1.1.1<br>mistune==0.8.4<br>mock==3.0.5<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.3<br>nbconvert==5.6.1<br>nbdime==2.0.0<br>nbexamples==0.0.0<br>nbformat==5.0.4<br>nbserverproxy==0.3.2<br>nose==1.3.7<br>notebook==5.7.8<br>numexpr==2.7.1<br>numpy==1.18.1<br>oauthlib==3.1.0<br>packaging==20.3<br>pandas==0.25.3<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.6.2<br>pexpect==4.8.0<br>pickleshare==0.7.5<br>pid==3.0.0<br>pip-tools==4.5.1<br>plotly==4.5.4<br>poyo==0.5.0<br>prometheus-client==0.7.1<br>prompt-toolkit==3.0.3<br>protobuf==3.11.3<br>protobuf3-to-dict==0.1.5<br>psutil==5.7.0<br>psycopg2==2.8.4<br>ptyprocess==0.6.0<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycparser==2.20<br>pydata-google-auth==0.3.0<br>pygal==2.4.0<br>Pygments==2.6.1<br>pykerberos==1.1.14<br>PyNaCl==1.3.0<br>pyOpenSSL==19.1.0<br>pyparsing==2.4.6<br>pyrsistent==0.15.7<br>PySocks==1.7.1<br>pyspark==2.3.2<br>python-dateutil==2.8.1<br>python-json-logger==0.1.11<br>pytz==2019.3<br>PyYAML==5.3.1<br>pyzmq==18.1.1<br>qtconsole==4.7.1<br>QtPy==1.9.0<br>requests==2.23.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rsa==3.4.2<br>s3fs==0.4.0<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-experiments==0.1.10<br>sagemaker-nbi-agent==1.0<br>sagemaker-pyspark==1.2.8<br>scipy==1.4.1<br>Send2Trash==1.5.0<br>six==1.14.0<br>smdebug-rulesconfig==0.1.2<br>smmap==3.0.1<br>sparkmagic==0.15.0<br>SQLAlchemy==1.3.15<br>tables==3.5.2<br>terminado==0.8.3<br>testpath==0.4.4<br>texttable==1.6.2<br>toposort==1.5<br>tornado==6.0.4<br>traitlets==4.3.3<br>urllib3==1.22<br>wcwidth==0.1.8<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>whichcraft==0.6.1<br>widgetsnbextension==3.5.1<br>xlrd==1.2.0<br>XlsxWriter==1.2.8<br>zipp==2.2.0 | alabaster==0.7.10<br>anaconda-client==1.6.14<br>anaconda-project==0.8.2<br>anyconfig==0.9.10<br>arrow==0.15.5<br>asn1crypto==0.24.0<br>astroid==1.6.3<br>astropy==3.0.2<br>attrs==18.1.0<br>Automat==0.3.0<br>autovizwidget==0.15.0<br>awscli==1.18.27<br>azure-common==1.1.25<br>azure-storage-blob==1.5.0<br>azure-storage-common==1.4.2<br>azure-storage-file==1.4.0<br>azure-storage-queue==1.4.0<br>Babel==2.5.3<br>backcall==0.1.0<br>backports.shutil-get-terminal-size==1.0.0<br>bcrypt==3.1.7<br>beautifulsoup4==4.6.0<br>binaryornot==0.4.4<br>bitarray==0.8.1<br>bkcharts==0.2<br>blaze==0.11.3<br>bleach==2.1.3<br>bokeh==1.0.4<br>boto==2.48.0<br>boto3==1.12.27<br>botocore==1.15.27<br>Bottleneck==1.2.1<br>cached-property==1.5.1<br>cachetools==4.0.0<br>certifi==2019.11.28<br>cffi==1.11.5<br>characteristic==14.3.0<br>chardet==3.0.4<br>click==6.7<br>cloudpickle==0.5.3<br>clyent==1.2.2<br>colorama==0.3.9<br>contextlib2==0.5.5<br>cookiecutter==1.7.0<br>cryptography==2.8<br>cycler==0.10.0<br>Cython==0.28.4<br>cytoolz==0.9.0.1<br>dask==0.17.5<br>datashape==0.5.4<br>decorator==4.3.0<br>defusedxml==0.6.0<br>distributed==1.21.8<br>docker==4.2.0<br>docker-compose==1.25.4<br>dockerpty==0.4.1<br>docopt==0.6.2<br>docutils==0.14<br>entrypoints==0.2.3<br>enum34==1.1.9<br>environment-kernels==1.1.1<br>et-xmlfile==1.0.1<br>fastcache==1.0.2<br>filelock==3.0.4<br>Flask==1.0.2<br>Flask-Cors==3.0.4<br>fsspec==0.7.1<br>future==0.18.2<br>gevent==1.3.0<br>glob2==0.6<br>gmpy2==2.0.8<br>google-api-core==1.16.0<br>google-auth==1.12.0<br>google-auth-oauthlib==0.4.1<br>google-cloud-bigquery==1.24.0<br>google-cloud-core==1.3.0<br>google-resumable-media==0.5.0<br>googleapis-common-protos==1.51.0<br>greenlet==0.4.13<br>h5py==2.8.0<br>hdijupyterutils==0.15.0<br>heapdict==1.0.0<br>html5lib==1.0.1<br>idna==2.6<br>imageio==2.3.0<br>imagesize==1.0.0<br>importlib-metadata==1.5.0<br>ipykernel==4.8.2<br>ipyparallel==6.2.2<br>ipython==6.4.0<br>ipython-genutils==0.2.0<br>ipywidgets==7.4.0<br>isort==4.3.4<br>itsdangerous==0.24<br>jdcal==1.4<br>jedi==0.12.0<br>Jinja2==2.10<br>jinja2-time==0.2.0<br>jmespath==0.9.4<br>jsonschema==2.6.0<br>jupyter==1.0.0<br>jupyter-client==5.2.3<br>jupyter-console==5.2.0<br>jupyter-core==4.4.0<br>jupyterlab==0.32.1<br>jupyterlab-launcher==0.10.5<br>kedro==0.15.8<br>kiwisolver==1.0.1<br>lazy-object-proxy==1.3.1<br>llvmlite==0.23.1<br>locket==0.2.0<br>lxml==4.2.1<br>MarkupSafe==1.0<br>matplotlib==3.0.3<br>mccabe==0.6.1<br>mistune==0.8.3<br>mkl-fft==1.0.0<br>mkl-random==1.0.1<br>mock==4.0.1<br>more-itertools==4.1.0<br>mpmath==1.0.0<br>msgpack==0.6.0<br>msgpack-python==0.5.6<br>multipledispatch==0.5.0<br>nb-conda==2.2.1<br>nb-conda-kernels==2.2.2<br>nbconvert==5.4.1<br>nbformat==4.4.0<br>networkx==2.1<br>nltk==3.3<br>nose==1.3.7<br>notebook==5.5.0<br>numba==0.38.0<br>numexpr==2.6.5<br>numpy==1.14.3<br>numpydoc==0.8.0<br>oauthlib==3.1.0<br>odo==0.5.1<br>olefile==0.45.1<br>opencv-python==3.4.2.17<br>openpyxl==2.5.3<br>packaging==20.1<br>pandas==0.24.2<br>pandas-gbq==0.13.1<br>pandocfilters==1.4.2<br>paramiko==2.7.1<br>parso==0.2.0<br>partd==0.3.8<br>path.py==11.0.1<br>pathlib2==2.3.2<br>patsy==0.5.0<br>pep8==1.7.1<br>pexpect==4.5.0<br>pickleshare==0.7.4<br>Pillow==5.1.0<br>pip-tools==4.5.1<br>pkginfo==1.4.2<br>plotly==4.5.2<br>pluggy==0.6.0<br>ply==3.11<br>poyo==0.5.0<br>prompt-toolkit==1.0.15<br>protobuf==3.6.1<br>protobuf3-to-dict==0.1.5<br>psutil==5.4.5<br>psycopg2==2.7.5<br>ptyprocess==0.5.2<br>py==1.5.3<br>py4j==0.10.7<br>pyarrow==0.16.0<br>pyasn1==0.4.8<br>pyasn1-modules==0.2.8<br>pycodestyle==2.4.0<br>pycosat==0.6.3<br>pycparser==2.18<br>pycrypto==2.6.1<br>pycurl==7.43.0.1<br>pydata-google-auth==0.3.0<br>pyflakes==1.6.0<br>pygal==2.4.0<br>Pygments==2.2.0<br>pykerberos==1.2.1<br>pylint==1.8.4<br>PyNaCl==1.3.0<br>pyodbc==4.0.23<br>pyOpenSSL==18.0.0<br>pyparsing==2.2.0<br>PySocks==1.6.8<br>pyspark==2.3.2<br>pytest==3.5.1<br>pytest-arraydiff==0.2<br>pytest-astropy==0.3.0<br>pytest-doctestplus==0.1.3<br>pytest-openfiles==0.3.0<br>pytest-remotedata==0.2.1<br>python-dateutil==2.7.3<br>python-json-logger==0.1.11<br>pytz==2018.4<br>PyWavelets==0.5.2<br>PyYAML==5.3.1<br>pyzmq==17.0.0<br>QtAwesome==0.4.4<br>qtconsole==4.3.1<br>QtPy==1.4.1<br>requests==2.20.0<br>requests-kerberos==0.12.0<br>requests-oauthlib==1.3.0<br>retrying==1.3.3<br>rope==0.10.7<br>rsa==3.4.2<br>ruamel-yaml==0.15.35<br>s3fs==0.4.2<br>s3transfer==0.3.3<br>sagemaker==1.51.4<br>sagemaker-pyspark==1.2.8<br>scikit-image==0.13.1<br>scikit-learn==0.20.3<br>scipy==1.1.0<br>seaborn==0.8.1<br>Send2Trash==1.5.0<br>simplegeneric==0.8.1<br>singledispatch==3.4.0.3<br>six==1.11.0<br>smdebug-rulesconfig==0.1.2<br>snowballstemmer==1.2.1<br>sortedcollections==0.6.1<br>sortedcontainers==1.5.10<br>sparkmagic==0.12.5<br>Sphinx==1.7.4<br>sphinxcontrib-websupport==1.0.1<br>spyder==3.2.8<br>SQLAlchemy==1.2.11<br>statsmodels==0.9.0<br>sympy==1.1.1<br>tables==3.5.2<br>TBB==0.1<br>tblib==1.3.2<br>terminado==0.8.1<br>testpath==0.3.1<br>texttable==1.6.2<br>toolz==0.9.0<br>toposort==1.5<br>tornado==5.0.2<br>traitlets==4.3.2<br>typing==3.6.4<br>unicodecsv==0.14.1<br>urllib3==1.23<br>wcwidth==0.1.7<br>webencodings==0.5.1<br>websocket-client==0.57.0<br>Werkzeug==0.14.1<br>whichcraft==0.6.1<br>widgetsnbextension==3.4.2<br>wrapt==1.10.11<br>xlrd==1.1.0<br>XlsxWriter==1.0.4<br>xlwt==1.3.0<br>zict==0.1.3<br>zipp==3.0.0|\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: notebooks raise error for `pandas.csvdataset`; ## description the conda environment for python3.6 in notebooks cannot find `pandas.csvdataset` ## context i'm wanting to use as my development environment. however, i cannot get to run as expected in both the notebooks (for exploration and node development) and the terminal (for running pipelines). ## steps to reproduce 0. startup a instance with defaults terminal success: 1. `pip install ` in the terminal 2. ` new` 2a. `testing` for name 2b. `y` for example project 3. `cd testing; run` => success! notebook fail: 1. create a new `conda_python3` notebook in `testing\/notebooks\/` 2. `!pip install ` in a notebook > the environments for the terminal and notebooks are separate by design in 2. load the context as described [here](https:\/\/.readthedocs.io\/en\/stable\/04_user_guide\/11_ipython.html#what-if-i-cannot-run--jupyter-notebook) > note that i've started to use the code below; Content: without checking if `current_dir` exists, you need to restart the kernel if you want to reload the context as something in the last 2 lines of code causes the next invocation of `path.cwd()` to point to the root dir not `notebook\/`, as intended. ``` if \"current_dir\" not in locals(): # check it exists first. for some reason this is not an idempotent operation? current_dir = path.cwd() # this points to 'notebooks\/' folder proj_path = current_dir.parent # point back to the root of the project context = load_context(proj_path) ``` 3. run `context.catalog.list()` ## expected result the notebook should print: ``` ['example_iris_data', 'parameters', 'params:example_test_data_ratio', 'params:example_num_train_iter', 'params:example_learning_rate'] ``` ## actual result ``` class `pandas.csvdataset` not found. ``` full trace. ``` --------------------------------------------------------------------------- stopiteration traceback (most recent call last) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/io\/core.py in parse_dataset_definition(config, load_version, save_version) 416 try: --> 417 class_obj = next(obj for obj in trials if obj is not none) 418 except stopiteration: stopiteration: during handling of the above exception, another exception occurred: dataseterror traceback (most recent call last) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/io\/core.py in from_config(cls, name, config, load_version, save_version) 148 class_obj, config = parse_dataset_definition( --> 149 config, load_version, save_version 150 ) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/io\/core.py in parse_dataset_definition(config, load_version, save_version) 418 except stopiteration: --> 419 raise dataseterror(\"class `{}` not found.\".format(class_obj)) 420 dataseterror: class `pandas.csvdataset` not found. during handling of the above exception, another exception occurred: dataseterror traceback (most recent call last) in () ----> 1 context.catalog.list() ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/context\/context.py in catalog(self) 206 207 \"\"\" --> 208 return self._get_catalog() 209 210 @property ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/context\/context.py in _get_catalog(self, save_version, journal, load_versions) 243 conf_creds = self._get_config_credentials() 244 catalog = self._create_catalog( --> 245 conf_catalog, conf_creds, save_version, journal, load_versions 246 ) 247 catalog.add_feed_dict(self._get_feed_dict()) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/context\/context.py in _create_catalog(self, conf_catalog, conf_creds, save_version, journal, load_versions) 267 save_version=save_version, 268 journal=journal, --> 269 load_versions=load_versions, 270 ) 271 ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/io\/data_catalog.py in from_config(cls, catalog, credentials, load_versions, save_version, journal) 298 ds_config = _resolve_credentials(ds_config, credentials) 299 data_sets[ds_name] = abstractdataset.from_config( --> 300 ds_name, ds_config, load_versions.get(ds_name), save_version 301 ) 302 return cls(data_sets=data_sets, journal=journal) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/io\/core.py in from_config(cls, name, config, load_version, save_version) 152 raise dataseterror( 153 \"an exception occurred when parsing config \" --> 154 \"for dataset `{}`:\\n{}\".format(name, str(ex)) 155 ) 156 dataseterror: an exception occurred when parsing config for dataset `example_iris_data`: class `pandas.csvdataset` not found. ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the conda environment for python3.6 in notebooks could not find `pandas.csvdataset`, resulting in an error when running `context.catalog.list()`.",
        "Issue_preprocessed_content":"Title: notebooks raise error for ; Content: description the conda environment for in notebooks cannot find context i'm wanting to use as my development environment. however, i cannot get to run as expected in both the notebooks and the terminal . steps to reproduce . startup a instance with defaults terminal success . in the terminal . a. for name b. for example project . success! notebook fail . create a new notebook in . in a notebook the environments for the terminal and notebooks are separate by design in . load the context as described note that i've started to use the code below; without checking if exists, you need to restart the kernel if you want to reload the context as something in the last lines of code causes the next invocation of to point to the root dir not , as intended. . run expected result the notebook should print actual result full trace. investigations so far upon changing the yaml type for from to , we get success on both the terminal and the notebook. however, this is not my desired outcome; the transition to using makes it easier, for me at least, to use both s and local datasets. output from notebook output from terminal your environment include as many relevant details about the environment in which you experienced the bug environment terminal notebook , version , version python anaconda, inc. python anaconda, os"
    },
    {
        "Issue_link":"https:\/\/github.com\/turbot\/steampipe-plugin-aws\/issues\/364",
        "Issue_title":"Getting an error from `aws_sagemaker_notebook_instance` table. Please see the detail below.",
        "Issue_creation_time":1620111416000,
        "Issue_closed_time":1623682749000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n\r\nCreate a notebook instance in one of the configured region.\r\nRan the below query and got that error\r\n\r\n```\r\nselect * from aws_sagemaker_notebook_instance;\r\nError: hydrate call listAwsSageMakerNotebookInstanceTags failed with panic interface conversion: interface {} is *sagemaker.NotebookInstanceSummary, not *sagemaker.DescribeNotebookInstanceOutput\r\n\r\n```\r\n\r\n\r\n\r\n**Steampipe version (`steampipe -v`)**\r\n: v0.4.1\r\n\r\n**Plugin version (`steampipe plugin list`)**\r\naws: v0.15.0\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: getting an error from `aws__notebook_instance` table. please see the detail below.; Content: **describe the bug** create a notebook instance in one of the configured region. ran the below query and got that error ``` select * from aws__notebook_instance; error: hydrate call listawsnotebookinstancetags failed with panic interface conversion: interface {} is *.notebookinstancesummary, not *.describenotebookinstanceoutput ``` **steampipe version (`steampipe -v`)** : v0.4.1 **plugin version (`steampipe plugin list`)** aws: v0.15.0",
        "Issue_original_content_gpt_summary":"The user encountered an error when running a query on the `aws__notebook_instance` table using Steampipe v0.4.1 and the AWS plugin v0.15.0.",
        "Issue_preprocessed_content":"Title: getting an error from table. please see the detail below.; Content: describe the bug create a notebook instance in one of the configured region. ran the below query and got that error steampipe version plugin version aws"
    },
    {
        "Issue_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/767",
        "Issue_title":"[BUG]: SageMaker + S3 artifact store fails trying to create a new bucket",
        "Issue_creation_time":1657726485000,
        "Issue_closed_time":1657782683000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### Contact Details [Optional]\n\n_No response_\n\n### System Information\n\nZenml == 0.10.0\n\n### What happened?\n\nZenml is trying to create a s3 bucket and fails due to incorrect regex in its name.\n\n### Reproduction steps\n\n1. Create a SageMaker pipeline.\r\n2. Create a s3 artifact store.\r\n3. Run the pipeline\r\n\n\n### Relevant log output\n\n```shell\nCreating run for pipeline: mnist_pipeline\r\nCache enabled for pipeline mnist_pipeline\r\nUsing stack sagemaker_stack to run pipeline mnist_pipeline...\r\nStep importer has started.\r\nUsing cached version of importer.\r\nStep importer has finished in 0.045s.\r\nStep trainer has started.\r\nINFO:botocore.credentials:Found credentials in shared credentials file: ~\/.aws\/credentials\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:752 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    749 \u2502   \u2502   \u2502   \u2502   \u2502   params[\"CreateBucketConfiguration\"] = {          \u2502\r\n\u2502    750 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \"LocationConstraint\": region_name            \u2502\r\n\u2502    751 \u2502   \u2502   \u2502   \u2502   \u2502   }                                                \u2502\r\n\u2502 >  752 \u2502   \u2502   \u2502   \u2502   await self._call_s3(\"create_bucket\", **params)       \u2502\r\n\u2502    753 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(\"\")                            \u2502\r\n\u2502    754 \u2502   \u2502   \u2502   \u2502   self.invalidate_cache(bucket)                        \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:302 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    299 \u2502   \u2502   \u2502   except Exception as e:                                   \u2502\r\n\u2502    300 \u2502   \u2502   \u2502   \u2502   err = e                                              \u2502\r\n\u2502    301 \u2502   \u2502   err = translate_boto_error(err)                              \u2502\r\n\u2502 >  302 \u2502   \u2502   raise err                                                    \u2502\r\n\u2502    303 \u2502                                                                    \u2502\r\n\u2502    304 \u2502   call_s3 = sync_wrapper(_call_s3)                                 \u2502\r\n\u2502    305                                                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:282 in _call_s3                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    279 \u2502   \u2502   additional_kwargs = self._get_s3_method_kwargs(method, *akwa \u2502\r\n\u2502    280 \u2502   \u2502   for i in range(self.retries):                                \u2502\r\n\u2502    281 \u2502   \u2502   \u2502   try:                                                     \u2502\r\n\u2502 >  282 \u2502   \u2502   \u2502   \u2502   out = await method(**additional_kwargs)              \u2502\r\n\u2502    283 \u2502   \u2502   \u2502   \u2502   return out                                           \u2502\r\n\u2502    284 \u2502   \u2502   \u2502   except S3_RETRYABLE_ERRORS as e:                         \u2502\r\n\u2502    285 \u2502   \u2502   \u2502   \u2502   logger.debug(\"Retryable error: %s\", e)               \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:198 in _make_api_call                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   195 \u2502   \u2502   \u2502   'has_streaming_input': operation_model.has_streaming_inpu \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   'auth_type': operation_model.auth_type,                   \u2502\r\n\u2502   197 \u2502   \u2502   }                                                             \u2502\r\n\u2502 > 198 \u2502   \u2502   request_dict = await self._convert_to_request_dict(           \u2502\r\n\u2502   199 \u2502   \u2502   \u2502   api_params, operation_model, context=request_context)     \u2502\r\n\u2502   200 \u2502   \u2502   resolve_checksum_context(request_dict, operation_model, api_p \u2502\r\n\u2502   201                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:246 in _convert_to_request_dict                            \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   243 \u2502                                                                     \u2502\r\n\u2502   244 \u2502   async def _convert_to_request_dict(self, api_params, operation_mo \u2502\r\n\u2502   245 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      context=None):                 \u2502\r\n\u2502 > 246 \u2502   \u2502   api_params = await self._emit_api_params(                     \u2502\r\n\u2502   247 \u2502   \u2502   \u2502   api_params, operation_model, context)                     \u2502\r\n\u2502   248 \u2502   \u2502   request_dict = self._serializer.serialize_to_request(         \u2502\r\n\u2502   249 \u2502   \u2502   \u2502   api_params, operation_model)                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\client.py:275 in _emit_api_params                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502                                                                 \u2502\r\n\u2502   273 \u2502   \u2502   event_name = (                                                \u2502\r\n\u2502   274 \u2502   \u2502   \u2502   'before-parameter-build.{service_id}.{operation_name}')   \u2502\r\n\u2502 > 275 \u2502   \u2502   await self.meta.events.emit(                                  \u2502\r\n\u2502   276 \u2502   \u2502   \u2502   event_name.format(                                        \u2502\r\n\u2502   277 \u2502   \u2502   \u2502   \u2502   service_id=service_id,                                \u2502\r\n\u2502   278 \u2502   \u2502   \u2502   \u2502   operation_name=operation_name),                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\aiobo \u2502\r\n\u2502 tocore\\hooks.py:29 in _emit                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   26 \u2502   \u2502   \u2502   if asyncio.iscoroutinefunction(handler):                   \u2502\r\n\u2502   27 \u2502   \u2502   \u2502   \u2502   response = await handler(**kwargs)                     \u2502\r\n\u2502   28 \u2502   \u2502   \u2502   else:                                                      \u2502\r\n\u2502 > 29 \u2502   \u2502   \u2502   \u2502   response = handler(**kwargs)                           \u2502\r\n\u2502   30 \u2502   \u2502   \u2502                                                              \u2502\r\n\u2502   31 \u2502   \u2502   \u2502   responses.append((handler, response))                      \u2502\r\n\u2502   32 \u2502   \u2502   \u2502   if stop_on_response and response is not None:              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\botoc \u2502\r\n\u2502 ore\\handlers.py:243 in validate_bucket_name                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    240 \u2502   \u2502   \u2502   'Invalid bucket name \"%s\": Bucket name must match '      \u2502\r\n\u2502    241 \u2502   \u2502   \u2502   'the regex \"%s\" or be an ARN matching the regex \"%s\"' %  \u2502\r\n\u2502    242 \u2502   \u2502   \u2502   \u2502   bucket, VALID_BUCKET.pattern, VALID_S3_ARN.pattern)) \u2502\r\n\u2502 >  243 \u2502   \u2502   raise ParamValidationError(report=error_msg)                 \u2502\r\n\u2502    244                                                                      \u2502\r\n\u2502    245                                                                      \u2502\r\n\u2502    246 def sse_md5(params, **kwargs):                                       \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nParamValidationError: Parameter validation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\run-sagemaker.py:87 in       \u2502\r\n\u2502 <module>                                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   84 \u2502   \u2502   trainer=trainer(),                                             \u2502\r\n\u2502   85 \u2502   \u2502   evaluator=evaluator(),                                         \u2502\r\n\u2502   86 \u2502   )                                                                  \u2502\r\n\u2502 > 87 \u2502   pipeline.run()                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\pipelines\\base_pipeline.py:489 in run                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   486 \u2502   \u2502   self._reset_step_flags()                                      \u2502\r\n\u2502   487 \u2502   \u2502   self.validate_stack(stack)                                    \u2502\r\n\u2502   488 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 489 \u2502   \u2502   return stack.deploy_pipeline(                                 \u2502\r\n\u2502   490 \u2502   \u2502   \u2502   self, runtime_configuration=runtime_configuration         \u2502\r\n\u2502   491 \u2502   \u2502   )                                                             \u2502\r\n\u2502   492                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\stack\\stack.py:595 in deploy_pipeline                                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   pipeline=pipeline, runtime_configuration=runtime_configur \u2502\r\n\u2502   593 \u2502   \u2502   )                                                             \u2502\r\n\u2502   594 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 595 \u2502   \u2502   return_value = self.orchestrator.run(                         \u2502\r\n\u2502   596 \u2502   \u2502   \u2502   pipeline, stack=self, runtime_configuration=runtime_confi \u2502\r\n\u2502   597 \u2502   \u2502   )                                                             \u2502\r\n\u2502   598                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:212 in run                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   pipeline=pipeline, pb2_pipeline=pb2_pipeline              \u2502\r\n\u2502   210 \u2502   \u2502   )                                                             \u2502\r\n\u2502   211 \u2502   \u2502                                                                 \u2502\r\n\u2502 > 212 \u2502   \u2502   result = self.prepare_or_run_pipeline(                        \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   sorted_steps=sorted_steps,                                \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   pipeline=pipeline,                                        \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                                \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\local\\local_orchestrator.py:68 in prepare_or_run_pipeline    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   65 \u2502   \u2502                                                                  \u2502\r\n\u2502   66 \u2502   \u2502   # Run each step                                                \u2502\r\n\u2502   67 \u2502   \u2502   for step in sorted_steps:                                      \u2502\r\n\u2502 > 68 \u2502   \u2502   \u2502   self.run_step(                                             \u2502\r\n\u2502   69 \u2502   \u2502   \u2502   \u2502   step=step,                                             \u2502\r\n\u2502   70 \u2502   \u2502   \u2502   \u2502   run_name=runtime_configuration.run_name,               \u2502\r\n\u2502   71 \u2502   \u2502   \u2502   \u2502   pb2_pipeline=pb2_pipeline,                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:316 in run_step                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   313 \u2502   \u2502   # This is where the step actually gets executed using the     \u2502\r\n\u2502   314 \u2502   \u2502   # component_launcher                                          \u2502\r\n\u2502   315 \u2502   \u2502   repo.active_stack.prepare_step_run()                          \u2502\r\n\u2502 > 316 \u2502   \u2502   execution_info = self._execute_step(component_launcher)       \u2502\r\n\u2502   317 \u2502   \u2502   repo.active_stack.cleanup_step_run()                          \u2502\r\n\u2502   318 \u2502   \u2502                                                                 \u2502\r\n\u2502   319 \u2502   \u2502   return execution_info                                         \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\orchestrators\\base_orchestrator.py:340 in _execute_step                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   337 \u2502   \u2502   start_time = time.time()                                      \u2502\r\n\u2502   338 \u2502   \u2502   logger.info(f\"Step `{pipeline_step_name}` has started.\")      \u2502\r\n\u2502   339 \u2502   \u2502   try:                                                          \u2502\r\n\u2502 > 340 \u2502   \u2502   \u2502   execution_info = tfx_launcher.launch()                    \u2502\r\n\u2502   341 \u2502   \u2502   \u2502   if execution_info and get_cache_status(execution_info):   \u2502\r\n\u2502   342 \u2502   \u2502   \u2502   \u2502   logger.info(f\"Using cached version of `{pipeline_step \u2502\r\n\u2502   343 \u2502   \u2502   except RuntimeError as e:                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:528 in launch                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   525 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502      self._pipeline_runtime_spe \u2502\r\n\u2502   526 \u2502                                                                     \u2502\r\n\u2502   527 \u2502   # Runs as a normal node.                                          \u2502\r\n\u2502 > 528 \u2502   execution_preparation_result = self._prepare_execution()          \u2502\r\n\u2502   529 \u2502   (execution_info, contexts,                                        \u2502\r\n\u2502   530 \u2502    is_execution_needed) = (execution_preparation_result.execution_i \u2502\r\n\u2502   531 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    execution_preparation_result.contexts,   \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\launcher.py:388 in _prepare_execution                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   385 \u2502   \u2502   \u2502     output_dict=output_artifacts,                           \u2502\r\n\u2502   386 \u2502   \u2502   \u2502     exec_properties=exec_properties,                        \u2502\r\n\u2502   387 \u2502   \u2502   \u2502     execution_output_uri=(                                  \u2502\r\n\u2502 > 388 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_executor_output_uri(execu \u2502\r\n\u2502   389 \u2502   \u2502   \u2502     stateful_working_dir=(                                  \u2502\r\n\u2502   390 \u2502   \u2502   \u2502   \u2502     self._output_resolver.get_stateful_working_director \u2502\r\n\u2502   391 \u2502   \u2502   \u2502     tmp_dir=self._output_resolver.make_tmp_dir(execution.id \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\o \u2502\r\n\u2502 rchestration\\portable\\outputs_utils.py:172 in get_executor_output_uri       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   169 \u2502   \"\"\"Generates executor output uri given execution_id.\"\"\"           \u2502\r\n\u2502   170 \u2502   execution_dir = os.path.join(self._node_dir, _SYSTEM, _EXECUTOR_E \u2502\r\n\u2502   171 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502    str(execution_id))                   \u2502\r\n\u2502 > 172 \u2502   fileio.makedirs(execution_dir)                                    \u2502\r\n\u2502   173 \u2502   return os.path.join(execution_dir, _EXECUTOR_OUTPUT_FILE)         \u2502\r\n\u2502   174                                                                       \u2502\r\n\u2502   175   def get_driver_output_uri(self) -> str:                             \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\tfx\\d \u2502\r\n\u2502 sl\\io\\fileio.py:80 in makedirs                                              \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    77                                                                       \u2502\r\n\u2502    78 def makedirs(path: PathType) -> None:                                 \u2502\r\n\u2502    79   \"\"\"Make a directory at the given path, recursively creating parents \u2502\r\n\u2502 >  80   _get_filesystem(path).makedirs(path)                                \u2502\r\n\u2502    81                                                                       \u2502\r\n\u2502    82                                                                       \u2502\r\n\u2502    83 def mkdir(path: PathType) -> None:                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\zenml \u2502\r\n\u2502 \\integrations\\s3\\artifact_stores\\s3_artifact_store.py:275 in makedirs       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502   272 \u2502   \u2502   Args:                                                         \u2502\r\n\u2502   273 \u2502   \u2502   \u2502   path: The path to create.                                 \u2502\r\n\u2502   274 \u2502   \u2502   \"\"\"                                                           \u2502\r\n\u2502 > 275 \u2502   \u2502   self.filesystem.makedirs(path=path, exist_ok=True)            \u2502\r\n\u2502   276 \u2502                                                                     \u2502\r\n\u2502   277 \u2502   def mkdir(self, path: PathType) -> None:                          \u2502\r\n\u2502   278 \u2502   \u2502   \"\"\"Create a directory at the given path.                      \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:85 in wrapper                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    82 \u2502   @functools.wraps(func)                                            \u2502\r\n\u2502    83 \u2502   def wrapper(*args, **kwargs):                                     \u2502\r\n\u2502    84 \u2502   \u2502   self = obj or args[0]                                         \u2502\r\n\u2502 >  85 \u2502   \u2502   return sync(self.loop, func, *args, **kwargs)                 \u2502\r\n\u2502    86 \u2502                                                                     \u2502\r\n\u2502    87 \u2502   return wrapper                                                    \u2502\r\n\u2502    88                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:65 in sync                                                        \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    62 \u2502   \u2502   # suppress asyncio.TimeoutError, raise FSTimeoutError         \u2502\r\n\u2502    63 \u2502   \u2502   raise FSTimeoutError from return_result                       \u2502\r\n\u2502    64 \u2502   elif isinstance(return_result, BaseException):                    \u2502\r\n\u2502 >  65 \u2502   \u2502   raise return_result                                           \u2502\r\n\u2502    66 \u2502   else:                                                             \u2502\r\n\u2502    67 \u2502   \u2502   return return_result                                          \u2502\r\n\u2502    68                                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\fsspe \u2502\r\n\u2502 c\\asyn.py:25 in _runner                                                     \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    22 \u2502   if timeout is not None:                                           \u2502\r\n\u2502    23 \u2502   \u2502   coro = asyncio.wait_for(coro, timeout=timeout)                \u2502\r\n\u2502    24 \u2502   try:                                                              \u2502\r\n\u2502 >  25 \u2502   \u2502   result[0] = await coro                                        \u2502\r\n\u2502    26 \u2502   except Exception as ex:                                           \u2502\r\n\u2502    27 \u2502   \u2502   result[0] = ex                                                \u2502\r\n\u2502    28 \u2502   finally:                                                          \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:767 in _makedirs                                                    \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    764 \u2502                                                                    \u2502\r\n\u2502    765 \u2502   async def _makedirs(self, path, exist_ok=False):                 \u2502\r\n\u2502    766 \u2502   \u2502   try:                                                         \u2502\r\n\u2502 >  767 \u2502   \u2502   \u2502   await self._mkdir(path, create_parents=True)             \u2502\r\n\u2502    768 \u2502   \u2502   except FileExistsError:                                      \u2502\r\n\u2502    769 \u2502   \u2502   \u2502   if exist_ok:                                             \u2502\r\n\u2502    770 \u2502   \u2502   \u2502   \u2502   pass                                                 \u2502\r\n\u2502                                                                             \u2502\r\n\u2502 C:\\Users\\i25262\\PycharmProjects\\ZenML-Kubeflow\\venv\\lib\\site-packages\\s3fs\\ \u2502\r\n\u2502 core.py:758 in _mkdir                                                       \u2502\r\n\u2502                                                                             \u2502\r\n\u2502    755 \u2502   \u2502   \u2502   except ClientError as e:                                 \u2502\r\n\u2502    756 \u2502   \u2502   \u2502   \u2502   raise translate_boto_error(e)                        \u2502\r\n\u2502    757 \u2502   \u2502   \u2502   except ParamValidationError as e:                        \u2502\r\n\u2502 >  758 \u2502   \u2502   \u2502   \u2502   raise ValueError(\"Bucket create failed %r: %s\" % (bu \u2502\r\n\u2502    759 \u2502   \u2502   else:                                                        \u2502\r\n\u2502    760 \u2502   \u2502   \u2502   # raises if bucket doesn't exist and doesn't get create  \u2502\r\n\u2502    761 \u2502   \u2502   \u2502   await self._ls(bucket)                                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nValueError: Bucket create failed \r\n'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': Parameter \r\nvalidation failed:\r\nInvalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": \r\nBucket name must match the regex \"^[a-zA-Z0-9.\\-_]{1,255}$\" or be an ARN \r\nmatching the regex \r\n\"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-zA-\r\nZ0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA\r\n-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$\"\n```\n\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: + s3 artifact store fails trying to create a new bucket; Content: ### what happened? zenml is trying to create a s3 bucket and fails due to incorrect regex in its name. ### reproduction steps 1. create a pipeline. 2. create a s3 artifact store. 3. run the pipeline ### relevant log output ```shell creating run for pipeline: mnist_pipeline cache enabled for pipeline mnist_pipeline using stack _stack to run pipeline mnist_pipeline... step importer has started. using cached version of importer. step importer has finished in 0.045s. step trainer has started. info:botocore.credentials:found credentials in shared credentials file: ~\/.aws\/credentials paramvalidationerror: parameter validation failed: invalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": bucket name must match the regex \"^[a-za-z0-9.\\-_]{1,255}$\" or be an arn matching the regex \"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-za- z0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-za -z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-za-z0-9\\-]{1,63}$\" during handling of the above exception, another exception occurred: valueerror: bucket create failed 'zenml-training\\\\trainer\\\\.system\\\\executor_execution\\\\24': parameter validation failed: invalid bucket name \"zenml-training\\trainer\\.system\\executor_execution\\24\": bucket name must match the regex \"^[a-za-z0-9.\\-_]{1,255}$\" or be an arn matching the regex \"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[\/:][a-za- z0-9\\-.]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-za -z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-za-z0-9\\-]{1,63}$\" ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where zenml was failing to create a s3 bucket due to an incorrect regex in its name.",
        "Issue_preprocessed_content":"Title: + s artifact store fails trying to create a new bucket; Content: contact details system information zenml what happened? zenml is trying to create a s bucket and fails due to incorrect regex in its name. reproduction steps . create a pipeline. . create a s artifact store. . run the pipeline relevant log output code of conduct i agree to follow this project's code of conduct"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1089",
        "Issue_title":"[Bug] study fail to mount in SWB 5.2.6 SageMaker Jupyter Notebook",
        "Issue_creation_time":1671836610000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nSWB 5.2.6 version - SageMaker jupyter notebook workspace can not mount a study.  see error in \/var\/log\/message\r\n\/usr\/local\/bin\/goofys[7248]: main.FATAL Mounting file system: Mount: mount: running fusermount: exec: \"fusermount\": executable file not found in $PATH#012#012stderr:\r\n\r\nLooks like the FUSE package failed to install during on-start.  if run \"sudo yum install fuse\" then you can run \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study. \r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v1.0.3]\r\n - Is the deployment from a forked version of the repository?\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] study fail to mount in swb 5.2.6 jupyter notebook; Content: **describe the bug** swb 5.2.6 version - jupyter notebook workspace can not mount a study. see error in \/var\/log\/message \/usr\/local\/bin\/goofys[7248]: main.fatal mounting file system: mount: mount: running fusermount: exec: \"fusermount\": executable file not found in $path#012#012stderr: looks like the fuse package failed to install during on-start. if run \"sudo yum install fuse\" then you can run \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json to mount the study. **to reproduce** steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error **expected behavior** a clear and concise description of what you expected to happen. **screenshots** if applicable, add screenshots to help explain your problem. **versions (please complete the following information):** - release version installed [e.g. v1.0.3] - is the deployment from a forked version of the repository? **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with mounting a study in swb 5.2.6 jupyter notebook, which was resolved by running \"sudo yum install fuse\" and then running \/usr\/local.\/share\/workspace-environment\/bin\/mount_sh.sh \/usr\/local\/etc\/s3-mounts.json.",
        "Issue_preprocessed_content":"Title: study fail to mount in swb jupyter notebook; Content: describe the bug swb version jupyter notebook workspace can not mount a study. see error in mounting file system mount mount running fusermount exec fusermount executable file not found in $path stderr looks like the fuse package failed to install during if run sudo yum install fuse then you can run to mount the study. to reproduce steps to reproduce the behavior . go to . click on . scroll down to . see error expected behavior a clear and concise description of what you expected to happen. screenshots if applicable, add screenshots to help explain your problem. versions release version installed is the deployment from a forked version of the repository? additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1081",
        "Issue_title":"[Bug] More descriptive error message for \"null is not an object\" while trying to connect to Sagemaker notebook. ",
        "Issue_creation_time":1670601495000,
        "Issue_closed_time":1671209314000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nUsers get the error \"null is not an object\" when pop-ups are enabled in SWB (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620))\r\nThis error is illegible to the user and causes confusion. Can we make the error message more clear such as:\r\n\"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Diable pop-ups \r\n2. Connect to a workspace\r\n\r\n**Expected behavior**\r\nIf the workspace is unable to open, a more legible error message should be shown, such as \"Service Workbench is encountering an error showing content. Please enable pop-ups and refresh the page.\"\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed [e.g. v4.3.1 and v5.0.0]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] more descriptive error message for \"null is not an object\" while trying to connect to notebook. ; Content: **describe the bug** users get the error \"null is not an object\" when pop-ups are enabled in swb (reference:[ issue #620](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620)) this error is illegible to the user and causes confusion. can we make the error message more clear such as: \"service workbench is encountering an error showing content. please enable pop-ups and refresh the page.\" **to reproduce** steps to reproduce the behavior: 1. diable pop-ups 2. connect to a workspace **expected behavior** if the workspace is unable to open, a more legible error message should be shown, such as \"service workbench is encountering an error showing content. please enable pop-ups and refresh the page.\" **screenshots** if applicable, add screenshots to help explain your problem. **versions (please complete the following information):** - release version installed [e.g. v4.3.1 and v5.0.0] **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encounters an illegible error message when trying to connect to a workspace, and is unable to open the workspace due to pop-ups being disabled, requiring a more legible error message to be shown.",
        "Issue_preprocessed_content":"Title: more descriptive error message for null is not an object while trying to connect to notebook. ; Content: describe the bug users get the error null is not an object when are enabled in swb this error is illegible to the user and causes confusion. can we make the error message more clear such as service workbench is encountering an error showing content. please enable and refresh the to reproduce steps to reproduce the behavior . diable . connect to a workspace expected behavior if the workspace is unable to open, a more legible error message should be shown, such as service workbench is encountering an error showing content. please enable and refresh the screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1076",
        "Issue_title":"[Bug] Sagemaker instance does not stop automatically",
        "Issue_creation_time":1670370218000,
        "Issue_closed_time":1671059684000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"**Describe the bug**\r\n\r\nIdle Sagemaker Notebook instances do not stop after specified time.\r\n\r\nSWB runs autostop.py script to automatically stop Sagemaker Notebook instance. The script is used by `on-start` lifecycle rule of the instance CFN template. According to LifecycleConfigOnStart logs, some packages are missing and autostop script doesn\u2019t work.\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Make sure AutoStopIdleTimeInMinutes parameter in workspace type config is set to a required time (30 minutes in our case)\r\n2. Create a new workspace with Sagemaker notebook instance\r\n3. Leave the instance idle for the time specified (AutoStopIdleTimeInMinutes )\r\n4. After the specified time see that the instance is not stopped\r\n\r\n**Expected behavior**\r\nIdle Sagemaker Notebook instance automatically stops after specified time.\r\n\r\n**Screenshots**\r\n<img width=\"1308\" alt=\"Screen Shot 2022-12-07 at 10 43 09 am\" src=\"https:\/\/user-images.githubusercontent.com\/47466926\/206049662-5ff12457-8bd4-42bd-b12f-ce68fdfacaf6.png\">\r\n\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed v5.0.0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] instance does not stop automatically; Content: **describe the bug** idle notebook instances do not stop after specified time. swb runs autostop.py script to automatically stop notebook instance. the script is used by `on-start` lifecycle rule of the instance cfn template. according to lifecycleconfigonstart logs, some packages are missing and autostop script doesnt work. **to reproduce** steps to reproduce the behavior: 1. make sure autostopidletimeinminutes parameter in workspace type config is set to a required time (30 minutes in our case) 2. create a new workspace with notebook instance 3. leave the instance idle for the time specified (autostopidletimeinminutes ) 4. after the specified time see that the instance is not stopped **expected behavior** idle notebook instance automatically stops after specified time. **screenshots** **versions (please complete the following information):** - release version installed v5.0.0 **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where idle notebook instances do not stop automatically after the specified time, despite the autostop.py script being used by the instance cfn template.",
        "Issue_preprocessed_content":"Title: instance does not stop automatically; Content: describe the bug idle notebook instances do not stop after specified time. swb runs script to automatically stop notebook instance. the script is used by lifecycle rule of the instance cfn template. according to lifecycleconfigonstart logs, some packages are missing and autostop script doesnt work. to reproduce steps to reproduce the behavior . make sure autostopidletimeinminutes parameter in workspace type config is set to a required time . create a new workspace with notebook instance . leave the instance idle for the time specified . after the specified time see that the instance is not stopped expected behavior idle notebook instance automatically stops after specified time. screenshots img width alt screen shot at am versions release version installed additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1073",
        "Issue_title":"[Bug] Sagemaker AWS Credentials not Populating",
        "Issue_creation_time":1669825870000,
        "Issue_closed_time":1671215597000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"When a Sagemaker notebook is launched with an associated study, the study folders are not mounted. Digging into this, I see that in the `~\/.aws` folder there is no credentials file, which is generated by the `mount_s3.sh` script.\r\n\r\n**To Reproduce**\r\n- Create a new data source (or use an existing one).\r\n- Select it and create a new Sagemaker instance using those studies. (Ensure that the folder has files so you can see them if they mount.)\r\n- After the instance launches, connect to it and see if the study folders are connected. If not, open a terminal and run `ls ~\/.aws` to see if the credential file is there.\r\n\r\n**Expected behavior**\r\nThe study folders are mounted using the assumed roles in the AWS credentials file, generated by `mount_s3.sh` script.\r\n\r\n**Screenshots**\r\n<img width=\"702\" alt=\"Screen Shot 2022-11-30 at 11 27 45 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853707-789607d5-6fca-4a77-911d-50a5c36a549b.png\">\r\n<img width=\"526\" alt=\"Screen Shot 2022-11-30 at 11 27 36 AM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/204853710-824ddc98-dfb5-4c8b-abbe-f19fa2453640.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - Versions 5.0.0 & 4.3.1\r\n\r\n**Additional context**\r\nThis may or may not be associated with the other bug I noted with mounting s3 studies folders, [[Bug] SWB Sagemaker Study permission denied](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067). It seems both of these issues are new, within the last couple weeks. And the environment has had no new deployments or changes within that time. Previous to these last couple weeks we had no issues with Sagemaker and study folders mounting.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] aws credentials not populating; Content: when a notebook is launched with an associated study, the study folders are not mounted. digging into this, i see that in the `~\/.aws` folder there is no credentials file, which is generated by the `mount_s3.sh` script. **to reproduce** - create a new data source (or use an existing one). - select it and create a new instance using those studies. (ensure that the folder has files so you can see them if they mount.) - after the instance launches, connect to it and see if the study folders are connected. if not, open a terminal and run `ls ~\/.aws` to see if the credential file is there. **expected behavior** the study folders are mounted using the assumed roles in the aws credentials file, generated by `mount_s3.sh` script. **screenshots** **versions (please complete the following information):** - versions 5.0.0 & 4.3.1 **additional context** this may or may not be associated with the other bug i noted with mounting s3 studies folders, [[bug] swb study permission denied](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067). it seems both of these issues are new, within the last couple weeks. and the environment has had no new deployments or changes within that time. previous to these last couple weeks we had no issues with and study folders mounting.",
        "Issue_original_content_gpt_summary":"The user encountered challenges with AWS credentials not populating when launching a notebook with an associated study, resulting in the study folders not being mounted.",
        "Issue_preprocessed_content":"Title: aws credentials not populating; Content: when a notebook is launched with an associated study, the study folders are not mounted. digging into this, i see that in the folder there is no credentials file, which is generated by the script. to reproduce create a new data source . select it and create a new instance using those studies. after the instance launches, connect to it and see if the study folders are connected. if not, open a terminal and run to see if the credential file is there. expected behavior the study folders are mounted using the assumed roles in the aws credentials file, generated by script. screenshots img width alt screen shot at am img width alt screen shot at am versions versions & additional context this may or may not be associated with the other bug i noted with mounting s studies folders, swb study permission it seems both of these issues are new, within the last couple weeks. and the environment has had no new deployments or changes within that time. previous to these last couple weeks we had no issues with and study folders mounting."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1067",
        "Issue_title":"[Bug] SWB Sagemaker Study permission denied",
        "Issue_creation_time":1668634688000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids, but the existing studies don't typically change.\r\n\r\nWhat could cause the fs role number to change for a study? What else could cause this permissions denied error? \r\n\r\nThis is a pretty big problem for us, as we have had people actively using SWB and all their work is gone on Sagemaker stop, because the folder they saved to isn't syncing.\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] swb study permission denied; Content: we encountered an issue where an older instance (>2 months) was turned on. after starting, one of the two study folders associated were not syncing any of the files. in the system logs there's this error: `nov 11 16:21:45 \/usr\/local\/bin\/goofys[9204]: main.error unable to access '': permission denied` comparing the s3mounts parameter for the stack of the older instance that fails to sync, and a newer instance (with the same studies), i see that the fs role number for the private workspace study that wouldn't sync is different. old stack s3mounts parameter: ``` [ { \"id\": \"private-workspace\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1662735997814\", \"prefix\": \"private-workspace\/\", \"readable\": true, \"writeable\": true }, { \"id\": \"read-only\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1661808852807\", \"prefix\": \"read-only\/\", \"readable\": true, \"writeable\": false } ] ``` new stack s3mounts parameter: ``` [ { \"id\": \"private-workspace\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1668521384862\", \"prefix\": \"private-workspace\/\", \"readable\": true, \"writeable\": true }, { \"id\": \"read-only\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1661808852807\", \"prefix\": \"read-only\/\", \"readable\": true, \"writeable\": false } ] ``` some additional context, this bucket (and the associated swb data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids, but the existing studies don't typically change. what could cause the fs role number to change for a study? what else could cause this permissions denied error? this is a pretty big problem for us, as we have had people actively using swb and all their work is gone on stop, because the folder they saved to isn't syncing. **versions (please complete the following information):** - swb 4.3.1",
        "Issue_original_content_gpt_summary":"The user encountered an issue where an older instance of SWB (>2 months) was turned on and one of the two study folders associated were not syncing any of the files, resulting in a \"permission denied\" error in the system logs, and causing a major problem for users who had been actively using SWB and had their work disappear.",
        "Issue_preprocessed_content":"Title: swb study permission denied; Content: we encountered an issue where an older instance was turned on. after starting, one of the two study folders associated were not syncing any of the files. in the system logs there's this error comparing the s mounts parameter for the stack of the older instance that fails to sync, and a newer instance , i see that the fs role number for the private workspace study that wouldn't sync is different. old stack s mounts parameter new stack s mounts parameter some additional context, this bucket that the two studies are a part of gets updated every couple months to add new study but the existing studies don't typically change. what could cause the fs role number to change for a study? what else could cause this permissions denied error? this is a pretty big problem for us, as we have had people actively using swb and all their work is gone on stop, because the folder they saved to isn't syncing. versions swb"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1066",
        "Issue_title":"[Bug] SWB Sagemaker Study permission denied",
        "Issue_creation_time":1668633119000,
        "Issue_closed_time":1668634670000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"We encountered an issue where an older Sagemaker instance (>2 months) was turned on. After starting, one of the two study folders associated were not syncing any of the files. In the system logs there's this error: `Nov 11 16:21:45 <ip redacted> \/usr\/local\/bin\/goofys[9204]: main.ERROR Unable to access '<bucket A, name redacted>': permission denied`\r\n\r\nComparing the S3mounts parameter for the Sagemaker stack of the older instance that fails to sync, and a newer instance (with the same studies), I see that the FS role number for the private workspace study that wouldn't sync is different.\r\n\r\nOld stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1662735997814\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nNew stack S3mounts parameter:\r\n```\r\n[\r\n  {\r\n    \"id\": \"Private-workspace\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1668521384862\",\r\n    \"prefix\": \"Private-workspace\/\",\r\n    \"readable\": true,\r\n    \"writeable\": true\r\n  },\r\n  {\r\n    \"id\": \"READ-only\",\r\n    \"bucket\": \"<bucket A, name redacted>\",\r\n    \"region\": \"us-east-1\",\r\n    \"roleArn\": \"arn:aws:iam::<account redacted>:role\/swb-LhDhyIAqCHc0a4vrlU256w-fs-1661808852807\",\r\n    \"prefix\": \"READ-only\/\",\r\n    \"readable\": true,\r\n    \"writeable\": false\r\n  }\r\n]\r\n```\r\n\r\nSome additional context, this bucket (and the associated SWB data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids.\r\n\r\nMy question is: What could cause the fs role number to change for a study?\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] swb study permission denied; Content: we encountered an issue where an older instance (>2 months) was turned on. after starting, one of the two study folders associated were not syncing any of the files. in the system logs there's this error: `nov 11 16:21:45 \/usr\/local\/bin\/goofys[9204]: main.error unable to access '': permission denied` comparing the s3mounts parameter for the stack of the older instance that fails to sync, and a newer instance (with the same studies), i see that the fs role number for the private workspace study that wouldn't sync is different. old stack s3mounts parameter: ``` [ { \"id\": \"private-workspace\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1662735997814\", \"prefix\": \"private-workspace\/\", \"readable\": true, \"writeable\": true }, { \"id\": \"read-only\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1661808852807\", \"prefix\": \"read-only\/\", \"readable\": true, \"writeable\": false } ] ``` new stack s3mounts parameter: ``` [ { \"id\": \"private-workspace\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1668521384862\", \"prefix\": \"private-workspace\/\", \"readable\": true, \"writeable\": true }, { \"id\": \"read-only\", \"bucket\": \"\", \"region\": \"us-east-1\", \"rolearn\": \"arn:aws:iam:::role\/swb-lhdhyiaqchc0a4vrlu256w-fs-1661808852807\", \"prefix\": \"read-only\/\", \"readable\": true, \"writeable\": false } ] ``` some additional context, this bucket (and the associated swb data source) that the two studies are a part of gets updated every couple months to add new study folders\/ids. my question is: what could cause the fs role number to change for a study? **versions (please complete the following information):** - swb 4.3.1",
        "Issue_original_content_gpt_summary":"The user encountered an issue where an older instance of SWB was not syncing one of the two study folders associated with it, and the system logs showed a \"permission denied\" error, which was caused by a difference in the fs role number for the private workspace study between the old and new stacks.",
        "Issue_preprocessed_content":"Title: swb study permission denied; Content: we encountered an issue where an older instance was turned on. after starting, one of the two study folders associated were not syncing any of the files. in the system logs there's this error comparing the s mounts parameter for the stack of the older instance that fails to sync, and a newer instance , i see that the fs role number for the private workspace study that wouldn't sync is different. old stack s mounts parameter new stack s mounts parameter some additional context, this bucket that the two studies are a part of gets updated every couple months to add new study my question is what could cause the fs role number to change for a study? versions swb"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1065",
        "Issue_title":"[Bug] Sagemaker autostop script not pulling from s3 bucket",
        "Issue_creation_time":1668620515000,
        "Issue_closed_time":1671215663000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":16.0,
        "Issue_body":"**Describe the bug**\r\nWe encountered an interesting issue regarding the auto stop script. We had no code changes, but suddenly, Sagemaker instances started hanging around for days, with no use. Looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. When I look at the script, it has this line `print(f'Notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. However, the file on the repo, as well as the s3 bucket, does not contain this line. So, after some digging, I found that this line was introduced here, in this commit [aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). What I don't understand is how it got into the Sagemaker notebook, and why it's not being overridden by the custom config start we have here [sagemaker-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/sagemaker-notebook-instance.cfn.yml#L264-L272) This script and repo was updated in the last 16 hours to remove this syntax error.\r\n\r\n**To Reproduce**\r\nLaunch a Sagemaker instance. You can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101.\r\n\r\nThe AWS version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/sagemaker\/autostop.py#L96-L100)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n```\r\nAnd on the `aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#L96-L101)\r\n```\r\nif notebook['kernel']['connections'] == 0:\r\n    if not is_idle(notebook['kernel']['last_activity']):\r\n        idle = False\r\nelse:\r\n    idle = False\r\n    print('Notebook idle state set as %s because no kernel has been detected.' % idle)\r\n```\r\n\r\n**Expected behavior**\r\nThe autostop script in the s3 bucket should be the one used for SWB Sagemaker instances.\r\n\r\n**Screenshots**\r\n<img width=\"1510\" alt=\"Screen Shot 2022-11-16 at 12 14 21 PM\" src=\"https:\/\/user-images.githubusercontent.com\/21109191\/202251401-67da0253-e74e-40e9-8150-99a4a27017ff.png\">\r\n\r\n**Versions (please complete the following information):**\r\n - SWB 4.3.1\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] autostop script not pulling from s3 bucket; Content: **describe the bug** we encountered an interesting issue regarding the auto stop script. we had no code changes, but suddenly, instances started hanging around for days, with no use. looking into the instance, the cron job was failing, because the autostop.py script had a syntax error. when i look at the script, it has this line `print(f'notebook idle state set as {idle} because no kernel has been detected.')` which caused the syntax error. however, the file on the repo, as well as the s3 bucket, does not contain this line. so, after some digging, i found that this line was introduced here, in this commit [aws-samples\/amazon--notebook-instance-lifecycle-config-samples\/](https:\/\/github.com\/aws-samples\/amazon--notebook-instance-lifecycle-config-samples\/commit\/fdace58a6b9401c53dc17f5c64bef3ec40dbc70e). what i don't understand is how it got into the notebook, and why it's not being overridden by the custom config start we have here [-notebook-instance.cfn.yml](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/addons\/addon-base-raas\/packages\/base-raas-cfn-templates\/src\/templates\/service-catalog\/-notebook-instance.cfn.yml#l264-l272) this script and repo was updated in the last 16 hours to remove this syntax error. **to reproduce** launch a instance. you can tell which version of the script it's using by looking at the autostop script, `less \/usr\/local\/bin\/autostop.py` and find lines 96-101. the aws version of the script on the `awslabs\/service-workbench-on-aws` repo has these lines, [reference](https:\/\/github.com\/awslabs\/service-workbench-on-aws\/blob\/mainline\/main\/solution\/post-deployment\/config\/environment-files\/offline-packages\/\/autostop.py#l96-l100) ``` if notebook['kernel']['connections'] == 0: if not is_idle(notebook['kernel']['last_activity']): idle = false else: idle = false ``` and on the `aws-samples\/amazon--notebook-instance-lifecycle-config-samples` repo, [reference](https:\/\/github.com\/aws-samples\/amazon--notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py#l96-l101) ``` if notebook['kernel']['connections'] == 0: if not is_idle(notebook['kernel']['last_activity']): idle = false else: idle = false print('notebook idle state set as %s because no kernel has been detected.' % idle) ``` **expected behavior** the autostop script in the s3 bucket should be the one used for swb instances. **screenshots** **versions (please complete the following information):** - swb 4.3.1",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the autostop script not pulling from the s3 bucket, resulting in a syntax error and instances hanging around for days with no use.",
        "Issue_preprocessed_content":"Title: autostop script not pulling from s bucket; Content: describe the bug we encountered an interesting issue regarding the auto stop script. we had no code changes, but suddenly, instances started hanging around for days, with no use. looking into the instance, the cron job was failing, because the script had a syntax error. when i look at the script, it has this line which caused the syntax error. however, the file on the repo, as well as the s bucket, does not contain this line. so, after some digging, i found that this line was introduced here, in this commit . what i don't understand is how it got into the notebook, and why it's not being overridden by the custom config start we have here this script and repo was updated in the last hours to remove this syntax error. to reproduce launch a instance. you can tell which version of the script it's using by looking at the autostop script, and find lines the aws version of the script on the repo has these lines, and on the repo, expected behavior the autostop script in the s bucket should be the one used for swb instances. screenshots img width alt screen shot at pm versions swb"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/1018",
        "Issue_title":"[Bug] SageMaker instances can't be launched due to missing tags permission",
        "Issue_creation_time":1658897296000,
        "Issue_closed_time":1659686697000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"**Describe the bug**\r\nService Workbench appears to be unable to launch SageMaker notebook instances at all, due to a missing permission for `sagemaker:AddTags`. This seems to also be the case when custom tags aren't included in the workspace configuration.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install Service Workbench from the latest version.\r\n2. Create a workspace configuration for a SageMaker notebook.\r\n3. Launch a workspace using the new configuration.\r\n4. Wait a few minutes and observe the error.\r\n\r\n**Expected behavior**\r\nExpected the notebook to launch :)\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png)\r\n```\r\nError provisioning environment TestNotebook1. Reason: Errors from CloudFormation: [{LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : The following resource(s) failed to create: [BasicNotebookInstance]. Rollback requested by user.}, {LogicalResourceId : BasicNotebookInstance, ResourceType : AWS::SageMaker::NotebookInstance, StatusReason : User: arn:aws:sts::XXXXXXXXXXXX:assumed-role\/dev-syd-timswb-LaunchConstraint\/servicecatalog is not authorized to perform: sagemaker:AddTags on resource: arn:aws:sagemaker:ap-southeast-2:XXXXXXXXXXXX:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the sagemaker:AddTags action (Service: AmazonSageMaker; Status Code: 400; Error Code: AccessDeniedException; Request ID: adee97b7-1c89-47e2-8ca7-5aa374a80004; Proxy: null)}, {LogicalResourceId : IAMRole, ResourceType : AWS::IAM::Role, StatusReason : Resource creation Initiated}, {LogicalResourceId : SecurityGroup, ResourceType : AWS::EC2::SecurityGroup, StatusReason : Resource creation Initiated}, {LogicalResourceId : InstanceRolePermissionBoundary, ResourceType : AWS::IAM::ManagedPolicy, StatusReason : Resource creation Initiated}, {LogicalResourceId : BasicNotebookInstanceLifecycleConfig, ResourceType : AWS::SageMaker::NotebookInstanceLifecycleConfig, StatusReason : Resource creation Initiated}, {LogicalResourceId : SC-455040667691-pp-auh6sv7j6dwr2, ResourceType : AWS::CloudFormation::Stack, StatusReason : User Initiated}]\r\n```\r\n\r\n**Versions (please complete the following information):**\r\n5.2.0\r\n(also replicated on an older 5.0.0 install)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] instances can't be launched due to missing tags permission; **describe the bug** service workbench appears to be unable to launch notebook instances at all, due to a missing permission for `:addtags`. this seems to also be the case when custom tags aren't included in the workspace configuration. **to reproduce** steps to reproduce the behavior: 1. install service workbench from the latest version. 2. create a workspace configuration for a notebook. 3. launch a workspace using the new configuration. 4. wait a few minutes and observe the error. **expected behavior** expected the notebook to launch :) **screenshots** ![image](https:\/\/user-images.githubusercontent.com\/900469\/181163664-98441ee8-7316-4d29-8f85-79d3e5e6ed3c.png) ``` error provisioning environment testnotebook1. reason: errors from cloudformation: [{logicalresourceid : sc-455040667691-pp-auh6sv7j6dwr2, resourcetype : aws::cloudformation::stack, statusreason : the following resource(s) failed to create: [basicnotebookinstance]. rollback requested by user.}, {logicalresourceid : basicnotebookinstance, resourcetype : aws::::notebookinstance, statusreason : user: arn:aws:sts::xxxxxxxxxxxx:assumed-role\/dev-syd-timswb-launchconstraint\/servicecatalog is not authorized to perform: :addtags on resource: arn:aws::ap-southeast-2:xxxxxxxxxxxx:assumed:notebook-instance\/basicnotebookinstance-y4ices04e3sv because no identity-based policy allows the :addtags action (service: amazon; status code: 400; error code: accessdeniedexception; request id: adee97b7-1c89-47e2-8ca7-5aa374a80004; Content: proxy: null)}, {logicalresourceid : iamrole, resourcetype : aws::iam::role, statusreason : resource creation initiated}, {logicalresourceid : securitygroup, resourcetype : aws::ec2::securitygroup, statusreason : resource creation initiated}, {logicalresourceid : instancerolepermissionboundary, resourcetype : aws::iam::managedpolicy, statusreason : resource creation initiated}, {logicalresourceid : basicnotebookinstancelifecycleconfig, resourcetype : aws::::notebookinstancelifecycleconfig, statusreason : resource creation initiated}, {logicalresourceid : sc-455040667691-pp-auh6sv7j6dwr2, resourcetype : aws::cloudformation::stack, statusreason : user initiated}] ``` **versions (please complete the following information):** 5.2.0 (also replicated on an older 5.0.0 install) **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where Service Workbench was unable to launch notebook instances due to a missing permission for `:addtags`.",
        "Issue_preprocessed_content":"Title: instances can't be launched due to missing tags permission; Content: describe the bug service workbench appears to be unable to launch notebook instances at all, due to a missing permission for . this seems to also be the case when custom tags aren't included in the workspace configuration. to reproduce steps to reproduce the behavior . install service workbench from the latest version. . create a workspace configuration for a notebook. . launch a workspace using the new configuration. . wait a few minutes and observe the error. expected behavior expected the notebook to launch screenshots versions also replicated on an older install additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/708",
        "Issue_title":"[Bug] SageMaker Notebook-v3 Workspace changed to \"Unknown\" status and cannot connect anymore",
        "Issue_creation_time":1631554276000,
        "Issue_closed_time":1633460282000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\nA SageMaker Notebook-v3 workspace that was working fine on Friday today appears with the status as \"Unknown\". \r\nWhen clicking on connect the new window pop up but is empty, and when going back to the SWB page, we see the message, \"We have a problem! Something went wrong\"\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to 'Workspaces'\r\n2. Look for the workspace that was expected to be \"Stoped\"\r\n2. Click on 'connect'\r\n4. See error\r\n\r\n**Expected behavior**\r\nThat the workspace was \"Stopped\" and when clicking on Connect we can access to the workspace. \r\n\r\n**Screenshots**\r\n![Screen Shot 2021-09-13 at 1 27 57 PM](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png)\r\n\r\n**Versions (please complete the following information):**\r\nRelease Version installed: 3.3.1\r\n\r\n**Additional context**\r\nThe workspace was working fine all previous week, autostop and connect without any issue. Unknown status found today.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] notebook-v3 workspace changed to \"unknown\" status and cannot connect anymore; Content: **describe the bug** a notebook-v3 workspace that was working fine on friday today appears with the status as \"unknown\". when clicking on connect the new window pop up but is empty, and when going back to the swb page, we see the message, \"we have a problem! something went wrong\" **to reproduce** steps to reproduce the behavior: 1. go to 'workspaces' 2. look for the workspace that was expected to be \"stoped\" 2. click on 'connect' 4. see error **expected behavior** that the workspace was \"stopped\" and when clicking on connect we can access to the workspace. **screenshots** ![screen shot 2021-09-13 at 1 27 57 pm](https:\/\/user-images.githubusercontent.com\/19646530\/133129766-85139082-e6e7-4fe1-8624-dedebf573ea5.png) **versions (please complete the following information):** release version installed: 3.3.1 **additional context** the workspace was working fine all previous week, autostop and connect without any issue. unknown status found today.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where a notebook-v3 workspace changed to an \"unknown\" status and could not connect, resulting in an error message when attempting to access the workspace.",
        "Issue_preprocessed_content":"Title: workspace changed to unknown status and cannot connect anymore; Content: describe the bug a workspace that was working fine on friday today appears with the status as unknown . when clicking on connect the new window pop up but is empty, and when going back to the swb page, we see the message, we have a problem! something went wrong to reproduce steps to reproduce the behavior . go to 'workspaces' . look for the workspace that was expected to be stoped . click on 'connect' . see error expected behavior that the workspace was stopped and when clicking on connect we can access to the workspace. screenshots versions release version installed additional context the workspace was working fine all previous week, autostop and connect without any issue. unknown status found today."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/620",
        "Issue_title":"\"null is not an object\" while trying to connect to Sagemaker notebook.",
        "Issue_creation_time":1628006476000,
        "Issue_closed_time":1643923114000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nOccasionally after starting a Sagemaker workspace, clicking 'Connect' gives an error in the bottom right-hand corner of the screen:\r\n\r\n> We have a problem!\r\n> null is not an object (evaluating 'l.location=s') \r\n\r\nin a little red box on the bottom-right of the screen. The notebook window is not opened after clicking on 'Connect'.\r\n\r\n**To Reproduce**\r\nThe error is intermittent. I *think* it may happen after the SW window has been open a while, because I noticed that the SW window automatically logged me out shortly after seeing this error.\r\n\r\n1. Click 'Start' for Sagemaker workspace and wait for the status to change to 'Available'. \r\n2. Click 'Connections', then 'Connect'\r\n3. See error\r\n\r\nWhen I logged out and back into Service Workbench, and was able to connect to the workspace successfully. \r\n\r\n**Expected behavior**\r\nA new window should open with a Jupyter\/Sagemaker notebook in a new window. \r\n\r\n**Versions (please complete the following information):**\r\n - 3.2.0\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: \"null is not an object\" while trying to connect to notebook.; Content: **describe the bug** occasionally after starting a workspace, clicking 'connect' gives an error in the bottom right-hand corner of the screen: > we have a problem! > null is not an object (evaluating 'l.location=s') in a little red box on the bottom-right of the screen. the notebook window is not opened after clicking on 'connect'. **to reproduce** the error is intermittent. i *think* it may happen after the sw window has been open a while, because i noticed that the sw window automatically logged me out shortly after seeing this error. 1. click 'start' for workspace and wait for the status to change to 'available'. 2. click 'connections', then 'connect' 3. see error when i logged out and back into service workbench, and was able to connect to the workspace successfully. **expected behavior** a new window should open with a jupyter\/ notebook in a new window. **versions (please complete the following information):** - 3.2.0",
        "Issue_original_content_gpt_summary":"The user encountered an intermittent issue where clicking 'connect' to a workspace in Service Workbench 3.2.0 resulted in an error message of \"null is not an object (evaluating 'l.location=s')\" in a red box on the bottom-right of the screen, preventing the notebook window from opening.",
        "Issue_preprocessed_content":"Title: null is not an object while trying to connect to notebook.; Content: describe the bug occasionally after starting a workspace, clicking 'connect' gives an error in the bottom corner of the screen we have a problem! null is not an object in a little red box on the of the screen. the notebook window is not opened after clicking on 'connect'. to reproduce the error is intermittent. i think it may happen after the sw window has been open a while, because i noticed that the sw window automatically logged me out shortly after seeing this error. . click 'start' for workspace and wait for the status to change to 'available'. . click 'connections', then 'connect' . see error when i logged out and back into service workbench, and was able to connect to the workspace successfully. expected behavior a new window should open with a notebook in a new window. versions"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/service-workbench-on-aws\/issues\/509",
        "Issue_title":"[Bug] Blank Page on Sagemaker Workspace Connect",
        "Issue_creation_time":1622734645000,
        "Issue_closed_time":1624287519000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nWhen connecting to Sagemaker workspaces, there is an intermittent issue where a blank browser launches instead of Sagemaker.  The issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nSTEP 1: Login as an Admin \r\nSTEP 2: Create a workspace (SageMaker)\r\nSTEP 3: Start the Workspace \r\nSTEP 5: Click \"Connect\"\r\nSTEP 6: A new blank web browser tab opens \r\nSTEP 7: Click \"Connect\" again, another blank web browser tab opens\r\n\r\nUser receives a \"Something Went Wrong\" general error in SWB at Step 6\r\n\r\nIn the client logs for the browser, there is also this error noted:\r\n              \"name\": \"x-cache\",\r\n              \"value\": \"Error from cloudfront\"\r\n   \r\n\r\n**Expected behavior**\r\nSagemaker workspace launch in browser\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions (please complete the following information):**\r\n - Release Version installed 3.0.0\r\n\r\n**Additional context**\r\nUser has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. \r\n\r\nThe issue is experienced approximately once a week. Sometimes clearing cache solves the issue. Other times going to incognito, and it does not solve the issue.\r\n\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] blank page on workspace connect; Content: **describe the bug** when connecting to workspaces, there is an intermittent issue where a blank browser launches instead of . the issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted. **to reproduce** steps to reproduce the behavior: step 1: login as an admin step 2: create a workspace () step 3: start the workspace step 5: click \"connect\" step 6: a new blank web browser tab opens step 7: click \"connect\" again, another blank web browser tab opens user receives a \"something went wrong\" general error in swb at step 6 in the client logs for the browser, there is also this error noted: \"name\": \"x-cache\", \"value\": \"error from cloudfront\" **expected behavior** workspace launch in browser **screenshots** if applicable, add screenshots to help explain your problem. **versions (please complete the following information):** - release version installed 3.0.0 **additional context** user has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. the issue is experienced approximately once a week. sometimes clearing cache solves the issue. other times going to incognito, and it does not solve the issue.",
        "Issue_original_content_gpt_summary":"The user is experiencing an intermittent issue where a blank browser launches instead of the workspace when connecting to workspaces, which presents for both newly created and restarted workspaces, and is accompanied by an \"x-cache\" error in the client logs.",
        "Issue_preprocessed_content":"Title: blank page on workspace connect; Content: describe the bug when connecting to workspaces, there is an intermittent issue where a blank browser launches instead of . the issue presents for workspaces that are newly created as well as for workspaces that were already created, but were stopped and are being restarted. to reproduce steps to reproduce the behavior step login as an admin step create a workspace step start the workspace step click connect step a new blank web browser tab opens step click connect again, another blank web browser tab opens user receives a something went wrong general error in swb at step in the client logs for the browser, there is also this error noted name value error from cloudfront expected behavior workspace launch in browser screenshots if applicable, add screenshots to help explain your problem. versions release version installed additional context user has cleared cache and it solved the issue, but for one of her employees clearing the cache did not solve the issue. the issue is experienced approximately once a week. sometimes clearing cache solves the issue. other times going to incognito, and it does not solve the issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/928",
        "Issue_title":"[SM-Executor] SageMaker.stop_training_job hangs",
        "Issue_creation_time":1570606746000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Calling `stop_training_job` from the SageMaker client against an existing by not \"InProgress\" job, causes the client to hang. This only seems to happen within the sm-executor though. \r\n\r\nHere's the output calling the method from the python interpreter within the pod:\r\n```\r\n# .\/python\r\nPython 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21)\r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import boto3\r\n>>> client = boto3.client(\"sagemaker\")\r\n>>> client.stop_training_job(TrainingJobName=\"98cb7232-02b1-4a1b-a59e-55a8eca9e048\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (ValidationException) when calling the StopTrainingJob operation: The request was rejected because the training job is in status Stopped.\r\n>>>\r\n```\r\n\r\nHere is the DEBUG output from the sm-executor\r\n```\r\n2019-10-09 06:39:38,252 INFO: Attempting to stop training job 98cb7232-02b1-4a1b-a59e-55a8eca9e048\r\n2019-10-09 06:39:38,252 DEBUG: Event before-parameter-build.sagemaker.StopTrainingJob: calling handler <function generate_idempotent_uuid at 0x7f54d82671e0>\r\n2019-10-09 06:39:38,253 DEBUG: Event before-call.sagemaker.StopTrainingJob: calling handler <function inject_api_version_header_if_needed at 0x7f54d8268b70>\r\n2019-10-09 06:39:38,253 DEBUG: Making request for OperationModel(name=StopTrainingJob) with params: {'url_path': '\/', 'query_string': '', 'method': 'POST', 'headers': {'X-Amz-Target': 'SageMaker.StopTrainingJob', 'Content-Type': 'application\/x-amz-json-1.1', 'User-Agent': 'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221'}, 'body': b'{\"TrainingJobName\": \"98cb7232-02b1-4a1b-a59e-55a8eca9e048\"}', 'url': 'https:\/\/api.sagemaker.us-east-1.amazonaws.com\/', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x7f54bacde518>, 'has_streaming_input': False, 'auth_type': None}}\r\n2019-10-09 06:39:38,253 DEBUG: Event request-created.sagemaker.StopTrainingJob: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x7f54bacde4e0>>\r\n2019-10-09 06:39:38,254 DEBUG: Event choose-signer.sagemaker.StopTrainingJob: calling handler <function set_operation_specific_signer at 0x7f54d82670d0>\r\n2019-10-09 06:39:38,254 DEBUG: Calculating signature using v4 auth.\r\n2019-10-09 06:39:38,254 DEBUG: CanonicalRequest:\r\nPOST\r\n\/\r\n\r\ncontent-type:application\/x-amz-json-1.1\r\nhost:api.sagemaker.us-east-1.amazonaws.com\r\nx-amz-date:20191009T063938Z\r\nx-amz-security-token:FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF\r\nx-amz-target:SageMaker.StopTrainingJob\r\n\r\ncontent-type;host;x-amz-date;x-amz-security-token;x-amz-target\r\n84e242897f2f826cc224094427e7ba8bc4c2f559097741460b59e162e8114c40\r\n2019-10-09 06:39:38,254 DEBUG: StringToSign:\r\nAWS4-HMAC-SHA256\r\n20191009T063938Z\r\n20191009\/us-east-1\/sagemaker\/aws4_request\r\n4320231908e4cd91204a6044a6201b1a74c0a63a2f708f9c4c27df2d6a6344db\r\n2019-10-09 06:39:38,255 DEBUG: Signature:\r\nc074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae\r\n2019-10-09 06:39:38,255 DEBUG: Sending http request: <AWSPreparedRequest stream_output=False, method=POST, url=https:\/\/api.sagemaker.us-east-1.amazonaws.com\/, headers={'X-Amz-Target': b'SageMaker.StopTrainingJob', 'Content-Type': b'application\/x-amz-json-1.1', 'User-Agent': b'Boto3\/1.9.221 Python\/3.7.3 Linux\/4.14.128-112.105.amzn2.x86_64 Botocore\/1.12.221', 'X-Amz-Date': b'20191009T063938Z', 'X-Amz-Security-Token': b'FQoGZXIvYXdzEMj\/\/\/\/\/\/\/\/\/\/wEaDFbwYhfMhbwcrxMnQiKEAh9qXHxpmHbCDKDDcH4UNekdyuxX+8R3yub8KIGVZjEuvcH64xIAOgWnkb2ZtrIsoYUFWGQB2C6+NSptni65YVATyi6+ZedRB0RHjLyFE98l5b0DEcM5IE7O0xq7zflpIFTtOK9h7QeNh9n8MAe69xEvthv0Gd34dalXMlUFALYSvb6+Ewo7rvFPjDEZ+1xqlSLKwMbpA8YJ+ngJdhXCkiBGpCwXuXIP+zvSSx5+gENSWdzOJ\/OTdCKepxD25OutUvf5WN+usAkv1U4dDiG8MfPumZJg\/m93LUUzX3ok88XC6dMwajhayc9XH5n89ZyzgXmq5np\/wkCoU\/wbOLsMdvDaAy41KPSA9uwF', 'Authorization': b'AWS4-HMAC-SHA256 Credential=ASIAYNI7SS57NFEDAZHQ\/20191009\/us-east-1\/sagemaker\/aws4_request, SignedHeaders=content-type;host;x-amz-date;x-amz-security-token;x-amz-target, Signature=c074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae', 'Content-Length': '59'}>\r\n2019-10-09 06:39:38,320 DEBUG: Response headers: {'x-amzn-RequestId': '03dd9de3-3672-4f4b-b575-a06d29e15e6b', 'Content-Type': 'application\/x-amz-json-1.1', 'Content-Length': '116', 'Date': 'Wed, 09 Oct 2019 06:39:37 GMT', 'Connection': 'close'}\r\n2019-10-09 06:39:38,320 DEBUG: Response body:\r\nb'{\"__type\":\"ValidationException\",\"message\":\"The request was rejected because the training job is in status Stopped.\"}'\r\n2019-10-09 06:39:38,320 DEBUG: Event needs-retry.sagemaker.StopTrainingJob: calling handler <botocore.retryhandler.RetryHandler object at 0x7f54bacde898>\r\n2019-10-09 06:39:38,321 DEBUG: No retry needed.\r\n```",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [sm-executor] .stop_training_job hangs; Content: calling `stop_training_job` from the client against an existing by not \"inprogress\" job, causes the client to hang. this only seems to happen within the sm-executor though. here's the output calling the method from the python interpreter within the pod: ``` # .\/python python 3.7.3 | packaged by conda-forge | (default, jul 1 2019, 21:52:21) [gcc 7.3.0] :: anaconda, inc. on linux type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import boto3 >>> client = boto3.client(\"\") >>> client.stop_training_job(trainingjobname=\"98cb7232-02b1-4a1b-a59e-55a8eca9e048\") traceback (most recent call last): file \"\", line 1, in file \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 357, in _api_call return self._make_api_call(operation_name, kwargs) file \"\/opt\/env\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 661, in _make_api_call raise error_class(parsed_response, operation_name) botocore.exceptions.clienterror: an error occurred (validationexception) when calling the stoptrainingjob operation: the request was rejected because the training job is in status stopped. >>> ``` here is the debug output from the sm-executor ``` 2019-10-09 06:39:38,252 info: attempting to stop training job 98cb7232-02b1-4a1b-a59e-55a8eca9e048 2019-10-09 06:39:38,252 debug: event before-parameter-build..stoptrainingjob: calling handler 2019-10-09 06:39:38,253 debug: event before-call..stoptrainingjob: calling handler 2019-10-09 06:39:38,253 debug: making request for operationmodel(name=stoptrainingjob) with params: {'url_path': '\/', 'query_string': '', 'method': 'post', 'headers': {'x-amz-target': '.stoptrainingjob', 'content-type': 'application\/x-amz-json-1.1', 'user-agent': 'boto3\/1.9.221 python\/3.7.3 linux\/4.14.128-112.105.amzn2.x86_64 botocore\/1.12.221'}, 'body': b'{\"trainingjobname\": \"98cb7232-02b1-4a1b-a59e-55a8eca9e048\"}', 'url': 'https:\/\/api..us-east-1.amazonaws.com\/', 'context': {'client_region': 'us-east-1', 'client_config': , 'has_streaming_input': false, 'auth_type': none}} 2019-10-09 06:39:38,253 debug: event request-created..stoptrainingjob: calling handler > 2019-10-09 06:39:38,254 debug: event choose-signer..stoptrainingjob: calling handler 2019-10-09 06:39:38,254 debug: calculating signature using v4 auth. 2019-10-09 06:39:38,254 debug: canonicalrequest: post \/ content-type:application\/x-amz-json-1.1 host:api..us-east-1.amazonaws.com x-amz-date:20191009t063938z x-amz-security-token:fqogzxivyxdzemj\/\/\/\/\/\/\/\/\/\/weadfbwyhfmhbwcrxmnqikeah9qxhxpmhbcdkddch4unekdyuxx+8r3yub8kigvzjeuvch64xiaogwnkb2ztrisoyufwgqb2c6+nsptni65yvatyi6+zedrb0rhjlyfe98l5b0decm5ie7o0xq7zflpifttok9h7qenh9n8mae69xevthv0gd34dalxmlufalysvb6+ewo7rvfpjdez+1xqlslkwmbpa8yj+ngjdhxckibgpcwxuxip+zvssx5+genswdzoj\/otdckepxd25outuvf5wn+usakv1u4ddig8mfpumzjg\/m93luuzx3ok88xc6dmwajhayc9xh5n89zyzgxmq5np\/wkcou\/wbolsmdvdaay41kpsa9uwf x-amz-target:.stoptrainingjob content-type;host;x-amz-date;x-amz-security-token;x-amz-target 84e242897f2f826cc224094427e7ba8bc4c2f559097741460b59e162e8114c40 2019-10-09 06:39:38,254 debug: stringtosign: aws4-hmac-sha256 20191009t063938z 20191009\/us-east-1\/\/aws4_request 4320231908e4cd91204a6044a6201b1a74c0a63a2f708f9c4c27df2d6a6344db 2019-10-09 06:39:38,255 debug: signature: c074cec50d69498f53c9f9283884363ee86010ccabece0d64f94e298f6d322ae 2019-10-09 06:39:38,255 debug: sending http request: 2019-10-09 06:39:38,320 debug: response headers: {'x-amzn-requestid': '03dd9de3-3672-4f4b-b575-a06d29e15e6b', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '116', 'date': 'wed, 09 oct 2019 06:39:37 gmt', 'connection': 'close'} 2019-10-09 06:39:38,320 debug: response body: b'{\"__type\":\"validationexception\",\"message\":\"the request was rejected because the training job is in status stopped.\"}' 2019-10-09 06:39:38,320 debug: event needs-retry..stoptrainingjob: calling handler 2019-10-09 06:39:38,321 debug: no retry needed. ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where calling the `stop_training_job` method from the client against an existing, but not \"inprogress\" job, caused the client to hang.",
        "Issue_preprocessed_content":"Title: hangs; Content: calling from the client against an existing by not inprogress job, causes the client to hang. this only seems to happen within the though. here's the output calling the method from the python interpreter within the pod here is the debug output from the"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/benchmark-ai\/issues\/907",
        "Issue_title":"Cannot run benchmark for sagemaker",
        "Issue_creation_time":1570150277000,
        "Issue_closed_time":1571326528000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"When I tried to run benchmark on sagemaker with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. \r\n<img width=\"1038\" alt=\"smmrcnn\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169329-e0ee3800-e5f4-11e9-887f-8e6fce87a917.png\">\r\n\r\nI also tried to run the sample for sagemaker https:\/\/github.com\/MXNetEdge\/benchmark-ai\/blob\/master\/sample-benchmarks\/sagemaker\/horovod.toml   and it showed with the same error\r\n<img width=\"1018\" alt=\"smsample\" src=\"https:\/\/user-images.githubusercontent.com\/54413235\/66169407-201c8900-e5f5-11e9-9de7-b46a7e9501a4.png\">\r\n\r\n\r\nBTW, when we wanna run with sagemaker, besides specify  execution_engine = \"aws.sagemaker\" and framework , is there anything else we need to specify or change?\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: cannot run benchmark for ; Content: when i tried to run benchmark on with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. i also tried to run the sample for https:\/\/github.com\/mxnetedge\/benchmark-ai\/blob\/master\/sample-benchmarks\/\/horovod.toml and it showed with the same error btw, when we wanna run with , besides specify execution_engine = \"aws.\" and framework , is there anything else we need to specify or change?",
        "Issue_original_content_gpt_summary":"The user encountered challenges when trying to run a benchmark on Anubis with either their own code or a sample code, resulting in an error message.",
        "Issue_preprocessed_content":"Title: cannot run benchmark for ; Content: when i tried to run benchmark on with anubis, it showed processing benchmark submission request and then cannot execute the requested benchmark. img width alt smmrcnn i also tried to run the sample for and it showed with the same error img width alt smsample btw, when we wanna run with , besides specify and framework , is there anything else we need to specify or change?"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/103",
        "Issue_title":"The data path inside sagemaker notebook does not work",
        "Issue_creation_time":1620285557000,
        "Issue_closed_time":1621931178000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The bucket of processed data does not exist (src\/sagemaker\/FD_SL_Training_BYO_Codes.ipynb)\r\n\r\n\r\n### Reproduction Steps\r\n\r\naws s3 ls s3:\/\/fraud-detection-solution\/processed_data\r\n\r\n\r\n\r\n### Error Log\r\n\r\nAn error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** 1.75.0 (build 7708242)\r\n  - **Framework Version:** not installed\r\n  - **Node.js Version:**  not installed\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\n<!-- e.g. detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated pull-request, stackoverflow, gitter, etc -->\r\n\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: the data path inside notebook does not work; Content: the bucket of processed data does not exist (src\/\/fd_sl_training_byo_codes.ipynb) ### reproduction steps aws s3 ls s3:\/\/fraud-detection-solution\/processed_data ### error log an error occurred (nosuchbucket) when calling the listobjectsv2 operation: the specified bucket does not exist ### environment - **cdk cli version:** 1.75.0 (build 7708242) - **framework version:** not installed - **node.js version:** not installed - **os :** ### other --- this is :bug: bug report",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the data path inside the notebook did not work, resulting in an error log of \"an error occurred (nosuchbucket) when calling the listobjectsv2 operation: the specified bucket does not exist\".",
        "Issue_preprocessed_content":"Title: the data path inside notebook does not work; Content: the bucket of processed data does not exist reproduction steps aws s ls error log an error occurred when calling the listobjectsv operation the specified bucket does not exist environment cdk cli version framework version not installed version not installed os other detailed explanation, stacktraces, related issues, suggestions on how to fix, links for us to have context, eg. associated stackoverflow, gitter, etc this is bug bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/realtime-fraud-detection-with-gnn-on-dgl\/issues\/57",
        "Issue_title":"sagemaker endpoint fail to deploy or time out server error(0) bug",
        "Issue_creation_time":1618212282000,
        "Issue_closed_time":1618279737000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Invoke Endpoint response time out. \r\n\r\n### Reproduction Steps\r\n\r\n{\r\n  \"trainingJob\": {\r\n    \"hyperparameters\": {\r\n    \"n-hidden\": \"2\",\r\n    \"n-epochs\": \"100\",\r\n    \"lr\":\"1e-2\"\r\n    },\r\n    \"instanceType\": \"ml.c5.9xlarge\",\r\n    \"timeoutInSeconds\": 10800    \r\n  }\r\n}\r\n\r\n\r\n\r\n### Error Log\r\nIn Inference Lambda CloudWatch:\r\n\r\nTask timed out after 120.10 seconds\r\n\r\n\r\nIn Sagemaker Training CloudWatch:\r\n\r\n2021-04-09   04:53:46,902 [INFO ] main org.pytorch.serve.ModelServer - Loading initial   models: model.mar\r\n--\r\n2021-04-09 04:53:49,837 [INFO ] main   org.pytorch.serve.archive.ModelArchive - eTag   8ff2b3de4bed4fb1bc7fe969652117ff\r\n2021-04-09 04:53:49,847 [INFO ] main   org.pytorch.serve.wlm.ModelManager - Model model loaded.\r\n2021-04-09 04:53:49,865 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Inference server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Inference API bind to: http:\/\/0.0.0.0:8080\r\n2021-04-09 04:53:49,930 [INFO ] main   org.pytorch.serve.ModelServer - Initialize Metrics server with:   EpollServerSocketChannel.\r\n2021-04-09 04:53:49,931 [INFO ] main   org.pytorch.serve.ModelServer - Metrics API bind to: http:\/\/127.0.0.1:8082\r\nModel server started.\r\n2021-04-09 04:53:49,957 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on   port: \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker   started.\r\n2021-04-09 04:53:49,959 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime:   3.6.13\r\n2021-04-09 04:53:49,963 [INFO ]   W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to:   \/home\/model-server\/tmp\/.ts.sock.9000\r\n2021-04-09 04:53:49,972 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection   accepted: \/home\/model-server\/tmp\/.ts.sock.9000.\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   CPUUtilization.Percent:33.3\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskAvailable.Gigabytes:19.622234344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUsage.Gigabytes:4.731609344482422\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,017 [INFO ]   pool-2-thread-1 TS_METRICS -   DiskUtilization.Percent:19.4\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryAvailable.Megabytes:30089.12109375\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUsed.Megabytes:902.6953125\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:50,018 [INFO ]   pool-2-thread-1 TS_METRICS -   MemoryUtilization.Percent:4.1\\|#Level:Host\\|#hostname:model.aws.local,timestamp:1617944030\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Setting the   default backend to \"pytorch\". You can change it in the   ~\/.dgl\/config.json file or export the DGLBACKEND environment variable.\u00a0 Valid options are: pytorch, mxnet,   tensorflow (all lowercase)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -   ------------------ Loading model -------------------\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Backend worker   process died.\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Traceback (most   recent call last):\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 176, in <module>\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 worker.run_server()\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 148, in run_server\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self.handle_connection(cl_socket)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 112, in handle_connection\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service, result, code =   self.load_model(msg)\r\n2021-04-09 04:53:51,250 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_service_worker.py\",   line 85, in load_model\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 service = model_loader.load(model_name,   model_dir, handler, gpu, batch_size)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/ts\/model_loader.py\", line   117, in load\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   model_service.initialize(service.context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/home\/model-server\/tmp\/models\/8ff2b3de4bed4fb1bc7fe969652117ff\/handler_service.py\",   line 51, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 super().initialize(context)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/default_handler_service.py\",   line 66, in initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self._service.validate_and_initialize(model_dir=model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/sagemaker_inference\/transformer.py\",   line 158, in validate_and_initialize\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 self._model = self._model_fn(model_dir)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/ml\/model\/code\/fd_sl_deployment_entry_point.py\", line 149, in   model_fn\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0 rgcn_model.load_state_dict(stat_dict)\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0 File   \"\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\",   line 1045, in load_state_dict\r\n2021-04-09   04:53:51,251 [INFO ] W-9000-model_1-stdout   org.pytorch.serve.wlm.WorkerLifeCycle -\u00a0\u00a0\u00a0\u00a0   self.__class__.__name__, \"     \\t\".join(error_msgs)))\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - RuntimeError:   Error(s) in loading state_dict for HeteroRGCN:\r\n2021-04-09 04:53:51,251 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceInfo<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.weight: copying a param   with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.DeviceType<>target.bias: copying a param   with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([16]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.weight: copying a   param with shape torch.Size([2, 390]) from checkpoint, the shape in current model   is torch.Size([16, 390]).\r\n2021-04-09 04:53:51,252 [INFO ]   W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - #011size   mismatch for layers.0.weight.P_emaildomain<>target.bias: copying a   param with shape torch.Size([2]) from checkpoint, the shape in current model   is torch.Size([16]).\r\n\r\n\r\n\r\n\r\n\r\n### Environment\r\n\r\n  - **CDK CLI Version:** <!-- Output of `cdk version` -->\r\n  - **Framework Version:**\r\n  - **Node.js Version:** <!-- Version of Node.js (run the command `node -v`) -->\r\n  - **OS               :**\r\n\r\n### Other\r\n\r\nCause of this bug:\r\n\r\nBackend worker process died.\r\nSagemaker Endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size.\r\n\r\n\r\n--- \r\n\r\nThis is :bug: Bug Report",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: endpoint fail to deploy or time out server error(0) bug; Content: invoke endpoint response time out. ### reproduction steps { \"trainingjob\": { \"hyperparameters\": { \"n-hidden\": \"2\", \"n-epochs\": \"100\", \"lr\":\"1e-2\" }, \"instancetype\": \"ml.c5.9xlarge\", \"timeoutinseconds\": 10800 } } ### error log in inference lambda cloudwatch: task timed out after 120.10 seconds in training cloudwatch: ### other cause of this bug: backend worker process died. endpoint deployment code and model training code parameter conflict on n-hidden and hidden_size. ---",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the endpoint failed to deploy or timed out with a server error (0), caused by a conflict between the endpoint deployment code and model training code parameters on n-hidden and hidden_size.",
        "Issue_preprocessed_content":"Title: endpoint fail to deploy or time out server error bug; Content: invoke endpoint response time out. reproduction steps trainingjob , instancetype timeoutinseconds error log in inference lambda cloudwatch task timed out after seconds in training cloudwatch , main loading initial models , main etag ff b de bed fb bc fe ff , main model model loaded. , main initialize inference server with epollserversocketchannel. , main inference api bind to , main initialize metrics server with epollserversocketchannel. , main metrics api bind to model server started. , listening on port , , torch worker started. , python runtime , connecting to , connection accepted , , , , , , , , setting the default backend to pytorch . you can change it in the file or export the dglbackend environment variable. valid options are pytorch, mxnet, tensorflow , loading model , backend worker process died. , traceback , file line , in , , file line , in , , file line , in , service, result, code , file line , in , service handler, gpu, , file line , in load , , file line , in initialize , , file line , in initialize , , file line , in , , file line , in , , file line , in , , runtimeerror error in loading for heterorgcn , size mismatch for copying a param with shape from checkpoint, the shape in current model is . , size mismatch for copying a param with shape from checkpoint, the shape in current model is , size mismatch for copying a param with shape from checkpoint, the shape in current model is . , size mismatch for copying a param with shape from checkpoint, the shape in current model is , size mismatch for copying a param with shape from checkpoint, the shape in current model is . , size mismatch for copying a param with shape from checkpoint, the shape in current model is environment cdk cli version framework version version os other cause of this bug backend worker process died. endpoint deployment code and model training code parameter conflict on and this is bug bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/accelerate\/issues\/706",
        "Issue_title":"Have accelerate for  Distributed Training: Data Parallelism feature working on AWS Sagemaker yet?",
        "Issue_creation_time":1663741563000,
        "Issue_closed_time":1664255368000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"### System Info\n\n```Shell\npytorch: 1.10.2\r\npython:3.8\n```\n\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] One of the scripts in the examples\/ folder of Accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nSagemaker Multi-GPU distributed data training, while \"model.generate\" it always returns empty tensors.\n\n### Expected behavior\n\n```Shell\nI'm trying to run a distributed training in a Sagemaker training job, the inference is not working properly, I found it as a future work on huggingface documentation so I'm wondering If that's why it's not working yet on sagemaker Multi-GPU.\r\n\r\nThanks\n```\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: have accelerate for distributed training: data parallelism feature working on yet?; Content: ### system info ```shell pytorch: 1.10.2 python:3.8 ``` ### information - [ ] the official example scripts - [ ] my own modified scripts ### tasks - [ ] one of the scripts in the examples\/ folder of accelerate or an officially supported `no_trainer` script in the `examples` folder of the `transformers` repo (such as `run_no_trainer_glue.py`) - [ ] my own task or dataset (give details below) ### reproduction multi-gpu distributed data training, while \"model.generate\" it always returns empty tensors. ### expected behavior ```shell i'm trying to run a distributed training in a training job, the inference is not working properly, i found it as a future work on huggingface documentation so i'm wondering if that's why it's not working yet on multi-gpu. thanks ```",
        "Issue_original_content_gpt_summary":"The user is encountering challenges with multi-GPU distributed data training, where the inference is not working properly, and is wondering if this is due to the feature not being available yet.",
        "Issue_preprocessed_content":"Title: have accelerate for distributed training data parallelism feature working on yet?; Content: system info information the official example scripts my own modified scripts tasks one of the scripts in the examples\/ folder of accelerate or an officially supported script in the folder of the repo my own task or dataset reproduction distributed data training, while it always returns empty tensors. expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/738",
        "Issue_title":"Sagemaker example DAG to use aws session token",
        "Issue_creation_time":1666950742000,
        "Issue_closed_time":1666960895000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nCurrently the example DAG for sagemaker just uses access key and secret key. We need to use a temporary  access token\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to '...'\r\n2. Click on '....'\r\n3. Scroll down to '....'\r\n4. See error\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Desktop (please complete the following information):**\r\n - OS: [e.g. iOS]\r\n - Browser [e.g. chrome, safari]\r\n - Version [e.g. 22]\r\n\r\n**Smartphone (please complete the following information):**\r\n - Device: [e.g. iPhone6]\r\n - OS: [e.g. iOS8.1]\r\n - Browser [e.g. stock browser, safari]\r\n - Version [e.g. 22]\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: example dag to use aws session token; Content: **describe the bug** currently the example dag for just uses access key and secret key. we need to use a temporary access token **to reproduce** steps to reproduce the behavior: 1. go to '...' 2. click on '....' 3. scroll down to '....' 4. see error **expected behavior** a clear and concise description of what you expected to happen. **screenshots** if applicable, add screenshots to help explain your problem. **desktop (please complete the following information):** - os: [e.g. ios] - browser [e.g. chrome, safari] - version [e.g. 22] **smartphone (please complete the following information):** - device: [e.g. iphone6] - os: [e.g. ios8.1] - browser [e.g. stock browser, safari] - version [e.g. 22] **additional context** add any other context about the problem here.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the example dag for using an AWS session token, where the access key and secret key were not being used correctly, and needed to be replaced with a temporary access token.",
        "Issue_preprocessed_content":"Title: example dag to use aws session token; Content: describe the bug currently the example dag for just uses access key and secret key. we need to use a temporary access token to reproduce steps to reproduce the behavior . go to . click on . scroll down to . see error expected behavior a clear and concise description of what you expected to happen. screenshots if applicable, add screenshots to help explain your problem. desktop os browser version smartphone device os browser version additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/736",
        "Issue_title":"XCom Output of Sagemaker Async Operators",
        "Issue_creation_time":1666892144000,
        "Issue_closed_time":1666957435000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nXCom return value of `SageMakerTransformOperatorAsync`  and `SageMakerTrainingOperatorAsync` does not produce the expected output.\r\n\r\nIt seems like some key(s) don't match the non-async operator output.\r\n\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run a dag with traditional operators\r\n2. Run same dag with Async operators\r\n3. Compare outputs\r\n\r\n**Expected behavior**\r\nThe Xcom keys and values should match whatever the traditional non-async version of the operators output.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: xcom output of async operators; Content: **describe the bug** xcom return value of `transformoperatorasync` and `trainingoperatorasync` does not produce the expected output. it seems like some key(s) don't match the non-async operator output. **to reproduce** steps to reproduce the behavior: 1. run a dag with traditional operators 2. run same dag with async operators 3. compare outputs **expected behavior** the xcom keys and values should match whatever the traditional non-async version of the operators output. **screenshots** if applicable, add screenshots to help explain your problem.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the xcom return value of `transformoperatorasync` and `trainingoperatorasync` does not produce the expected output, with the xcom keys and values not matching the non-async operator output.",
        "Issue_preprocessed_content":"Title: xcom output of async operators; Content: describe the bug xcom return value of and does not produce the expected output. it seems like some key don't match the operator output. to reproduce steps to reproduce the behavior . run a dag with traditional operators . run same dag with async operators . compare outputs expected behavior the xcom keys and values should match whatever the traditional version of the operators output. screenshots if applicable, add screenshots to help explain your problem."
    },
    {
        "Issue_link":"https:\/\/github.com\/astronomer\/astronomer-providers\/issues\/725",
        "Issue_title":"Token error with Sagemaker Async Operators",
        "Issue_creation_time":1666713848000,
        "Issue_closed_time":1666859588000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nGetting errors with the new Sagemaker Async Operators that I don't get with the traditional ones. I'm using a personal Access Key, Secret, and Session Token as I did with the non async operators for auth.\r\n\r\n```\r\nbotocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the DescribeTrainingJob operation: The security token included in the request is invalid.\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\nUse the SageMaker async operators with user Access Key, Secret, and Session Token\r\n\r\n**Expected behavior**\r\nExpect it to not have auth\/token errors.\r\n\r\n\r\n**Additional context**\r\nWhen I switch back to the traditional operators in the same dag with the same auth creds it works fine.\r\n\r\n\r\n@kentdanas also had similar issues and her auth was setup a little different.",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: token error with async operators; Content: **describe the bug** getting errors with the new async operators that i don't get with the traditional ones. i'm using a personal access key, secret, and session token as i did with the non async operators for auth. ``` botocore.exceptions.clienterror: an error occurred (unrecognizedclientexception) when calling the describetrainingjob operation: the security token included in the request is invalid. ``` **to reproduce** steps to reproduce the behavior: use the async operators with user access key, secret, and session token **expected behavior** expect it to not have auth\/token errors. **additional context** when i switch back to the traditional operators in the same dag with the same auth creds it works fine. @kentdanas also had similar issues and her auth was setup a little different.",
        "Issue_original_content_gpt_summary":"The user encountered an error with the new async operators when using a personal access key, secret, and session token for authentication, which was not encountered when using the traditional operators.",
        "Issue_preprocessed_content":"Title: token error with async operators; Content: describe the bug getting errors with the new async operators that i don't get with the traditional ones. i'm using a personal access key, secret, and session token as i did with the non async operators for auth. to reproduce steps to reproduce the behavior use the async operators with user access key, secret, and session token expected behavior expect it to not have errors. additional context when i switch back to the traditional operators in the same dag with the same auth creds it works fine. also had similar issues and her auth was setup a little different."
    },
    {
        "Issue_link":"https:\/\/github.com\/awslabs\/sagemaker-debugger\/issues\/325",
        "Issue_title":"Sagemaker Debugger with HPO",
        "Issue_creation_time":1597167105000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Can you please confirm if Sagemaker Debugger works with HPO. I get errors when the code that works perfectly fine with SM script mode fails when extended to HPO.\r\n\r\n` FileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/input\/config\/debughookconfig.json'`",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: debugger with hpo; Content: can you please confirm if debugger works with hpo. i get errors when the code that works perfectly fine with sm script mode fails when extended to hpo. ` filenotfounderror: [errno 2] no such file or directory: '\/opt\/ml\/input\/config\/debughookconfig.json'`",
        "Issue_original_content_gpt_summary":"The user is encountering errors when attempting to use the debugger with HPO, as they are receiving a \"filenotfounderror\" when running the code.",
        "Issue_preprocessed_content":"Title: debugger with hpo; Content: can you please confirm if debugger works with hpo. i get errors when the code that works perfectly fine with sm script mode fails when extended to hpo."
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/54",
        "Issue_title":"[BUG] Graph tab doesn't render in Amazon SageMaker Studio - Jupyter Lab",
        "Issue_creation_time":1609843354000,
        "Issue_closed_time":1646674184000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"**Describe the bug**\r\nWhen trying to execute a .path() query in Jupyter Lab the Graph tab doesn't render, instead it shows\r\n`\"Tab(children=(Output(layout=Layout(max_height='600px', overflow='scroll', width='100%')), Force(network=<graph\u2026\"`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Go to Jupyter Lab\r\n2. Run a query with .path()\r\n\r\n**Current behavior**\r\nScreenshot taken from JupyterLab\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png)\r\n\r\n\r\n**Expected behavior**\r\nScreenshot taken from Jupyter\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] graph tab doesn't render in studio - jupyter lab; Content: **describe the bug** when trying to execute a .path() query in jupyter lab the graph tab doesn't render, instead it shows `\"tab(children=(output(layout=layout(max_height='600px', overflow='scroll', width='100%')), force(network=<graph\"` **to reproduce** steps to reproduce the behavior: 1. go to jupyter lab 2. run a query with .path() **current behavior** screenshot taken from jupyterlab ![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637313-fb2f6800-4f53-11eb-9eac-8fd446c240bf.png) **expected behavior** screenshot taken from jupyter ![image](https:\/\/user-images.githubusercontent.com\/4501996\/103637180-bf949e00-4f53-11eb-8090-b2057c62cea3.png)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the graph tab does not render in Jupyter Lab when running a query with .path(), instead of the expected behavior of a graph being displayed.",
        "Issue_preprocessed_content":"Title: graph tab doesn't render in studio jupyter lab; Content: describe the bug when trying to execute a .path query in jupyter lab the graph tab doesn't render, instead it shows to reproduce steps to reproduce the behavior . go to jupyter lab . run a query with .path current behavior screenshot taken from jupyterlab expected behavior screenshot taken from jupyter"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18060",
        "Issue_title":"LED Model returns AlgorithmError when using SageMaker SMP training #16890",
        "Issue_creation_time":1657213837000,
        "Issue_closed_time":1660575729000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### System Info\n\ncc @philschmid  , cc @ydshieh  , cc @sgugger \r\n\r\nHello,\r\n\r\nThis is a follow up on a related post with the below link) with the same title:\r\nhttps:\/\/github.com\/huggingface\/transformers\/issues\/16890\r\n\r\nWe ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively):\r\n\r\n-ValueError: not enough values to unpack (expected 2, got 1)\r\n\r\nThe error is in the \u201cmodeling_led\u201d within the transformers module expecting a different input_ids shape. \r\n\r\nNew Update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error:\r\ndef unsqueeze_col(example):\r\nreturn {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)}\r\npubmed_train = pubmed_train.map(unsqueeze_col)\r\n\r\n\r\nIt helped moving forward in the process, but we got another error, below, a little further down in the code:\r\n\r\nUnexpectedStatusException: Error for Training job huggingface-pytorch-training-2022-06-29-04-04-58-606: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\r\nExitCode 1\r\nErrorMessage \":RuntimeError: Tensors must have same number of dimensions: got 4 and 3\r\n :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set :Environment variable SAGEMAKER_INSTANCE_TYPE is not set -------------------------------------------------------------------------- Primary job  terminated normally, but 1 process returned a non-zero exit code. Per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. The first process to do so was:    Process name: [[41154,1],0]   Exit code:    1\"\r\nCommand \"mpirun --host algo-1:8 \r\n\r\n\r\nI\u2019d greatly appreciate your feedback. Please let me know if you need any further information about the project.\n\n### Who can help?\n\n[SageMakerAprilTraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/SageMakerAprilTraining.zip)\r\n\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nRunning this attached file with the training python file\n\n### Expected behavior\n\nI have shared the notebook and the error raised in it for clarification",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: led model returns algorithmerror when using smp training #16890; Content: ### system info cc @philschmid , cc @ydshieh , cc @sgugger hello, this is a follow up on a related post with the below link) with the same title: https:\/\/github.com\/huggingface\/transformers\/issues\/16890 we ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations (3.8, 4.16.2, and 1.10.2, respectively): -valueerror: not enough values to unpack (expected 2, got 1) the error is in the modeling_led within the transformers module expecting a different input_ids shape. new update is we tried below to unsqueeze input tensors to the \"modeling_led\" to solve the above error: def unsqueeze_col(example): return {\"input_ids\": torch.unsqueeze(example[\"input_ids\"], 0)} pubmed_train = pubmed_train.map(unsqueeze_col) it helped moving forward in the process, but we got another error, below, a little further down in the code: unexpectedstatusexception: error for training job huggingface-pytorch-training-2022-06-29-04-04-58-606: failed. reason: algorithmerror: executeuserscripterror: exitcode 1 errormessage \":runtimeerror: tensors must have same number of dimensions: got 4 and 3 :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set :environment variable _instance_type is not set -------------------------------------------------------------------------- primary job terminated normally, but 1 process returned a non-zero exit code. per user-direction, the job has been aborted. mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. the first process to do so was: process name: [[41154,1],0] exit code: 1\" command \"mpirun --host algo-1:8 id greatly appreciate your feedback. please let me know if you need any further information about the project. ### who can help? [apriltraining.zip](https:\/\/github.com\/huggingface\/transformers\/files\/9065968\/apriltraining.zip) ### information - [ ] the official example scripts - [x] my own modified scripts ### tasks - [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [x] my own task or dataset (give details below) ### reproduction running this attached file with the training python file ### expected behavior i have shared the notebook and the error raised in it for clarification",
        "Issue_original_content_gpt_summary":"The user encountered multiple challenges while attempting to use the Transformers library to train a model, including a ValueError and an UnexpectedStatusException, both related to the shape of the input tensors.",
        "Issue_preprocessed_content":"Title: led model returns algorithmerror when using smp training ; Content: system info cc , cc , cc hello, this is a follow up on a related post with the below link with the same title we ade a bit of more progress but are still facing with some issues and are trying to fix them after trying out several fixes including matching the python, transformers, and pytorch versions according to the recommendations valueerror not enough values to unpack the error is in the within the transformers module expecting a different shape. new update is we tried below to unsqueeze input tensors to the to solve the above error def return it helped moving forward in the process, but we got another error, below, a little further down in the code unexpectedstatusexception error for training job failed. reason algorithmerror executeuserscripterror exitcode errormessage runtimeerror tensors must have same number of dimensions got and environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set environment variable is not set primary job terminated normally, but process returned a exit code. per the job has been aborted. detected that one or more processes exited with status, thus causing the job to be terminated. the first process to do so was process name , exit code command mpirun id greatly appreciate your feedback. please let me know if you need any further information about the project. who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction running this attached file with the training python file expected behavior i have shared the notebook and the error raised in it for clarification"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17855",
        "Issue_title":"LayoutLMv2 training on sagemaker error: undefined value has_torch_function_variadic",
        "Issue_creation_time":1656007789000,
        "Issue_closed_time":1656429784000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"### System Info\r\n\r\n```shell\r\ntransformer: 4.17.0\r\ntorch: 1.10.2\r\n\r\nPlatform: Sagemaker Deep Learning Container\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\n@NielsRogge\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nThe error only comes when training on Sagemaker using Huggingface.\r\n\r\nScripts to start training on Sagemaker:\r\n\r\nFolder organization:\r\n```\r\n.\/\r\n----sg_training.py\r\n----scripts\r\n-------requirements.txt\r\n-------train.py \r\n```\r\n\r\nsg_training.py:\r\n```\r\nimport boto3\r\nimport sagemaker\r\nfrom sagemaker.huggingface import HuggingFace\r\n\r\nif __name__ == \"__main__\":\r\n    iam_client = boto3.client(...)\r\n\r\n    role = iam_client.get_role(...)['Role']['Arn']\r\n    sess = sagemaker.Session()\r\n\r\n    sagemaker_session_bucket = 's3-sagemaker-session'\r\n\r\n    hyperparameters = {'epochs': 20,\r\n                       'train_batch_size': 1,\r\n                       'model_name': \"microsoft\/layoutxlm-base\",\r\n                       'output_dir': '\/opt\/ml\/model\/',\r\n                       'checkpoints': '\/opt\/ml\/checkpoints\/',\r\n                       'combine_train_val': True,\r\n                       'exp_tracker': \"all\",\r\n                       'exp_name': 'Sagemaker Training'\r\n                       }\r\n\r\n    huggingface_estimator = HuggingFace(entry_point='train.py',\r\n                                        source_dir='scripts',\r\n                                        instance_type='ml.p3.2xlarge',\r\n                                        instance_count=1,\r\n                                        role=role,\r\n                                        transformers_version='4.17.0',\r\n                                        pytorch_version='1.10.2',\r\n                                        py_version='py38',\r\n                                        hyperparameters=hyperparameters,\r\n                                        environment={'HF_TASK': 'text-classification'},\r\n                                        code_location='s3:\/\/dummy_code_location')\r\n\r\n    huggingface_estimator.fit()\r\n```\r\n\r\nEntrypoint scripts folder:\r\n\r\n\r\nrequirements.txt:\r\n```\r\ngit+https:\/\/github.com\/facebookresearch\/detectron2.git\r\n```\r\n\r\ntrain.py:\r\n```\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport sys\r\n\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n\r\n\r\ndef run():\r\n    model = LayoutLMv2ForSequenceClassification.from_pretrained('microsoft\/layoutxlm-base',\r\n                                                                num_labels=5)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--epochs\", type=int, default=3)\r\n    parser.add_argument(\"--exp_name\", type=str, default=\"Sagemaker Training\")\r\n    parser.add_argument(\"--train-batch-size\", type=int, default=2)\r\n    parser.add_argument(\"--eval-batch-size\", type=int, default=1)\r\n    parser.add_argument(\"--warmup_steps\", type=int, default=500)\r\n    parser.add_argument(\"--model_name\", type=str)\r\n    parser.add_argument(\"--learning_rate\", type=str, default=1e-5)\r\n    parser.add_argument(\"--combine_train_val\", type=bool, default=False)\r\n    # Data, model, and output directories\r\n    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\r\n    parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\")\r\n    parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model')\r\n    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\r\n    args, _ = parser.parse_known_args()\r\n\r\n    logger = logging.getLogger(__name__)\r\n    logging.basicConfig(\r\n        level=logging.getLevelName(\"INFO\"),\r\n        handlers=[logging.StreamHandler(sys.stdout)],\r\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\r\n    )\r\n\r\n    run()\r\n\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\n```shell\r\nHere the log on the error from AWS Cloud Watch:\r\n\r\nInvoking script with the following command:\r\n\/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val True --epochs 20 --exp_name Sagemaker_Training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module\r\nreturn importlib.import_module(\".\" + module_name, self.__name__)\r\n  File \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\nFile \"<frozen importlib._bootstrap_external>\", line 848, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nFile \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in <module>\r\nfrom detectron2.modeling import META_ARCH_REGISTRY\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in <module>\r\nfrom detectron2.layers import ShapeSpec\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in <module>\r\nfrom .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm, CycleBatchNormList\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in <module>\r\n    from fvcore.nn.distributed import differentiable_all_reduce\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in <module>\r\n    from .focal_loss import (\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in <module>\r\n    sigmoid_focal_loss_jit: \"torch.jit.ScriptModule\" = torch.jit.script(sigmoid_focal_loss)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn\r\nreturn torch.jit.script(fn, _rcb=rcb)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script\r\nfn = torch._C._jit_script_compile(\r\nRuntimeError: \r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 6, in <module>\r\nfrom transformers import LayoutLMv2ForSequenceClassification\r\n  File \"<frozen importlib._bootstrap>\", line 1039, in _handle_fromlist\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__\r\nvalue = getattr(module, name)\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__\r\nmodule = self._get_module(self._class_to_module[name])\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module\r\nraise RuntimeError(\r\nRuntimeError: Failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback):\r\nundefined value has_torch_function_variadic:\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962\r\n         >>> loss.backward()\r\n    \"\"\"\r\n    if has_torch_function_variadic(input, target, weight, pos_weight):\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        return handle_torch_function(\r\n            binary_cross_entropy_with_logits,\r\n'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss'\r\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36\r\n    targets = targets.float()\r\n    p = torch.sigmoid(inputs)\r\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n    p_t = p * targets + (1 - p) * (1 - targets)\r\n    loss = ce_loss * ((1 - p_t) ** gamma)\r\n\r\n```\r\n```\r\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: layoutlmv2 training on error: undefined value has_torch_function_variadic; Content: ### system info ```shell transformer: 4.17.0 torch: 1.10.2 platform: deep learning container ``` ### who can help? @nielsrogge ### information - [ ] the official example scripts - [x] my own modified scripts ### tasks - [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction the error only comes when training on using huggingface. scripts to start training on : folder organization: ``` .\/ ----sg_training.py ----scripts -------requirements.txt -------train.py ``` sg_training.py: ``` import boto3 import from .huggingface import huggingface if __name__ == \"__main__\": iam_client = boto3.client(...) role = iam_client.get_role(...)['role']['arn'] sess = .session() _session_bucket = 's3--session' hyperparameters = {'epochs': 20, 'train_batch_size': 1, 'model_name': \"microsoft\/layoutxlm-base\", 'output_dir': '\/opt\/ml\/model\/', 'checkpoints': '\/opt\/ml\/checkpoints\/', 'combine_train_val': true, 'exp_tracker': \"all\", 'exp_name': ' training' } huggingface_estimator = huggingface(entry_point='train.py', source_dir='scripts', instance_type='ml.p3.2xlarge', instance_count=1, role=role, transformers_version='4.17.0', pytorch_version='1.10.2', py_version='py38', hyperparameters=hyperparameters, environment={'hf_task': 'text-classification'}, code_location='s3:\/\/dummy_code_location') huggingface_estimator.fit() ``` entrypoint scripts folder: requirements.txt: ``` git+https:\/\/github.com\/facebookresearch\/detectron2.git ``` train.py: ``` import argparse import logging import os import sys from transformers import layoutlmv2forsequenceclassification def run(): model = layoutlmv2forsequenceclassification.from_pretrained('microsoft\/layoutxlm-base', num_labels=5) if __name__ == \"__main__\": parser = argparse.argumentparser() parser.add_argument(\"--epochs\", type=int, default=3) parser.add_argument(\"--exp_name\", type=str, default=\" training\") parser.add_argument(\"--train-batch-size\", type=int, default=2) parser.add_argument(\"--eval-batch-size\", type=int, default=1) parser.add_argument(\"--warmup_steps\", type=int, default=500) parser.add_argument(\"--model_name\", type=str) parser.add_argument(\"--learning_rate\", type=str, default=1e-5) parser.add_argument(\"--combine_train_val\", type=bool, default=false) # data, model, and output directories parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"sm_output_data_dir\"]) parser.add_argument(\"--checkpoints\", type=str, default=\"\/opt\/ml\/checkpoints\") parser.add_argument(\"--model-dir\", type=str, default='\/opt\/ml\/code\/model') parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"sm_num_gpus\"]) args, _ = parser.parse_known_args() logger = logging.getlogger(__name__) logging.basicconfig( level=logging.getlevelname(\"info\"), handlers=[logging.streamhandler(sys.stdout)], format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", ) run() ``` ### expected behavior ```shell here the log on the error from aws cloud watch: invoking script with the following command: \/opt\/conda\/bin\/python3.8 train.py --checkpoints \/opt\/ml\/checkpoints\/ --combine_train_val true --epochs 20 --exp_name _training_doc_cls --exp_tracker all --model_name microsoft\/layoutxlm-base --output_dir \/opt\/ml\/model\/ --train_batch_size 1 traceback (most recent call last): file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2777, in _get_module return importlib.import_module(\".\" + module_name, self.__name__) file \"\/opt\/conda\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module return _bootstrap._gcd_import(name[level:], package, level) file \"\", line 1014, in _gcd_import file \"\", line 991, in _find_and_load file \"\", line 975, in _find_and_load_unlocked file \"\", line 671, in _load_unlocked file \"\", line 848, in exec_module file \"\", line 219, in _call_with_frames_removed file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/models\/layoutlmv2\/modeling_layoutlmv2.py\", line 48, in from detectron2.modeling import meta_arch_registry file \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/modeling\/__init__.py\", line 2, in from detectron2.layers import shapespec file \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/__init__.py\", line 2, in from .batch_norm import frozenbatchnorm2d, get_norm, naivesyncbatchnorm, cyclebatchnormlist file \"\/opt\/conda\/lib\/python3.8\/site-packages\/detectron2\/layers\/batch_norm.py\", line 4, in from fvcore.nn.distributed import differentiable_all_reduce file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/__init__.py\", line 4, in from .focal_loss import ( file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 52, in sigmoid_focal_loss_jit: \"torch.jit.scriptmodule\" = torch.jit.script(sigmoid_focal_loss) file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script fn = torch._c._jit_script_compile( file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_recursive.py\", line 838, in try_compile_fn return torch.jit.script(fn, _rcb=rcb) file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/jit\/_script.py\", line 1310, in script fn = torch._c._jit_script_compile( runtimeerror: undefined value has_torch_function_variadic: file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962 >>> loss.backward() \"\"\" if has_torch_function_variadic(input, target, weight, pos_weight): ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here return handle_torch_function( binary_cross_entropy_with_logits, 'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss' file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36 targets = targets.float() p = torch.sigmoid(inputs) ce_loss = f.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\") ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here p_t = p * targets + (1 - p) * (1 - targets) loss = ce_loss * ((1 - p_t) ** gamma) the above exception was the direct cause of the following exception: traceback (most recent call last): file \"train.py\", line 6, in from transformers import layoutlmv2forsequenceclassification file \"\", line 1039, in _handle_fromlist file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2768, in __getattr__ value = getattr(module, name) file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2767, in __getattr__ module = self._get_module(self._class_to_module[name]) file \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 2779, in _get_module raise runtimeerror( runtimeerror: failed to import transformers.models.layoutlmv2.modeling_layoutlmv2 because of the following error (look up to see its traceback): undefined value has_torch_function_variadic: file \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch\/utils\/smdebug.py\", line 2962 >>> loss.backward() \"\"\" if has_torch_function_variadic(input, target, weight, pos_weight): ~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here return handle_torch_function( binary_cross_entropy_with_logits, 'binary_cross_entropy_with_logits' is being compiled since it was called from 'sigmoid_focal_loss' file \"\/opt\/conda\/lib\/python3.8\/site-packages\/fvcore\/nn\/focal_loss.py\", line 36 targets = targets.float() p = torch.sigmoid(inputs) ce_loss = f.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\") ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- here p_t = p * targets + (1 - p) * (1 - targets) loss = ce_loss * ((1 - p_t) ** gamma) ``` ```",
        "Issue_original_content_gpt_summary":"The user encountered an error when training layoutlmv2 on Huggingface, with an undefined value has_torch_function_variadic.",
        "Issue_preprocessed_content":"Title: layoutlmv training on error undefined value ; Content: system info who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction the error only comes when training on using huggingface. scripts to start training on folder organization entrypoint scripts folder expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17096",
        "Issue_title":"pip install \"sacremoses>=0.0.50\" breaks on SageMaker Studio",
        "Issue_creation_time":1651754767000,
        "Issue_closed_time":1654502136000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"### System Info\n\n```shell\nThis was verified today on a fresh SageMaker Studio instance running in us-west-2.\r\n\r\nIt's not a Transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on SageMaker Studio at some point.\n```\n\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1) Open an SM Studio notebook\r\n\r\n2) Run the following cell:\r\n```\r\n%%sh\r\npip install \"sacremoses>=0.0.50\"\r\n```\r\n\r\nThe obvious workaround for now is\r\n```\r\npip install \"sacremoses==0.0.49\"\r\n```\r\n\r\n\n\n### Expected behavior\n\n```shell\nsacremoses should install without error.\n```\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: pip install \"sacremoses>=0.0.50\" breaks on studio; Content: ### system info ```shell this was verified today on a fresh studio instance running in us-west-2. it's not a transformer issue, but as sacremoses is a dependency, this is likely to break 'pip install transformers' on studio at some point. ``` ### who can help? _no response_ ### information - [ ] the official example scripts - [ ] my own modified scripts ### tasks - [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction 1) open an sm studio notebook 2) run the following cell: ``` %%sh pip install \"sacremoses>=0.0.50\" ``` the obvious workaround for now is ``` pip install \"sacremoses==0.0.49\" ``` ### expected behavior ```shell sacremoses should install without error. ```",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to install the sacremoses package version 0.0.50 on an SM Studio notebook, resulting in an error, and a workaround of installing version 0.0.49 was suggested.",
        "Issue_preprocessed_content":"Title: pip install breaks on studio; Content: system info who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction open an sm studio notebook run the following cell the obvious workaround for now is expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/16890",
        "Issue_title":"LED Model returns AlgorithmError when using SageMaker SMP training",
        "Issue_creation_time":1650634004000,
        "Issue_closed_time":1653922922000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### System Info\n\n```shell\nusing sagemaker \r\nmpi_options = {\r\n    \"enabled\" : True,\r\n    \"processes_per_host\" : 8\r\n}\r\n\r\nsmp_options = {\r\n    \"enabled\":True,\r\n    \"parameters\": {\r\n        \"microbatches\": 1,\r\n        \"placement_strategy\": \"spread\",\r\n        \"pipeline\": \"interleaved\",\r\n        \"optimize\": \"memory\",\r\n        \"partitions\": 2,\r\n        \"ddp\": True,\r\n    }\r\n}\r\n\r\ndistribution={\r\n    \"smdistributed\": {\"modelparallel\": smp_options},\r\n    \"mpi\": mpi_options\r\n}\r\nhyperparameters={'epochs': 1,\r\n                 'train_batch_size': 1,\r\n                 'eval_batch_size': 1,\r\n                 'model_name':HHousen\/distil-led-large-cnn-16384,\r\n                 'output_dir': 'bucket',\r\n                 'warmup_steps': 25,\r\n                 'checkpoint_s3_uri': 'bucket',\r\n                 'logging_steps':100,\r\n                 'evaluation_strategy':\"steps\",\r\n                 'gradient_accumulation_steps':10\r\n                 }\r\nhuggingface_estimator = HuggingFace(entry_point='trainer.py',\r\n                            source_dir='.\/scripts',\r\n                            instance_type='ml.p3.16xlarge',\r\n                            instance_count=1,\r\n                            role=role,\r\n                            volume=100,\r\n                            transformers_version='4.6.1',\r\n                            pytorch_version='1.8.1',\r\n                            py_version='py36',\r\n                            hyperparameters=hyperparameters,\r\n                                   distribution=distribution)\n```\n\n\n### Who can help?\n\n@ydshieh @sgugger\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Create huggingface estimator\r\n2.     training_args = Seq2SeqTrainingArguments(\r\n        predict_with_generate=True,\r\n        evaluation_strategy=\"steps\",\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        fp16=True,\r\n        fp16_backend=\"apex\",\r\n        output_dir=s3_bucket,\r\n        logging_steps=50,\r\n        warmup_steps=25,\r\n        gradient_accumulation_steps=10,\r\n    )\r\n\r\nError I get:\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward\r\n[1,0]<stderr>:    return super().forward(positions)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward\r\n[1,0]<stderr>:    raise e\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward\r\n[1,0]<stderr>:    output = original_forward(self, *args, **kwargs)\r\n[1,0]<stderr>:  File \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward\r\n[1,0]<stderr>:    bsz, seq_len = input_ids_shape[:2]\r\n[1,0]<stderr>:ValueError: not enough values to unpack (expected 2, got 1)\r\n--------------------------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n--------------------------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun.real detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n  Process name: [[41156,1],0]\r\n  Exit code:    1\r\n--------------------------------------------------------------------------\r\n\n\n### Expected behavior\n\n```shell\nTraining on a sagemaker notebook p3dn.24xlarge using fairscale `simple` and these versions\r\ntransformers-4.16.2\r\ntorch-1.10.2\r\nfairscale-0.4.5\r\npy37\r\n\r\nI can successfully train the LED model with my training data. Trying to get it to work with Huggingface estimator and sagemaker SMP I would assume the same outcome.\n```\n",
        "Tool":"Amazon SageMaker",
        "Platform":"Github",
        "Issue_original_content":"Title: led model returns algorithmerror when using smp training; Content: ### system info ```shell using mpi_options = { \"enabled\" : true, \"processes_per_host\" : 8 } smp_options = { \"enabled\":true, \"parameters\": { \"microbatches\": 1, \"placement_strategy\": \"spread\", \"pipeline\": \"interleaved\", \"optimize\": \"memory\", \"partitions\": 2, \"ddp\": true, } } distribution={ \"smdistributed\": {\"modelparallel\": smp_options}, \"mpi\": mpi_options } hyperparameters={'epochs': 1, 'train_batch_size': 1, 'eval_batch_size': 1, 'model_name':hhousen\/distil-led-large-cnn-16384, 'output_dir': 'bucket', 'warmup_steps': 25, 'checkpoint_s3_uri': 'bucket', 'logging_steps':100, 'evaluation_strategy':\"steps\", 'gradient_accumulation_steps':10 } huggingface_estimator = huggingface(entry_point='trainer.py', source_dir='.\/scripts', instance_type='ml.p3.16xlarge', instance_count=1, role=role, volume=100, transformers_version='4.6.1', pytorch_version='1.8.1', py_version='py36', hyperparameters=hyperparameters, distribution=distribution) ``` ### who can help? @ydshieh @sgugger ### information - [ ] the official example scripts - [ ] my own modified scripts ### tasks - [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction 1. create huggingface estimator 2. training_args = seq2seqtrainingarguments( predict_with_generate=true, evaluation_strategy=\"steps\", per_device_train_batch_size=1, per_device_eval_batch_size=1, fp16=true, fp16_backend=\"apex\", output_dir=s3_bucket, logging_steps=50, warmup_steps=25, gradient_accumulation_steps=10, ) error i get: [1,0]: file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward [1,0]: raise e [1,0]: file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward [1,0]: output = original_forward(self, *args, **kwargs) [1,0]: file \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 125, in forward [1,0]: return super().forward(positions) [1,0]: file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 68, in trace_forward [1,0]: raise e [1,0]: file \"\/opt\/conda\/lib\/python3.6\/site-packages\/smdistributed\/modelparallel\/torch\/patches\/tracing.py\", line 51, in trace_forward [1,0]: output = original_forward(self, *args, **kwargs) [1,0]: file \"\/opt\/conda\/lib\/python3.6\/site-packages\/transformers\/models\/led\/modeling_led.py\", line 121, in forward [1,0]: bsz, seq_len = input_ids_shape[:2] [1,0]:valueerror: not enough values to unpack (expected 2, got 1) -------------------------------------------------------------------------- primary job terminated normally, but 1 process returned a non-zero exit code. per user-direction, the job has been aborted. -------------------------------------------------------------------------- -------------------------------------------------------------------------- mpirun.real detected that one or more processes exited with non-zero status, thus causing the job to be terminated. the first process to do so was: process name: [[41156,1],0] exit code: 1 -------------------------------------------------------------------------- ### expected behavior ```shell training on a notebook p3dn.24xlarge using fairscale `simple` and these versions transformers-4.16.2 torch-1.10.2 fairscale-0.4.5 py37 i can successfully train the led model with my training data. trying to get it to work with huggingface estimator and smp i would assume the same outcome. ```",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to use the Huggingface Estimator and SMP to train a LED model, resulting in an AlgorithmError.",
        "Issue_preprocessed_content":"Title: led model returns algorithmerror when using smp training; Content: system info who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction . create huggingface estimator . seq seqtrainingarguments error i get , stderr file line , in , stderr raise e , stderr file line , in , stderr output args, kwargs , stderr file line , in forward , stderr return , stderr file line , in , stderr raise e , stderr file line , in , stderr output args, kwargs , stderr file line , in forward , stderr bsz, , stderr valueerror not enough values to unpack primary job terminated normally, but process returned a exit code. per the job has been aborted. detected that one or more processes exited with status, thus causing the job to be terminated. the first process to do so was process name , exit code expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/977",
        "Issue_title":"ImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)",
        "Issue_creation_time":1645497684000,
        "Issue_closed_time":1645604942000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Which example? Describe the issue\r\n\r\nexample:  az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file azureml-examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic\r\ndescription:\r\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\r\n    import create_app\r\n  File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\r\n    import aml_framework\r\n  File \"\/var\/azureml-server\/aml_framework.py\", line 9, in <module>\r\n    from synchronous.framework import *\r\n  File \"\/var\/azureml-server\/synchronous\/framework.py\", line 3, in <module>\r\n    from flask import Flask, request, g, Request, Response, Blueprint\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in <module>\r\n    from .app import Flask, Request, Response\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in <module>\r\n    from . import cli, json\r\n  File \"\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in <module>\r\n    from itsdangerous import json as _json\r\nImportError: cannot import name 'json' from 'itsdangerous' (\/azureml-envs\/azureml_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py)\r\n\r\n## Additional context\r\n\r\nhttps:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\r\n\r\n-\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: importerror: cannot import name 'json' from 'itsdangerous' (\/-envs\/_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py); Content: ## which example? describe the issue example: az ml online-deployment create --name blue --endpoint-name amlarc-runner-simple-849b --resource-group lt-westus2-r6-amlarc-rg --workspace-name lt-westus2-r6-arc-ws --file -examples\/cli\/endpoints\/online\/\/amlarc\/blue-deployment.yml --all-traffic description: file \"\/var\/-server\/entry.py\", line 1, in import create_app file \"\/var\/-server\/create_app.py\", line 3, in import aml_framework file \"\/var\/-server\/aml_framework.py\", line 9, in from synchronous.framework import * file \"\/var\/-server\/synchronous\/framework.py\", line 3, in from flask import flask, request, g, request, response, blueprint file \"\/-envs\/_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/__init__.py\", line 21, in from .app import flask, request, response file \"\/-envs\/_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/app.py\", line 26, in from . import cli, json file \"\/-envs\/_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/flask\/json\/__init__.py\", line 21, in from itsdangerous import json as _json importerror: cannot import name 'json' from 'itsdangerous' (\/-envs\/_9560a2159e2635db8931fa24bcadb555\/lib\/python3.7\/site-packages\/itsdangerous\/__init__.py) ## additional context https:\/\/ml.azure.com\/endpoints\/realtime\/amlarc-runner-simple-849b\/logs?wsid=\/subscriptions\/589c7ae9-223e-45e3-a191-98433e0821a9\/resourcegroups\/lt-westus2-r6-amlarc-rg\/workspaces\/lt-westus2-r6-arc-ws&tid=72f988bf-86f1-41af-91ab-2d7cd011db47 -",
        "Issue_original_content_gpt_summary":"The user encountered an \"importerror: cannot import name 'json' from 'itsdangerous'\" error while attempting to create an online deployment with Azure ML.",
        "Issue_preprocessed_content":"Title: importerror cannot import name 'json' from 'itsdangerous' ; Content: which example? describe the issue example az ml create blue description file line , in import file line , in import file line , in from import file line , in from flask import flask, request, g, request, response, blueprint file line , in from .app import flask, request, response file line , in from . import cli, json file line , in from itsdangerous import json as importerror cannot import name 'json' from 'itsdangerous' additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1862",
        "Issue_title":"[BUG] Update test documentation to connect AzureML with GitHub actions",
        "Issue_creation_time":1669630421000,
        "Issue_closed_time":1669646142000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nSteps:\r\n1. Create a new AzureML workspace.\r\n    - Name: `azureml-test-workspace`\r\n    - Resource group: `recommenders_project_resources`\r\n    - Location: *Make sure you have enough quota in the location you choose*\r\n2. Create two new clusters: `cpu-cluster` and `gpu-cluster`. Go to compute, then compute cluster, then new.\r\n    - Select the CPU VM base. Anything above 32GB of RAM, and 8 cores should be fine.\r\n    - Select the GPU VM base. Anything above 56GB of RAM, and 6 cores, and an NVIDIA K80 should be fine.\r\n3. Add the subscription ID to GitHub action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). Create a new repository secret called `AZUREML_TEST_SUBID` and add the subscription ID as the value.\r\n4. Make sure you have installed [Azure CLI](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`.\r\n5. Select your subscription: `az account set -s $AZURE_SUBSCRIPTION_ID`.\r\n5. Create a Service Principal: `az ad sp create-for-rbac --name \"CICD\" --role contributor --scopes \/subscriptions\/$AZURE_SUBSCRIPTION_ID --sdk-auth`.\r\n6. Add the output from the Service Principal (should be a JSON blob) as an action secret `AZUREML_TEST_CREDENTIALS`.\r\n\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] update test documentation to connect with github actions; Content: ### description steps: 1. create a new workspace. - name: `-test-workspace` - resource group: `recommenders_project_resources` - location: *make sure you have enough quota in the location you choose* 2. create two new clusters: `cpu-cluster` and `gpu-cluster`. go to compute, then compute cluster, then new. - select the cpu vm base. anything above 32gb of ram, and 8 cores should be fine. - select the gpu vm base. anything above 56gb of ram, and 6 cores, and an nvidia k80 should be fine. 3. add the subscription id to github action secrets [here](https:\/\/github.com\/microsoft\/recommenders\/settings\/secrets\/actions). create a new repository secret called `_test_subid` and add the subscription id as the value. 4. make sure you have installed [azure cli](https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/install-azure-cli), and that you are logged in: `az login`. 5. select your subscription: `az account set -s $azure_subscription_id`. 5. create a service principal: `az ad sp create-for-rbac --name \"cicd\" --role contributor --scopes \/subscriptions\/$azure_subscription_id --sdk-auth`. 6. add the output from the service principal (should be a json blob) as an action secret `_test_credentials`. ### in which platform does it happen? ### how do we replicate the issue? ### expected behavior (i.e. solution) ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered challenges in updating test documentation to connect with GitHub Actions, requiring the creation of a new workspace, two new clusters, adding the subscription ID to GitHub Action secrets, installing Azure CLI, selecting the subscription, creating a service principal, and adding the output from the service principal as an action secret.",
        "Issue_preprocessed_content":"Title: update test documentation to connect with github actions; Content: description describe your in detail steps . create a new workspace. name resource group location make sure you have enough quota in the location you choose . create two new clusters and . go to compute, then compute cluster, then new. select the cpu vm base. anything above gb of ram, and cores should be fine. select the gpu vm base. anything above gb of ram, and cores, and an nvidia k should be fine. . add the subscription id to github action secrets . create a new repository secret called and add the subscription id as the value. . make sure you have installed , and that you are logged in . . select your subscription . . create a service principal . . add the output from the service principal as an action secret . in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. azure databricks. other platforms. how do we replicate the issue? please be specific as possible . for example create a conda environment for pyspark run unit test with expected behavior for example the tests for sar pyspark should pass successfully. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1852",
        "Issue_title":"[BUG] AzureML test process is not failing if there is an error in the tests",
        "Issue_creation_time":1668674781000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nAzureML tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3485981939\/jobs\/5832009213\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\nWe want to send back a signal to GitHub so if the tests fail, the badge is red and we are notified\r\n\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] test process is not failing if there is an error in the tests; Content: ### description tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors ### in which platform does it happen? ### how do we replicate the issue? see https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3485981939\/jobs\/5832009213 ### expected behavior (i.e. solution) we want to send back a signal to github so if the tests fail, the badge is red and we are notified ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the test process was not failing if there was an error in the tests, making it difficult to identify errors.",
        "Issue_preprocessed_content":"Title: test process is not failing if there is an error in the tests; Content: description describe your in detail tests execute the code, but if the process fail, we are not getting a signal that is failing, which makes difficult to identify errors in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. azure databricks. other platforms. how do we replicate the issue? please be specific as possible . for example create a conda environment for pyspark run unit test with see expected behavior for example the tests for sar pyspark should pass successfully. we want to send back a signal to github so if the tests fail, the badge is red and we are notified other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1848",
        "Issue_title":"[BUG] xdeepfm error in AzureML test",
        "Issue_creation_time":1668591744000,
        "Issue_closed_time":1668600473000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n    @pytest.mark.gpu\r\n    @pytest.mark.notebooks\r\n    @pytest.mark.integration\r\n    @pytest.mark.parametrize(\r\n        \"syn_epochs, criteo_epochs, expected_values, seed\",\r\n        [\r\n            (\r\n                15,\r\n                10,\r\n                ***\r\n                    \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***,\r\n                    \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***,\r\n                ***,\r\n                42,\r\n            )\r\n        ],\r\n    )\r\n    def test_xdeepfm_integration(\r\n        notebooks,\r\n        output_notebook,\r\n        kernel_name,\r\n        syn_epochs,\r\n        criteo_epochs,\r\n        expected_values,\r\n        seed,\r\n    ):\r\n        notebook_path = notebooks[\"xdeepfm_quickstart\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            output_notebook,\r\n            kernel_name=kernel_name,\r\n            parameters=dict(\r\n                EPOCHS_FOR_SYNTHETIC_RUN=syn_epochs,\r\n                EPOCHS_FOR_CRITEO_RUN=criteo_epochs,\r\n                BATCH_SIZE_SYNTHETIC=1024,\r\n                BATCH_SIZE_CRITEO=1024,\r\n                RANDOM_SEED=seed,\r\n            ),\r\n        )\r\n        results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[\r\n            \"data\"\r\n        ]\r\n    \r\n        for key, value in expected_values.items():\r\n>           assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=TOL, abs=ABS_TOL)\r\nE           assert 0.5131 == 0.9716 \u00b1 9.7e-02\r\nE             comparison failed\r\nE             Obtained: 0.5131\r\nE             Expected: 0.9716 \u00b1 9.7e-02\r\n```\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nSee https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889\r\n\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] xdeepfm error in test; Content: ### description ``` @pytest.mark.gpu @pytest.mark.notebooks @pytest.mark.integration @pytest.mark.parametrize( \"syn_epochs, criteo_epochs, expected_values, seed\", [ ( 15, 10, *** \"res_syn\": ***\"auc\": 0.9716, \"logloss\": 0.699***, \"res_real\": ***\"auc\": 0.749, \"logloss\": 0.4926***, ***, 42, ) ], ) def test_xdeepfm_integration( notebooks, output_notebook, kernel_name, syn_epochs, criteo_epochs, expected_values, seed, ): notebook_path = notebooks[\"xdeepfm_quickstart\"] pm.execute_notebook( notebook_path, output_notebook, kernel_name=kernel_name, parameters=dict( epochs_for_synthetic_run=syn_epochs, epochs_for_criteo_run=criteo_epochs, batch_size_synthetic=1024, batch_size_criteo=1024, random_seed=seed, ), ) results = sb.read_notebook(output_notebook).scraps.dataframe.set_index(\"name\")[ \"data\" ] for key, value in expected_values.items(): > assert results[key][\"auc\"] == pytest.approx(value[\"auc\"], rel=tol, abs=abs_tol) e assert 0.5131 == 0.9716 9.7e-02 e comparison failed e obtained: 0.5131 e expected: 0.9716 9.7e-02 ``` ### in which platform does it happen? ### how do we replicate the issue? see https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3459763061\/jobs\/5775521889 ### expected behavior (i.e. solution) ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered a bug in xDeepFM where the expected values for AUC and logloss were not met, and encountered challenges in replicating the issue and finding a solution.",
        "Issue_preprocessed_content":"Title: xdeepfm error in test; Content: description describe your in detail in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. azure databricks. other platforms. how do we replicate the issue? please be specific as possible . for example create a conda environment for pyspark run unit test with see expected behavior for example the tests for sar pyspark should pass successfully. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1841",
        "Issue_title":"[BUG] Error in some of the AzureML tests",
        "Issue_creation_time":1668089448000,
        "Issue_closed_time":1668591607000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThere are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022\r\n\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\nERROR tests\/integration\/examples\/test_notebooks_gpu.py\r\n======================== 48 warnings, 3 errors in 3.79s ========================\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nERROR: not found: \/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration\r\n(no name '\/mnt\/azureml\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of [<Module tests\/integration\/examples\/test_notebooks_gpu.py>])\r\n\r\nINFO:submit_groupwise_azureml_pytest.py:Test execution completed!\r\n\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] error in some of the tests; Content: ### description there are some errors: https:\/\/github.com\/microsoft\/recommenders\/actions\/runs\/3402182291\/jobs\/5657762171#step:3:1022 ``` =========================== short test summary info ============================ error tests\/integration\/examples\/test_notebooks_gpu.py error tests\/integration\/examples\/test_notebooks_gpu.py error tests\/integration\/examples\/test_notebooks_gpu.py ======================== 48 warnings, 3 errors in 3.79s ======================== error: not found: \/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration (no name '\/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_lightgcn_deep_dive_integration' in any of []) error: not found: \/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration (no name '\/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_dkn_quickstart_integration' in any of []) error: not found: \/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration (no name '\/mnt\/\/cr\/j\/445b60537f0546449ad2693000a5417e\/exe\/wd\/tests\/integration\/examples\/test_notebooks_gpu.py::test_slirec_quickstart_integration' in any of []) info:submit_groupwise__pytest.py:test execution completed! ``` ### in which platform does it happen? ### how do we replicate the issue? ### expected behavior (i.e. solution) ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered errors in some of the tests for the Microsoft Recommenders project, resulting in 48 warnings and 3 errors in 3.79s.",
        "Issue_preprocessed_content":"Title: error in some of the tests; Content: description describe your in detail there are some errors in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. azure databricks. other platforms. how do we replicate the issue? please be specific as possible . for example create a conda environment for pyspark run unit test with expected behavior for example the tests for sar pyspark should pass successfully. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1716",
        "Issue_title":"[BUG] SASRec integration test unusually long time on AzureML compute cluster",
        "Issue_creation_time":1652368299000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nRuntime of [tests\/integration\/examples\/test_notebooks_gpu.py::test_sasrec_quickstart_integration](https:\/\/github.com\/microsoft\/recommenders\/blob\/6987858116d21699f6d92661f03c1529383c7d88\/tests\/integration\/examples\/test_notebooks_gpu.py#L679) varies a lot on the following platforms:\r\n- As part of ADO pipeline, it takes ~562 sec to complete.\r\n- When run as an experiment on AzureML compute cluster triggered using a [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml), it takes ~7080 sec.\r\n\r\nWe need to investigate why this happens.\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nBoth the machines are of same type (NC6s_V2), and use the same CUDA and CuDNN versions:\r\n`cudatoolkit=11.2`\r\n`cudnn=8.1`\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a conda environment for pyspark -->\r\n<!--- * Run unit test `test_sar_pyspark.py` with `pytest -m 'spark'` -->\r\n<!--- * ... -->\r\nTrigger the [GitHub workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml) manually and take a look at pytest logs in the dashboard to see the execution times.\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] sasrec integration test unusually long time on compute cluster; Content: ### description runtime of [tests\/integration\/examples\/test_notebooks_gpu.py::test_sasrec_quickstart_integration](https:\/\/github.com\/microsoft\/recommenders\/blob\/6987858116d21699f6d92661f03c1529383c7d88\/tests\/integration\/examples\/test_notebooks_gpu.py#l679) varies a lot on the following platforms: - as part of ado pipeline, it takes ~562 sec to complete. - when run as an experiment on compute cluster triggered using a [github workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml), it takes ~7080 sec. we need to investigate why this happens. ### in which platform does it happen? both the machines are of same type (nc6s_v2), and use the same cuda and cudnn versions: `cudatoolkit=11.2` `cudnn=8.1` ### how do we replicate the issue? trigger the [github workflow](https:\/\/github.com\/microsoft\/recommenders\/blob\/pradjoshi\/aml_tests\/.github\/workflows\/aml-nightly.yml) manually and take a look at pytest logs in the dashboard to see the execution times. ### expected behavior (i.e. solution) ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the runtime of a SASRec integration test varies significantly between two machines of the same type, and needs to investigate why this happens.",
        "Issue_preprocessed_content":"Title: sasrec integration test unusually long time on compute cluster; Content: description describe your in detail runtime of varies a lot on the following platforms as part of ado pipeline, it takes sec to complete. when run as an experiment on compute cluster triggered using a , it takes sec. we need to investigate why this happens. in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. azure databricks. other platforms. both the machines are of same type , and use the same cuda and cudnn versions how do we replicate the issue? please be specific as possible . for example create a conda environment for pyspark run unit test with trigger the manually and take a look at pytest logs in the dashboard to see the execution times. expected behavior for example the tests for sar pyspark should pass successfully. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/1171",
        "Issue_title":"[BUG] New ver. of Azure CLI is not compatible with the old Azure ML package",
        "Issue_creation_time":1596320174000,
        "Issue_closed_time":1603980914000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\nWe fixed azureml-sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75).\r\nThe new version of azure-cli is not compatible with the old azureml package and throws an error when creating AzureML workspace:\r\n\r\n```\r\nUnable to create the workspace. \r\n Azure Error: InvalidRequestContent\r\nMessage: The request content was invalid and could not be deserialized: 'Could not find member 'template' on object of type 'DeploymentDefinition'. Path 'template', line 1, position 12.'.\r\n```\r\n\r\nThere is an open issue at Azure cli about the similar error: https:\/\/github.com\/Azure\/azure-cli-extensions\/issues\/1591\r\n\r\n### In which platform does it happen?\r\nLinux Ubuntu\r\n(Haven't tested on other platforms)\r\n\r\n### How do we replicate the issue?\r\nInstall reco_pyspark and run operationalization notebook.\r\n\r\n### Expected behavior (i.e. solution)\r\nFix the version of azure-cli\r\n```\r\nazure-cli-core==2.0.75\r\n```\r\n\r\n### Other Comments\r\nI'm working on #1158 and #900.\r\nIf fixing the azure-cli-core version is okay, then I will address this issue together.\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] new ver. of azure cli is not compatible with the old package; Content: ### description we fixed -sdk ver (==1.0.69) but not on azure-cli-core (>=2.0.75). the new version of azure-cli is not compatible with the old package and throws an error when creating workspace: ``` unable to create the workspace. azure error: invalidrequestcontent message: the request content was invalid and could not be deserialized: 'could not find member 'template' on object of type 'deploymentdefinition'. path 'template', line 1, position 12.'. ``` there is an open issue at azure cli about the similar error: https:\/\/github.com\/azure\/azure-cli-extensions\/issues\/1591 ### in which platform does it happen? linux ubuntu (haven't tested on other platforms) ### how do we replicate the issue? install reco_pyspark and run operationalization notebook. ### expected behavior (i.e. solution) fix the version of azure-cli ``` azure-cli-core==2.0.75 ``` ### other comments i'm working on #1158 and #900. if fixing the azure-cli-core version is okay, then i will address this issue together.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the new version of Azure CLI was not compatible with the old package, resulting in an error when creating a workspace.",
        "Issue_preprocessed_content":"Title: new ver. of azure cli is not compatible with the old package; Content: description we fixed ver but not on . the new version of is not compatible with the old package and throws an error when creating workspace there is an open issue at azure cli about the similar error in which platform does it happen? linux ubuntu haven't tested on other platforms how do we replicate the issue? install and run operationalization notebook. expected behavior fix the version of other comments i'm working on and . if fixing the version is okay, then i will address this issue together."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/695",
        "Issue_title":"[BUG] Remove contrib from azureml",
        "Issue_creation_time":1553860230000,
        "Issue_closed_time":1555413510000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":12.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\nThe product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `azureml-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n<!--- * Azure Databricks.  -->\r\n<!--- * Other platforms.  -->\r\nDSVM, DB\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The tests for SAR PySpark should pass successfully. -->\r\neverything runs\r\n\r\n### Other Comments\r\nquestion to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] remove contrib from ; Content: ### description the product team mentioned that contrib package is not recomended for production, we need to remove contrib from here `-sdk[notebooks,tensorboard,contrib]==1.0.18` and check that all the tests pass ### in which platform does it happen? dsvm, db ### expected behavior (i.e. solution) everything runs ### other comments question to @anargyri @loomlike @jreynolds01 @gramhagen @bethz @heatherbshapiro @jingyanwangms are we using contrib anywhere (or planning to use)?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they needed to remove the contrib package from a product and check that all the tests pass, while also asking other users if they were using or planning to use the contrib package.",
        "Issue_preprocessed_content":"Title: remove contrib from ; Content: description describe your in detail the product team mentioned that contrib package is not recomended for production, we need to remove contrib from here and check that all the tests pass in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. azure databricks. other platforms. dsvm, db expected behavior for example the tests for sar pyspark should pass successfully. everything runs other comments question to are we using contrib anywhere ?"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/recommenders\/issues\/451",
        "Issue_title":"Remove azureml sdk preview private PyPi index from operationalize notebook",
        "Issue_creation_time":1548435846000,
        "Issue_closed_time":1548948415000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"This [notebook](https:\/\/github.com\/Microsoft\/Recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to Azure ML SDK preview private index. \r\n\r\n    # Required packages for AzureML execution, history, and data preparation.\r\n    - --extra-index-url https:\/\/azuremlsdktestpypi.azureedge.net\/sdk-release\/Preview\/E7501C02541B433786111FE8E140CAA1\r\n\r\nGiven that Azure ML SDK is now available though regular PyPi as a GA product, and preview versions are unsupported, the extra-index-url should be removed.\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: remove sdk preview private pypi index from operationalize notebook; Content: this [notebook](https:\/\/github.com\/microsoft\/recommenders\/blob\/master\/notebooks\/04_operationalize\/als_movie_o16n.ipynb) contains a reference to sdk preview private index. # required packages for execution, history, and data preparation. - --extra-index-url https:\/\/sdktestpypi.azureedge.net\/sdk-release\/preview\/e7501c02541b433786111fe8e140caa1 given that sdk is now available though regular pypi as a ga product, and preview versions are unsupported, the extra-index-url should be removed.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to remove the reference to the SDK Preview private index from an operationalize notebook.",
        "Issue_preprocessed_content":"Title: remove sdk preview private pypi index from operationalize notebook; Content: this contains a reference to sdk preview private index. required packages for execution, history, and data preparation. given that sdk is now available though regular pypi as a ga product, and preview versions are unsupported, the should be removed."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1590",
        "Issue_title":"Unable to open R locfit package in Azure Machine Learning",
        "Issue_creation_time":1631291354000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I have trained a model locally using the R package locfit. I am now trying to run this in Azure Machine Learning.\r\n\r\nMost guides\/previous questions appear to be in relation to Azure Machine Learning (classic). Although I believe the process outlined in similar posts will be similar (e.g. here, here, I am still unable to get it to work.\r\n\r\nI have outlined the steps I have followed below:\r\n\r\nDownload locfit R package for windows Zip file from here\r\n\r\nPut this downloaded Zip file into a new Zip file entitled \"locfit_package\"\r\n\r\nI upload this \"locfit_package\" zip folder to AML as a dataset (Create Dataset > From Local Files > name: locfit_package dataset type: file > Upload the zip (\"locfit_package\") > Confirm upload is correct\r\n\r\nIn the R terminal I then execute the following code:\r\n\r\n```\r\ninstall.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, verbose = TRUE)\r\n\r\nlibrary(locfit_package, lib.loc=\".\", verbose=TRUE)\r\n\r\nlibrary(locfit)\r\n\r\n```\r\nThe following error message is then returned:\r\n\r\n```\r\nsystem (cmd0): \/usr\/lib\/R\/bin\/R CMD INSTALL\r\n\r\nWarning: invalid package \u2018src\/locfit_package.zip\u2019 Error: ERROR: no packages specified Warning message:\r\n\r\nIn install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = NULL, : installation of package \u2018src\/locfit_package.zip\u2019 had non-zero exit status Error in library(locfit_package, lib.loc = \".\", verbose = TRUE) : there is no package called \u2018locfit_package\u2019 Execution halted\r\n\r\n\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to open r locfit package in ; Content: i have trained a model locally using the r package locfit. i am now trying to run this in . most guides\/previous questions appear to be in relation to (classic). although i believe the process outlined in similar posts will be similar (e.g. here, here, i am still unable to get it to work. i have outlined the steps i have followed below: download locfit r package for windows zip file from here put this downloaded zip file into a new zip file entitled \"locfit_package\" i upload this \"locfit_package\" zip folder to aml as a dataset (create dataset > from local files > name: locfit_package dataset type: file > upload the zip (\"locfit_package\") > confirm upload is correct in the r terminal i then execute the following code: ``` install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = null, verbose = true) library(locfit_package, lib.loc=\".\", verbose=true) library(locfit) ``` the following error message is then returned: ``` system (cmd0): \/usr\/lib\/r\/bin\/r cmd install warning: invalid package src\/locfit_package.zip error: error: no packages specified warning message: in install.packages(\"src\/locfit_package.zip\", lib = \".\", repos = null, : installation of package src\/locfit_package.zip had non-zero exit status error in library(locfit_package, lib.loc = \".\", verbose = true) : there is no package called locfit_package execution halted ```",
        "Issue_original_content_gpt_summary":"The user encountered challenges while attempting to open the r locfit package in , with the installation of the package resulting in a non-zero exit status.",
        "Issue_preprocessed_content":"Title: unable to open r locfit package in ; Content: i have trained a model locally using the r package locfit. i am now trying to run this in . most questions appear to be in relation to . although i believe the process outlined in similar posts will be similar confirm upload is correct in the r terminal i then execute the following code the following error message is then returned"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1589",
        "Issue_title":"Azure ML mounting Storage Account",
        "Issue_creation_time":1631278836000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Hello, \r\n\r\nWe are trying to mount an Azure Storage account in Azure ML. This works perfectly fine, until we start a child run. In the logs of the child run, we can see the following:\r\nSet Dataset input__c79bd306's target path to \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4\r\n\r\nBut when we try to access the mount, we get the following error: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ml-studio-01\/azureml\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4': No such file or directory\r\n\r\nThe code to start the child run can be found below.\r\nThank you for your help.\r\n\r\n`child_config = ScriptRunConfig(source_directory='.',\r\n                                       script='src\/main_child.py',\r\n                                       arguments=arguments,\r\n                                       environment=environment,\r\n                                       docker_runtime_config=DockerConfiguration(use_docker=True),\r\n                                       compute_target=compute_target)\r\nrun.submit_child(child_config)`\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: mounting storage account; Content: hello, we are trying to mount an azure storage account in . this works perfectly fine, until we start a child run. in the logs of the child run, we can see the following: set dataset input__c79bd306's target path to \/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/ml-studio-01\/\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4 but when we try to access the mount, we get the following error: '\/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/ml-studio-01\/\/train_classification_model_20210909_fr_1631216080_e3eca838\/wd\/input__c79bd306_f7faa3c3-938e-4cfc-950b-c91c9827dfa4': no such file or directory the code to start the child run can be found below. thank you for your help. `child_config = scriptrunconfig(source_directory='.', script='src\/main_child.py', arguments=arguments, environment=environment, docker_runtime_config=dockerconfiguration(use_docker=true), compute_target=compute_target) run.submit_child(child_config)`",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to mount an Azure Storage Account in a child run, resulting in an error when trying to access the mount.",
        "Issue_preprocessed_content":"Title: mounting storage account; Content: hello, we are trying to mount an azure storage account in . this works perfectly fine, until we start a child run. in the logs of the child run, we can see the following set dataset target path to but when we try to access the mount, we get the following error no such file or directory the code to start the child run can be found below. thank you for your help."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1587",
        "Issue_title":"Pandas dataframes with array column values are not correctly persisted as AzureML datasets",
        "Issue_creation_time":1630657501000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Pandas dataframes with arrays as column values seem to be incorrectly persisted. An example:\r\n\r\n```python\r\ntest_df = pd.DataFrame({'x': [np.random.rand(1000) for _ in range(1000)]})\r\nds = Datastore.get_default(ws)\r\nDataset.Tabular.register_pandas_dataframe(test_df, ds, 'test_dataset')\r\n\r\ntest_df.head()\r\n###\r\n\tx\r\n0\t[0.5044850335733219, 0.6054305053424696, 0.669...\r\n1\t[0.41759815476145723, 0.266477750018155, 0.511...\r\n2\t[0.6777708610872593, 0.16925324567267985, 0.16...\r\n3\t[0.4268294269387616, 0.6540643485117185, 0.033...\r\n4\t[0.6560106490417036, 0.5804652379458484, 0.582...\r\n\r\nDataset.get_by_name(ws, 'test_dataset').to_pandas_dataframe().head()\r\n###\r\nx\r\n0\tERROR\r\n1\tERROR\r\n2\tERROR\r\n3\tERROR\r\n4\tERROR\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: pandas dataframes with array column values are not correctly persisted as datasets; Content: pandas dataframes with arrays as column values seem to be incorrectly persisted. an example: ```python test_df = pd.dataframe({'x': [np.random.rand(1000) for _ in range(1000)]}) ds = datastore.get_default(ws) dataset.tabular.register_pandas_dataframe(test_df, ds, 'test_dataset') test_df.head() ### x 0 [0.5044850335733219, 0.6054305053424696, 0.669... 1 [0.41759815476145723, 0.266477750018155, 0.511... 2 [0.6777708610872593, 0.16925324567267985, 0.16... 3 [0.4268294269387616, 0.6540643485117185, 0.033... 4 [0.6560106490417036, 0.5804652379458484, 0.582... dataset.get_by_name(ws, 'test_dataset').to_pandas_dataframe().head() ### x 0 error 1 error 2 error 3 error 4 error ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where pandas dataframes with array column values were not correctly persisted as datasets.",
        "Issue_preprocessed_content":"Title: pandas dataframes with array column values are not correctly persisted as datasets; Content: pandas dataframes with arrays as column values seem to be incorrectly persisted. an example"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1584",
        "Issue_title":"Identity Based Access No longer works (with Azure SQL DB datastore) in V1.33 of Azure ML SDK",
        "Issue_creation_time":1630006433000,
        "Issue_closed_time":1632248052000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"Seems like recent upgrade to V1.33 for Azure ML SDK has changed how identity based access worked? Previously if you had a datastore (ex. SQL) with no credentials and then tried to register a dataset, it would prompt you to login to get your AAD auth token to see if you had permission to get access to the underlying data source. Seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab AD auth token:\r\n**_Getting data access token with Assigned Identity (client_id=clientid)._**\r\n\r\n\r\nI have verified the underlying datastore does not have Managed Identity on and V1.32 SDK Prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. Has any changes been made to the identity based access feature from on V1.33 SDK? According to the SDK docs, running the TabularDataset.to_pandas_dataframe() command should prompt an AD login if using no credentialed datastore into dataset creation. FYI currently using Azure SQL DB as datastore, any clarifications would be appreciated!\r\nazureml.core.Datastore class - Azure Machine Learning Python | Microsoft Docs\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: identity based access no longer works (with azure sql db datastore) in v1.33 of sdk; Content: seems like recent upgrade to v1.33 for sdk has changed how identity based access worked? previously if you had a datastore (ex. sql) with no credentials and then tried to register a dataset, it would prompt you to login to get your aad auth token to see if you had permission to get access to the underlying data source. seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab ad auth token: **_getting data access token with assigned identity (client_id=clientid)._** i have verified the underlying datastore does not have managed identity on and v1.32 sdk prompts me to log in at microsoft.com\/devicelogin and gives a code to enter and identity based access works normally after. has any changes been made to the identity based access feature from on v1.33 sdk? according to the sdk docs, running the tabulardataset.to_pandas_dataframe() command should prompt an ad login if using no credentialed datastore into dataset creation. fyi currently using azure sql db as datastore, any clarifications would be appreciated! .core.datastore class - python | microsoft docs",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with identity based access no longer working in v1.33 of the SDK when trying to register a dataset with an Azure SQL DB datastore.",
        "Issue_preprocessed_content":"Title: identity based access no longer works in of sdk; Content: seems like recent upgrade to for sdk has changed how identity based access worked? previously if you had a datastore with no credentials and then tried to register a dataset, it would prompt you to login to get your aad auth token to see if you had permission to get access to the underlying data source. seems like recent update the same code now seems to prompt this message instead of asking for user to login to and grab ad auth token data access token with assigned identity i have verified the underlying datastore does not have managed identity on and sdk prompts me to log in at and gives a code to enter and identity based access works normally after. has any changes been made to the identity based access feature from on sdk? according to the sdk docs, running the command should prompt an ad login if using no credentialed datastore into dataset creation. fyi currently using azure sql db as datastore, any clarifications would be appreciated! class python microsoft docs"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1564",
        "Issue_title":"pip install `azureml-core` fails on `ruamel.yaml`",
        "Issue_creation_time":1627861519000,
        "Issue_closed_time":1629244160000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### System Specs\r\n**Operating System:** Windows 10\r\n**Python Version:** 3.9.5 64-bit\r\n\r\nWhen I run the command:\r\n\r\n```terminal\r\npip install azureml-core\r\n```\r\n\r\nI get an error during the installation, specifically on the `ruamel.yaml` package. I guess the first question I have is there any reason we are restricted to that specific version of `ruamel.yaml`? I was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix.\r\n\r\n### Partial Log\r\n```terminal\r\nAttempting uninstall: ruamel.yaml\r\nFound existing installation: ruamel.yaml 0.17.10\r\nUninstalling ruamel.yaml-0.17.10:\r\nSuccessfully uninstalled ruamel.yaml-0.17.10\r\nRunning setup.py install for ruamel.yaml ... error\r\nERROR: Command errored out with exit status 1:\r\n```\r\n\r\n### Full Log\r\n[Error Log From Installation Run](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/files\/6913613\/error.log)",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: pip install `-core` fails on `ruamel.yaml`; Content: ### system specs **operating system:** windows 10 **python version:** 3.9.5 64-bit when i run the command: ```terminal pip install -core ``` i get an error during the installation, specifically on the `ruamel.yaml` package. i guess the first question i have is there any reason we are restricted to that specific version of `ruamel.yaml`? i was able to install the latest version **(0.17.10)** no problem, so if we could use a later version that would be the easiest fix. ### partial log ```terminal attempting uninstall: ruamel.yaml found existing installation: ruamel.yaml 0.17.10 uninstalling ruamel.yaml-0.17.10: successfully uninstalled ruamel.yaml-0.17.10 running setup.py install for ruamel.yaml ... error error: command errored out with exit status 1: ``` ### full log [error log from installation run](https:\/\/github.com\/azure\/machinelearningnotebooks\/files\/6913613\/error.log)",
        "Issue_original_content_gpt_summary":"The user encountered an error during the installation of the `ruamel.yaml` package when running the command `pip install -core`, and is wondering if they are restricted to a specific version of the package.",
        "Issue_preprocessed_content":"Title: pip install fails on ; Content: system specs operating system windows python version when i run the command i get an error during the installation, specifically on the package. i guess the first question i have is there any reason we are restricted to that specific version of ? i was able to install the latest version no problem, so if we could use a later version that would be the easiest fix. partial log full log error log from installation"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1545",
        "Issue_title":"AzureMLException with model.download",
        "Issue_creation_time":1625795312000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Hi!\r\n\r\nWhen trying to download a registered model from the AMLS workspace, I'm getting the following traceback. The file shows up in the `target_dir` (and ADLS path) however the size is 0 bytes, so it is making the file, however no data is being transferred into it.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    432         try:\r\n--> 433             return exec_func()\r\n    434         except exceptions as request_exception:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in exec_func()\r\n    212                                                           max_connections=max_concurrency,\r\n--> 213                                                           validate_content=_validate_check_sum)\r\n    214             file_size = os.stat(path).st_size\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/baseblobservice.py in get_blob_to_path(self, container_name, blob_name, file_path, open_mode, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout)\r\n   1855 \r\n-> 1856         with open(file_path, open_mode) as stream:\r\n   1857             blob = self.get_blob_to_stream(\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAzureMLException                          Traceback (most recent call last)\r\n<command-3894832347418984> in <module>\r\n----> 1 existing_model.download(target_dir=\"\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\")\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in download(self, target_dir, exist_ok, exists_ok)\r\n    999 \r\n   1000         # download files using sas\r\n-> 1001         file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\r\n   1002         if len(file_paths) == 0:\r\n   1003             raise WebserviceException(\"Illegal state. Unpack={}, Paths in target_dir is \"\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/core\/model.py in _download_model_files(self, sas_to_relative_download_path, target_dir, exist_ok)\r\n    940                                           \"{}\".format(target_path), logger=module_logger)\r\n    941             sas_to_relative_download_path[sas] = target_path\r\n--> 942             download_file(sas, target_path, stream=True)\r\n    943 \r\n    944         if self.unpack:\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in download_file(source_uri, path, max_retries, stream, protocol, session, _validate_check_sum, max_concurrency)\r\n    219                                        'present in blob.'.format(file_size, content_length))\r\n    220 \r\n--> 221         return _retry(exec_func, max_retries=max_retries)\r\n    222 \r\n    223     # download using requests.Session\r\n\r\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions)\r\n    443             else:\r\n    444                 module_logger.error('Failed to download file with error: {}'.format(request_exception))\r\n--> 445                 raise AzureMLException('Download of file failed with error: {}'.format(request_exception))\r\n    446         finally:\r\n    447             clean_up_func()\r\n\r\nAzureMLException: AzureMLException:\r\n\tMessage: Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Download of file failed with error: [Errno 2] No such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\"\r\n    }\r\n}\r\n\r\n```\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/7530947\/125011096-a8c32700-e01c-11eb-83b4-4305be4095df.png)\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: exception with model.download; Content: hi! when trying to download a registered model from the amls workspace, i'm getting the following traceback. the file shows up in the `target_dir` (and adls path) however the size is 0 bytes, so it is making the file, however no data is being transferred into it. ```python --------------------------------------------------------------------------- filenotfounderror traceback (most recent call last) \/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions) 432 try: --> 433 return exec_func() 434 except exceptions as request_exception: \/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in exec_func() 212 max_connections=max_concurrency, --> 213 validate_content=_validate_check_sum) 214 file_size = os.stat(path).st_size \/databricks\/python\/lib\/python3.7\/site-packages\/\/_vendor\/azure_storage\/blob\/baseblobservice.py in get_blob_to_path(self, container_name, blob_name, file_path, open_mode, snapshot, start_range, end_range, validate_content, progress_callback, max_connections, lease_id, if_modified_since, if_unmodified_since, if_match, if_none_match, timeout) 1855 -> 1856 with open(file_path, open_mode) as stream: 1857 blob = self.get_blob_to_stream( filenotfounderror: [errno 2] no such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001' during handling of the above exception, another exception occurred: exception traceback (most recent call last) in ----> 1 existing_model.download(target_dir=\"\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\") \/databricks\/python\/lib\/python3.7\/site-packages\/\/core\/model.py in download(self, target_dir, exist_ok, exists_ok) 999 1000 # download files using sas -> 1001 file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok) 1002 if len(file_paths) == 0: 1003 raise webserviceexception(\"illegal state. unpack={}, paths in target_dir is \" \/databricks\/python\/lib\/python3.7\/site-packages\/\/core\/model.py in _download_model_files(self, sas_to_relative_download_path, target_dir, exist_ok) 940 \"{}\".format(target_path), logger=module_logger) 941 sas_to_relative_download_path[sas] = target_path --> 942 download_file(sas, target_path, stream=true) 943 944 if self.unpack: \/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in download_file(source_uri, path, max_retries, stream, protocol, session, _validate_check_sum, max_concurrency) 219 'present in blob.'.format(file_size, content_length)) 220 --> 221 return _retry(exec_func, max_retries=max_retries) 222 223 # download using requests.session \/databricks\/python\/lib\/python3.7\/site-packages\/\/_file_utils\/file_utils.py in _retry(exec_func, clean_up_func, max_retries, exceptions) 443 else: 444 module_logger.error('failed to download file with error: {}'.format(request_exception)) --> 445 raise exception('download of file failed with error: {}'.format(request_exception)) 446 finally: 447 clean_up_func() exception: exception: message: download of file failed with error: [errno 2] no such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001' innerexception none errorresponse { \"error\": { \"message\": \"download of file failed with error: [errno 2] no such file or directory: '\/dbfs\/mnt\/prism0stg0dls\/amls\/enablers\/amls_model_saving\/models\/test2\/test2\/variables\/variables.data-00000-of-00001'\" } } ``` ![image](https:\/\/user-images.githubusercontent.com\/7530947\/125011096-a8c32700-e01c-11eb-83b4-4305be4095df.png)",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when trying to download a registered model from the AMLS workspace, resulting in a 0 byte file.",
        "Issue_preprocessed_content":"Title: exception with ; Content: hi! when trying to download a registered model from the amls workspace, i'm getting the following traceback. the file shows up in the however the size is bytes, so it is making the file, however no data is being transferred into it."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1537",
        "Issue_title":"Bug: Failure while loading azureml_run_type_providers",
        "Issue_creation_time":1625218579000,
        "Issue_closed_time":1630367426000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"In a fresh conda environment, I get several warnings that halt the script execution:\r\n```\r\n...\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), Requirement.parse('docker<5.0.0'), {'azureml-core'}).\r\n...\r\n```\r\n\r\nMy environment is specified by:\r\n```yaml\r\nname: xxx\r\nchannels:\r\n  - anaconda\r\n  - pytorch-lts\r\ndependencies:\r\n  - python=3.6\r\n  - pandas=1.1.3\r\n  - numpy=1.19.2\r\n  - scikit-learn=0.23.2\r\n  - matplotlib\r\n  - mkl=2020.2\r\n  - pytorch=1.8.1\r\n  - cpuonly=1.0\r\n  - pip\r\n  - pip:\r\n      - azureml-sdk==1.31.0\r\n      - azureml-defaults==1.31.0\r\n      - azure-storage-blob==12.8.1\r\n      - mlflow==1.18.0\r\n      - azureml-mlflow==1.31.0\r\n      - pytorch-lightning==1.3.8\r\n      - onnxruntime==1.8.0\r\n      - docker<5.0.0 # this is the fix needed\r\n```\r\nThe fix is to specify `docker<5.0.0`. Perhaps, there are some wrong deps checks somewhere.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: bug: failure while loading _run_type_providers; Content: in a fresh conda environment, i get several warnings that halt the script execution: ``` ... failure while loading _run_type_providers. failed to load entrypoint .pipelinerun = .pipeline.core.run:pipelinerun._from_dto with exception (docker 5.0.0 (c:\\dev\\miniconda\\envs\\xxx\\lib\\site-packages), requirement.parse('docker<5.0.0'), {'-core'}). ... ``` my environment is specified by: ```yaml name: xxx channels: - anaconda - pytorch-lts dependencies: - python=3.6 - pandas=1.1.3 - numpy=1.19.2 - scikit-learn=0.23.2 - matplotlib - mkl=2020.2 - pytorch=1.8.1 - cpuonly=1.0 - pip - pip: - -sdk==1.31.0 - -defaults==1.31.0 - azure-storage-blob==12.8.1 - ==1.18.0 - -==1.31.0 - pytorch-lightning==1.3.8 - onnxruntime==1.8.0 - docker<5.0.0 # this is the fix needed ``` the fix is to specify `docker<5.0.0`. perhaps, there are some wrong deps checks somewhere. --- #### document details *do not edit this section. it is required for docs.microsoft.com github issue linking.* * id: eb938463-51c2-43f3-d528-76a07a28bec8 * version independent id: e15753c0-6fe1-100a-0efc-08c1f845dc83 * [ sdk for python - python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py) * content source: [-docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/docs-ref-conceptual\/index.md) * service: **machine-learning** * sub-service: **core** * github login: @trevorbye * microsoft alias: **trbye**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while loading _run_type_providers in a fresh conda environment, which was resolved by specifying 'docker<5.0.0' in the dependencies.",
        "Issue_preprocessed_content":"Title: bug failure while loading ; Content: in a fresh conda environment, i get several warnings that halt the script execution my environment is specified by the fix is to specify . perhaps, there are some wrong deps checks somewhere. document details do not edit this section. it is required for github issue id version independent id content content source service core github login microsoft alias trbye"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1534",
        "Issue_title":"Broken link in AML doc to azureml.core.runconfig.MpiConfiguration",
        "Issue_creation_time":1624997173000,
        "Issue_closed_time":1626388206000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"\r\n<img width=\"1430\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/5203025\/123860354-63399680-d958-11eb-9dc8-dc0a52d67cc2.png\">\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 109d9284-e234-5086-5da6-4155291361c8\r\n* Version Independent ID: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d\r\n* Content: [azureml.core.ScriptRunConfig class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.scriptrunconfig?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.ScriptRunConfig.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: broken link in aml doc to .core.runconfig.mpiconfiguration; Content: --- #### document details *do not edit this section. it is required for docs.microsoft.com github issue linking.* * id: 109d9284-e234-5086-5da6-4155291361c8 * version independent id: 57cc0c7a-faa7-1a86-ee14-b9cf99fb540d * [.core.scriptrunconfig class - python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.scriptrunconfig?view=azure-ml-py) * content source: [-docset\/stable\/docs-ref-autogen\/-core\/.core.scriptrunconfig.yml](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/stable\/docs-ref-autogen\/-core\/.core.scriptrunconfig.yml) * service: **machine-learning** * sub-service: **core** * github login: @debfro * microsoft alias: **debfro**",
        "Issue_original_content_gpt_summary":"The user @debfro encountered a broken link in the Azure Machine Learning Python API documentation for the .core.scriptrunconfig class to the .core.runconfig.mpiconfiguration class.",
        "Issue_preprocessed_content":"Title: broken link in aml doc to ; Content: img width alt image document details do not edit this section. it is required for github issue id version independent id content content source service core github login microsoft alias debfro"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1523",
        "Issue_title":"Warning while loading the azureml.core",
        "Issue_creation_time":1624519136000,
        "Issue_closed_time":1626388114000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi,\r\n\r\nI have installed the azure ml using below environment yml, installation happened without any issues but when I import the azureml.core I am getting exception.\r\n\r\n**conda environment yml**\r\n```\r\nname: ati_reranking_automl_py36\r\ndependencies:\r\n  # The python interpreter version.\r\n  # Currently Azure ML only supports 3.5.2 and later.\r\n- pip==20.2.4\r\n- python==3.6.13\r\n- nb_conda\r\n- matplotlib==2.1.0\r\n- numpy==1.18.5\r\n- seaborn==0.9.0\r\n- urllib3<1.24\r\n- scipy>=1.4.1,<=1.5.2\r\n- scikit-learn==0.22.1\r\n- pandas==0.25.1\r\n- py-xgboost<=1.3.3\r\n- jupyterlab==1.0.2\r\n- ipykernel==5.3.4\r\n- pytorch::pytorch=1.4.0\r\n\r\n- pip:\r\n  # Base AzureML SDK\r\n  - azureml-sdk\r\n      \r\n  - pytorch-transformers==1.0.0\r\n\r\n  # Scoring deps\r\n  - inference-schema[numpy-support]\r\n```\r\n\r\n\r\n**Exception**\r\nimport azureml.core\r\n`Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).`\r\n\r\nAzure ML SDK Version:  1.31.0\r\n\r\n\r\nPlease help.\r\nThanks",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: warning while loading the .core; hi, i have installed the using below environment yml, installation happened without any issues but when i import the .core i am getting exception. **conda environment yml** ``` name: ati_reranking_automl_py36 dependencies: # the python interpreter version. # currently only supports 3.5.2 and later. - pip==20.2.4 - python==3.6.13 - nb_conda - matplotlib==2.1.0 - numpy==1.18.5 - seaborn==0.9.0 - urllib3<1.24 - scipy>=1.4.1,<=1.5.2 - scikit-learn==0.22.1 - pandas==0.25.1 - py-xgboost<=1.3.3 - jupyterlab==1.0.2 - ipykernel==5.3.4 - pytorch::pytorch=1.4.0 - pip: # base sdk - -sdk - pytorch-transformers==1.0.0 # scoring deps - inference-schema[numpy-support] ``` **exception** import .core `failure while loading _run_type_providers. failed to load entrypoint automl = .train.automl.run:auto._from_run_dto with exception (cryptography 2.3.1 (c:\\miniconda\\envs\\ati_reranking_automl_py36\\lib\\site-packages), requirement.parse('cryptography<4.0.0,>=3.3.1; Content: extra == \"crypto\"'), {'pyjwt'}).` sdk version: 1.31.0 please help. thanks",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while loading the .core, resulting in an exception due to a conflict between the cryptography version and the pyjwt version.",
        "Issue_preprocessed_content":"Title: warning while loading the ; Content: hi, i have installed the using below environment yml, installation happened without any issues but when i import the i am getting exception. conda environment yml exception import sdk version please help. thanks"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1517",
        "Issue_title":"AzureML Pipelines: Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.",
        "Issue_creation_time":1624292044000,
        "Issue_closed_time":1626719342000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"I am running a lightly edited version of this pipeline example: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-azureml\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb\r\n\r\nand it is yielding me this error (or warning): `Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.`\r\n\r\nI am also getting this same warning in other pipelines I make and I cannot figure out what is causing it.\r\n\r\nHere is a slightly reduced MWE for (hopefully) clarity:\r\n\r\n\r\n```\r\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication\r\nfrom azureml.core.runconfig import RunConfiguration, DEFAULT_CPU_IMAGE\r\nfrom azureml.core.conda_dependencies import CondaDependencies\r\nfrom azureml.core.compute import ComputeTarget, AmlCompute\r\nfrom azureml.core.compute_target import ComputeTargetException\r\nfrom azureml.data import OutputFileDatasetConfig\r\nfrom azureml.pipeline.steps import PythonScriptStep\r\nfrom azureml.pipeline.core import Pipeline\r\n\r\nimport os\r\n\r\n# environment data\r\nfrom dotenv import load_dotenv  # pip install python-dotenv\r\nload_dotenv('.env') # load .env file with sp info\r\n```\r\n\r\n\r\n```\r\n# instantiate the service principal\r\nsp = ServicePrincipalAuthentication(tenant_id=os.environ['AML_TENANT_ID'],\r\n                                    service_principal_id=os.environ['AML_PRINCIPAL_ID'],\r\n                                    service_principal_password=os.environ['AML_PRINCIPAL_PASS'])\r\n```\r\n\r\n\r\n\r\n```\r\n# instantiate a workspace\r\nws = Workspace(subscription_id = \"redacted\",\r\n               resource_group = \"redacted\",\r\n               auth=sp,  # use service principal auth\r\n               workspace_name = \"redacted\")\r\n\r\nprint(\"Found workspace {} at location {}\".format(ws.name, ws.location))\r\n```\r\n\r\n\r\n```\r\n# pipeline step 1\r\nstep1 = PythonScriptStep(\r\n    name=\"generate_data\",\r\n    script_name=\"scripts\/mwe.py\",\r\n    arguments=[\"--save\", 'hello world'],\r\n    runconfig=RunConfiguration(),\r\n    compute_target='retry2',\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\n```\r\n%%writefile scripts\/mwe.py\r\n\r\n# load packages\r\nimport os\r\nfrom azureml.core import Run\r\nimport argparse\r\nimport pandas as pd\r\n\r\nprint('hello world')\r\n```\r\n\r\n\r\n```\r\n# build the pipeline\r\npipeline1 = Pipeline(workspace=ws, steps=[step1])\r\n# validate the pipeline\r\npipeline1.validate()\r\n# submit a pipeline run\r\npipeline_run1 = Experiment(ws, 'mwe').submit(pipeline1)\r\n# run and wait for completion to check its results\r\npipeline_run1.wait_for_completion(show_output=True)\r\n\r\n```\r\n\r\n\r\n\r\n```\r\nExpected a StepRun object but received <class 'azureml.core.run.Run'> instead.\r\nThis usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\r\nPlease check for package conflicts in your python environment\r\n```\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: pipelines: expected a steprun object but received instead.; Content: i am running a lightly edited version of this pipeline example: https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/8f7717014b7e9b431c11857956982f0f718eb362\/how-to-use-\/machine-learning-pipelines\/nyc-taxi-data-regression-model-building\/nyc-taxi-data-regression-model-building.ipynb and it is yielding me this error (or warning): `expected a steprun object but received instead.` i am also getting this same warning in other pipelines i make and i cannot figure out what is causing it. here is a slightly reduced mwe for (hopefully) clarity: ``` from .core import workspace, datastore, dataset, experiment from .core.authentication import serviceprincipalauthentication from .core.runconfig import runconfiguration, default_cpu_image from .core.conda_dependencies import condadependencies from .core.compute import computetarget, amlcompute from .core.compute_target import computetargetexception from .data import outputfiledatasetconfig from .pipeline.steps import pythonscriptstep from .pipeline.core import pipeline import os # environment data from dotenv import load_dotenv # pip install python-dotenv load_dotenv('.env') # load .env file with sp info ``` ``` # instantiate the service principal sp = serviceprincipalauthentication(tenant_id=os.environ['aml_tenant_id'], service_principal_id=os.environ['aml_principal_id'], service_principal_password=os.environ['aml_principal_pass']) ``` ``` # instantiate a workspace ws = workspace(subscription_id = \"redacted\", resource_group = \"redacted\", auth=sp, # use service principal auth workspace_name = \"redacted\") print(\"found workspace {} at location {}\".format(ws.name, ws.location)) ``` ``` # pipeline step 1 step1 = pythonscriptstep( name=\"generate_data\", script_name=\"scripts\/mwe.py\", arguments=[\"--save\", 'hello world'], runconfig=runconfiguration(), compute_target='retry2', allow_reuse=true ) ``` ``` %%writefile scripts\/mwe.py # load packages import os from .core import run import argparse import pandas as pd print('hello world') ``` ``` # build the pipeline pipeline1 = pipeline(workspace=ws, steps=[step1]) # validate the pipeline pipeline1.validate() # submit a pipeline run pipeline_run1 = experiment(ws, 'mwe').submit(pipeline1) # run and wait for completion to check its results pipeline_run1.wait_for_completion(show_output=true) ``` ``` expected a steprun object but received instead. this usually indicates a package conflict with one of the dependencies of -core or -pipeline-core. please check for package conflicts in your python environment ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they received an error message indicating a package conflict with one of the dependencies of -core or -pipeline-core when running a pipeline.",
        "Issue_preprocessed_content":"Title: pipelines expected a steprun object but received instead.; Content: i am running a lightly edited version of this pipeline example and it is yielding me this error i am also getting this same warning in other pipelines i make and i cannot figure out what is causing it. here is a slightly reduced mwe for clarity"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1509",
        "Issue_title":"How to copy files into  docker image while deploying ml model using azure ml model deploy command",
        "Issue_creation_time":1623592472000,
        "Issue_closed_time":1626798489000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I am trying to deploy ml model using az ml model deploy command with additional files.\r\n\r\nEg:-\r\n\r\naz ml model deploy --ds  docker-additional-steps.txt \r\n```\r\ndocker-additional-steps.txt\r\n\r\nCOPY *.txt \/var\/azureml-app\/\r\n```\r\n\r\nbut it gives an error as below\r\n```\r\nFailed\r\nERROR: {'Azure-cli-ml Version': '1.29.0', 'Error': WebserviceException:\r\n\tMessage: Image creation polling reached non-successful terminal state, current state: Failed\r\nError response from server:\r\nStatusCode: 400\r\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Image creation polling reached non-successful terminal state, current state: Failed\\nError response from server:\\nStatusCode: 400\\nMessage: Failed to parse steps: COPY is not an allowed Dockerfile instruction. Allowed instructions: ARG, ENV, EXPOSE, LABEL, RUN\"\r\n    }\r\n}}\r\n\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: how to copy files into docker image while deploying ml model using model deploy command; Content: i am trying to deploy ml model using az ml model deploy command with additional files. eg:- az ml model deploy --ds docker-additional-steps.txt ``` docker-additional-steps.txt copy *.txt \/var\/-app\/ ``` but it gives an error as below ``` failed error: {'azure-cli-ml version': '1.29.0', 'error': webserviceexception: message: image creation polling reached non-successful terminal state, current state: failed error response from server: statuscode: 400 message: failed to parse steps: copy is not an allowed dockerfile instruction. allowed instructions: arg, env, expose, label, run innerexception none errorresponse { \"error\": { \"message\": \"image creation polling reached non-successful terminal state, current state: failed\\nerror response from server:\\nstatuscode: 400\\nmessage: failed to parse steps: copy is not an allowed dockerfile instruction. allowed instructions: arg, env, expose, label, run\" } }} ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while trying to deploy an ML model using the az ml model deploy command with additional files, as the copy instruction was not allowed in the dockerfile.",
        "Issue_preprocessed_content":"Title: how to copy files into docker image while deploying ml model using model deploy command; Content: i am trying to deploy ml model using az ml model deploy command with additional files. eg az ml model deploy but it gives an error as below"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1501",
        "Issue_title":"'ClientRequestError' when trying to use Azure Computer Vision API from Azure Machine Learning Notebook",
        "Issue_creation_time":1622675126000,
        "Issue_closed_time":1631108652000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I'm trying to use Azure Computer Vision's OCR API in an Azure Machine Learning Notebook. However there seems to be an error when trying to call the Computer Vision API from an Azure Machine Learning Notebook. The same code works when I'm running it on a local machine.\r\nI'm following Azure Computer Vision's OCR Quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python\r\n\r\nWhen running the following code, `computervision_client.read(read_image_url, raw=True)` does not return but throws an exception.\r\nException:\r\n`ClientRequestError: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='some-host.cognitiveservices.azure.com', port=443): Max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingOrder=basic (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f59e0102820>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))`\r\n\r\nCode:\r\n```python\r\nfrom azure.cognitiveservices.vision.computervision import ComputerVisionClient\r\nfrom azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\r\nfrom azure.cognitiveservices.vision.computervision.models import VisualFeatureTypes\r\nfrom msrest.authentication import CognitiveServicesCredentials\r\n\r\nfrom array import array\r\nimport os\r\nfrom PIL import Image\r\nimport sys\r\nimport time\r\n\r\n'''\r\nAuthenticate\r\nAuthenticates your credentials and creates a client.\r\n'''\r\nsubscription_key = os.environ[\"COMPUTERVISION_KEY\"]\r\nendpoint = os.environ[\"COMPUTERVISION_URL\"]\r\n\r\ncomputervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\r\n\r\n'''\r\nOCR: Read File using the Read API, extract text - remote\r\nThis example will extract text in an image, then print results, line by line.\r\nThis API call can also extract handwriting style text (not shown).\r\n'''\r\nprint(\"===== Read File - remote =====\")\r\n# Get an image with text\r\nread_image_url = \"https:\/\/raw.githubusercontent.com\/MicrosoftDocs\/azure-docs\/master\/articles\/cognitive-services\/Computer-vision\/Images\/readsample.jpg\"\r\n\r\n# Call API with URL and raw response (allows you to get the operation location)\r\nread_response = computervision_client.read(read_image_url,  raw=True) # <- THROWS EXCEPTION\r\n```\r\n\r\nUsed azure packages:\r\n```\r\nazure-ai-textanalytics                        5.1.0b7\r\nazure-cognitiveservices-vision-computervision 0.9.0\r\nazure-common                                  1.1.27\r\nazure-core                                    1.14.0\r\nazure-cosmos                                  4.2.0\r\nazure-graphrbac                               0.61.1\r\nazure-identity                                1.4.1\r\nazure-mgmt-authorization                      0.61.0\r\nazure-mgmt-containerregistry                  8.0.0\r\nazure-mgmt-core                               1.2.2\r\nazure-mgmt-keyvault                           2.2.0\r\nazure-mgmt-resource                           13.0.0\r\nazure-mgmt-storage                            11.2.0\r\nazure-storage-blob                            12.8.0\r\nazureml-automl-core                           1.29.0\r\nazureml-contrib-dataset                       1.29.0\r\nazureml-core                                  1.29.0.post1\r\nazureml-dataprep                              2.15.1\r\nazureml-dataprep-native                       33.0.0\r\nazureml-dataprep-rslex                        1.13.0\r\nazureml-dataset-runtime                       1.29.0\r\nazureml-pipeline-core                         1.29.0\r\nazureml-pipeline-steps                        1.29.0\r\nazureml-telemetry                             1.29.0\r\nazureml-train-automl-client                   1.29.0\r\nazureml-train-core                            1.29.0\r\nazureml-train-restclients-hyperdrive          1.29.0\r\nazureml-widgets                               1.29.0.post1\r\n```\r\n\r\nMaybe related to #1107",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: 'clientrequesterror' when trying to use azure computer vision api from notebook; Content: i'm trying to use azure computer vision's ocr api in an notebook. however there seems to be an error when trying to call the computer vision api from an notebook. the same code works when i'm running it on a local machine. i'm following azure computer vision's ocr quickstart: https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/computer-vision\/quickstarts-sdk\/client-library?tabs=visual-studio&pivots=programming-language-python when running the following code, `computervision_client.read(read_image_url, raw=true)` does not return but throws an exception. exception: `clientrequesterror: error occurred in request., connectionerror: httpsconnectionpool(host='some-host.cognitiveservices.azure.com', port=443): max retries exceeded with url: \/vision\/v3.2\/read\/analyze?model-version=latest&readingorder=basic (caused by newconnectionerror(': failed to establish a new connection: [errno -3] temporary failure in name resolution'))` code: ```python from azure.cognitiveservices.vision.computervision import computervisionclient from azure.cognitiveservices.vision.computervision.models import operationstatuscodes from azure.cognitiveservices.vision.computervision.models import visualfeaturetypes from msrest.authentication import cognitiveservicescredentials from array import array import os from pil import image import sys import time ''' authenticate authenticates your credentials and creates a client. ''' subscription_key = os.environ[\"computervision_key\"] endpoint = os.environ[\"computervision_url\"] computervision_client = computervisionclient(endpoint, cognitiveservicescredentials(subscription_key)) ''' ocr: read file using the read api, extract text - remote this example will extract text in an image, then print results, line by line. this api call can also extract handwriting style text (not shown). ''' print(\"===== read file - remote =====\") # get an image with text read_image_url = \"https:\/\/raw.githubusercontent.com\/microsoftdocs\/azure-docs\/master\/articles\/cognitive-services\/computer-vision\/images\/readsample.jpg\" # call api with url and raw response (allows you to get the operation location) read_response = computervision_client.read(read_image_url, raw=true) # <- throws exception ``` used azure packages: ``` azure-ai-textanalytics 5.1.0b7 azure-cognitiveservices-vision-computervision 0.9.0 azure-common 1.1.27 azure-core 1.14.0 azure-cosmos 4.2.0 azure-graphrbac 0.61.1 azure-identity 1.4.1 azure-mgmt-authorization 0.61.0 azure-mgmt-containerregistry 8.0.0 azure-mgmt-core 1.2.2 azure-mgmt-keyvault 2.2.0 azure-mgmt-resource 13.0.0 azure-mgmt-storage 11.2.0 azure-storage-blob 12.8.0 -automl-core 1.29.0 -contrib-dataset 1.29.0 -core 1.29.0.post1 -dataprep 2.15.1 -dataprep-native 33.0.0 -dataprep-rslex 1.13.0 -dataset-runtime 1.29.0 -pipeline-core 1.29.0 -pipeline-steps 1.29.0 -telemetry 1.29.0 -train-automl-client 1.29.0 -train-core 1.29.0 -train-restclients-hyperdrive 1.29.0 -widgets 1.29.0.post1 ``` maybe related to #1107",
        "Issue_original_content_gpt_summary":"The user encountered an error when trying to use the Azure Computer Vision API from an notebook, despite the same code working on a local machine.",
        "Issue_preprocessed_content":"Title: 'clientrequesterror' when trying to use azure computer vision api from notebook; Content: i'm trying to use azure computer vision's ocr api in an notebook. however there seems to be an error when trying to call the computer vision api from an notebook. the same code works when i'm running it on a local machine. i'm following azure computer vision's ocr quickstart when running the following code, does not return but throws an exception. exception code used azure packages maybe related to"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1468",
        "Issue_title":"AutoMLPipelineBuilder.get_many_models_train_steps - Error \"Environment name can not start with the prefix AzureML...\"",
        "Issue_creation_time":1620766573000,
        "Issue_closed_time":1622654301000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"Hello,\r\n\r\nWhen running the experiment, the error message **Environment name can not start with the prefix AzureML** was displayed. How can I set the name of the environment? I'm following the GitHub tutorial and haven't found anything about it.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png)\r\n\r\nCode used:\r\n\r\n- Registering Dataset\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png)\r\n\r\n- Training Pipeline\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png)\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png)\r\n\r\nReferences:\r\n\r\n- https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/Automated_ML\/02_AutoML_Training_Pipeline\r\n- https:\/\/github.com\/MicrosoftDocs\/azure-docs\/issues\/65770\r\n\r\nBest regards,\r\nCristina\r\n\r\n\r\n---\r\n#### Detalhes do documento\r\n\r\n\u26a0 *N\u00e3o edite esta se\u00e7\u00e3o. \u00c9 necess\u00e1rio para a vincula\u00e7\u00e3o do problema do docs.microsoft.com \u279f GitHub.*\r\n\r\n* ID: 49399a7d-d4e8-370e-ce62-d60a6b64e412\r\n* Version Independent ID: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf\r\n* Content: [azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr.pt-BR\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-contrib-automl-pipeline-steps\/azureml.contrib.automl.pipeline.steps.AutoMLPipelineBuilder.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: automlpipelinebuilder.get_many_models_train_steps - error \"environment name can not start with the prefix ...\"; Content: hello, when running the experiment, the error message **environment name can not start with the prefix ** was displayed. how can i set the name of the environment? i'm following the github tutorial and haven't found anything about it. ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117882725-01767d80-b281-11eb-8df5-36d8683523e7.png) code used: - registering dataset ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883192-81044c80-b281-11eb-9dec-d73431948061.png) ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883230-8c577800-b281-11eb-8445-060839369fe5.png) - training pipeline ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883313-a5602900-b281-11eb-818d-3972111d7f9c.png) ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883356-b315ae80-b281-11eb-99d9-1ac6c0989186.png) ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883429-c7f24200-b281-11eb-88de-3979570adb55.png) ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883465-d2144080-b281-11eb-8b9c-f74756bedd01.png) ![image](https:\/\/user-images.githubusercontent.com\/29444086\/117883535-e48e7a00-b281-11eb-9d31-e035f44a0871.png) references: - https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/tree\/master\/automated_ml\/02_automl_training_pipeline - https:\/\/github.com\/microsoftdocs\/azure-docs\/issues\/65770 best regards, cristina --- #### detalhes do documento *no edite esta seo. necessrio para a vinculao do problema do docs.microsoft.com github.* * id: 49399a7d-d4e8-370e-ce62-d60a6b64e412 * version independent id: 782d8ba4-75dd-27c3-5a46-a921c3ead4bf * content: [.contrib.automl.pipeline.steps.automlpipelinebuilder class - python](https:\/\/docs.microsoft.com\/pt-br\/python\/api\/-contrib-automl-pipeline-steps\/.contrib.automl.pipeline.steps.automlpipelinebuilder?view=azure-ml-py) * content source: [-docset\/stable\/docs-ref-autogen\/-contrib-automl-pipeline-steps\/.contrib.automl.pipeline.steps.automlpipelinebuilder.yml](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr.pt-br\/blob\/live\/-docset\/stable\/docs-ref-autogen\/-contrib-automl-pipeline-steps\/.contrib.automl.pipeline.steps.automlpipelinebuilder.yml) * service: **machine-learning** * sub-service: **core** * github login: @debfro * microsoft alias: **debfro**",
        "Issue_original_content_gpt_summary":"The user encountered an error message when running an experiment with the automlpipelinebuilder.get_many_models_train_steps function, stating that the environment name cannot start with a certain prefix.",
        "Issue_preprocessed_content":"Title: error environment name can not start with the prefix ; Content: hello, when running the experiment, the error message environment name can not start with the prefix was displayed. how can i set the name of the environment? i'm following the github tutorial and haven't found anything about it. code used registering dataset training pipeline references best regards, cristina detalhes do documento no edite esta seo. necessrio para a vinculao do problema do id version independent id content content source service core github login microsoft alias debfro"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1413",
        "Issue_title":"How to effectively use azureml.exceptions.WebserviceException for efficient REST API Error Management?",
        "Issue_creation_time":1617254943000,
        "Issue_closed_time":1617344140000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"In Azure Machine Learning Service, when we deploy a Model as an AKS Webservice Endpoint, how can we raise exceptions to let the end-user get proper feedback if their API call is unsuccessful? Azure mentions using `azureml.exceptions.WebserviceException` in their documentation. However, how do we use this class to raise exceptions in case the API call cannot be processed properly and the end-user is responsible for it?",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: how to effectively use .exceptions.webserviceexception for efficient rest api error management?; Content: in service, when we deploy a model as an aks webservice endpoint, how can we raise exceptions to let the end-user get proper feedback if their api call is unsuccessful? azure mentions using `.exceptions.webserviceexception` in their documentation. however, how do we use this class to raise exceptions in case the api call cannot be processed properly and the end-user is responsible for it?",
        "Issue_original_content_gpt_summary":"The user is looking for an efficient way to use the .exceptions.webserviceexception class to raise exceptions and provide proper feedback to the end-user in case their API call is unsuccessful.",
        "Issue_preprocessed_content":"Title: how to effectively use for efficient rest api error management?; Content: in service, when we deploy a model as an aks webservice endpoint, how can we raise exceptions to let the get proper feedback if their api call is unsuccessful? azure mentions using in their documentation. however, how do we use this class to raise exceptions in case the api call cannot be processed properly and the is responsible for it?"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1387",
        "Issue_title":"azure ml Python SDK 1.24.0 image build fails with the error failed to get layer was working fine before",
        "Issue_creation_time":1615361317000,
        "Issue_closed_time":1623755236000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"![image](https:\/\/user-images.githubusercontent.com\/74793968\/110592377-36daee00-81a0-11eb-8fb0-de7e2ba93af1.png)\r\n\r\nThis issue wasn't present until a few days ago. Issue shows up when we submit an experiment to azure ml workspace in the image build logs. We are using mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04 base image",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: python sdk 1.24.0 image build fails with the error failed to get layer was working fine before; Content: ![image](https:\/\/user-images.githubusercontent.com\/74793968\/110592377-36daee00-81a0-11eb-8fb0-de7e2ba93af1.png) this issue wasn't present until a few days ago. issue shows up when we submit an experiment to workspace in the image build logs. we are using mcr.microsoft.com\/\/base:intelmpi2018.3-ubuntu16.04 base image",
        "Issue_original_content_gpt_summary":"The user encountered an issue with their Python SDK 1.24.0 image build failing with an error when submitting an experiment to their workspace in the image build logs.",
        "Issue_preprocessed_content":"Title: python sdk image build fails with the error failed to get layer was working fine before; Content: this issue wasn't present until a few days ago. issue shows up when we submit an experiment to workspace in the image build logs. we are using base image"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1378",
        "Issue_title":"No instructions on where to submit bugs\/glitches related to the Python azureml-sdk",
        "Issue_creation_time":1615198296000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"\r\nAs, title says: there are no instructions on where to submit bugs\/glitches related to the Python azureml-sdk. \r\nIs it through github somewhere? Is it through an Azure support ticket? \r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: eb938463-51c2-43f3-d528-76a07a28bec8\r\n* Version Independent ID: e15753c0-6fe1-100a-0efc-08c1f845dc83\r\n* Content: [Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/index.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @trevorbye\r\n* Microsoft Alias: **trbye**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: no instructions on where to submit bugs\/glitches related to the python -sdk; Content: as, title says: there are no instructions on where to submit bugs\/glitches related to the python -sdk. is it through github somewhere? is it through an azure support ticket? --- #### document details *do not edit this section. it is required for docs.microsoft.com github issue linking.* * id: eb938463-51c2-43f3-d528-76a07a28bec8 * version independent id: e15753c0-6fe1-100a-0efc-08c1f845dc83 * content: [ sdk for python - python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/?view=azure-ml-py) * content source: [-docset\/docs-ref-conceptual\/index.md](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/docs-ref-conceptual\/index.md) * service: **machine-learning** * sub-service: **core** * github login: @trevorbye * microsoft alias: **trbye**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of not knowing where to submit bugs\/glitches related to the Python -SDK.",
        "Issue_preprocessed_content":"Title: no instructions on where to submit related to the python ; Content: as, title says there are no instructions on where to submit related to the python is it through github somewhere? is it through an azure support ticket? document details do not edit this section. it is required for github issue id version independent id content content source service core github login microsoft alias trbye"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1371",
        "Issue_title":"Azure ML Run docker command to pull public image failed ",
        "Issue_creation_time":1614273729000,
        "Issue_closed_time":1614604742000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"'m going through this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\r\n\r\nI need to start the training using the docker image from my local registry. I provided all required data in the environment I created:\r\n\r\nconda_env.docker.enabled = True\r\nconda_env.docker.base_image = \"tf_od_api:latest\"\r\nconda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\"\r\nconda_env.docker.base_image_registry.username = \"MyToken\"\r\nconda_env.docker.base_image_registry.password = \"MyPassword\"\r\n\r\nconda_env.python.user_managed_dependencies = True\r\n\r\nsrc = ScriptRunConfig(source_directory='azureml-examples\/workflows\/train\/fastai\/pets\/src',\r\n                      script='aml_wrapper.py',\r\n                      compute_target=attached_dsvm_compute,\r\n                      environment=conda_env)\r\nrun = exp.submit(config=src)\r\nrun.wait_for_completion(show_output=True)\r\n\r\nAnd when I start the pipeline I got: \"FailedPullingImage: Unable to pull docker image\\n\\timageName: Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"\r\n\r\nIf I set conda_env.python.user_managed_dependencies = False\r\n\r\nthen the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. But on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"Run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: run docker command to pull public image failed ; Content: 'm going through this notebook: https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb i need to start the training using the docker image from my local registry. i provided all required data in the environment i created: conda_env.docker.enabled = true conda_env.docker.base_image = \"tf_od_api:latest\" conda_env.docker.base_image_registry.address = \"mylocalacr.azurecr.io\" conda_env.docker.base_image_registry.username = \"mytoken\" conda_env.docker.base_image_registry.password = \"mypassword\" conda_env.python.user_managed_dependencies = true src = scriptrunconfig(source_directory='-examples\/workflows\/train\/fastai\/pets\/src', script='aml_wrapper.py', compute_target=attached_dsvm_compute, environment=conda_env) run = exp.submit(config=src) run.wait_for_completion(show_output=true) and when i start the pipeline i got: \"failedpullingimage: unable to pull docker image\\n\\timagename: run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\" if i set conda_env.python.user_managed_dependencies = false then the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. but on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error: \"run docker command to pull public image failed with error: error response from daemon: unauthorized: authentication required\"",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to pull a public image from their local registry for training a machine learning model, resulting in an \"unauthorized: authentication required\" error.",
        "Issue_preprocessed_content":"Title: run docker command to pull public image failed ; Content: 'm going through this notebook i need to start the training using the docker image from my local registry. i provided all required data in the environment i created true mytoken mypassword true src run and when i start the pipeline i got failedpullingimage unable to pull docker run docker command to pull public image failed with error error response from daemon unauthorized authentication required if i set false then the pipeline can pull my image from my local registry, build a new image with all required python dependencies on top of my base image and push the new image to my local registry. but on the second step of the pipeline, when it tries to pull the image for running it, that was just created and pushed, it again crashes with the same error run docker command to pull public image failed with error error response from daemon unauthorized authentication required"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1348",
        "Issue_title":"ERROR:  Learning: Build AI solutions with Azure Machine Learning - 06 - Work with Data - Upload data to a datastore",
        "Issue_creation_time":1613675010000,
        "Issue_closed_time":1613973498000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Upload data to a datastore**\r\n![AzureUpload](https:\/\/user-images.githubusercontent.com\/947785\/108407641-3e325b80-71e1-11eb-85df-58479ed8db52.png)\r\n\r\nNow that you have determined the available datastores, you can upload files from your local file system to a datastore so that it will be accessible to experiments running in the workspace, regardless of where the experiment script is actually being run.\r\n\r\n_default_ds.upload_files(files=['.\/data\/diabetes.csv', '.\/data\/diabetes2.csv'], # Upload the diabetes csv files in \/data\r\n                       target_path='diabetes-data\/', # Put it in a folder path in the datastore\r\n                       overwrite=True, # Replace existing files of the same name\r\n                       show_progress=True)_\r\n\r\nUploading an estimated of 2 files\r\nUploading .\/data\/diabetes.csv\r\nUploading .\/data\/diabetes2.csv\r\nUploaded 0 files\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 332, in handler\r\n    result = future.result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in <lambda>\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 463, in create_blob_from_path\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 582, in create_blob_from_stream\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 971, in _put_blob\r\n    return self._perform_request(request, _parse_base_properties)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 381, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 306, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 292, in _perform_request\r\n    HTTPError(response.status, response.message, response.headers, response.body))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\r\n    raise ex\r\nazure.common.AzureHttpError: This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\r\nRequestId:a9ffd72c-c01e-00d9-5220-064b2e000000\r\nTime:2021-02-18T18:02:54.3372191Z<\/Message><\/Error>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 994, in emit\r\n    msg = self.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 840, in format\r\n    return fmt.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 577, in format\r\n    record.message = record.getMessage()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/traitlets\/config\/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelapp.py\", line 612, in start\r\n    self.io_loop.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/platform\/asyncio.py\", line 199, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 814, in inner\r\n    self.ctx_run(self.run)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 775, in run\r\n    yielded = self.gen.send(value)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 362, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 265, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2867, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2895, in _run_cell\r\n    return runner(coro)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3072, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3263, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3343, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-0f28dc9194af>\", line 4, in <module>\r\n    show_progress=True)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in upload_files\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 321, in _start_upload_task\r\n    tq.add_task(async_task)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 55, in __exit__\r\n    self.flush(self.identity)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in flush\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in <genexpr>\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/async_task.py\", line 58, in wait\r\n    res = self._handler(self._future, self._logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 340, in handler\r\n    exception_handler(e, logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 304, in exception_handler\r\n    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\r\nMessage: 'Upload failed, please make sure target_path does not start with invalid characters.'\r\nArguments: (AzureHttpError('This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\\nRequestId:a9ffd72c-c01e-00d9-5220-064b2e000000\\nTime:2021-02-18T18:02:54.3372191Z<\/Message><\/Error>',),)\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 332, in handler\r\n    result = future.result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 425, in result\r\n    return self.__get_result()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/concurrent\/futures\/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in <lambda>\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 463, in create_blob_from_path\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 582, in create_blob_from_stream\r\n    timeout=timeout)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 971, in _put_blob\r\n    return self._perform_request(request, _parse_base_properties)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 381, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 306, in _perform_request\r\n    raise ex\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 292, in _perform_request\r\n    HTTPError(response.status, response.message, response.headers, response.body))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\r\n    raise ex\r\nazure.common.AzureHttpError: This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\r\nRequestId:5488fc90-001e-0080-5d20-064ea8000000\r\nTime:2021-02-18T18:02:54.3372332Z<\/Message><\/Error>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 994, in emit\r\n    msg = self.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 840, in format\r\n    return fmt.format(record)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 577, in format\r\n    record.message = record.getMessage()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/logging\/__init__.py\", line 338, in getMessage\r\n    msg = msg % self.args\r\nTypeError: not all arguments converted during string formatting\r\nCall stack:\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/traitlets\/config\/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelapp.py\", line 612, in start\r\n    self.io_loop.start()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/platform\/asyncio.py\", line 199, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/asyncio\/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 688, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/ioloop.py\", line 741, in _run_callback\r\n    ret = callback()\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 814, in inner\r\n    self.ctx_run(self.run)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 775, in run\r\n    yielded = self.gen.send(value)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 362, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 265, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/tornado\/gen.py\", line 234, in wrapper\r\n    yielded = ctx_run(next, result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/contextvars\/__init__.py\", line 38, in run\r\n    return callable(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2867, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 2895, in _run_cell\r\n    return runner(coro)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3072, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3263, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/IPython\/core\/interactiveshell.py\", line 3343, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-30-0f28dc9194af>\", line 4, in <module>\r\n    show_progress=True)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 787, in upload_files\r\n    lambda target, source: lambda: self.blob_service.create_blob_from_path(self.container_name, target, source)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 321, in _start_upload_task\r\n    tq.add_task(async_task)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 55, in __exit__\r\n    self.flush(self.identity)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in flush\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/task_queue.py\", line 118, in <genexpr>\r\n    self._results.extend((task.wait(awaiter_name=self.identity) for task in completed_tasks))\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_common\/async_utils\/async_task.py\", line 58, in wait\r\n    res = self._handler(self._future, self._logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 340, in handler\r\n    exception_handler(e, logger)\r\n  File \"\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/data\/azure_storage_datastore.py\", line 304, in exception_handler\r\n    logger.error(\"Upload failed, please make sure target_path does not start with invalid characters.\", e)\r\nMessage: 'Upload failed, please make sure target_path does not start with invalid characters.'\r\nArguments: (AzureHttpError('This request is not authorized to perform this operation using this permission. ErrorCode: AuthorizationPermissionMismatch\\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthorizationPermissionMismatch<\/Code><Message>This request is not authorized to perform this operation using this permission.\\nRequestId:5488fc90-001e-0080-5d20-064ea8000000\\nTime:2021-02-18T18:02:54.3372332Z<\/Message><\/Error>',),)\r\n$AZUREML_DATAREFERENCE_010e49b94ea645928f99f4a15d7b3a00\r\nfrom azurem",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: error: learning: build ai solutions with - 06 - work with data - upload data to a datastore; Content: **upload data to a datastore** ![azureupload](https:\/\/user-images.githubusercontent.com\/947785\/108407641-3e325b80-71e1-11eb-85df-58479ed8db52.png) now that you have the available datastores, you can upload files from your local file system to a datastore so that it will be accessible to experiments running in the workspace, regardless of where the experiment script is actually being run. _default_ds.upload_files(files=['.\/data\/diabetes.csv', '.\/data\/diabetes2.csv'], # upload the diabetes csv files in \/data target_path='diabetes-data\/', # put it in a folder path in the datastore overwrite=true, # replace existing files of the same name show_progress=true)_ uploading an estimated of 2 files uploading .\/data\/diabetes.csv uploading .\/data\/diabetes2.csv uploaded 0 files --- logging error --- httperror(response.status, response.message, response.headers, response.body)) file \"\/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler raise ex azure.common.azurehttperror: this request is not authorized to perform this operation using this permission. errorcode: authorizationpermissionmismatch authorizationpermissionmismatchthis request is not authorized to perform this operation using this permission. requestid:a9ffd72c-c01e-00d9-5220-064b2e000000 time:2021-02-18t18:02:54.3372191z",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to upload data to a datastore, resulting in an \"authorization permission mismatch\" error.",
        "Issue_preprocessed_content":"Title: error learning build ai solutions with work with data upload data to a datastore; Content: upload data to a datastore now that you have the available datastores, you can upload files from your local file system to a datastore so that it will be accessible to experiments running in the workspace, regardless of where the experiment script is actually being run. upload the diabetes csv files in put it in a folder path in the datastore overwrite true, replace existing files of the same name uploading an estimated of files uploading uploading uploaded files logging error traceback file line , in handler result file line , in result return file line , in raise file line , in run result file line , in lambda target, source lambda target, source file line , in timeout timeout file line , in timeout timeout file line , in return file line , in raise ex file line , in raise ex file line , in file line , in raise ex this request is not authorized to perform this operation using this permission. errorcode authorizationpermissionmismatch ?xml request is not authorized to perform this operation using this permission. during handling of the above exception, another exception occurred traceback file line , in emit msg file line , in format return file line , in format file line , in getmessage msg msg % typeerror not all arguments converted during string formatting call stack file line , in file line , in exec file line , in file line , in file line , in start file line , in start file line , in file line , in file line , in file line , in lambda f future file line , in ret callback file line , in inner file line , in run return callable file line , in run yielded file line , in yield file line , in wrapper yielded result file line , in run return callable file line , in yield idents, msg file line , in wrapper yielded result file line , in run return callable file line , in file line , in wrapper yielded result file line , in run return callable file line , in res silent silent file line , in return super kwargs file line , in silent, file line , in return runner file line , in file line , in interactivity interactivity, compiler compiler, result result file line , in if file line , in file line , in file line , in lambda target, source lambda target, source file line , in file line , in file line , in flush for task in file line , in for task in file line , in wait res file line , in handler logger file line , in failed, please make sure does not start with invalid e message 'upload failed, please make sure does not start with invalid arguments , logging error traceback file line , in handler result file line , in result return file line , in raise file line , in run result file line , in lambda target, source lambda target, source file line , in timeout timeout file line , in timeout timeout file line , in return file line , in raise ex file line , in raise ex file line , in file line , in raise ex this request is not authorized to perform this operation using this permission. errorcode authorizationpermissionmismatch ?xml request is not authorized to perform this operation using this permission. during handling of the above exception, another exception occurred traceback file line , in emit msg file line , in format return file line , in format file line , in getmessage msg msg % typeerror not all arguments converted during string formatting call stack file line , in file line , in exec file line , in file line , in file line , in start file line , in start file line , in file line , in file line , in file line , in lambda f future file line , in ret callback file line , in inner file line , in run return callable file line , in run yielded file line , in yield file line , in wrapper yielded result file line , in run return callable file line , in yield idents, msg file line , in wrapper yielded result file line , in run return callable file line , in file line , in wrapper yielded result file line , in run return callable file line , in res silent silent file line , in return super kwargs file line , in silent, file line , in return runner file line , in file line , in interactivity interactivity, compiler compiler, result result file line , in if file line , in file line , in file line , in lambda target, source lambda target, source file line , in file line , in file line , in flush for task in file line , in for task in file line , in wait res file line , in handler logger file line , in failed, please make sure does not start with invalid e message 'upload failed, please make sure does not start with invalid arguments , from azurem"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1341",
        "Issue_title":"azureml-defaults not described ",
        "Issue_creation_time":1613523098000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"\r\n[Enter feedback here]\r\n\r\nWe need details description of `azureml-defaults`. \r\n\r\nWe need this when deployment. In training, we usually use `azureml-core`. In deployment, `azureml-defaults` is necessary (only `azureml-core` is not enough to deploy). I heard `azureml-defaults` includes `azureml-core`. But it is not documented.\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: -defaults not described ; Content: [enter feedback here] we need details description of `-defaults`. we need this when deployment. in training, we usually use `-core`. in deployment, `-defaults` is necessary (only `-core` is not enough to deploy). i heard `-defaults` includes `-core`. but it is not documented. --- #### document details *do not edit this section. it is required for docs.microsoft.com github issue linking.* * id: 8e0e12a4-b363-2726-06b4-9db2015efb32 * version independent id: e39a91ac-375b-a2cc-350d-a82cb7b0b035 * [install the sdk for python - python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py) * content source: [-docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/docs-ref-conceptual\/install.md) * service: **machine-learning** * sub-service: **core** * github login: @harneetvirk * microsoft alias: **harnvir**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the lack of documentation regarding the `-defaults` option when deploying the Azure Machine Learning SDK for Python, which is necessary for deployment but not included in the training process.",
        "Issue_preprocessed_content":"Title: not described ; Content: enter feedback here we need details description of . we need this when deployment. in training, we usually use . in deployment, is necessary . i heard includes . but it is not documented. document details do not edit this section. it is required for github issue id version independent id content content source service core github login microsoft alias harnvir"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1316",
        "Issue_title":"Local execution is not supported for Azure ML pipelines. ValueError: Please specify a remote compute_target. ",
        "Issue_creation_time":1612300739000,
        "Issue_closed_time":1620257629000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"\r\nWhen I try to run a pipeline with target as \"local\" it gives me an error. \r\nValueError: Please specify a remote compute_target. \r\nThis should be mentioned somewhere in the end of the page under target section. \r\nAlso please specify why pipelines cannot be run on local target? People like me waste a lot of time trying this & then realize its a shortcoming in the Azure ML Python SDK. \r\nPlease update this documentation page as soon as possible.\r\n![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png)\r\n\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: f2c8e18c-8443-67fe-b1f9-531de3599c8f\r\n* Version Independent ID: a8c897b7-c44b-1a72-52f2-f81bbdbce753\r\n* Content: [azureml.core.runconfig.RunConfiguration class - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.runconfiguration?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/stable\/docs-ref-autogen\/azureml-core\/azureml.core.runconfig.RunConfiguration.yml)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @DebFro\r\n* Microsoft Alias: **debfro**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: local execution is not supported for pipelines. valueerror: please specify a remote compute_target. ; Content: when i try to run a pipeline with target as \"local\" it gives me an error. valueerror: please specify a remote compute_target. this should be mentioned somewhere in the end of the page under target section. also please specify why pipelines cannot be run on local target? people like me waste a lot of time trying this & then realize its a shortcoming in the python sdk. please update this documentation page as soon as possible. ![image](https:\/\/user-images.githubusercontent.com\/17008122\/106663751-73fe0000-65a4-11eb-87f7-fcc7613dd42f.png) --- #### document details *do not edit this section. it is required for docs.microsoft.com github issue linking.* * id: f2c8e18c-8443-67fe-b1f9-531de3599c8f * version independent id: a8c897b7-c44b-1a72-52f2-f81bbdbce753 * [.core.runconfig.runconfiguration class - python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.runconfig.runconfiguration?view=azure-ml-py) * content source: [-docset\/stable\/docs-ref-autogen\/-core\/.core.runconfig.runconfiguration.yml](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/stable\/docs-ref-autogen\/-core\/.core.runconfig.runconfiguration.yml) * service: **machine-learning** * sub-service: **core** * github login: @debfro * microsoft alias: **debfro**",
        "Issue_original_content_gpt_summary":"The user encountered an issue when attempting to run a pipeline with a local target, resulting in a ValueError and a lack of documentation on why pipelines cannot be run on local targets.",
        "Issue_preprocessed_content":"Title: local execution is not supported for pipelines. valueerror please specify a remote ; Content: when i try to run a pipeline with target as local it gives me an error. valueerror please specify a remote this should be mentioned somewhere in the end of the page under target section. also please specify why pipelines cannot be run on local target? people like me waste a lot of time trying this & then realize its a shortcoming in the python sdk. please update this documentation page as soon as possible. document details do not edit this section. it is required for github issue id version independent id content content source service core github login microsoft alias debfro"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1299",
        "Issue_title":"AzureML TabularDataSet via parquet and pandas index error",
        "Issue_creation_time":1611344395000,
        "Issue_closed_time":null,
        "Issue_upvote_count":4,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"Azure's TabularDataset implementation introduces an index, \\_\\_index\\_level_0\\_\\_ when creating or reading parquet files that were originally written by Pandas\/Python.  This occurs when an index is unnamed but has been modified at some point; if an index is named we get an extra column with the same name as the index.\r\n\r\nWhen making changes to datasets, this additional field causes Azure errors if not handled.  Depending on what's been done to the index of the original dataset, you may or may not get that additional field.\r\n\r\nI have an example notebook that can be run to reproduce the issue.  It's here: https:\/\/github.com\/vla6\/Azure_notes\/blob\/main\/tabulardataset_parquet_index_di_issue.ipynb\r\n\r\nThe notebook requires an Azure Machine Learning workspace and a storage account to run",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: tabulardataset via parquet and pandas index error; Content: azure's tabulardataset implementation introduces an index, \\_\\_index\\_level_0\\_\\_ when creating or reading parquet files that were originally written by pandas\/python. this occurs when an index is unnamed but has been modified at some point; if an index is named we get an extra column with the same name as the index. when making changes to datasets, this additional field causes azure errors if not handled. depending on what's been done to the index of the original dataset, you may or may not get that additional field. i have an example notebook that can be run to reproduce the issue. it's here: https:\/\/github.com\/vla6\/azure_notes\/blob\/main\/tabulardataset_parquet_index_di_issue.ipynb the notebook requires an workspace and a storage account to run",
        "Issue_original_content_gpt_summary":"The user encountered an issue with Azure's tabulardataset implementation introducing an index, \\_\\_index\\_level_0\\_\\_ when creating or reading parquet files that were originally written by pandas\/python.",
        "Issue_preprocessed_content":"Title: tabulardataset via parquet and pandas index error; Content: azure's tabulardataset implementation introduces an index, when creating or reading parquet files that were originally written by this occurs when an index is unnamed but has been modified at some point; if an index is named we get an extra column with the same name as the index. when making changes to datasets, this additional field causes azure errors if not handled. depending on what's been done to the index of the original dataset, you may or may not get that additional field. i have an example notebook that can be run to reproduce the issue. it's here the notebook requires an workspace and a storage account to run"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1417",
        "Issue_title":"python package azureml-contrib-pipeline-steps 1.20.0 not working ",
        "Issue_creation_time":1611092820000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"## Describe the issue\r\n\r\nversion 1.20.0 of python package azureml-contrib-pipeline-steps throws (works fine on version 1.19 or 1.18)\r\n\r\n File \"C:\/Users\/v-songshanli\/projects\/ashexplore\/object_identification\/obj_segmentation_azure_2_steps.py\", line 88, in run\r\n    pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\core\\_experiment_method.py\", line 97, in wrapper\r\n    return init_func(self, *args, **kwargs)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\pipeline.py\", line 177, in __init__\r\n    self._graph = self._graph_builder.build(self._name, steps, finalize=False)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1481, in build\r\n    graph = self.construct(name, steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1503, in construct\r\n    self.process_collection(steps)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1539, in process_collection\r\n    builder.process_collection(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1830, in process_collection\r\n    self._base_builder.process_collection(item)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1533, in process_collection\r\n    return self.process_step(collection)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\builder.py\", line 1577, in process_step\r\n    node = step.create_node(self._graph, self._default_datastore, self._context)\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\steps\\python_script_step.py\", line 243, in create_node\r\n    return super(PythonScriptStep, self).create_node(\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 140, in create_node\r\n    self._set_compute_params_to_node(node,\r\n  File \"C:\\Users\\v-songshanli\\Anaconda3\\envs\\pytouchEnv\\lib\\site-packages\\azureml\\pipeline\\core\\_python_script_step_base.py\", line 229, in _set_compute_params_to_node\r\n    self._module_param_provider.set_params_to_node(\r\nTypeError: _set_params_to_node_hook() got an unexpected keyword argument 'command'\r\n\r\n## Minimal example\r\n\r\n```python\r\nfrom azureml.core import Workspace\r\n\r\nws = Workspace.from_config()\r\n\r\n\r\nsplit_step = PythonScriptStep(\r\n        name=\"Train Test Split\",\r\n        script_name=\"obj_segment_step_data_process.py\",\r\n        arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(),\r\n                   \"--train-split\", train_split_data, \"--test-split\", test_split_data,\r\n                   \"--test-size\", 50],\r\n        compute_target=compute_target,\r\n        runconfig=aml_run_config,\r\n        source_directory=source_directory,\r\n        allow_reuse=False\r\n    )\r\n\r\npipeline_steps = [split_step ]\r\n\r\npipeline = Pipeline(workspace=ws, steps=pipeline_steps)\r\n```\r\n\r\n## Additional context\r\nI am using aml sdk 1.20. no type errors with version 1.19\/1.18 of azureml-contrib-pipeline-steps.\r\n-\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: python package -contrib-pipeline-steps 1.20.0 not working ; Content: ## describe the issue version 1.20.0 of python package -contrib-pipeline-steps throws (works fine on version 1.19 or 1.18) file \"c:\/users\/v-songshanli\/projects\/ashexplore\/object_identification\/obj_segmentation_azure_2_steps.py\", line 88, in run pipeline = pipeline(workspace=ws, steps=pipeline_steps) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\core\\_experiment_method.py\", line 97, in wrapper return init_func(self, *args, **kwargs) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\pipeline.py\", line 177, in __init__ self._graph = self._graph_builder.build(self._name, steps, finalize=false) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1481, in build graph = self.construct(name, steps) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1503, in construct self.process_collection(steps) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1539, in process_collection builder.process_collection(collection) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1830, in process_collection self._base_builder.process_collection(item) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1533, in process_collection return self.process_step(collection) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\builder.py\", line 1577, in process_step node = step.create_node(self._graph, self._default_datastore, self._context) file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\steps\\python_script_step.py\", line 243, in create_node return super(pythonscriptstep, self).create_node( file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\_python_script_step_base.py\", line 140, in create_node self._set_compute_params_to_node(node, file \"c:\\users\\v-songshanli\\anaconda3\\envs\\pytouchenv\\lib\\site-packages\\\\pipeline\\core\\_python_script_step_base.py\", line 229, in _set_compute_params_to_node self._module_param_provider.set_params_to_node( typeerror: _set_params_to_node_hook() got an unexpected keyword argument 'command' ## minimal example ```python from .core import workspace ws = workspace.from_config() split_step = pythonscriptstep( name=\"train test split\", script_name=\"obj_segment_step_data_process.py\", arguments=[\"--data-path\", dataset.as_named_input('pennfudan_data').as_mount(), \"--train-split\", train_split_data, \"--test-split\", test_split_data, \"--test-size\", 50], compute_target=compute_target, runconfig=aml_run_config, source_directory=source_directory, allow_reuse=false ) pipeline_steps = [split_step ] pipeline = pipeline(workspace=ws, steps=pipeline_steps) ``` ## additional context i am using aml sdk 1.20. no type errors with version 1.19\/1.18 of -contrib-pipeline-steps. -",
        "Issue_original_content_gpt_summary":"The user encountered an issue with version 1.20.0 of python package -contrib-pipeline-steps, which threw a typeerror when running a pipeline, while versions 1.19 and 1.18 of the package worked fine.",
        "Issue_preprocessed_content":"Title: python package not working ; Content: describe the issue version of python package throws file line , in run pipeline pipeline file line , in wrapper return args, kwargs file line , in steps, finalize false file line , in build graph steps file line , in construct file line , in file line , in file line , in return file line , in node file line , in return super file line , in file line , in typeerror got an unexpected keyword argument 'command' minimal example additional context i am using aml sdk no type errors with version of"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1285",
        "Issue_title":"Error Installing Azureml. (Python 3.9 support)",
        "Issue_creation_time":1609957275000,
        "Issue_closed_time":1637097588000,
        "Issue_upvote_count":3,
        "Issue_downvote_count":0,
        "Issue_comment_count":15.0,
        "Issue_body":"This guidance results in an error:\r\n\r\n\"To install the default packages in an environment without a previous version of the package installed, run the following command.\" \r\n\r\nPS C:\\> pip install azureml-sdk\r\n\r\n`ERROR: Could not find a version that satisfies the requirement azureml-sdk (from versions: none)\r\nERROR: No matching distribution found for azureml-sdk`\r\n\r\nWhat am I missing?\r\n\r\nThanks,\r\nclaw\r\n---\r\n#### Document Details\r\n\r\n\u26a0 *Do not edit this section. It is required for docs.microsoft.com \u279f GitHub issue linking.*\r\n\r\n* ID: 8e0e12a4-b363-2726-06b4-9db2015efb32\r\n* Version Independent ID: e39a91ac-375b-a2cc-350d-a82cb7b0b035\r\n* Content: [Install the Azure Machine Learning SDK for Python - Azure Machine Learning Python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py)\r\n* Content Source: [AzureML-Docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/MicrosoftDocs\/MachineLearning-Python-pr\/blob\/live\/AzureML-Docset\/docs-ref-conceptual\/install.md)\r\n* Service: **machine-learning**\r\n* Sub-service: **core**\r\n* GitHub Login: @harneetvirk\r\n* Microsoft Alias: **harnvir**",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: error installing . (python 3.9 support); Content: this guidance results in an error: \"to install the default packages in an environment without a previous version of the package installed, run the following command.\" ps c:\\> pip install -sdk `error: could not find a version that satisfies the requirement -sdk (from versions: none) error: no matching distribution found for -sdk` what am i missing? thanks, claw --- #### document details *do not edit this section. it is required for docs.microsoft.com github issue linking.* * id: 8e0e12a4-b363-2726-06b4-9db2015efb32 * version independent id: e39a91ac-375b-a2cc-350d-a82cb7b0b035 * content: [install the sdk for python - python](https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/install?view=azure-ml-py) * content source: [-docset\/docs-ref-conceptual\/install.md](https:\/\/github.com\/microsoftdocs\/machinelearning-python-pr\/blob\/live\/-docset\/docs-ref-conceptual\/install.md) * service: **machine-learning** * sub-service: **core** * github login: @harneetvirk * microsoft alias: **harnvir**",
        "Issue_original_content_gpt_summary":"The user encountered an error while attempting to install the SDK for Python 3.9, resulting in a \"no matching distribution found\" error.",
        "Issue_preprocessed_content":"Title: error installing . ; Content: this guidance results in an error to install the default packages in an environment without a previous version of the package installed, run the following ps pip install what am i missing? thanks, claw document details do not edit this section. it is required for github issue id version independent id content content source service core github login microsoft alias harnvir"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1111",
        "Issue_title":"error: azureml-train-automl-runtime is required however it is included",
        "Issue_creation_time":1598046974000,
        "Issue_closed_time":1598387113000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"```Azure ML SDK Version:  1.11.0```\r\n\r\nIn a ```PythonScriptStep``` I'm getting a crash error that: \"\r\n```\r\nazureml-train-automl-runtime must be installed in the current environment to run local in process runs. Please install this dependency or provide a RunConfiguration.\r\n```\r\n\r\nHere is my RunConfiguration:\r\n```\r\ncompute_target = ComputeTarget(workspace=f.ws, name=compute_name)\r\n\r\ncd = CondaDependencies.create(\r\n    pip_packages=[\"pandas\", \"numpy\",\r\n                  \"azureml-defaults\", \"azureml-sdk[explain,automl]\", \"azureml-train-automl-runtime\"],\r\n    conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"])\r\namlcompute_run_config = RunConfiguration(conda_dependencies=cd)\r\namlcompute_run_config.environment.docker.enabled = True\r\n```\r\n\r\nhere is the step:\r\n```\r\nadd_vendor_sets = PythonScriptStep(\r\n    name='Add Vendor set',\r\n    script_name='add_vendor_set.py',\r\n    arguments=['--respondent_dir', level_respondent,\r\n                '--my_dir', my_raw,\r\n                '--output_dir', factset_processed],\r\n    compute_target=compute_target,\r\n    inputs=[level_respondent, my_raw],\r\n    outputs=[my_processed],\r\n    runconfig=amlcompute_run_config,\r\n    source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'),\r\n    allow_reuse=True\r\n)\r\n```\r\n\r\nThe environment is obviously included, but also definitely missing.  I'm stuck and now none of my pipelines, that were running in previous version, will work. \r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: error: -train-automl-runtime is required however it is included; Content: ``` sdk version: 1.11.0``` in a ```pythonscriptstep``` i'm getting a crash error that: \" ``` -train-automl-runtime must be installed in the current environment to run local in process runs. please install this dependency or provide a runconfiguration. ``` here is my runconfiguration: ``` compute_target = computetarget(workspace=f.ws, name=compute_name) cd = condadependencies.create( pip_packages=[\"pandas\", \"numpy\", \"-defaults\", \"-sdk[explain,automl]\", \"-train-automl-runtime\"], conda_packages=[\"xlrd\", \"scikit-learn\", \"numpy\", \"pyyaml\", \"pip\"]) amlcompute_run_config = runconfiguration(conda_dependencies=cd) amlcompute_run_config.environment.docker.enabled = true ``` here is the step: ``` add_vendor_sets = pythonscriptstep( name='add vendor set', script_name='add_vendor_set.py', arguments=['--respondent_dir', level_respondent, '--my_dir', my_raw, '--output_dir', factset_processed], compute_target=compute_target, inputs=[level_respondent, my_raw], outputs=[my_processed], runconfig=amlcompute_run_config, source_directory=os.path.join(os.getcwd(), 'pipes\/add_vendor_set'), allow_reuse=true ) ``` the environment is obviously included, but also definitely missing. i'm stuck and now none of my pipelines, that were running in previous version, will work.",
        "Issue_original_content_gpt_summary":"The user is encountering an error that \"-train-automl-runtime must be installed in the current environment to run local in process runs\" despite the environment being included in the run configuration.",
        "Issue_preprocessed_content":"Title: error is required however it is included; Content: in a i'm getting a crash error that here is my runconfiguration here is the step the environment is obviously included, but also definitely missing. i'm stuck and now none of my pipelines, that were running in previous version, will work."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/944",
        "Issue_title":"BUG: Versioning not enabled when pulling data from SQL DB\/DW into Azure ML datasets",
        "Issue_creation_time":1587712154000,
        "Issue_closed_time":1599067481000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"\r\nWhenever I pull the data from an azure SQL DB or DW, the version history is not maintained. Everytime I pull a new data, the first version is only refreshing.\r\nI have created a reproducible example to explain my issue. \r\n\r\nhttps:\/\/github.com\/swaticolab\/MachineLearningNotebooks\/blob\/SQL_to_ML\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/Connect_SQL_to_ML_dataset.ipynb",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: bug: versioning not enabled when pulling data from sql db\/dw into datasets; Content: whenever i pull the data from an azure sql db or dw, the version history is not maintained. everytime i pull a new data, the first version is only refreshing. i have created a reproducible example to explain my issue. https:\/\/github.com\/swaticolab\/machinelearningnotebooks\/blob\/sql_to_ml\/how-to-use-\/machine-learning-pipelines\/intro-to-pipelines\/connect_sql_to_ml_dataset.ipynb",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge where versioning is not enabled when pulling data from an Azure SQL database or data warehouse into datasets.",
        "Issue_preprocessed_content":"Title: bug versioning not enabled when pulling data from sql into datasets; Content: whenever i pull the data from an azure sql db or dw, the version history is not maintained. everytime i pull a new data, the first version is only refreshing. i have created a reproducible example to explain my issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/841",
        "Issue_title":"Internal server error when deploying from Azure ML to AKS",
        "Issue_creation_time":1583404471000,
        "Issue_closed_time":1583847372000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"The team uses Azure ML CLI to deploy a container to AKS (az ml model deploy). Now and then (not always), they get an internal server error, see stack trace. They could not detect a clear pattern when this error occurs. Although it would be possible to create a retry loop in their Azure DevOps pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue.\r\n\r\n```\r\n2020-02-14T11:11:07.1739375Z ERROR: {'Azure-cli-ml Version': '1.0.85', 'Error': WebserviceException:\r\n\r\n2020-02-14T11:11:07.1739694Z \tMessage: Received bad response from Model Management Service:\r\n\r\n2020-02-14T11:11:07.1739785Z Response Code: 500\r\n\r\n2020-02-14T11:11:07.1740533Z Headers: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\r\n\r\n2020-02-14T11:11:07.1741400Z Content: b'{\"code\":\"InternalServerError\",\"statusCode\":500,\"message\":\"An internal server error occurred. Please try again. If the problem persists, contact support\"}'\r\n\r\n2020-02-14T11:11:07.1741516Z \tInnerException None\r\n\r\n2020-02-14T11:11:07.1741641Z \tErrorResponse \r\n\r\n2020-02-14T11:11:07.1741708Z {\r\n\r\n2020-02-14T11:11:07.1741813Z     \"error\": {\r\n\r\n2020-02-14T11:11:07.1742819Z         \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 500\\nHeaders: {'Date': 'Fri, 14 Feb 2020 11:11:07 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"InternalServerError\\\",\\\"statusCode\\\":500,\\\"message\\\":\\\"An internal server error occurred. Please try again. If the problem persists, contact support\\\"}'\"\r\n\r\n2020-02-14T11:11:07.1743119Z     }\r\n\r\n2020-02-14T11:11:07.1743227Z }}\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: internal server error when deploying from to aks; Content: the team uses cli to deploy a container to aks (az ml model deploy). now and then (not always), they get an internal server error, see stack trace. they could not detect a clear pattern when this error occurs. although it would be possible to create a retry loop in their azure devops pipeline when this error occurs (as the error message also tells), this would not resolve the underlying issue. ``` 2020-02-14t11:11:07.1739375z error: {'azure-cli-ml version': '1.0.85', 'error': webserviceexception: 2020-02-14t11:11:07.1739694z message: received bad response from model management service: 2020-02-14t11:11:07.1739785z response code: 500 2020-02-14t11:11:07.1740533z headers: {'date': 'fri, 14 feb 2020 11:11:07 gmt', 'content-type': 'application\/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'request-context': 'appid=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'strict-transport-security': 'max-age=15724800; includesubdomains; preload'} 2020-02-14t11:11:07.1741400z content: b'{\"code\":\"internalservererror\",\"statuscode\":500,\"message\":\"an internal server error occurred. please try again. if the problem persists, contact support\"}' 2020-02-14t11:11:07.1741516z innerexception none 2020-02-14t11:11:07.1741641z errorresponse 2020-02-14t11:11:07.1741708z { 2020-02-14t11:11:07.1741813z \"error\": { 2020-02-14t11:11:07.1742819z \"message\": \"received bad response from model management service:\\nresponse code: 500\\nheaders: {'date': 'fri, 14 feb 2020 11:11:07 gmt', 'content-type': 'application\/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'request-context': 'appid=cid-v1:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'strict-transport-security': 'max-age=15724800; includesubdomains; preload'}\\ncontent: b'{\\\"code\\\":\\\"internalservererror\\\",\\\"statuscode\\\":500,\\\"message\\\":\\\"an internal server error occurred. please try again. if the problem persists, contact support\\\"}'\" 2020-02-14t11:11:07.1743119z } 2020-02-14t11:11:07.1743227z }} ```",
        "Issue_original_content_gpt_summary":"The user is encountering an internal server error when deploying a container to aks (az ml model deploy) using the cli, with no clear pattern as to when the error occurs.",
        "Issue_preprocessed_content":"Title: internal server error when deploying from to aks; Content: the team uses cli to deploy a container to aks . now and then , they get an internal server error, see stack trace. they could not detect a clear pattern when this error occurs. although it would be possible to create a retry loop in their azure devops pipeline when this error occurs , this would not resolve the underlying issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/787",
        "Issue_title":"Import Error - from azureml.core import Dataset  - ImportError: cannot import name 'Dataset'",
        "Issue_creation_time":1581438052000,
        "Issue_closed_time":1581440044000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"The following sample notebook fails \r\n### img-classification-part1-training.ipynb\r\n\r\nwhen running:\r\n\r\n### from azureml.core import Dataset\r\n\r\nfrom azureml.core import Dataset\r\nfrom azureml.opendatasets import MNIST\r\n\r\ndata_folder = os.path.join(os.getcwd(), 'data')\r\nos.makedirs(data_folder, exist_ok=True)\r\n\r\nmnist_file_dataset = MNIST.get_file_dataset()\r\nmnist_file_dataset.download(data_folder, overwrite=True)\r\n\r\nmnist_file_dataset = mnist_file_dataset.register(workspace=ws,\r\n                                                 name='mnist_opendataset',\r\n                                                 description='training and test dataset',\r\n                                                 create_new_version=True)\r\n\r\n\r\n**Here is the error**\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-ac2e91b46eec> in <module>\r\n----> 1 from azureml.core import Dataset\r\n      2 from azureml.opendatasets import MNIST\r\n      3 \r\n      4 data_folder = os.path.join(os.getcwd(), 'data')\r\n      5 os.makedirs(data_folder, exist_ok=True)\r\n\r\nImportError: cannot import name 'Dataset'\r\n\r\nreference: yml file:\r\nname: img-classification-part1-training\r\ndependencies:\r\n- pip:\r\n  - azureml-sdk\r\n  - azureml-widgets\r\n  - matplotlib\r\n  - sklearn\r\n  - pandas\r\n  - azureml-opendatasets\r\n\r\nAzure ML SDK Version:  1.0.17\r\nPython 3.6 - AzureML\r\n\r\n@microsoft\r\nPlease kindly investigate.\r\nMany thanks :)",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: import error - from .core import dataset - importerror: cannot import name 'dataset'; Content: the following sample notebook fails ### img-classification-part1-training.ipynb when running: ### from .core import dataset from .core import dataset from .opendatasets import mnist data_folder = os.path.join(os.getcwd(), 'data') os.makedirs(data_folder, exist_ok=true) mnist_file_dataset = mnist.get_file_dataset() mnist_file_dataset.download(data_folder, overwrite=true) mnist_file_dataset = mnist_file_dataset.register(workspace=ws, name='mnist_opendataset', description='training and test dataset', create_new_version=true) **here is the error** --------------------------------------------------------------------------- importerror traceback (most recent call last) in ----> 1 from .core import dataset 2 from .opendatasets import mnist 3 4 data_folder = os.path.join(os.getcwd(), 'data') 5 os.makedirs(data_folder, exist_ok=true) importerror: cannot import name 'dataset' reference: yml file: name: img-classification-part1-training dependencies: - pip: - -sdk - -widgets - matplotlib - sklearn - pandas - -opendatasets sdk version: 1.0.17 python 3.6 - @microsoft please kindly investigate. many thanks :)",
        "Issue_original_content_gpt_summary":"The user encountered an import error when running a sample notebook, which prevented them from importing the 'dataset' module from the 'core' package.",
        "Issue_preprocessed_content":"Title: import error from import dataset importerror cannot import name 'dataset'; Content: the following sample notebook fails when running from import dataset from import dataset from import mnist 'data' overwrite true description 'training and test dataset', here is the error importerror traceback in from import dataset from import mnist 'data' importerror cannot import name 'dataset' reference yml file name dependencies pip matplotlib sklearn pandas sdk version python please kindly investigate. many thanks"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/767",
        "Issue_title":"Azure Machine Learning error: Can not use 'XGBoostClassifier'",
        "Issue_creation_time":1581000215000,
        "Issue_closed_time":1583863460000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"I got this error with Azure Machine Learning. \r\n\r\nConfigException: ConfigException:\r\n\tMessage: blacklisted and whitelisted models are exactly the same. Found: {'XGBoostClassifier'}.Please remove models from the blacklist or add models to the whitelist.\r\n\r\nThe settings are as follow. 'XGBoostClassifier' is in the whitelist; and backlist is None. Would you please help with the error?\r\n\r\nautoml_settings = {\r\n    \"iteration_timeout_minutes\": 2,\r\n    \"experiment_timeout_minutes\": 20,\r\n    \"enable_early_stopping\": True,\r\n    \"primary_metric\": 'accuracy',\r\n    \"featurization\": 'auto',\r\n    \"verbosity\": logging.INFO,\r\n    \"n_cross_validations\": 5\r\n}\r\n\r\nfrom azureml.train.automl import AutoMLConfig\r\n\r\nautoml_config = AutoMLConfig(task='classification',\r\n                             enable_tf = True,\r\n                             debug_log='automated_ml_errors.log',\r\n                             X=x_train.values,\r\n                             y=y_train.values.flatten(),\r\n                             blacklist_models = None,\r\n                             whitelist_models = ['XGBoostClassifier'],\r\n                             **automl_settings)\r\n\r\n(Note: XGBoostClassifier was installed in the notebook)",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: error: can not use 'xgboostclassifier'; Content: i got this error with . configexception: configexception: message: blacklisted and whitelisted models are exactly the same. found: {'xgboostclassifier'}.please remove models from the blacklist or add models to the whitelist. the settings are as follow. 'xgboostclassifier' is in the whitelist; and backlist is none. would you please help with the error? automl_settings = { \"iteration_timeout_minutes\": 2, \"experiment_timeout_minutes\": 20, \"enable_early_stopping\": true, \"primary_metric\": 'accuracy', \"featurization\": 'auto', \"verbosity\": logging.info, \"n_cross_validations\": 5 } from .train.automl import automlconfig automl_config = automlconfig(task='classification', enable_tf = true, debug_log='automated_ml_errors.log', x=x_train.values, y=y_train.values.flatten(), blacklist_models = none, whitelist_models = ['xgboostclassifier'], **automl_settings) (note: xgboostclassifier was installed in the notebook)",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to use an XGBoostClassifier model in an automated machine learning configuration.",
        "Issue_preprocessed_content":"Title: error can not use 'xgboostclassifier'; Content: i got this error with . configexception configexception message blacklisted and whitelisted models are exactly the same. found .please remove models from the blacklist or add models to the whitelist. the settings are as follow. 'xgboostclassifier' is in the whitelist; and backlist is none. would you please help with the error? from import automlconfig automlconfig , none, , note xgboostclassifier was installed in the notebook"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/735",
        "Issue_title":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Issue_creation_time":1578967122000,
        "Issue_closed_time":1582730951000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":14.0,
        "Issue_body":"ImportError: cannot import name 'AutoMLStep' from 'azureml.train.automl",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: importerror: cannot import name 'automlstep' from '.train.automl; Content: importerror: cannot import name 'automlstep' from '.train.automl",
        "Issue_original_content_gpt_summary":"The user encountered an ImportError when attempting to import the 'automlstep' module from the '.train.automl' package.",
        "Issue_preprocessed_content":"Title: importerror cannot import name 'automlstep' from ; Content: importerror cannot import name 'automlstep' from"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/644",
        "Issue_title":"Error trying to load azureml.train.automl",
        "Issue_creation_time":1572995244000,
        "Issue_closed_time":1587086020000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Hello, receiving the following error in an Azure Notebook VM while trying to import the ML library - \r\n\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\n# error here!!!\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\nimport json\r\nimport pickle\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom azureml.train.automl import AutoMLConfig\r\nfrom sklearn.externals import joblib\r\nfrom azureml.core.model import Model\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-b8d543bb7111> in <module>\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n----> 5 from azureml.train.automl import AutoMLConfig\r\n      6 from sklearn.externals import joblib\r\n      7 from azureml.core.model import Model\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/__init__.py in <module>\r\n     23     # Suppress the warnings at the import phase.\r\n     24     warnings.simplefilter(\"ignore\")\r\n---> 25     from ._automl import fit_pipeline\r\n     26     from .automlconfig import AutoMLConfig\r\n     27     from .automl_step import AutoMLStep, AutoMLStepRun\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/_automl.py in <module>\r\n     17 from automl.client.core.runtime.cache_store import CacheStore\r\n     18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities\r\n---> 19 from azureml.automl.core import data_transformation, fit_pipeline as fit_pipeline_helper\r\n     20 from azureml.automl.core.automl_pipeline import AutoMLPipeline\r\n     21 from azureml.automl.core.data_context import RawDataContext, TransformedDataContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/fit_pipeline.py in <module>\r\n     18 from automl.client.core.common.limit_function_call_exceptions import TimeoutException\r\n     19 from automl.client.core.runtime.datasets import DatasetBase\r\n---> 20 from . import package_utilities, pipeline_run_helper, training_utilities\r\n     21 from .automl_base_settings import AutoMLBaseSettings\r\n     22 from .automl_pipeline import AutoMLPipeline\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/pipeline_run_helper.py in <module>\r\n     18 from automl.client.core.common.exceptions import ClientException\r\n     19 from automl.client.core.runtime import metrics\r\n---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module\r\n     21 from automl.client.core.runtime.datasets import DatasetBase\r\n     22 from automl.client.core.runtime.execution_context import ExecutionContext\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in <module>\r\n     21 \r\n     22 from automl.client.core.common import constants\r\n---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers\r\n     24 from automl.client.core.runtime.nimbus_wrappers import AveragedPerceptronBinaryClassifier, \\\r\n     25     AveragedPerceptronMulticlassClassifier, NimbusMlClassifierMixin, NimbusMlRegressorMixin\r\n \r\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in <module>\r\n     34 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n     35 if tf_found:\r\n---> 36     tf.logging.set_verbosity(tf.logging.ERROR)\r\n     37 \r\n     38     OPTIMIZERS = {\r\n \r\nAttributeError: module 'tensorflow' has no attribute 'logging'\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: error trying to load .train.automl; Content: hello, receiving the following error in an azure notebook vm while trying to import the ml library - import json import pickle import numpy as np import pandas as pd # error here!!! from .train.automl import automlconfig from sklearn.externals import joblib from .core.model import model import json import pickle import numpy as np import pandas as pd from .train.automl import automlconfig from sklearn.externals import joblib from .core.model import model --------------------------------------------------------------------------- attributeerror traceback (most recent call last) in 3 import numpy as np 4 import pandas as pd ----> 5 from .train.automl import automlconfig 6 from sklearn.externals import joblib 7 from .core.model import model \/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/train\/automl\/__init__.py in 23 # suppress the warnings at the import phase. 24 warnings.simplefilter(\"ignore\") ---> 25 from ._automl import fit_pipeline 26 from .automlconfig import automlconfig 27 from .automl_step import automlstep, automlsteprun \/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/train\/automl\/_automl.py in 17 from automl.client.core.runtime.cache_store import cachestore 18 from automl.client.core.runtime import logging_utilities as runtime_logging_utilities ---> 19 from .automl.core import data_transformation, fit_pipeline as fit_pipeline_helper 20 from .automl.core.automl_pipeline import automlpipeline 21 from .automl.core.data_context import rawdatacontext, transformeddatacontext \/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/automl\/core\/fit_pipeline.py in 18 from automl.client.core.common.limit_function_call_exceptions import timeoutexception 19 from automl.client.core.runtime.datasets import datasetbase ---> 20 from . import package_utilities, pipeline_run_helper, training_utilities 21 from .automl_base_settings import automlbasesettings 22 from .automl_pipeline import automlpipeline \/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/automl\/core\/pipeline_run_helper.py in 18 from automl.client.core.common.exceptions import clientexception 19 from automl.client.core.runtime import metrics ---> 20 from automl.client.core.runtime import pipeline_spec as pipeline_spec_module 21 from automl.client.core.runtime.datasets import datasetbase 22 from automl.client.core.runtime.execution_context import executioncontext \/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/pipeline_spec.py in 21 22 from automl.client.core.common import constants ---> 23 from automl.client.core.runtime import model_wrappers, tf_wrappers 24 from automl.client.core.runtime.nimbus_wrappers import averagedperceptronbinaryclassifier, \\ 25 averagedperceptronmulticlassclassifier, nimbusmlclassifiermixin, nimbusmlregressormixin \/anaconda\/envs\/_py36\/lib\/python3.6\/site-packages\/\/automl\/core\/_vendor\/automl\/client\/core\/runtime\/tf_wrappers.py in 34 os.environ['tf_cpp_min_log_level'] = '2' 35 if tf_found: ---> 36 tf.logging.set_verbosity(tf.logging.error) 37 38 optimizers = { attributeerror: module 'tensorflow' has no attribute 'logging'",
        "Issue_original_content_gpt_summary":"The user is encountering an AttributeError while trying to import the ml library from .train.automl.",
        "Issue_preprocessed_content":"Title: error trying to load ; Content: hello, receiving the following error in an azure notebook vm while trying to import the ml library import json import pickle import numpy as np import pandas as pd error here!!! from import automlconfig from import joblib from import model import json import pickle import numpy as np import pandas as pd from import automlconfig from import joblib from import model attributeerror traceback in import numpy as np import pandas as pd from import automlconfig from import joblib from import model in suppress the warnings at the import phase. from import from .automlconfig import automlconfig from import automlstep, automlsteprun in from import cachestore from import as from import as from import automlpipeline from import rawdatacontext, transformeddatacontext in from import timeoutexception from import datasetbase from . import from import automlbasesettings from import automlpipeline in from import clientexception from import metrics from import as from import datasetbase from import executioncontext in from import constants from import from import averagedperceptronbinaryclassifier, \\ averagedperceptronmulticlassclassifier, nimbusmlclassifiermixin, nimbusmlregressormixin in ' ' if optimizers attributeerror module 'tensorflow' has no attribute 'logging'"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/639",
        "Issue_title":"azureml.contrib.interpret - ModuleNotFoundError - build 1.0.72",
        "Issue_creation_time":1572788594000,
        "Issue_closed_time":1573060395000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I've installed and updated the sdk yet when attempting to import the module for the ExplainationDashboard i keep getting the error that the interpret module does not exist.\r\ni am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb \r\nthe failing line in the sample notebook is:\r\n**from azureml.contrib.interpret.visualize import ExplanationDashboard**\r\n**\"ModuleNotFoundError: No module named 'azureml.contrib.interpret' \"**\r\nthis is the update command i ran:\r\npip install --upgrade azureml-sdk[explain,automl,contrib] \r\n(the install ran fine - no errors)\r\njim",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: .contrib.interpret - modulenotfounderror - build 1.0.72; Content: i've installed and updated the sdk yet when attempting to import the module for the explainationdashboard i keep getting the error that the interpret module does not exist. i am running build 1.0.72 and following the sample in 'how to use\"\/explain-model\/tabular-data\/explain-regression-local.ipynb the failing line in the sample notebook is: **from .contrib.interpret.visualize import explanationdashboard** **\"modulenotfounderror: no module named '.contrib.interpret' \"** this is the update command i ran: pip install --upgrade -sdk[explain,automl,contrib] (the install ran fine - no errors) jim",
        "Issue_original_content_gpt_summary":"The user is encountering a ModuleNotFoundError when attempting to import the module for the explanationdashboard, despite having installed and updated the SDK and running build 1.0.72.",
        "Issue_preprocessed_content":"Title: modulenotfounderror build ; Content: i've installed and updated the sdk yet when attempting to import the module for the explainationdashboard i keep getting the error that the interpret module does not exist. i am running build and following the sample in 'how to the failing line in the sample notebook is from import explanationdashboard modulenotfounderror no module named this is the update command i ran pip install the install ran fine no errors jim"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/567",
        "Issue_title":"ModuleNotFoundError: No module named 'azureml.tensorboard'",
        "Issue_creation_time":1568117676000,
        "Issue_closed_time":1568118389000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: modulenotfounderror: no module named '.tensorboard'; Content:",
        "Issue_original_content_gpt_summary":"The user encountered a ModuleNotFoundError when attempting to use TensorBoard, indicating that the module was not found.",
        "Issue_preprocessed_content":"Title: modulenotfounderror no module named ; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/534",
        "Issue_title":"Azureml Automl \"Error: Null\" Vague Error",
        "Issue_creation_time":1566233711000,
        "Issue_closed_time":1581034133000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"I'm not exactly sure on how to interpret this and what to refine. The error I'm getting is in the azureml.PipelineStep automl step. Here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourceGroups\/RG-ITSMLTeam-Dev\/providers\/Microsoft.MachineLearningServices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0).\r\n\r\nWhen my dataset has more than ~1200 features, I consistently get this error, but when there are fewer features it works fine. Is there some limitation here?",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: automl \"error: null\" vague error; Content: i'm not exactly sure on how to interpret this and what to refine. the error i'm getting is in the .pipelinestep automl step. here is the [link](https:\/\/mlworkspace.azure.ai\/portal\/subscriptions\/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\/resourcegroups\/rg-itsmlteam-dev\/providers\/microsoft.machinelearningservices\/workspaces\/avadevitsmlsvc\/experiments\/deal-deal-nema\/runs\/7abe9617-ac79-413b-8843-7fd3878313f0). when my dataset has more than ~1200 features, i consistently get this error, but when there are fewer features it works fine. is there some limitation here?",
        "Issue_original_content_gpt_summary":"The user is encountering an \"error: null\" vague error in the .pipelinestep automl step when their dataset has more than ~1200 features.",
        "Issue_preprocessed_content":"Title: automl error null vague error; Content: i'm not exactly sure on how to interpret this and what to refine. the error i'm getting is in the automl step. here is the . when my dataset has more than features, i consistently get this error, but when there are fewer features it works fine. is there some limitation here?"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/518",
        "Issue_title":"No name 'opendatasets' in module 'azureml' Error",
        "Issue_creation_time":1565216029000,
        "Issue_closed_time":1565217619000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi,\r\nI was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (Go to \"Data access\" tab)to use opendatasets module to access historical weather data. But it gives me the error message `No name 'opendatasets' in module 'azureml'`. \r\nI tried `pip install azureml-sdk[opendatasets]` as well, it shows `WARNING: azureml-sdk 1.0.55 does not provide the extra 'opendatasets'`.\r\nDo you know how to use the opendatasets module in azureml?\r\n\r\nThanks!",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: no name 'opendatasets' in module '' error; Content: hi, i was trying to follow this documentation: https:\/\/azure.microsoft.com\/en-us\/services\/open-datasets\/catalog\/noaa-integrated-surface-data\/ (go to \"data access\" tab)to use opendatasets module to access historical weather data. but it gives me the error message `no name 'opendatasets' in module ''`. i tried `pip install -sdk[opendatasets]` as well, it shows `warning: -sdk 1.0.55 does not provide the extra 'opendatasets'`. do you know how to use the opendatasets module in ? thanks!",
        "Issue_original_content_gpt_summary":"The user is encountering an error when attempting to use the 'opendatasets' module to access historical weather data from the NOAA Integrated Surface Data catalog.",
        "Issue_preprocessed_content":"Title: no name 'opendatasets' in module '' error; Content: hi, i was trying to follow this documentation to use opendatasets module to access historical weather data. but it gives me the error message . i tried as well, it shows . do you know how to use the opendatasets module in ? thanks!"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/243",
        "Issue_title":"Unable to register the model using Jupyter Notebook with error message: \"HttpOperationError: Operation returned an invalid status code 'Service invocation failed!Request: GET https:\/\/cert-westeurope.experiments.azureml.net\/rp\/workspaces'\"",
        "Issue_creation_time":1552039718000,
        "Issue_closed_time":1587148100000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"> from azureml.core.model import Model\r\nmodel = Model.register(model_path = MODEL_FILENAME,\r\n                       model_name = \"MyONNXmodel\",\r\n                       tags = {\"onnx\":\"V0\"},\r\n                       description = \"test\",\r\n                       workspace = ws)\r\nthis is the python code I am using to register the model.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to register the model using jupyter notebook with error message: \"httpoperationerror: operation returned an invalid status code 'service invocation failed!request: get https:\/\/cert-westeurope.experiments..net\/rp\/workspaces'\"; Content: > from .core.model import model model = model.register(model_path = model_filename, model_name = \"myonnxmodel\", tags = {\"onnx\":\"v0\"}, description = \"test\", workspace = ws) this is the python code i am using to register the model.",
        "Issue_original_content_gpt_summary":"The user is unable to register the model using jupyter notebook with an error message of \"httpoperationerror: operation returned an invalid status code 'service invocation failed!request: get https:\/\/cert-westeurope.experiments..net\/rp\/workspaces'\".",
        "Issue_preprocessed_content":"Title: unable to register the model using jupyter notebook with error message httpoperationerror operation returned an invalid status code 'service invocation failed!request get ; Content: from import model model myonnxmodel , tags , description test , workspace ws this is the python code i am using to register the model."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1850",
        "Issue_title":"502 Bad Gateway error after running Tesla K80 compute instance on azure ML",
        "Issue_creation_time":1671822949000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? YES <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Create compute instance in azure ML (Tesla K80) and add schedule\r\n2. Connect with VSCode on day 2. \r\n\r\nAction: Resolver.resolve\r\nError type: ce\r\nError Message: Failed to connect to target. Make sure that it exists and is in a running state (Error: Invalid response: 502 Bad Gateway)\r\n\r\n\r\nVersion: 0.22.0\r\nOS: darwin\r\nOS Release: 22.1.0\r\nProduct: Visual Studio Code\r\nProduct Version: 1.74.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nce.NotAvailable extensionHostProcess.js:92:17513\r\nb.handle400BadRequestAnd502BadGateway extension.js:2:1949684\r\nb.handleError extension.js:2:1948588\r\no.value extension.js:2:1911817\r\nextension.js:2:1960308\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: 502 bad gateway error after running tesla k80 compute instance on ; Content: does this occur consistently? yes repro steps: 1. create compute instance in (tesla k80) and add schedule 2. connect with vscode on day 2. action: resolver.resolve error type: ce error message: failed to connect to target. make sure that it exists and is in a running state (error: invalid response: 502 bad gateway) version: 0.22.0 os: darwin os release: 22.1.0 product: visual studio code product version: 1.74.1 language: en call stack ``` ce.notavailable extensionhostprocess.js:92:17513 b.handle400badrequestand502badgateway extension.js:2:1949684 b.handleerror extension.js:2:1948588 o.value extension.js:2:1911817 extension.js:2:1960308 ```",
        "Issue_original_content_gpt_summary":"The user encountered a 502 bad gateway error after running a tesla k80 compute instance on Visual Studio Code, which consistently occurred despite attempts to resolve it.",
        "Issue_preprocessed_content":"Title: bad gateway error after running tesla k compute instance on ; Content: does this occur consistently? yes repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . create compute instance in and add schedule . connect with vscode on day . action error type ce error message failed to connect to target. make sure that it exists and is in a running state version os darwin os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1817",
        "Issue_title":"Unable to login in Azure ML extension in VSCode",
        "Issue_creation_time":1669213419000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.22.0\r\nOS: linux\r\nOS Release: 5.15.0-1022-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.68.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1976096\r\ns extension.js:2:1972783\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to login in extension in vscode; Content: does this occur consistently? repro steps: 1. 2. action: azureaccount.onsessionschanged error type: 123 error message: unknown error retrieving susbcriptions from azure account extension version: 0.22.0 os: linux os release: 5.15.0-1022-azure product: visual studio code product version: 1.68.1 language: en call stack ``` b. extension.js:2:1976096 s extension.js:2:1972783 ```",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the Azure Account extension in Visual Studio Code, where they were unable to login and received an unknown error retrieving subscriptions from the extension.",
        "Issue_preprocessed_content":"Title: unable to login in extension in vscode; Content: does this occur consistently? repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . . action error type error message unknown error retrieving susbcriptions from azure account extension version os linux os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1797",
        "Issue_title":"connecting to VScode to AzureML",
        "Issue_creation_time":1668197837000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: connecting to vscode to ; Content: does this occur consistently? repro steps: 1. 2. action: azureaccount.onsessionschanged error type: 123 error message: unknown error retrieving susbcriptions from azure account extension version: 0.20.0 os: win32 os release: 10.0.19044 product: visual studio code product version: 1.72.2 language: en call stack ``` b. extension.js:2:1975925 s extension.js:2:1972612 ```",
        "Issue_original_content_gpt_summary":"The user encountered an issue connecting to VSCode, with an unknown error retrieving subscriptions from the Azure Account Extension, while using Visual Studio Code version 1.72.2 on Windows 10.",
        "Issue_preprocessed_content":"Title: connecting to vscode to ; Content: does this occur consistently? repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . . action error type error message unknown error retrieving susbcriptions from azure account extension version os win os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1783",
        "Issue_title":"Visual Studio Azure ML extension error on \"View Properties\" ",
        "Issue_creation_time":1667838269000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"To reproduce the issue:\r\nRight click on ML default project in left hand bar.\r\nFrom the menu click \"View Properties\" and getting:\r\n\r\n[UriError]: If a URI does not contain an authority component, then the path cannot begin with two slash characters (\"\/\/\")",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: visual studio extension error on \"view properties\" ; Content: to reproduce the issue: right click on ml default project in left hand bar. from the menu click \"view properties\" and getting: [urierror]: if a uri does not contain an authority component, then the path cannot begin with two slash characters (\"\/\/\")",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to view properties of an ML default project in Visual Studio by right-clicking and selecting \"View Properties\".",
        "Issue_preprocessed_content":"Title: visual studio extension error on view properties ; Content: to reproduce the issue right click on ml default project in left hand bar. from the menu click view properties and getting urierror if a uri does not contain an authority component, then the path cannot begin with two slash characters"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1754",
        "Issue_title":"Mutliple consecutive sign-in requests from Azure ML plugin VS Code",
        "Issue_creation_time":1665696667000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: linux\r\nOS Release: 5.15.0-1017-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.72.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: mutliple consecutive sign-in requests from plugin vs code; Content: does this occur consistently? repro steps: 1. 2. action: azureaccount.onsessionschanged error type: 123 error message: unknown error retrieving susbcriptions from azure account extension version: 0.20.0 os: linux os release: 5.15.0-1017-azure product: visual studio code product version: 1.72.2 language: en call stack ``` b. extension.js:2:1975925 s extension.js:2:1972612 ```",
        "Issue_original_content_gpt_summary":"The user encountered multiple consecutive sign-in requests from the plugin VS Code, resulting in an unknown error retrieving subscriptions from the Azure Account Extension.",
        "Issue_preprocessed_content":"Title: mutliple consecutive requests from plugin vs code; Content: does this occur consistently? repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . . action error type error message unknown error retrieving susbcriptions from azure account extension version os linux os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1745",
        "Issue_title":"Problem signing into Azure ML using VSCode with latest version",
        "Issue_creation_time":1664990324000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.20.0\r\nOS: win32\r\nOS Release: 10.0.19044\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:1975925\r\ns extension.js:2:1972612\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: problem signing into using vscode with latest version; Content: does this occur consistently? repro steps: 1. 2. action: azureaccount.onsessionschanged error type: 123 error message: unknown error retrieving susbcriptions from azure account extension version: 0.20.0 os: win32 os release: 10.0.19044 product: visual studio code product version: 1.71.2 language: en call stack ``` b. extension.js:2:1975925 s extension.js:2:1972612 ```",
        "Issue_original_content_gpt_summary":"The user encountered a problem signing into Azure using VSCode with the latest version, resulting in an unknown error retrieving subscriptions from the Azure Account Extension.",
        "Issue_preprocessed_content":"Title: problem signing into using vscode with latest version; Content: does this occur consistently? repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . . action error type error message unknown error retrieving susbcriptions from azure account extension version os win os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1737",
        "Issue_title":"Working with Azure ML Studio on VSCode",
        "Issue_creation_time":1664468718000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1.\r\n2.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.18.0\r\nOS: darwin\r\nOS Release: 21.6.0\r\nProduct: Visual Studio Code\r\nProduct Version: 1.71.2\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: working with studio on vscode; Content: does this occur consistently? repro steps: 1. 2. action: azureaccount.onsessionschanged error type: 123 error message: unknown error retrieving susbcriptions from azure account extension version: 0.18.0 os: darwin os release: 21.6.0 product: visual studio code product version: 1.71.2 language: en call stack ``` b. extension.js:2:2030116 s extension.js:2:2026803 ```",
        "Issue_original_content_gpt_summary":"The user encountered an unknown error retrieving subscriptions from the Azure Account Extension while working with Studio on VSCode, with the action \"azureaccount.onsessionschanged\", error type \"123\", and call stack provided.",
        "Issue_preprocessed_content":"Title: working with studio on vscode; Content: does this occur consistently? repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . . action error type error message unknown error retrieving susbcriptions from azure account extension version os darwin os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1714",
        "Issue_title":"I keep on getting this error continuously for Azure Machine Learning extension",
        "Issue_creation_time":1662929331000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No -->\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Azure sign in\r\n2. Sign in using Azure portal. You get the sign in successful, you may close the window message, but Azure asks to sign in again.\r\n\r\nAction: azureAccount.onSessionsChanged\r\nError type: 123\r\nError Message: Unknown Error retrieving susbcriptions from Azure Account extension\r\n\r\n\r\nVersion: 0.17.2022090809\r\nOS: win32\r\nOS Release: 10.0.19042\r\nProduct: Visual Studio Code - Insiders\r\nProduct Version: 1.72.0-insider\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nb.<anonymous> extension.js:2:2030116\r\ns extension.js:2:2026803\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: i keep on getting this error continuously for extension; Content: does this occur consistently? repro steps: 1. azure sign in 2. sign in using azure portal. you get the sign in successful, you may close the window message, but azure asks to sign in again. action: azureaccount.onsessionschanged error type: 123 error message: unknown error retrieving susbcriptions from azure account extension version: 0.17.2022090809 os: win32 os release: 10.0.19042 product: visual studio code - insiders product version: 1.72.0-insider language: en call stack ``` b. extension.js:2:2030116 s extension.js:2:2026803 ```",
        "Issue_original_content_gpt_summary":"The user encountered an error when signing into Azure using the Visual Studio Code - Insiders extension, with an unknown error retrieving subscriptions from the Azure Account extension.",
        "Issue_preprocessed_content":"Title: i keep on getting this error continuously for extension; Content: does this occur consistently? repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . azure sign in . sign in using azure portal. you get the sign in successful, you may close the window message, but azure asks to sign in again. action error type error message unknown error retrieving susbcriptions from azure account extension version os win os release product visual studio code insiders product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1691",
        "Issue_title":"Update Treeview asset labels to match Azure ML Studio.",
        "Issue_creation_time":1661895451000,
        "Issue_closed_time":1663956142000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"In particular:\r\n- Experiments needs to be renamed to Jobs\r\n- Datasets needs to be renamed to Data\r\n\r\nFurther changes probably aren't absolutely necessary right now, but should be considered as well. See #616.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: update treeview asset labels to match studio.; Content: in particular: - experiments needs to be renamed to jobs - datasets needs to be renamed to data further changes probably aren't absolutely necessary right now, but should be considered as well. see #616.",
        "Issue_original_content_gpt_summary":"The user encountered challenges in updating the treeview asset labels to match studio, in particular renaming experiments to jobs and datasets to data, with further changes to be considered.",
        "Issue_preprocessed_content":"Title: update treeview asset labels to match studio.; Content: in particular experiments needs to be renamed to jobs datasets needs to be renamed to data further changes probably aren't absolutely necessary right now, but should be considered as well. see ."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1655",
        "Issue_title":"Can't use Azure ML features when remotely connected to a compute",
        "Issue_creation_time":1661811043000,
        "Issue_closed_time":1665527881000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Since we changed the vscode-azureml-remote extension to be of UI type it is not supported anymore in the web or codespaces.\r\n\r\nGiven that main extension depends on vscode-azureml-remote, main is also unavailable in the web or codespaces.\r\n\r\nChanging the dependency should enable the main extension in the web context again.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: can't use features when remotely connected to a compute; Content: since we changed the vscode--remote extension to be of ui type it is not supported anymore in the web or codespaces. given that main extension depends on vscode--remote, main is also unavailable in the web or codespaces. changing the dependency should enable the main extension in the web context again.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they were unable to use certain features when remotely connected to a compute due to a dependency on the vscode--remote extension.",
        "Issue_preprocessed_content":"Title: can't use features when remotely connected to a compute; Content: since we changed the extension to be of ui type it is not supported anymore in the web or codespaces. given that main extension depends on main is also unavailable in the web or codespaces. changing the dependency should enable the main extension in the web context again."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1627",
        "Issue_title":"AzureML Prompts twice to login when VS Code (Insiders) loads",
        "Issue_creation_time":1659626460000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Expected Behavior\r\nIf the user is logged out, the AML extension should not prompt to login until the user specifically tries to run an AzureML command. Prompting when VS Code loads is disruptive and unnecessary, and no other extensions for AWS or Azure do this.\r\n\r\n## Actual Behavior\r\nIf you are signed out of the Azure ML extension and reload VS Code, you are prompted to login when it loads (Issue #1). If you click cancel, you are prompted again (#2). \r\n\r\n## Steps to Reproduce the Problem\r\n  1. Install the Azure ML Extension\r\n  2. Login\r\n  3. Logout\r\n  4. Reload VS Code\r\n  5. Click \"Cancel\" when prompted to login\r\n\r\n\r\n## Specifications\r\nAzure ML Extension Version 0.16.0\r\n \r\nVersion: 1.70.0-insider (Universal)\r\nCommit: da76f93349a72022ca4670c1b84860304616aaa2\r\nDate: 2022-08-03T05:55:27.651Z (1 day ago)\r\nElectron: 18.3.5\r\nChromium: 100.0.4896.160\r\nNode.js: 16.13.2\r\nV8: 10.0.139.17-electron.0\r\nOS: Darwin x64 21.6.0\r\n\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: prompts twice to login when vs code (insiders) loads; Content: ## expected behavior if the user is logged out, the aml extension should not prompt to login until the user specifically tries to run an command. prompting when vs code loads is disruptive and unnecessary, and no other extensions for aws or azure do this. ## actual behavior if you are signed out of the extension and reload vs code, you are prompted to login when it loads (issue #1). if you click cancel, you are prompted again (#2). ## steps to reproduce the problem 1. install the extension 2. login 3. logout 4. reload vs code 5. click \"cancel\" when prompted to login ## specifications extension version 0.16.0 version: 1.70.0-insider (universal) commit: da76f93349a72022ca4670c1b84860304616aaa2 date: 2022-08-03t05:55:27.651z (1 day ago) electron: 18.3.5 chromium: 100.0.4896.160 node.js: 16.13.2 v8: 10.0.139.17-electron.0 os: darwin x64 21.6.0",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the AML extension prompts twice to login when VS Code (Insiders) loads, even after clicking \"cancel\" the first time.",
        "Issue_preprocessed_content":"Title: prompts twice to login when vs code loads; Content: expected behavior if the user is logged out, the aml extension should not prompt to login until the user specifically tries to run an command. prompting when vs code loads is disruptive and unnecessary, and no other extensions for aws or azure do this. actual behavior if you are signed out of the extension and reload vs code, you are prompted to login when it loads . if you click cancel, you are prompted again . steps to reproduce the problem . install the extension . login . logout . reload vs code . click cancel when prompted to login specifications extension version version commit da f a ca c b aaa date electron chromium v os darwin x"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1589",
        "Issue_title":"Run and debug experiments locally - azureML.CLI Compatibility Mode for CLI v1 - cannot find",
        "Issue_creation_time":1654678830000,
        "Issue_closed_time":1654701310000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Expected Behavior\r\nIt seems new version on AzureML extension to VS Code doesn't have this option in settings. I needed to downgrade to 0.6x.\r\n\r\n## Actual Behavior\r\nCurrent version 0.10.0 doesn't have the option. Cannot locally debug or documentation doesn't provide info about that.\r\n\r\n## Specifications\r\n\r\n  - Version: 0.10.0\r\n  - Platform: VS Code, Windows\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: run and debug experiments locally - .cli compatibility mode for cli v1 - cannot find; Content: ## expected behavior it seems new version on extension to vs code doesn't have this option in settings. i needed to downgrade to 0.6x. ## actual behavior current version 0.10.0 doesn't have the option. cannot locally debug or documentation doesn't provide info about that. ## specifications - version: 0.10.0 - platform: vs code, windows",
        "Issue_original_content_gpt_summary":"The user encountered challenges with the .cli compatibility mode for cli v1 in the new version of the extension to VS Code, and had to downgrade to 0.6x in order to find the option in the settings.",
        "Issue_preprocessed_content":"Title: run and debug experiments locally compatibility mode for cli v cannot find; Content: expected behavior it seems new version on extension to vs code doesn't have this option in settings. i needed to downgrade to actual behavior current version doesn't have the option. cannot locally debug or documentation doesn't provide info about that. specifications version platform vs code, windows"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1588",
        "Issue_title":"Improve the error message when trying to execute a YAML that is not Azure ML realted",
        "Issue_creation_time":1654285462000,
        "Issue_closed_time":1655835283000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Currently the customer is shown an error message but also has the option to report an issue which is misleading, we should remove the report issue button for this scenario.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: improve the error message when trying to execute a yaml that is not realted; Content: currently the customer is shown an error message but also has the option to report an issue which is misleading, we should remove the report issue button for this scenario.",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with the error message displayed when attempting to execute a YAML file that is not related, as the option to report an issue is misleading.",
        "Issue_preprocessed_content":"Title: improve the error message when trying to execute a yaml that is not realted; Content: currently the customer is shown an error message but also has the option to report an issue which is misleading, we should remove the report issue button for this scenario."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/vscode-tools-for-ai\/issues\/1541",
        "Issue_title":"Reoccurring error on opening connection to Azure Machine Learning Compute Instance",
        "Issue_creation_time":1649744320000,
        "Issue_closed_time":1652117002000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"<!-- IMPORTANT: Please be sure to remove any private information before submitting. -->\r\n\r\nDoes this occur consistently? <!-- TODO: Type Yes or No --> Yes\r\nRepro steps:\r\n<!-- TODO: Share the steps needed to reliably reproduce the problem. Please include actual and expected results. -->\r\n\r\n1. Open remote connection to Azure Machine Learning Compute Instance\r\n\r\nThis does not seem to cause any issues, but it's annoying to see the error message every time.\r\n\r\nAction: azureAccount.onSubscriptionsChanged\r\nError type: REQUEST_SEND_ERROR\r\nError Message: request to redacted:url failed, reason: getaddrinfo ENOTFOUND redacted:idworkspace.westeurope.api.azureml.ms\r\n\r\n\r\nVersion: 0.8.2\r\nOS: linux\r\nOS Release: 5.4.0-1068-azure\r\nProduct: Visual Studio Code\r\nProduct Version: 1.66.1\r\nLanguage: en\r\n\r\n<details>\r\n<summary>Call Stack<\/summary>\r\n\r\n```\r\nnew t extension.js:2:486489\r\nt.<anonymous> extension.js:2:470040\r\nextension.js:2:2450576\r\nObject.throw extension.js:2:2450681\r\nc extension.js:2:2449471\r\n```\r\n\r\n<\/details>\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: reoccurring error on opening connection to compute instance; Content: does this occur consistently? yes repro steps: 1. open remote connection to compute instance this does not seem to cause any issues, but it's annoying to see the error message every time. action: azureaccount.onsubscriptionschanged error type: request_send_error error message: request to redacted:url failed, reason: getaddrinfo enotfound redacted:idworkspace.westeurope.api..ms version: 0.8.2 os: linux os release: 5.4.0-1068-azure product: visual studio code product version: 1.66.1 language: en call stack ``` new t extension.js:2:486489 t. extension.js:2:470040 extension.js:2:2450576 object.throw extension.js:2:2450681 c extension.js:2:2449471 ```",
        "Issue_original_content_gpt_summary":"The user encountered a reoccurring error when opening a remote connection to a compute instance, which resulted in an \"request_send_error\" error message.",
        "Issue_preprocessed_content":"Title: reoccurring error on opening connection to compute instance; Content: does this occur consistently? yes repro steps todo share the steps needed to reliably reproduce the problem. please include actual and expected results. . open remote connection to compute instance this does not seem to cause any issues, but it's annoying to see the error message every time. action error type error message request to redacted url failed, reason getaddrinfo enotfound version os linux os release product visual studio code product version language en details summary call"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/1213",
        "Issue_title":"Can not add AzureML extention on openshift cluster ",
        "Issue_creation_time":1653563264000,
        "Issue_closed_time":1654438640000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Error when adding extetnion azureml\r\naz k8s-extension create --name azureml-extension --extension-type Microsoft.AzureML.Kubernetes --config enableTraining= cluster-type conneced--cluster-name <your-AKS-cluster-name> --resource-group <your-RG-name> --scope cluster\r\n\r\n\r\ncrc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Creating\"}\r\ncli.azure.cli.core.sdk.policies: Request URL: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01'\r\ncli.azure.cli.core.sdk.policies: Request method: 'GET'\r\ncli.azure.cli.core.sdk.policies: Request headers:\r\ncli.azure.cli.core.sdk.policies:     'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d'\r\ncli.azure.cli.core.sdk.policies:     'CommandName': 'k8s-extension create'\r\ncli.azure.cli.core.sdk.policies:     'ParameterSetName': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config'\r\ncli.azure.cli.core.sdk.policies:     'User-Agent': 'AZURECLI\/2.36.0 (MSI) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 Python\/3.10.4 (Windows-10-10.0.19044-SP0)'\r\ncli.azure.cli.core.sdk.policies:     'Authorization': '*****'\r\ncli.azure.cli.core.sdk.policies: Request body:\r\ncli.azure.cli.core.sdk.policies: This request has no body\r\nurllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fmanagement.azure.com%2F&data=05%7C01%7Cjohan.andolf%40microsoft.com%7C37b5d083c3d7447f133208da3e347a4a%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637890692414003835%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=1kWOqV7FwAgqmYol4W7wfZRbf%2BCTKz9XucDBe%2FKgGKA%3D&reserved=0) \"GET \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-Version=2022-03-01 HTTP\/1.1\" 200 None\r\ncli.azure.cli.core.sdk.policies: Response status: 200\r\ncli.azure.cli.core.sdk.policies: Response headers:\r\ncli.azure.cli.core.sdk.policies:     'Cache-Control': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Pragma': 'no-cache'\r\ncli.azure.cli.core.sdk.policies:     'Transfer-Encoding': 'chunked'\r\ncli.azure.cli.core.sdk.policies:     'Content-Type': 'application\/json; charset=utf-8'\r\ncli.azure.cli.core.sdk.policies:     'Content-Encoding': 'gzip'\r\ncli.azure.cli.core.sdk.policies:     'Expires': '-1'\r\ncli.azure.cli.core.sdk.policies:     'Vary': 'Accept-Encoding'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-ratelimit-remaining-subscription-reads': '11968'\r\ncli.azure.cli.core.sdk.policies:     'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'\r\ncli.azure.cli.core.sdk.policies:     'api-supported-versions': '2019-11-01-Preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview'\r\ncli.azure.cli.core.sdk.policies:     'X-Content-Type-Options': 'nosniff'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'x-ms-routing-request-id': 'SWEDENCENTRAL:20220525T095135Z:8d41b858-4f6d-4d30-819c-c34bef28d668'\r\ncli.azure.cli.core.sdk.policies:     'Date': 'Wed, 25 May 2022 09:51:34 GMT'\r\ncli.azure.cli.core.sdk.policies: Response content:\r\ncli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourceGroups\/azurearctest\/providers\/Microsoft.Kubernetes\/ConnectedClusters\/tvl-crc\/providers\/Microsoft.KubernetesConfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"Failed\",\"error\":{\"code\":\"ExtensionCreationFailed\",\"message\":\" error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\"}}\r\ncli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception:\r\ncli.azure.cli.core.util: Traceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll\r\nazure.core.polling.base_polling.OperationFailed: Operation failed or canceled\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\knack\/cli.py\", line 231, in invoke\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start\r\n  File \"D:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\Lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run\r\nazure.core.exceptions.HttpResponseError: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\n\r\ncli.azure.cli.core.azclierror: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\naz_command_data_logger: (ExtensionCreationFailed)  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\nCode: ExtensionCreationFailed\r\nMessage:  error: Unable to get the status from the local CRD with the error : {Error : Retry for given duration didn't get any results with err {status not populated}}\r\ncli.knack.cli: Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x0387C190>]\r\naz_command_data_logger: exit code: 1\r\ncli.__main__: Command ran in 996.906 seconds (init: 0.535, invoke: 996.371)\r\ntelemetry.save: Save telemetry record of length 3581 in cache\r\ntelemetry.check: Returns Positive.\r\ntelemetry.main: Begin creating telemetry upload process.\r\ntelemetry.process: Creating upload process: \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\python.exe C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\Lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc C:\\Users\\ropa04\\.azure\"\r\ntelemetry.process: Return from creating process\r\ntelemetry.main: Finish creating telemetry upload process.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: can not add extention on openshift cluster ; error when adding extetnion az k8s-extension create --name -extension --extension-type microsoft..kubernetes --config enabletraining= cluster-type conneced--cluster-name --resource-group --scope cluster crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"creating\"} cli.azure.cli.core.sdk.policies: request url: 'https:\/\/management.azure.com\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourcegroups\/azurearctest\/providers\/microsoft.kubernetes\/connectedclusters\/tvl-crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-version=2022-03-01' cli.azure.cli.core.sdk.policies: request method: 'get' cli.azure.cli.core.sdk.policies: request headers: cli.azure.cli.core.sdk.policies: 'x-ms-client-request-id': 'f1bf020c-dc0d-11ec-a8c0-808abda5e54d' cli.azure.cli.core.sdk.policies: 'commandname': 'k8s-extension create' cli.azure.cli.core.sdk.policies: 'parametersetname': '--name --extension-type --cluster-type --cluster-name --resource-group --name --auto-upgrade --scope --debug --config' cli.azure.cli.core.sdk.policies: 'user-agent': 'azurecli\/2.36.0 (msi) azsdk-python-azure-mgmt-kubernetesconfiguration\/1.0.0 python\/3.10.4 (windows-10-10.0.19044-sp0)' cli.azure.cli.core.sdk.policies: 'authorization': '*****' cli.azure.cli.core.sdk.policies: request body: cli.azure.cli.core.sdk.policies: this request has no body urllib3.connectionpool: [https:\/\/management.azure.com:443](https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3a%2f%2fmanagement.azure.com%2f&data=05%7c01%7cjohan.andolf%40microsoft.com%7c37b5d083c3d7447f133208da3e347a4a%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c637890692414003835%7cunknown%7ctwfpbgzsb3d8eyjwijoimc4wljawmdailcjqijoiv2lumziilcjbtii6ik1hawwilcjxvci6mn0%3d%7c3000%7c%7c%7c&sdata=1kwoqv7fwagqmyol4w7wfzrbf%2bctkz9xucdbe%2fkggka%3d&reserved=0) \"get \/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourcegroups\/azurearctest\/providers\/microsoft.kubernetes\/connectedclusters\/tvl-crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e?api-version=2022-03-01 http\/1.1\" 200 none cli.azure.cli.core.sdk.policies: response status: 200 cli.azure.cli.core.sdk.policies: response headers: cli.azure.cli.core.sdk.policies: 'cache-control': 'no-cache' cli.azure.cli.core.sdk.policies: 'pragma': 'no-cache' cli.azure.cli.core.sdk.policies: 'transfer-encoding': 'chunked' cli.azure.cli.core.sdk.policies: 'content-type': 'application\/json; charset=utf-8' cli.azure.cli.core.sdk.policies: 'content-encoding': 'gzip' cli.azure.cli.core.sdk.policies: 'expires': '-1' cli.azure.cli.core.sdk.policies: 'vary': 'accept-encoding' cli.azure.cli.core.sdk.policies: 'x-ms-ratelimit-remaining-subscription-reads': '11968' cli.azure.cli.core.sdk.policies: 'strict-transport-security': 'max-age=31536000; Content: includesubdomains' cli.azure.cli.core.sdk.policies: 'api-supported-versions': '2019-11-01-preview, 2021-05-01-preview, 2021-06-01-preview, 2021-09-01, 2021-11-01-preview, 2022-01-01-preview, 2022-03-01, 2022-04-02-preview' cli.azure.cli.core.sdk.policies: 'x-content-type-options': 'nosniff' cli.azure.cli.core.sdk.policies: 'x-ms-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668' cli.azure.cli.core.sdk.policies: 'x-ms-correlation-request-id': '8d41b858-4f6d-4d30-819c-c34bef28d668' cli.azure.cli.core.sdk.policies: 'x-ms-routing-request-id': 'swedencentral:20220525t095135z:8d41b858-4f6d-4d30-819c-c34bef28d668' cli.azure.cli.core.sdk.policies: 'date': 'wed, 25 may 2022 09:51:34 gmt' cli.azure.cli.core.sdk.policies: response cli.azure.cli.core.sdk.policies: {\"id\":\"\/subscriptions\/0ebcf6f3-37c0-4ab6-bc4a-4299fd25192a\/resourcegroups\/azurearctest\/providers\/microsoft.kubernetes\/connectedclusters\/tvl-crc\/providers\/microsoft.kubernetesconfiguration\/extensions\/arcml-extension\/operations\/b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"name\":\"b5f5e30a-7439-4dd7-aab7-90bd438d320e\",\"status\":\"failed\",\"error\":{\"code\":\"extensioncreationfailed\",\"message\":\" error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}}\"}} cli.azure.cli.core.util: azure.cli.core.util.handle_exception is called with an exception: cli.azure.cli.core.util: traceback (most recent call last): file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 483, in run file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 522, in _poll azure.core.polling.base_polling.operationfailed: operation failed or canceled during handling of the above exception, another exception occurred: traceback (most recent call last): file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\knack\/cli.py\", line 231, in invoke file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 658, in execute file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 721, in _run_jobs_serially file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 703, in _run_job file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 1008, in __call__ file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/cli\/core\/commands\/__init__.py\", line 995, in __call__ file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 255, in result file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/tracing\/decorator.py\", line 83, in wrapper_use_tracer file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 275, in wait file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/_poller.py\", line 192, in _start file \"d:\\a\\1\\s\\build_scripts\\windows\\artifacts\\cli\\lib\\site-packages\\azure\/core\/polling\/base_polling.py\", line 501, in run azure.core.exceptions.httpresponseerror: (extensioncreationfailed) error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}} code: extensioncreationfailed message: error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}} cli.azure.cli.core.azclierror: (extensioncreationfailed) error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}} code: extensioncreationfailed message: error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}} az_command_data_logger: (extensioncreationfailed) error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}} code: extensioncreationfailed message: error: unable to get the status from the local crd with the error : {error : retry for given duration didn't get any results with err {status not populated}} cli.knack.cli: event: cli.postexecute [] az_command_data_logger: exit code: 1 cli.__main__: command ran in 996.906 seconds (init: 0.535, invoke: 996.371) telemetry.save: save telemetry record of length 3581 in cache telemetry.check: returns positive. telemetry.main: begin creating telemetry upload process. telemetry.process: creating upload process: \"c:\\program files (x86)\\microsoft sdks\\azure\\cli2\\python.exe c:\\program files (x86)\\microsoft sdks\\azure\\cli2\\lib\\site-packages\\azure\\cli\\telemetry\\__init__.pyc c:\\users\\ropa04\\.azure\" telemetry.process: return from creating process telemetry.main: finish creating telemetry upload process.",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to add an extension to an OpenShift cluster, resulting in an error due to an inability to get the status from the local crd.",
        "Issue_preprocessed_content":"Title: can not add extention on openshift cluster ; Content: error when adding extetnion az create enabletraining cluster request url request method 'get' request headers 'commandname' create' 'parametersetname' ' 'authorization' ' ' request body this request has no body get none response status response headers 'pragma' 'chunked' 'gzip' 'expires' 'vary' ' ' includesubdomains' 'nosniff' 'date' 'wed, may gmt' response content is called with an exception traceback file line , in run file line , in operation failed or canceled during handling of the above exception, another exception occurred traceback file line , in invoke file line , in execute file line , in file line , in file line , in file line , in file line , in result file line , in file line , in wait file line , in file line , in run error unable to get the status from the local crd with the error code extensioncreationfailed message error unable to get the status from the local crd with the error error unable to get the status from the local crd with the error code extensioncreationfailed message error unable to get the status from the local crd with the error error unable to get the status from the local crd with the error code extensioncreationfailed message error unable to get the status from the local crd with the error event exit code command ran in seconds save telemetry record of length in cache returns positive. begin creating telemetry upload process. creating upload process files files return from creating process finish creating telemetry upload process."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/azure_arc\/issues\/758",
        "Issue_title":"error when installing AZURE ML training model piece",
        "Issue_creation_time":1631563330000,
        "Issue_closed_time":1631711922000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"I was getting an error when azuremllogonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. So, I installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below.\r\n\r\ngrep executes but now the error says \"Dataset with name 'mnist_opendataset' is not found\".\r\n\r\nAny help troubleshooting this error will be appreciated, I am trying to demo this to a customer. next week.\r\n\r\n**TEXT of the OUTPUT when error is encountered:**\r\n\r\n\r\nInstalling amlarc-compute K8s extension was successful.\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nLibrary configuration succeeded\r\n\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\n\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nClass KubernetesCompute: This is an experimental class, and may change at any time. Please see https:\/\/aka.ms\/azuremlexperimental for more information.\r\nfound compute target: ARC-ml\r\n\"\r\n Training model:\r\n                               \r\n            .....                                             .....\r\n         .........                                           .........\r\n        .........                 (((((((((##                 .........\r\n       .....                      (((((((####                      .....\r\n      ......                      #((########                      ......\r\n     ....... .............        ###########        ............. .......\r\n     ......................       ###########       ......................\r\n    .................*.....       ###########       ....,*.................\r\n    .........*******......       (((((((((((         ......*******.........\r\n         ............          (((((((((((     (.         ............\r\n                            .(((((((((((     (((((\/\r\n                          ((((((((((((     #(((((((##\r\n                        \/\/\/\/(((((((*     ##############\r\n                      \/\/\/\/\/\/(((((.         ,#############.\r\n                   ,**\/\/\/\/\/\/\/((               #############\/\r\n                    *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%##########\r\n                    \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######(\r\n                     \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%####\r\n                     .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#.\r\n\r\n\"\r\nWarning: Falling back to use azure cli login credentials.\r\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\r\nPlease refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\r\nWARNING: Command group 'ml job' is experimental and under development. Reference and support levels: https:\/\/aka.ms\/CLI_refstatus\r\nUploading src:   0%|                                                                                                                                | 0.00\/3.08k [00:00<?, ?B\/s]\r\n\r\n**ERROR: Code: UserError**\r\n**Message: Dataset with name 'mnist_opendataset' is not found.**\r\n**You cannot call a method on a null-valued expression.**\r\n**At C:\\Temp\\AzureMLLogonScript.ps1:267 char:4**\r\n**+    $RunId = ($Job | grep '\\\"name\\\":').Split('\\\"')[3]**\r\n**+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~**\r\n    **+ CategoryInfo          : InvalidOperation: (:) [], RuntimeException**\r\n    **+ FullyQualifiedErrorId : InvokeMethodOnNull**\r\n\r\n**RunId:**\r\n**Training model, hold tight...**\r\n**ERROR: argument --name\/-n: expected one argument**_****\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n\r\nhttps:\/\/aka.ms\/cli_ref\r\nRead more about the command in reference docs\r\nJob Status:\r\nERROR: argument --name\/-n: expected one argument\r\n\r\nTRY THIS:\r\naz ml job show --name my-job-id --query \"{Name:name,Jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace\r\nShow the status of a job using --query argument to execute a JMESPath query on the results of commands.\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: error when installing training model piece; Content: i was getting an error when logonscript.ps1 was running and trying to use grep in one line, but it could not find grep anywhere. so, i installed grep via chocolatey, and now the script goes further to line 267,and gives me the error below. grep executes but now the error says \"dataset with name 'mnist_opendataset' is not found\". any help troubleshooting this error will be appreciated, i am trying to demo this to a customer. next week. **text of the output when error is encountered:** installing amlarc-compute k8s extension was successful. warning: falling back to use azure cli login credentials. if you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in -sdk. library configuration succeeded warning: falling back to use azure cli login credentials. if you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in -sdk. class kubernetescompute: this is an experimental class, and may change at any time. please see https:\/\/aka.ms\/experimental for more information. class kubernetescompute: this is an experimental class, and may change at any time. please see https:\/\/aka.ms\/experimental for more information. found compute target: arc-ml \" training model: ..... ..... ......... ......... ......... (((((((((## ......... ..... (((((((#### ..... ...... #((######## ...... ....... ............. ########### ............. ....... ...................... ########### ...................... .................*..... ########### ....,*................. .........*******...... ((((((((((( ......*******......... ............ ((((((((((( (. ............ .((((((((((( (((((\/ (((((((((((( #(((((((## \/\/\/\/(((((((* ############## \/\/\/\/\/\/(((((. ,#############. ,**\/\/\/\/\/\/\/(( #############\/ *\/\/\/\/\/\/\/\/&%%%%%%%%%%%%%%%%%%%########## \/\/\/\/\/\/\/&&&%&%%%%%%%%%%%%%%%&%&&#######( \/\/\/\/&&&&&&&%%%%%%%%%%%%%&&&&&&&&%#### .(&&&&&&&&&&&&&&%%%%%%&&&&&&&&&&&&&#. \" warning: falling back to use azure cli login credentials. if you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to aka.ms\/aml-notebook-auth for different authentication mechanisms in -sdk. warning: command group 'ml job' is experimental and under development. reference and support levels: https:\/\/aka.ms\/cli_refstatus uploading src: 0%| | 0.00\/3.08k [00:00<?, ?b\/s] **error: code: usererror** **message: dataset with name 'mnist_opendataset' is not found.** **you cannot call a method on a null-valued expression.** **at c:\\temp\\logonscript.ps1:267 char:4** **+ $runid = ($job | grep '\\\"name\\\":').split('\\\"')[3]** **+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~** **+ categoryinfo : invalidoperation: (:) [], runtimeexception** **+ fullyqualifiederrorid : invokemethodonnull** **runid:** **training model, hold tight...** **error: argument --name\/-n: expected one argument**_**** try this: az ml job show --name my-job-id --query \"{name:name,jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace show the status of a job using --query argument to execute a jmespath query on the results of commands. https:\/\/aka.ms\/cli_ref read more about the command in reference docs job status: error: argument --name\/-n: expected one argument try this: az ml job show --name my-job-id --query \"{name:name,jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace show the status of a job using --query argument to execute a jmespath query on the results of commands. https:\/\/aka.ms\/cli_ref read more about the command in reference docs job status: error: argument --name\/-n: expected one argument try this: az ml job show --name my-job-id --query \"{name:name,jobstatus:status}\" --output table --resource-group my-resource-group --workspace-name my-workspace show the status of a job using --query argument to execute a jmespath query on the results of commands.",
        "Issue_original_content_gpt_summary":"The user encountered challenges when installing a training model piece, including an error when running logonscript.ps1, an inability to find grep, and an error when trying to show the status of a job using the --query argument.",
        "Issue_preprocessed_content":"Title: error when installing training model piece; Content: i was getting an error when was running and trying to use grep in one line, but it could not find grep anywhere. so, i installed grep via chocolatey, and now the script goes further to line ,and gives me the error below. grep executes but now the error says dataset with name is not found . any help troubleshooting this error will be appreciated, i am trying to demo this to a customer. next week. text of the output when error is encountered installing k s extension was successful. warning falling back to use azure cli login credentials. if you run your code in unattended mode, where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to for different authentication mechanisms in library configuration succeeded warning falling back to use azure cli login credentials. if you run your code in unattended mode, where you can't give a user input, then we recommend to use serviceprincipalauthentication or msiauthentication. please refer to for different authentication mechanisms in class kubernetescompute this is an experimental class, and may change at any time. please see for more information. class kubernetescompute this is an experimental class, and may change at any time. please see for more information. found compute target training model + + categoryinfo invalidoperation , runtimeexception + fullyqualifiederrorid invokemethodonnull runid training model, hold error argument expected one try this az ml job show name name,jobstatus status table show the status of a job using argument to execute a jmespath query on the results of commands. read more about the command in reference docs job status error argument expected one argument try this az ml job show name name,jobstatus status table show the status of a job using argument to execute a jmespath query on the results of commands. read more about the command in reference docs job status error argument expected one argument try this az ml job show name name,jobstatus status table show the status of a job using argument to execute a jmespath query on the results of commands."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/hi-ml\/issues\/335",
        "Issue_title":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False",
        "Issue_creation_time":1652108924000,
        "Issue_closed_time":1657547192000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Models that override  crossval_count with a value bigger than 1 automatically switch to train on AzureML even if user overrides --azureml=False\r\n\r\nThis behaviour is a bit confusing and I had to debug the code to understand what was happening. I would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what I want that is train locally.\r\n\r\nRepro with:\r\n\r\n\/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.DeepSMILECrck \r\n\r\nAlso the histopathology.DeepSMILECrck is not trainable because it does not have a default encoder type. Should we flag base classes as not trainable and throw an error?",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: models that override crossval_count with a value bigger than 1 automatically switch to train on even if user overrides --=false; Content: models that override crossval_count with a value bigger than 1 automatically switch to train on even if user overrides --=false this behaviour is a bit confusing and i had to debug the code to understand what was happening. i would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what i want that is train locally. repro with: \/home\/azureuser\/hi-ml\/hi-ml\/src\/health_ml\/runner.py --model=histopathology.deepsmilecrck also the histopathology.deepsmilecrck is not trainable because it does not have a default encoder type. should we flag base classes as not trainable and throw an error?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where models that override crossval_count with a value bigger than 1 automatically switch to train on even if user overrides --=false, which is confusing and unexpected.",
        "Issue_preprocessed_content":"Title: models that override with a value bigger than automatically switch to train on even if user overrides ; Content: models that override with a value bigger than automatically switch to train on even if user overrides this behaviour is a bit confusing and i had to debug the code to understand what was happening. i would expect the runner to fail if there are contradicting parameters instead of overriding them for me and doing the opposite of what i want that is train locally. repro with also the is not trainable because it does not have a default encoder type. should we flag base classes as not trainable and throw an error?"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/410",
        "Issue_title":"[BUG] Some links to notebooks in introduction are broken in 11_exploring_hyperparameters_on_azureml notebook",
        "Issue_creation_time":1573072566000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n\r\nThe introduction section of the 11_exploring_hyperparameters_on_azureml notebook under object detection includes two broken links:\r\n` [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb)`\r\n`[03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb)`\r\n\r\nThe master branch of this repo (which I am working from...please tell me that was intended...) does not contain these notebooks. \r\n\r\n### In which platform does it happen?\r\nAll.\r\n\r\n### How do we replicate the issue?\r\nClick the links\r\n\r\n### Expected behavior (i.e. solution)\r\nNotebooks are present or links are removed\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] some links to notebooks in introduction are broken in 11_exploring_hyperparameters_on_ notebook; Content: ### description the introduction section of the 11_exploring_hyperparameters_on_ notebook under object detection includes two broken links: ` [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb)` `[03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb)` the master branch of this repo (which i am working from...please tell me that was intended...) does not contain these notebooks. ### in which platform does it happen? all. ### how do we replicate the issue? click the links ### expected behavior (i.e. solution) notebooks are present or links are removed ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered broken links in the introduction section of the 11_exploring_hyperparameters_on_ notebook under object detection, which were not present in the master branch of the repository.",
        "Issue_preprocessed_content":"Title: some links to notebooks in introduction are broken in notebook; Content: description the introduction section of the notebook under object detection includes two broken links the master branch of this repo does not contain these notebooks. in which platform does it happen? all. how do we replicate the issue? click the links expected behavior notebooks are present or links are removed other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/409",
        "Issue_title":"[FEATURE_REQUEST] Provide guidance on how to obtain a subscription id in 11_exploring_hyperparameters_on_azureml notebook",
        "Issue_creation_time":1573072237000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n\r\nUsers need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. Some guidance within the notebook on how to obtain these values and fill in the strings would be helpful.\r\n\r\nIt would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to `get_or_create_workspace()` later on.\r\n\r\n### Expected behavior with the suggested feature\r\n\r\nUsers who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. Novice users receive some guidance on how to obtain their Azure subscription id without having to reference other notebooks.\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [feature_request] provide guidance on how to obtain a subscription id in 11_exploring_hyperparameters_on_ notebook; Content: ### description users need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. some guidance within the notebook on how to obtain these values and fill in the strings would be helpful. it would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to `get_or_create_workspace()` later on. ### expected behavior with the suggested feature users who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. novice users receive some guidance on how to obtain their azure subscription id without having to reference other notebooks. ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered challenges in obtaining a subscription ID and filling in the strings in the third code cell of the 11_exploring_hyperparameters_on_ notebook, and requested guidance on how to do so, as well as an error message if the strings are not filled in.",
        "Issue_preprocessed_content":"Title: provide guidance on how to obtain a subscription id in notebook; Content: description users need to modify the third code cell to specific a subscription id and the names that will be used for creating a resource group, workspace, etc. some guidance within the notebook on how to obtain these values and fill in the strings would be helpful. it would also be nice to throw an error in this code cell if users forget to fill in the values, so that users don't encounter a cryptic error from the call to later on. expected behavior with the suggested feature users who forget to fill in the string values in this code cell are alerted to the issue by an error message from this code cell. novice users receive some guidance on how to obtain their azure subscription id without having to reference other notebooks. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/408",
        "Issue_title":"[BUG] Link to 20_azure_workspace_setup.ipynb in 11_exploring_hyperparameters_on_azureml notebook is broken",
        "Issue_creation_time":1573071661000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\n\r\nThe 11_exploring_hyperparameters_on_azureml notebook contains the following link in markdown:\r\n`[20_azure_workspace_setup.ipynb](..\/..\/classification\/notebooks\/20_azure_workspace_setup.ipynb)`\r\n\r\nThe link does not resolve properly -- it appears the relative location of the notebook has changed.\r\n\r\n### In which platform does it happen?\r\nAll\r\n\r\n### How do we replicate the issue?\r\nClick the link\r\n\r\n### Expected behavior (i.e. solution)\r\nLink works\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] link to 20_azure_workspace_setup.ipynb in 11_exploring_hyperparameters_on_ notebook is broken; Content: ### description the 11_exploring_hyperparameters_on_ notebook contains the following link in markdown: `[20_azure_workspace_setup.ipynb](..\/..\/classification\/notebooks\/20_azure_workspace_setup.ipynb)` the link does not resolve properly -- it appears the relative location of the notebook has changed. ### in which platform does it happen? all ### how do we replicate the issue? click the link ### expected behavior (i.e. solution) link works ### other comments",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the link to the 20_azure_workspace_setup.ipynb in the 11_exploring_hyperparameters_on_ notebook was broken.",
        "Issue_preprocessed_content":"Title: link to in notebook is broken; Content: description the notebook contains the following link in markdown the link does not resolve properly it appears the relative location of the notebook has changed. in which platform does it happen? all how do we replicate the issue? click the link expected behavior link works other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/404",
        "Issue_title":"[FEATURE_REQUEST] AzureML may need to be updated 1.0.30->1.0.72?",
        "Issue_creation_time":1573068707000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\nThe current version of AzureML is a little dated \r\nhttps:\/\/github.com\/microsoft\/ComputerVision\/blob\/3e0631e0dc7d5ddbfc6283b1e89b3ce51f0bd449\/environment.yml#L41\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [feature_request] may need to be updated 1.0.30->1.0.72?; Content: ### description the current version of is a little dated https:\/\/github.com\/microsoft\/computervision\/blob\/3e0631e0dc7d5ddbfc6283b1e89b3ce51f0bd449\/environment.yml#l41",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with needing to update the version of a feature from 1.0.30 to 1.0.72.",
        "Issue_preprocessed_content":"Title: may need to be updated ; Content: description the current version of is a little dated"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/396",
        "Issue_title":"[FEATURE_REQUEST] Install utils_cv as a pip wheel in AzureML",
        "Issue_creation_time":1573065169000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Description\r\nIn https:\/\/github.com\/microsoft\/ComputerVision\/blob\/master\/scenarios\/detection\/11_exploring_hyperparameters_on_azureml.ipynb\r\nyou copy the whole directory in order to make use of the utils_cv\r\nThis is a bit cumbersome and unecesarily copies things around. You can create a pip wheel package of your utils_cv and add it as a dependency see here https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#add-private-pip-wheel-workspace--file-path--exist-ok-false-\r\n\r\n\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [feature_request] install utils_cv as a pip wheel in ; Content: ### description in https:\/\/github.com\/microsoft\/computervision\/blob\/master\/scenarios\/detection\/11_exploring_hyperparameters_on_.ipynb you copy the whole directory in order to make use of the utils_cv this is a bit cumbersome and unecesarily copies things around. you can create a pip wheel package of your utils_cv and add it as a dependency see here https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.environment(class)?view=azure-ml-py#add-private-pip-wheel-workspace--file-path--exist-ok-false-",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of having to copy a whole directory in order to make use of the utils_cv, and proposed creating a pip wheel package of the utils_cv and adding it as a dependency.",
        "Issue_preprocessed_content":"Title: install as a pip wheel in ; Content: description in you copy the whole directory in order to make use of the this is a bit cumbersome and unecesarily copies things around. you can create a pip wheel package of your and add it as a dependency see here"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/332",
        "Issue_title":"[BUG] Error in o16n with AzureML  notebooks",
        "Issue_creation_time":1569349581000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n\r\nThis is the error, it looks it is related to the deployment of ACI and AKS resources. \r\n\r\n\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:35:17.380577', 'duration': 1113.334717, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [26]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-26-21aec20dbbb2> in <module>\r\nE                 1 # Deploy the web service\r\nE           ----> 2 service.wait_for_deployment(show_output=True)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"AciDeploymentFailed\",\r\nE             \"message\": \"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Unhealthy\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"AciDeploymentFailed\\\",\\n  \\\"message\\\": \\\"Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: im-classif-websvc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<01:03,  1.01cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:44,  1.40cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:31,  1.92cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:04<01:13,  1.24s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<01:00,  1.07s\/cell]\r\nExecuting:  15%|\u2588\u258c        | 10\/65 [00:05<00:43,  1.27cell\/s]\r\nExecuting:  18%|\u2588\u258a        | 12\/65 [00:05<00:30,  1.72cell\/s]\r\nExecuting:  20%|\u2588\u2588        | 13\/65 [00:06<00:22,  2.26cell\/s]\r\nExecuting:  23%|\u2588\u2588\u258e       | 15\/65 [00:07<00:23,  2.15cell\/s]\r\nExecuting:  26%|\u2588\u2588\u258c       | 17\/65 [00:07<00:16,  2.87cell\/s]\r\nExecuting:  28%|\u2588\u2588\u258a       | 18\/65 [00:13<01:34,  2.00s\/cell]\r\nExecuting:  31%|\u2588\u2588\u2588       | 20\/65 [00:13<01:04,  1.43s\/cell]\r\nExecuting:  32%|\u2588\u2588\u2588\u258f      | 21\/65 [00:15<01:06,  1.51s\/cell]\r\nExecuting:  35%|\u2588\u2588\u2588\u258c      | 23\/65 [00:15<00:45,  1.08s\/cell]\r\nExecuting:  37%|\u2588\u2588\u2588\u258b      | 24\/65 [00:16<00:45,  1.11s\/cell]\r\nExecuting:  38%|\u2588\u2588\u2588\u258a      | 25\/65 [00:18<00:54,  1.37s\/cell]\r\nExecuting:  42%|\u2588\u2588\u2588\u2588\u258f     | 27\/65 [00:18<00:37,  1.01cell\/s]\r\nExecuting:  43%|\u2588\u2588\u2588\u2588\u258e     | 28\/65 [00:20<00:50,  1.37s\/cell]\r\nExecuting:  45%|\u2588\u2588\u2588\u2588\u258d     | 29\/65 [00:21<00:38,  1.07s\/cell]\r\nExecuting:  48%|\u2588\u2588\u2588\u2588\u258a     | 31\/65 [00:22<00:33,  1.01cell\/s]\r\nExecuting:  51%|\u2588\u2588\u2588\u2588\u2588     | 33\/65 [00:22<00:22,  1.39cell\/s]\r\nExecuting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 34\/65 [00:23<00:19,  1.61cell\/s]\r\nExecuting:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 35\/65 [00:23<00:14,  2.11cell\/s]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 37\/65 [00:23<00:10,  2.76cell\/s]\r\nExecuting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 38\/65 [00:23<00:07,  3.41cell\/s]\r\nExecuting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 40\/65 [00:24<00:05,  4.18cell\/s]\r\nExecuting:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 42\/65 [00:24<00:04,  5.02cell\/s]\r\nExecuting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 43\/65 [00:32<00:59,  2.70s\/cell]\r\nExecuting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 44\/65 [11:52<1:12:00, 205.75s\/cell]\r\nExecuting:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 45\/65 [11:52<48:01, 144.08s\/cell]  \r\nExecuting:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 46\/65 [11:53<32:00, 101.08s\/cell]\r\nExecuting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 47\/65 [11:53<21:14, 70.80s\/cell] \r\nExecuting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 48\/65 [11:53<14:03, 49.63s\/cell]\r\nExecuting:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 49\/65 [11:53<09:16, 34.79s\/cell]\r\nExecuting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 50\/65 [11:53<06:05, 24.40s\/cell]\r\nExecuting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 51\/65 [11:56<04:07, 17.70s\/cell]\r\nExecuting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 52\/65 [11:56<02:41, 12.44s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:32<25:30, 127.58s\/cell]\r\nExecuting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 53\/65 [18:33<04:12, 21.01s\/cell] \r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:108: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour..._time': '2019-09-24T17:58:40.389449', 'duration': 1402.445046, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [12]\":\r\nE           ---------------------------------------------------------------------------\r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               511                                           'Error:\\n'\r\nE           --> 512                                           '{}'.format(self.state, logs_response, error_response), logger=module_logger)\r\nE               513             print('{} service creation operation finished, operation \"{}\"'.format(self._webservice_type,\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           WebserviceException                       Traceback (most recent call last)\r\nE           <ipython-input-12-ea5338712650> in <module>\r\nE                 8         deployment_target = aks_target\r\nE                 9     )\r\nE           ---> 10     aks_service.wait_for_deployment(show_output = True)\r\nE                11     print(f\"The web service is {aks_service.state}\")\r\nE                12 else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/azureml\/core\/webservice\/webservice.py in wait_for_deployment(self, show_output)\r\nE               519                                           'Current state is {}'.format(self.state), logger=module_logger)\r\nE               520             else:\r\nE           --> 521                 raise WebserviceException(e.message, logger=module_logger)\r\nE               522 \r\nE               523     def _wait_for_operation_to_complete(self, show_output):\r\nE           \r\nE           WebserviceException: WebserviceException:\r\nE           \tMessage: Service deployment polling reached non-successful terminal state, current service state: Failed\r\nE           More information can be found using '.get_logs()'\r\nE           Error:\r\nE           {\r\nE             \"code\": \"KubernetesDeploymentFailed\",\r\nE             \"statusCode\": 400,\r\nE             \"message\": \"Kubernetes Deployment failed\",\r\nE             \"details\": [\r\nE               {\r\nE                 \"code\": \"CrashLoopBackOff\",\r\nE                 \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\"\r\nE               }\r\nE             ]\r\nE           }\r\nE           \tInnerException None\r\nE           \tErrorResponse \r\nE           {\r\nE               \"error\": {\r\nE                   \"message\": \"Service deployment polling reached non-successful terminal state, current service state: Failed\\nMore information can be found using '.get_logs()'\\nError:\\n{\\n  \\\"code\\\": \\\"KubernetesDeploymentFailed\\\",\\n  \\\"statusCode\\\": 400,\\n  \\\"message\\\": \\\"Kubernetes Deployment failed\\\",\\n  \\\"details\\\": [\\n    {\\n      \\\"code\\\": \\\"CrashLoopBackOff\\\",\\n      \\\"message\\\": \\\"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\\\nPlease check the logs for your container instance: aks-cpu-image-classif-web-svc. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\\\nYou can also try to run image amlnotebookw04a7b513.azurecr.io\/image-classif-resnet18-f48:1 locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\\\"\\n    }\\n  ]\\n}\"\r\nE               }\r\nE           }\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:192: PapermillExecutionError\r\n```\r\n\r\nFYI @PatrickBue @jiata any idea of what could be happening?\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] error in o16n with notebooks; Content: ### description this is the error, it looks it is related to the deployment of aci and aks resources.",
        "Issue_original_content_gpt_summary":"The user encountered an error in o16n with notebooks related to the deployment of aci and aks resources.",
        "Issue_preprocessed_content":"Title: error in o n with notebooks; Content: description describe your in detail this is the error, it looks it is related to the deployment of aci and aks resources. fyi any idea of what could be happening? in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. how do we replicate the issue? please be specific as possible . for example create a linux data science virtual machine one azure with v gpu run unit test expected behavior for example the test for gpu model training should pass successfully. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/computervision-recipes\/issues\/320",
        "Issue_title":"[BUG] pipeline azureml-notebook-test-linux-cpu failing",
        "Issue_creation_time":1568285443000,
        "Issue_closed_time":1569234937000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\n<!--- Describe your issue\/bug\/request in detail -->\r\n```\r\n.FFF.                                                                    [100%]\r\n=================================== FAILURES ===================================\r\n_____________________________ test_21_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_21_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"21_deployment_on_azure_container_instances\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:58: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:40.699401', 'duration': 5.033488, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [2]\":\r\nE           ---------------------------------------------------------------------------\r\nE           SSLError                                  Traceback (most recent call last)\r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1317                 h.request(req.get_method(), req.selector, req.data, headers,\r\nE           -> 1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in request(self, method, url, body, headers, encode_chunked)\r\nE              1238         \"\"\"Send a complete request to the server.\"\"\"\r\nE           -> 1239         self._send_request(method, url, body, headers, encode_chunked)\r\nE              1240 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_request(self, method, url, body, headers, encode_chunked)\r\nE              1284             body = _encode(body, 'body')\r\nE           -> 1285         self.endheaders(body, encode_chunked=encode_chunked)\r\nE              1286 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in endheaders(self, message_body, encode_chunked)\r\nE              1233             raise CannotSendHeader()\r\nE           -> 1234         self._send_output(message_body, encode_chunked=encode_chunked)\r\nE              1235 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in _send_output(self, message_body, encode_chunked)\r\nE              1025         del self._buffer[:]\r\nE           -> 1026         self.send(msg)\r\nE              1027 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in send(self, data)\r\nE               963             if self.auto_open:\r\nE           --> 964                 self.connect()\r\nE               965             else:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/http\/client.py in connect(self)\r\nE              1399             self.sock = self._context.wrap_socket(self.sock,\r\nE           -> 1400                                                   server_hostname=server_hostname)\r\nE              1401             if not self._context.check_hostname and self._check_hostname:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in wrap_socket(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\r\nE               406                          server_hostname=server_hostname,\r\nE           --> 407                          _context=self, _session=session)\r\nE               408 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in __init__(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\r\nE               816                         raise ValueError(\"do_handshake_on_connect should not be specified for non-blocking sockets\")\r\nE           --> 817                     self.do_handshake()\r\nE               818 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self, block)\r\nE              1076                 self.settimeout(None)\r\nE           -> 1077             self._sslobj.do_handshake()\r\nE              1078         finally:\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/ssl.py in do_handshake(self)\r\nE               688         \"\"\"Start the SSL\/TLS handshake.\"\"\"\r\nE           --> 689         self._sslobj.do_handshake()\r\nE               690         if self.context.check_hostname:\r\nE           \r\nE           SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)\r\nE           \r\nE           During handling of the above exception, another exception occurred:\r\nE           \r\nE           URLError                                  Traceback (most recent call last)\r\nE           <ipython-input-2-2e2a8adec5e2> in <module>\r\nE           ----> 1 learn = model_to_learner(models.resnet18(pretrained=True), IMAGENET_IM_SIZE)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in resnet18(pretrained, progress, **kwargs)\r\nE               229     \"\"\"\r\nE               230     return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\r\nE           --> 231                    **kwargs)\r\nE               232 \r\nE               233 \r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torchvision\/models\/resnet.py in _resnet(arch, block, layers, pretrained, progress, **kwargs)\r\nE               215     if pretrained:\r\nE               216         state_dict = load_state_dict_from_url(model_urls[arch],\r\nE           --> 217                                               progress=progress)\r\nE               218         model.load_state_dict(state_dict)\r\nE               219     return model\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in load_state_dict_from_url(url, model_dir, map_location, progress)\r\nE               460         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\r\nE               461         hash_prefix = HASH_REGEX.search(filename).group(1)\r\nE           --> 462         _download_url_to_file(url, cached_file, hash_prefix, progress=progress)\r\nE               463     return torch.load(cached_file, map_location=map_location)\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/torch\/hub.py in _download_url_to_file(url, dst, hash_prefix, progress)\r\nE               370 def _download_url_to_file(url, dst, hash_prefix, progress):\r\nE               371     file_size = None\r\nE           --> 372     u = urlopen(url)\r\nE               373     meta = u.info()\r\nE               374     if hasattr(meta, 'getheaders'):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)\r\nE               221     else:\r\nE               222         opener = _opener\r\nE           --> 223     return opener.open(url, data, timeout)\r\nE               224 \r\nE               225 def install_opener(opener):\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in open(self, fullurl, data, timeout)\r\nE               524             req = meth(req)\r\nE               525 \r\nE           --> 526         response = self._open(req, data)\r\nE               527 \r\nE               528         # post-process response\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _open(self, req, data)\r\nE               542         protocol = req.type\r\nE               543         result = self._call_chain(self.handle_open, protocol, protocol +\r\nE           --> 544                                   '_open', req)\r\nE               545         if result:\r\nE               546             return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in _call_chain(self, chain, kind, meth_name, *args)\r\nE               502         for handler in handlers:\r\nE               503             func = getattr(handler, meth_name)\r\nE           --> 504             result = func(*args)\r\nE               505             if result is not None:\r\nE               506                 return result\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in https_open(self, req)\r\nE              1359         def https_open(self, req):\r\nE              1360             return self.do_open(http.client.HTTPSConnection, req,\r\nE           -> 1361                 context=self._context, check_hostname=self._check_hostname)\r\nE              1362 \r\nE              1363         https_request = AbstractHTTPHandler.do_request_\r\nE           \r\nE           \/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/urllib\/request.py in do_open(self, http_class, req, **http_conn_args)\r\nE              1318                           encode_chunked=req.has_header('Transfer-encoding'))\r\nE              1319             except OSError as err: # timeout error\r\nE           -> 1320                 raise URLError(err)\r\nE              1321             r = h.getresponse()\r\nE              1322         except:\r\nE           \r\nE           URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)>\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/65 [00:00<?, ?cell\/s]\r\nExecuting:   2%|\u258f         | 1\/65 [00:00<00:56,  1.14cell\/s]\r\nExecuting:   5%|\u258d         | 3\/65 [00:01<00:39,  1.58cell\/s]\r\nExecuting:   8%|\u258a         | 5\/65 [00:01<00:27,  2.16cell\/s]\r\nExecuting:   9%|\u2589         | 6\/65 [00:03<01:00,  1.03s\/cell]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:04<00:47,  1.19cell\/s]\r\nExecuting:  12%|\u2588\u258f        | 8\/65 [00:05<00:35,  1.59cell\/s]\r\n_____________________________ test_22_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_22_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\r\n            \"22_deployment_on_azure_kubernetes_service\"\r\n        ]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:83: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:46.959285', 'duration': 5.817276, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-af5043783823> in <module>\r\nE           ----> 1 docker_image = ws.images[\"image-classif-resnet18-f48\"]\r\nE           \r\nE           KeyError: 'image-classif-resnet18-f48'\r\n\r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:188: PapermillExecutionError\r\n----------------------------- Captured stderr call -----------------------------\r\n\r\nExecuting:   0%|          | 0\/36 [00:00<?, ?cell\/s]\r\nExecuting:   3%|\u258e         | 1\/36 [00:00<00:30,  1.16cell\/s]\r\nExecuting:  11%|\u2588         | 4\/36 [00:02<00:24,  1.32cell\/s]\r\nExecuting:  19%|\u2588\u2589        | 7\/36 [00:02<00:15,  1.84cell\/s]\r\nExecuting:  25%|\u2588\u2588\u258c       | 9\/36 [00:02<00:10,  2.52cell\/s]\r\nExecuting:  31%|\u2588\u2588\u2588       | 11\/36 [00:03<00:10,  2.47cell\/s]\r\nExecuting:  33%|\u2588\u2588\u2588\u258e      | 12\/36 [00:04<00:16,  1.50cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:12,  1.81cell\/s]\r\nExecuting:  39%|\u2588\u2588\u2588\u2589      | 14\/36 [00:05<00:09,  2.41cell\/s]\r\n_____________________________ test_23_notebook_run _____________________________\r\n\r\nclassification_notebooks = {'00_webcam': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/00_webcam.ipynb', '01_training_introduction': '\/home\/vsts\/...3_training_accuracy_vs_speed': '\/home\/vsts\/work\/1\/s\/classification\/notebooks\/03_training_accuracy_vs_speed.ipynb', ...}\r\nsubscription_id = '***'\r\nresource_group = 'amlnotebookrg', workspace_name = 'amlnotebookws'\r\nworkspace_region = '***2'\r\n\r\n    @pytest.mark.azuremlnotebooks\r\n    def test_23_notebook_run(\r\n        classification_notebooks,\r\n        subscription_id,\r\n        resource_group,\r\n        workspace_name,\r\n        workspace_region,\r\n    ):\r\n        notebook_path = classification_notebooks[\"23_aci_aks_web_service_testing\"]\r\n        pm.execute_notebook(\r\n            notebook_path,\r\n            OUTPUT_NOTEBOOK,\r\n            parameters=dict(\r\n                PM_VERSION=pm.__version__,\r\n                subscription_id=subscription_id,\r\n                resource_group=resource_group,\r\n                workspace_name=workspace_name,\r\n                workspace_region=workspace_region,\r\n            ),\r\n>           kernel_name=KERNEL_NAME,\r\n        )\r\n\r\ntests\/smoke\/test_azureml_notebooks.py:106: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/usr\/share\/miniconda\/envs\/cv\/lib\/python3.6\/site-packages\/papermill\/execute.py:104: in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nnb = {'cells': [{'cell_type': 'code', 'metadata': {'inputHidden': True, 'hide_input': True}, 'execution_count': None, 'sour...end_time': '2019-09-12T10:19:53.061402', 'duration': 6.023939, 'exception': True}}, 'nbformat': 4, 'nbformat_minor': 2}\r\noutput_path = 'output.ipynb'\r\n\r\n    def raise_for_execution_errors(nb, output_path):\r\n        \"\"\"Assigned parameters into the appropriate place in the input notebook\r\n    \r\n        Parameters\r\n        ----------\r\n        nb : NotebookNode\r\n           Executable notebook object\r\n        output_path : str\r\n           Path to write executed notebook\r\n        \"\"\"\r\n        error = None\r\n        for cell in nb.cells:\r\n            if cell.get(\"outputs\") is None:\r\n                continue\r\n    \r\n            for output in cell.outputs:\r\n                if output.output_type == \"error\":\r\n                    error = PapermillExecutionError(\r\n                        exec_count=cell.execution_count,\r\n                        source=cell.source,\r\n                        ename=output.ename,\r\n                        evalue=output.evalue,\r\n                        traceback=output.traceback,\r\n                    )\r\n                    break\r\n    \r\n        if error:\r\n            # Write notebook back out with the Error Message at the top of the Notebook.\r\n            error_msg = ERROR_MESSAGE_TEMPLATE % str(error.exec_count)\r\n            error_msg_cell = nbformat.v4.new_code_cell(\r\n                source=\"%%html\\n\" + error_msg,\r\n                outputs=[\r\n                    nbformat.v4.new_output(output_type=\"display_data\", data={\"text\/html\": error_msg})\r\n                ],\r\n                metadata={\"inputHidden\": True, \"hide_input\": True},\r\n            )\r\n            nb.cells = [error_msg_cell] + nb.cells\r\n            write_ipynb(nb, output_path)\r\n>           raise error\r\nE           papermill.exceptions.PapermillExecutionError: \r\nE           ---------------------------------------------------------------------------\r\nE           Exception encountered at \"In [6]\":\r\nE           ---------------------------------------------------------------------------\r\nE           KeyError                                  Traceback (most recent call last)\r\nE           <ipython-input-6-883397ed965d> in <module>\r\nE                 1 # Retrieve the web services\r\nE           ----> 2 aci_service = ws.webservices['im-classif-websvc']\r\nE                 3 aks_service = ws.webservices['aks-cpu-image-classif-web-svc']\r\nE           \r\nE           KeyError: 'im-classif-websvc'\r\n```\r\n\r\n\r\n### In which platform does it happen?\r\n<!--- Describe the platform where the issue is happening (use a list if needed) -->\r\n<!--- For example: -->\r\n<!--- * Windows\/Linux.  -->\r\n<!--- * CPU\/GPU.  -->\r\n<!--- * Azure Data Science Virtual Machine. -->\r\n\r\n### How do we replicate the issue?\r\n<!--- Please be specific as possible (use a list if needed). -->\r\n<!--- For example: -->\r\n<!--- * Create a Linux Data Science Virtual Machine one Azure with V100 GPU -->\r\n<!--- * Run unit test `test_classification_data.py` -->\r\n<!--- * ... -->\r\n\r\n### Expected behavior (i.e. solution)\r\n<!--- For example:  -->\r\n<!--- * The test `test_is_data_multilabel` for GPU model training should pass successfully. -->\r\n\r\n### Other Comments\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] pipeline -notebook-test-linux-cpu failing",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the pipeline -notebook-test-linux-cpu failing.",
        "Issue_preprocessed_content":"Title: pipeline failing; Content: description describe your in detail in which platform does it happen? describe the platform where the issue is happening for example azure data science virtual machine. how do we replicate the issue? please be specific as possible . for example create a linux data science virtual machine one azure with v gpu run unit test expected behavior for example the test for gpu model training should pass successfully. other comments"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/InnerEye-DeepLearning\/issues\/389",
        "Issue_title":"Memory utilization metrics are not correctly visible in AzureML",
        "Issue_creation_time":1612434474000,
        "Issue_closed_time":1613669349000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Run 2236 in experiment \"master\" in RadiomicsNN: \r\n- Only metrics for 3 out of the 4 GPUs are visible\r\n- The MemAllocated and MemReserved metrics are all zero and hence meaningless.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: memory utilization metrics are not correctly visible in ; Content: run 2236 in experiment \"master\" in radiomicsnn: - only metrics for 3 out of the 4 gpus are visible - the memallocated and memreserved metrics are all zero and hence meaningless.",
        "Issue_original_content_gpt_summary":"The user encountered challenges with memory utilization metrics not correctly visible in run 2236 in experiment \"master\" in radiomicsnn, with only metrics for 3 out of the 4 gpus visible and the memallocated and memreserved metrics all zero and hence meaningless.",
        "Issue_preprocessed_content":"Title: memory utilization metrics are not correctly visible in ; Content: run in experiment master in radiomicsnn only metrics for out of the gpus are visible the memallocated and memreserved metrics are all zero and hence meaningless."
    },
    {
        "Issue_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/175",
        "Issue_title":"azure credentials module should lazy-import any azureml.core modules",
        "Issue_creation_time":1589931356000,
        "Issue_closed_time":1589931676000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"A regression was introduced in https:\/\/github.com\/augerai\/a2ml\/commit\/c4f89d282fd951defe3e1d51d35386be2c55c7d9#diff-1cd4abe6fbca8804140fbb9b340e3cc8, where this import statement causes azureml.core.authentication to be loaded when it's not needed if you only have the default set of a2ml dependencies installed.\r\n\r\n```\r\n~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages\/a2ml\/api\/azure\/credentials.py\", line 4, in <module>\r\n    from azureml.core.authentication import ServicePrincipalAuthentication, InteractiveLoginAuthentication\r\nModuleNotFoundError: No module named 'azureml'\r\n```\r\n\r\nThe following import statement could be added around L34, right before `InteractiveLoginAuthentication` is called:\r\n\r\n```python\r\nfrom azureml.core.authentication import InteractiveLoginAuthentication\r\n```\r\n\r\nThen this could be removed from the top:\r\n\r\n```python\r\nfrom azureml.core.authentication import ServicePrincipalAuthentication, InteractiveLoginAuthentication\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: azure credentials module should lazy-import any .core modules; Content: a regression was introduced in https:\/\/github.com\/augerai\/a2ml\/commit\/c4f89d282fd951defe3e1d51d35386be2c55c7d9#diff-1cd4abe6fbca8804140fbb9b340e3cc8, where this import statement causes .core.authentication to be loaded when it's not needed if you only have the default set of a2ml dependencies installed. ``` ~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages\/a2ml\/api\/azure\/credentials.py\", line 4, in from .core.authentication import serviceprincipalauthentication, interactiveloginauthentication modulenotfounderror: no module named '' ``` the following import statement could be added around l34, right before `interactiveloginauthentication` is called: ```python from .core.authentication import interactiveloginauthentication ``` then this could be removed from the top: ```python from .core.authentication import serviceprincipalauthentication, interactiveloginauthentication ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the azure credentials module should lazy-import any .core modules, and a regression was introduced in a commit which caused a ModuleNotFoundError when trying to import the interactiveloginauthentication module.",
        "Issue_preprocessed_content":"Title: azure credentials module should any modules; Content: a regression was introduced in where this import statement causes to be loaded when it's not needed if you only have the default set of a ml dependencies installed. the following import statement could be added around l , right before is called then this could be removed from the top"
    },
    {
        "Issue_link":"https:\/\/github.com\/augerai\/a2ml\/issues\/173",
        "Issue_title":"Warning message about hyperdrive loading with azureml_run_type_providers",
        "Issue_creation_time":1589926894000,
        "Issue_closed_time":1597072927000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"If you run any command that uses azureml (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message:\r\n\r\n```\r\nFailure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), Requirement.parse('flake8<=3.7.9,>=3.1.0; python_version >= \"3.6\"')).\r\n```\r\n\r\n**Expected Behavior**\r\nNo warning message should be printed.\r\n\r\n**Steps to Reproduce the Issue**\r\n1. From latest master branch in a fresh virtualenv run: `make build install`\r\n2. `cd \/path\/to\/azure\/a2ml-project`\r\n3. `a2ml experiment leaderboard`\r\n4. Observe the warning message above.\r\n\r\n\r\n**Environment Details:**\r\n - OS: macOS 10.15\r\n - A2ML Version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34\r\n - Python Version: 3.7.7\r\n",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: warning message about hyperdrive loading with _run_type_providers; if you run any command that uses (i.e. `a2ml experiment leaderboard`, `a2ml model predict ...`), it prints out this strange warning message: ``` failure while loading _run_type_providers. failed to load entrypoint hyperdrive = .train.hyperdrive:hyperdriverun._from_run_dto with exception (flake8 3.8.1 (~\/.virtualenvs\/a2ml\/lib\/python3.7\/site-packages), requirement.parse('flake8<=3.7.9,>=3.1.0; Content: python_version >= \"3.6\"')). ``` **expected behavior** no warning message should be printed. **steps to reproduce the issue** 1. from latest master branch in a fresh virtualenv run: `make build install` 2. `cd \/path\/to\/azure\/a2ml-project` 3. `a2ml experiment leaderboard` 4. observe the warning message above. **environment details:** - os: macos 10.15 - a2ml version: master branch rev 6fe45a4619e0fc80efde5c84015afbfb91b54d34 - python version: 3.7.7",
        "Issue_original_content_gpt_summary":"The user encountered a strange warning message when running a command that uses _run_type_providers, which was not the expected behavior, and the issue was reproduced in a MacOS 10.15 environment with a2ml version from the master branch and Python version 3.7.7.",
        "Issue_preprocessed_content":"Title: warning message about hyperdrive loading with ; Content: if you run any command that uses , it prints out this strange warning message expected behavior no warning message should be printed. steps to reproduce the issue . from latest master branch in a fresh virtualenv run . . . observe the warning message above. environment details os macos a ml version master branch rev fe a e fc efde c afbfb b d python version"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/lightgbm-benchmark\/issues\/27",
        "Issue_title":"Show lightgbm logs in the logs in AzureML",
        "Issue_creation_time":1630081759000,
        "Issue_closed_time":1630110931000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in AzureML",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: show lightgbm logs in the logs in ; Content: current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of not being able to view LightGBM logs in the logs, as the current execution lets LightGBM handle its own logs, which are likely printed in stdout, but do not show up in the logs.",
        "Issue_preprocessed_content":"Title: show lightgbm logs in the logs in ; Content: current execution lets lightgbm handle its own logs, they are likely printed in stdout, but don't show up in"
    },
    {
        "Issue_link":"https:\/\/github.com\/rapidsai\/cloud-ml-examples\/issues\/165",
        "Issue_title":"azureml-sdk downgrades pyarrow to 3.0.0 which breaks cudf",
        "Issue_creation_time":1655998483000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Steps to reproduce\r\n\r\n1. Create a fresh RAPIDS conda environment <br\/> `conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.8 cudatoolkit=11.5`\r\n2. `conda activate rapids-22.06`\r\n3. `conda list | grep pyarrow` shows 7.0.0 installed\r\n4. Launch python\/ipython and `import cudf` should work\r\n5. `pip install azureml-sdk`\r\n6. Launch python\/ipython and `import cudf` fails\r\n7. `conda list | grep pyarrow` shows 3.0.0 installed\r\n\r\n#### Error:\r\n```\r\n$ python -m cudf\r\nTraceback (most recent call last):\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 185, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 144, in _get_module_details\r\n    return _get_module_details(pkg_main_name, error)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 111, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/__init__.py\", line 13, in <module>\r\n    from cudf import api, core, datasets, testing\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/datasets.py\", line 7, in <module>\r\n    from cudf._lib.transform import bools_to_mask\r\n  File \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/_lib\/__init__.py\", line 4, in <module>\r\n    from . import (\r\n  File \"cudf\/_lib\/avro.pyx\", line 1, in init cudf._lib.avro\r\n  File \"cudf\/_lib\/column.pyx\", line 1, in init cudf._lib.column\r\n  File \"cudf\/_lib\/scalar.pyx\", line 37, in init cudf._lib.scalar\r\n  File \"cudf\/_lib\/interop.pyx\", line 1, in init cudf._lib.interop\r\nAttributeError: module 'pyarrow.lib' has no attribute 'MonthDayNanoIntervalArray'\r\n```",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: -sdk downgrades pyarrow to 3.0.0 which breaks cudf; Content: ### steps to reproduce 1. create a fresh rapids conda environment `conda create -n rapids-22.06 -c rapidsai -c nvidia -c conda-forge rapids=22.06 python=3.8 cudatoolkit=11.5` 2. `conda activate rapids-22.06` 3. `conda list | grep pyarrow` shows 7.0.0 installed 4. launch python\/ipython and `import cudf` should work 5. `pip install -sdk` 6. launch python\/ipython and `import cudf` fails 7. `conda list | grep pyarrow` shows 3.0.0 installed #### error: ``` $ python -m cudf traceback (most recent call last): file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 185, in _run_module_as_main mod_name, mod_spec, code = _get_module_details(mod_name, _error) file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 144, in _get_module_details return _get_module_details(pkg_main_name, error) file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/runpy.py\", line 111, in _get_module_details __import__(pkg_name) file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/__init__.py\", line 13, in from cudf import api, core, datasets, testing file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/datasets.py\", line 7, in from cudf._lib.transform import bools_to_mask file \"\/home\/mmccarty\/miniconda3\/envs\/cloud-ml-examples-test\/lib\/python3.8\/site-packages\/cudf\/_lib\/__init__.py\", line 4, in from . import ( file \"cudf\/_lib\/avro.pyx\", line 1, in init cudf._lib.avro file \"cudf\/_lib\/column.pyx\", line 1, in init cudf._lib.column file \"cudf\/_lib\/scalar.pyx\", line 37, in init cudf._lib.scalar file \"cudf\/_lib\/interop.pyx\", line 1, in init cudf._lib.interop attributeerror: module 'pyarrow.lib' has no attribute 'monthdaynanointervalarray' ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running the 'pip install -sdk' command downgraded pyarrow to 3.0.0, which caused cudf to fail.",
        "Issue_preprocessed_content":"Title: downgrades pyarrow to which breaks cudf; Content: steps to reproduce . create a fresh rapids conda environment . . shows installed . launch and should work . . launch and fails . shows installed error"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/onnxruntime\/issues\/14030",
        "Issue_title":"CUDA error 46 with CUDA 11.6 on Azure ML Linux image",
        "Issue_creation_time":1671527243000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Describe the issue\r\n\r\n I am trying to run onnxruntime-gpu on an Azure Machine Learning instance with this base image: [openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04](https:\/\/github.com\/Azure\/AzureML-Containers\/blob\/master\/base\/gpu\/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04\/Dockerfile) using CUDA. When trying to create an inference session I get this error:\r\n\r\n```\r\n  File \"\/azureml-envs\/azureml_4ab38fdb3b18635e56f7e63921a429e8\/lib\/python3.9\/site-packages\/onnxruntime\/capi\/onnxruntime_inference_collection.py\", line 347, in __init__\r\n    self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n  File \"\/azureml-envs\/azureml_4ab38fdb3b18635e56f7e63921a429e8\/lib\/python3.9\/site-packages\/onnxruntime\/capi\/onnxruntime_inference_collection.py\", line 395, in _create_inference_session\r\n    sess.initialize_session(providers, provider_options, disabled_optimizers)\r\nRuntimeError: \/onnxruntime_src\/onnxruntime\/core\/providers\/cuda\/cuda_call.cc:122 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] \/onnxruntime_src\/onnxruntime\/core\/providers\/cuda\/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 46: all CUDA-capable devices are busy or unavailable ; GPU=0 ; hostname=c7b375a29d54407dad59b0dd621129a7000000 ; expr=cudaDeviceSynchronize(); \r\n```\r\n\r\nI previously had this working with near identical environments using earlier versions of CUDA (e.g. [11.3](https:\/\/github.com\/Azure\/AzureML-Containers\/blob\/master\/base\/gpu\/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04\/Dockerfile) with onnxruntime-gpu 1.12) which makes me think it is some error between CUDA versions and onnxruntime rather than a general configuration error.\r\n\r\nThe only potential difference I can see is that the docker image uses cudnn 8.4 while the compatibility list for onnxruntime says cudnn 8.2, though I think 8.4 should be back compatible? The error given doesn't really seem to match this case either\r\n\r\nAny advice would be much appreciated!\r\n\r\n### To reproduce\r\n\r\nInstall onnxruntime-gpu in a conda env with the above dockerfile and try to create an inference session\r\n\r\n### Urgency\r\n\r\n_No response_\r\n\r\n### Platform\r\n\r\nLinux\r\n\r\n### OS Version\r\n\r\n20.04\r\n\r\n### ONNX Runtime Installation\r\n\r\nReleased Package\r\n\r\n### ONNX Runtime Version or Commit ID\r\n\r\n1.13 and 1.12\r\n\r\n### ONNX Runtime API\r\n\r\nPython\r\n\r\n### Architecture\r\n\r\nX64\r\n\r\n### Execution Provider\r\n\r\nCUDA\r\n\r\n### Execution Provider Library Version\r\n\r\nCUDA 11.6",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: cuda error 46 with cuda 11.6 on linux image; Content: ### describe the issue i am trying to run onnxruntime-gpu on an instance with this base image: [openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04](https:\/\/github.com\/azure\/-containers\/blob\/master\/base\/gpu\/openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04\/dockerfile) using cuda. when trying to create an inference session i get this error: ``` file \"\/-envs\/_4ab38fdb3b18635e56f7e63921a429e8\/lib\/python3.9\/site-packages\/onnxruntime\/capi\/onnxruntime_inference_collection.py\", line 347, in __init__ self._create_inference_session(providers, provider_options, disabled_optimizers) file \"\/-envs\/_4ab38fdb3b18635e56f7e63921a429e8\/lib\/python3.9\/site-packages\/onnxruntime\/capi\/onnxruntime_inference_collection.py\", line 395, in _create_inference_session sess.initialize_session(providers, provider_options, disabled_optimizers) runtimeerror: \/onnxruntime_src\/onnxruntime\/core\/providers\/cuda\/cuda_call.cc:122 bool onnxruntime::cudacall(errtype, const char*, const char*, errtype, const char*) [with errtype = cudaerror; bool thrw = true] \/onnxruntime_src\/onnxruntime\/core\/providers\/cuda\/cuda_call.cc:116 bool onnxruntime::cudacall(errtype, const char*, const char*, errtype, const char*) [with errtype = cudaerror; bool thrw = true] cuda failure 46: all cuda-capable devices are busy or unavailable ; gpu=0 ; hostname=c7b375a29d54407dad59b0dd621129a7000000 ; expr=cudadevicesynchronize(); ``` i previously had this working with near identical environments using earlier versions of cuda (e.g. [11.3](https:\/\/github.com\/azure\/-containers\/blob\/master\/base\/gpu\/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04\/dockerfile) with onnxruntime-gpu 1.12) which makes me think it is some error between cuda versions and onnxruntime rather than a general configuration error. the only potential difference i can see is that the docker image uses cudnn 8.4 while the compatibility list for onnxruntime says cudnn 8.2, though i think 8.4 should be back compatible? the error given doesn't really seem to match this case either any advice would be much appreciated! ### to reproduce install onnxruntime-gpu in a conda env with the above dockerfile and try to create an inference session ### urgency _no response_ ### platform linux ### os version 20.04 ### onnx runtime installation released package ### onnx runtime version or commit id 1.13 and 1.12 ### onnx runtime api python ### architecture x64 ### execution provider cuda ### execution provider library version cuda 11.6",
        "Issue_original_content_gpt_summary":"The user is encountering an error when trying to create an inference session with onnxruntime-gpu on an instance with a base image of openmpi4.1.0-cuda11.6-cudnn8-ubuntu20.04.",
        "Issue_preprocessed_content":"Title: cuda error with cuda on linux image; Content: describe the issue i am trying to run on an instance with this base image using cuda. when trying to create an inference session i get this error i previously had this working with near identical environments using earlier versions of cuda with which makes me think it is some error between cuda versions and onnxruntime rather than a general configuration error. the only potential difference i can see is that the docker image uses cudnn while the compatibility list for onnxruntime says cudnn though i think should be back compatible? the error given doesn't really seem to match this case either any advice would be much appreciated! to reproduce install in a conda env with the above dockerfile and try to create an inference session urgency platform linux os version . onnx runtime installation released package onnx runtime version or commit id . and onnx runtime api python architecture x execution provider cuda execution provider library version cuda"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/nni\/issues\/3518",
        "Issue_title":"Training extremely slow with Azure Machine Learning",
        "Issue_creation_time":1617867548000,
        "Issue_closed_time":1662517763000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"**Environment**:\r\n- NNI version: 2.0\r\n- NNI mode (local|remote|pai): remote\r\n- Client OS: Windows 10\r\n- Server OS (for remote mode only): Linux\r\n- Python version: 3.6.12\r\n- PyTorch\/TensorFlow version:  PyTorch1.7.1\r\n- Is conda\/virtualenv\/venv used?: conda\r\n- Is running in Docker?: No\r\n\r\n**Log message**:\r\n - nnimanager.log: \r\n [2021-04-07 15:24:48] INFO [ 'Datastore initialization done' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer start' ]\r\n[2021-04-07 15:24:48] INFO [ 'RestServer base port is 8086' ]\r\n[2021-04-07 15:24:48] INFO [ 'Rest server listening on: http:\/\/0.0.0.0:8086' ]\r\n[2021-04-07 15:24:51] INFO [ 'NNIManager setClusterMetadata, key: aml_config, value: {\"subscriptionId\":\"xxxxxxxxxxxx\",\"resourceGroup\":\"xxxxxxxxxxxxxxx\",\"workspaceName\":\"xxxxxxxxxxxxxx\",\"computeTarget\":\"xxxxxxxxxxxxxxxx\"}' ]\r\n[2021-04-07 15:24:53] INFO [ 'NNIManager setClusterMetadata, key: nni_manager_ip, value: {\"nniManagerIp\":\"10.194.188.18\"}' ]\r\n[2021-04-07 15:24:55] INFO [ 'NNIManager setClusterMetadata, key: trial_config, value: {\"command\":\"python3 mnist.py\",\"codeDir\":\"C:\\\\\\\\Users\\\\\\\\yanmi\\\\\\\\nni\\\\\\\\examples\\\\\\\\trials\\\\\\\\mnist-pytorch\\\\\\\\.\",\"image\":\"msranni\/nni\"}' ]\r\n[2021-04-07 15:24:57] INFO [ 'Starting experiment: fy8bAx3K' ]\r\n[2021-04-07 15:24:57] INFO [ 'Change NNIManager status from: INITIALIZED to: RUNNING' ]\r\n[2021-04-07 15:24:57] INFO [ 'Add event listeners' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: started channel: AMLCommandChannel' ]\r\n[2021-04-07 15:24:57] INFO [ 'TrialDispatcher: copying code and settings.' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: ID, ' ]\r\n[2021-04-07 15:25:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 0, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.1, \"momentum\": 0.754420685055723}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:25:07] INFO [ 'Initialize environments total number: 0' ]\r\n[2021-04-07 15:25:07] INFO [ 'TrialDispatcher: run loop started.' ]\r\n[2021-04-07 15:25:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":0,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 0, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.754420685055723}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:25:12] INFO [ 'Assign environment service aml to environment XlEgg' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested environment nni_exp_fy8bAx3K_1617834318_1a1683cd and job id is nni_exp_fy8bAx3K_env_XlEgg.' ]\r\n[2021-04-07 15:25:24] INFO [ 'requested new environment, live trials: 1, live environments: 0, neededEnvironmentCount: 1, requestedCount: 1' ]\r\n[2021-04-07 15:25:42] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to WAITING.' ]\r\n[2021-04-07 15:28:27] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from WAITING to RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'TrialDispatcher: env nni_exp_fy8bAx3K_1617834318_1a1683cd received initialized message and runner is ready, env status: RUNNING.' ]\r\n[2021-04-07 15:29:35] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial KH7Ph.' ]\r\n[2021-04-07 15:29:36] INFO [ 'Trial job KH7Ph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:34:06] INFO [ 'Trial job KH7Ph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:34:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 1, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.48989819362825704}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:34:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":1,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 1, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.48989819362825704}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:34:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Uh6jK.' ]\r\n[2021-04-07 15:34:16] INFO [ 'Trial job Uh6jK status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:37:26] INFO [ 'Trial job Uh6jK status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:37:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 2, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 256, \"lr\": 0.01, \"momentum\": 0.7009004965885264}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:37:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":2,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 2, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 256, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.7009004965885264}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:37:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial JqjWi.' ]\r\n[2021-04-07 15:37:36] INFO [ 'Trial job JqjWi status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:41:26] INFO [ 'Trial job JqjWi status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:41:26] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 3, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.6258856288476062}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:41:31] INFO [ 'submitTrialJob: form: {\"sequenceId\":3,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 3, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.6258856288476062}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:41:32] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial ijhph.' ]\r\n[2021-04-07 15:41:36] INFO [ 'Trial job ijhph status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:46:31] INFO [ 'Trial job ijhph status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:46:31] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 4, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.30905289366545063}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:46:36] INFO [ 'submitTrialJob: form: {\"sequenceId\":4,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 4, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.30905289366545063}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:46:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial bElKu.' ]\r\n[2021-04-07 15:46:41] INFO [ 'Trial job bElKu status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:52:06] INFO [ 'Trial job bElKu status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:52:06] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 5, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 1024, \"lr\": 0.0001, \"momentum\": 0.0003307910747289977}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:52:11] INFO [ 'submitTrialJob: form: {\"sequenceId\":5,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 5, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.0001, \\\\\"momentum\\\\\": 0.0003307910747289977}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:52:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial upDtw.' ]\r\n[2021-04-07 15:52:16] INFO [ 'Trial job upDtw status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:56:07] INFO [ 'Trial job upDtw status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:56:07] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 6, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 64, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.876381947693324}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:56:12] INFO [ 'submitTrialJob: form: {\"sequenceId\":6,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 6, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 64, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.01, \\\\\"momentum\\\\\": 0.876381947693324}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:56:12] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial Zgo5Q.' ]\r\n[2021-04-07 15:56:17] INFO [ 'Trial job Zgo5Q status changed from WAITING to RUNNING' ]\r\n[2021-04-07 15:59:32] INFO [ 'Trial job Zgo5Q status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 15:59:32] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 7, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 128, \"hidden_size\": 512, \"lr\": 0.1, \"momentum\": 0.2948365715286464}, \"parameter_index\": 0}' ]\r\n[2021-04-07 15:59:37] INFO [ 'submitTrialJob: form: {\"sequenceId\":7,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 7, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 128, \\\\\"hidden_size\\\\\": 512, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.2948365715286464}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 15:59:38] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial T92cL.' ]\r\n[2021-04-07 15:59:42] INFO [ 'Trial job T92cL status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:02:49] INFO [ 'Trial job T92cL status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:02:49] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 8, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.001, \"momentum\": 0.5108633717497612}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:02:54] INFO [ 'submitTrialJob: form: {\"sequenceId\":8,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 8, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 16, \\\\\"hidden_size\\\\\": 128, \\\\\"lr\\\\\": 0.001, \\\\\"momentum\\\\\": 0.5108633717497612}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:02:54] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial RoHBk.' ]\r\n[2021-04-07 16:02:59] INFO [ 'Trial job RoHBk status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:06:58] INFO [ 'Trial job RoHBk status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:06:58] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 9, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 32, \"hidden_size\": 1024, \"lr\": 0.1, \"momentum\": 0.1371728116640185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:07:03] INFO [ 'submitTrialJob: form: {\"sequenceId\":9,\"hyperParameters\":{\"value\":\"{\\\\\"parameter_id\\\\\": 9, \\\\\"parameter_source\\\\\": \\\\\"algorithm\\\\\", \\\\\"parameters\\\\\": {\\\\\"batch_size\\\\\": 32, \\\\\"hidden_size\\\\\": 1024, \\\\\"lr\\\\\": 0.1, \\\\\"momentum\\\\\": 0.1371728116640185}, \\\\\"parameter_index\\\\\": 0}\",\"index\":0}}' ]\r\n[2021-04-07 16:07:06] INFO [ 'assigning environment nni_exp_fy8bAx3K_1617834318_1a1683cd to trial UURlR.' ]\r\n[2021-04-07 16:07:08] INFO [ 'Trial job UURlR status changed from WAITING to RUNNING' ]\r\n[2021-04-07 16:07:08] INFO [ 'Change NNIManager status from: RUNNING to: NO_MORE_TRIAL' ]\r\n[2021-04-07 16:10:36] INFO [ 'Trial job UURlR status changed from RUNNING to SUCCEEDED' ]\r\n[2021-04-07 16:10:36] INFO [ 'Change NNIManager status from: NO_MORE_TRIAL to: DONE' ]\r\n[2021-04-07 16:10:36] INFO [ 'NNIManager received command from dispatcher: TR, {\"parameter_id\": 10, \"parameter_source\": \"algorithm\", \"parameters\": {\"batch_size\": 16, \"hidden_size\": 128, \"lr\": 0.01, \"momentum\": 0.5296207133227185}, \"parameter_index\": 0}' ]\r\n[2021-04-07 16:10:36] INFO [ 'Experiment done.' ]\r\n[2021-04-07 16:20:40] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from RUNNING to UNKNOWN.' ]\r\n[2021-04-07 16:21:10] INFO [ 'EnvironmentInformation: nni_exp_fy8bAx3K_env_XlEgg change status from UNKNOWN to SUCCEEDED.' ]\r\n\r\n - dispatcher.log:\r\n [2021-04-07 15:24:58] INFO (nni.runtime.msg_dispatcher_base\/MainThread) Dispatcher started\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001968 seconds\r\n[2021-04-07 15:25:06] INFO (hyperopt.tpe\/Thread-1) TPE using 0 trials\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 15:34:06] INFO (hyperopt.tpe\/Thread-1) TPE using 1\/1 trials with best loss -98.950000\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001003 seconds\r\n[2021-04-07 15:37:26] INFO (hyperopt.tpe\/Thread-1) TPE using 2\/2 trials with best loss -98.950000\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001019 seconds\r\n[2021-04-07 15:41:26] INFO (hyperopt.tpe\/Thread-1) TPE using 3\/3 trials with best loss -99.220000\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001025 seconds\r\n[2021-04-07 15:46:31] INFO (hyperopt.tpe\/Thread-1) TPE using 4\/4 trials with best loss -99.220000\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000998 seconds\r\n[2021-04-07 15:52:06] INFO (hyperopt.tpe\/Thread-1) TPE using 5\/5 trials with best loss -99.300000\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000969 seconds\r\n[2021-04-07 15:56:07] INFO (hyperopt.tpe\/Thread-1) TPE using 6\/6 trials with best loss -99.300000\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001000 seconds\r\n[2021-04-07 15:59:32] INFO (hyperopt.tpe\/Thread-1) TPE using 7\/7 trials with best loss -99.300000\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.001994 seconds\r\n[2021-04-07 16:02:49] INFO (hyperopt.tpe\/Thread-1) TPE using 8\/8 trials with best loss -99.300000\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:06:58] INFO (hyperopt.tpe\/Thread-1) TPE using 9\/9 trials with best loss -99.300000\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) tpe_transform took 0.000997 seconds\r\n[2021-04-07 16:10:36] INFO (hyperopt.tpe\/Thread-1) TPE using 10\/10 trials with best loss -99.340000\r\n\r\n - nnictl stdout and stderr:\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n\r\n-----------------------------------------------------------------------\r\n                Experiment start time 2021-04-07 15:24:42\r\n-----------------------------------------------------------------------\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 message listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 error listeners added. Use emitter.setMaxListeners() to increase limit\r\n(node:16168) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 close listeners added. Use emitter.setMaxListeners() to increase limit\r\n\r\n<!-- Where can you find the log files: [log](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/HowToDebug.md#experiment-root-director), [stdout\/stderr](https:\/\/github.com\/microsoft\/nni\/blob\/master\/docs\/en_US\/Tutorial\/Nnictl.md#nnictl%20log%20stdout) -->\r\n\r\n**What issue meet, what's expected?**:\r\nThe mnist_pytorch example training with Azure ML is unreasonably slow, each trial take about 3 to 5 mins. The entire experiment took nearly 50 mins. I was expecting it to be much faster given that it's using STANDARD_NC6 with GPU - 1 x NVIDIA Tesla K80.\r\n\r\n**How to reproduce it?**: \r\nFollow this doc https:\/\/nni.readthedocs.io\/en\/latest\/TrainingService\/AMLMode.html\r\n\r\n**Additional information**:\r\nTried adding gpuNum: 1 and useActiveGpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Tool":"Azure Machine Learning",
        "Platform":"Github",
        "Issue_original_content":"Title: training extremely slow with ; Content: **what issue meet, what's expected?**: the mnist_pytorch example training with is unreasonably slow, each trial take about 3 to 5 mins. the entire experiment took nearly 50 mins. i was expecting it to be much faster given that it's using standard_nc6 with gpu - 1 x nvidia tesla k80. **additional information**: tried adding gpunum: 1 and useactivegpu: true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all 10 trials in 1 run, each trial take 1 run.",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the mnist_pytorch example training with Azure Machine Learning, which was unreasonably slow, taking 3-5 minutes per trial and nearly 50 minutes for the entire experiment, despite using a standard_nc6 with a single NVIDIA Tesla K80 GPU.",
        "Issue_preprocessed_content":"Title: training extremely slow with ; Content: environment nni version nni mode remote client os windows server os linux python version version is used? conda is running in docker? no log message info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info info dispatcher started info took seconds info tpe using trials info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss info took seconds info tpe using trials with best loss nnictl stdout and stderr experiment start time experiment start time node maxlistenersexceededwarning possible eventemitter memory leak detected. message listeners added. use to increase limit node maxlistenersexceededwarning possible eventemitter memory leak detected. error listeners added. use to increase limit node maxlistenersexceededwarning possible eventemitter memory leak detected. close listeners added. use to increase limit where can you find the log files , what issue meet, what's expected? the example training with is unreasonably slow, each trial take about to mins. the entire experiment took nearly mins. i was expecting it to be much faster given that it's using with gpu x nvidia tesla k . how to reproduce it? follow this doc additional information tried adding gpunum and useactivegpu true in config file, only made it even slower with trials spending more time in waiting status, also instead of doing all trials in run, each trial take run."
    },
    {
        "Issue_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/125",
        "Issue_title":"Child models need to copy the dict.*.txt files from the parent model when launching an experiment on ClearML",
        "Issue_creation_time":1644785881000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"If a parent model was trained with the Alignment Enhanced architecture and the dictionary on, preprocessing for the child model will look for the dict.*.txt files (dict.src.txt, dict.trg.txt, dict.vref.txt) from the parent model.  Those files are not currently being copied into the \/tmp directory on the AQUA server when the experiment is launched through ClearML, so preprocessing fails on the child model.\r\n\r\nSample [experiment ](https:\/\/app.pro.clear.ml\/projects\/2243ca6c76d642699db1f28951bbb78a\/experiments\/fc444552b21243149fd3c90a9a4c6c8d\/execution?columns=selected&columns=type&columns=name&columns=tags&columns=status&columns=project.name&columns=users&columns=started&columns=last_update&columns=last_iteration&columns=parent.name&order=-last_update&filter=)with this failure.",
        "Tool":"ClearML",
        "Platform":"Github",
        "Issue_original_content":"Title: child models need to copy the dict.*.txt files from the parent model when launching an experiment on ; Content: if a parent model was trained with the alignment enhanced architecture and the dictionary on, preprocessing for the child model will look for the dict.*.txt files (dict.src.txt, dict.trg.txt, dict.vref.txt) from the parent model. those files are not currently being copied into the \/tmp directory on the aqua server when the experiment is launched through , so preprocessing fails on the child model. sample [experiment ](https:\/\/app.pro.clear.ml\/projects\/2243ca6c76d642699db1f28951bbb78a\/experiments\/fc444552b21243149fd3c90a9a4c6c8d\/execution?columns=selected&columns=type&columns=name&columns=tags&columns=status&columns=project.name&columns=users&columns=started&columns=last_update&columns=last_iteration&columns=parent.name&order=-last_update&filter=)with this failure.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the dict.*.txt files from the parent model were not being copied into the \/tmp directory on the aqua server when launching an experiment, resulting in preprocessing failure on the child model.",
        "Issue_preprocessed_content":"Title: child models need to copy the files from the parent model when launching an experiment on ; Content: if a parent model was trained with the alignment enhanced architecture and the dictionary on, preprocessing for the child model will look for the files from the parent model. those files are not currently being copied into the directory on the aqua server when the experiment is launched through , so preprocessing fails on the child model. sample with this failure."
    },
    {
        "Issue_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/120",
        "Issue_title":"Execute translate script without creating ClearML task",
        "Issue_creation_time":1641546207000,
        "Issue_closed_time":1657980432000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Currently, the `silnlp.nmt.translate` script always creates a ClearML task. This should be optional. By default, it should just execute locally.",
        "Tool":"ClearML",
        "Platform":"Github",
        "Issue_original_content":"Title: execute translate script without creating task; Content: currently, the `silnlp.nmt.translate` script always creates a task. this should be optional. by default, it should just execute locally.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of having to create a task when executing the `silnlp.nmt.translate` script, which should be optional and execute locally by default.",
        "Issue_preprocessed_content":"Title: execute translate script without creating task; Content: currently, the script always creates a task. this should be optional. by default, it should just execute locally."
    },
    {
        "Issue_link":"https:\/\/github.com\/sillsdev\/silnlp\/issues\/109",
        "Issue_title":"Translate is trying to use ClearML even though it was not requested. Preventing translation on local machine.",
        "Issue_creation_time":1637586010000,
        "Issue_closed_time":1637601038000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I tried to translate with the following command line and trace.\r\nThe command is meant to run locally, but there is an error about ClearML credentials. The ClearML argument was not set in the command line.\r\n\r\n```\r\npython -m silnlp.nmt.translate --checkpoint 6000 --src-project GELA3_2021_11_22 --book OT --trg-iso en  nlg-en-4\r\n2021-11-22 12:53:27.859063: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-11-22 12:53:30,996 - silnlp.common.environment - INFO - Using workspace: \/home\/david\/Gutenberg_new as per environment variable SIL_NLP_DATA_PATH.\r\n2021-11-22 12:53:31,372 - silnlp.common.utils - INFO - Git commit: 12aca87cab\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in <module>\r\n    main()\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main\r\n    translator.translate_book(\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book\r\n    self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\")\r\n  File \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task\r\n    self.clearml = SILClearML(\r\n  File \"<string>\", line 8, in __init__\r\n  File \"\/home\/david\/silnlp\/silnlp\/common\/clearml.py\", line 27, in __post_init__\r\n    self.task = Task.init(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 491, in init\r\n    task = cls._create_dev_task(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 2554, in _create_dev_task\r\n    task = cls(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/task.py\", line 164, in __init__\r\n    super(Task, self).__init__(**kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/task\/task.py\", line 151, in __init__\r\n    super(Task, self).__init__(id=task_id, session=session, log=log)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 131, in __init__\r\n    super(IdObjectBase, self).__init__(session, log, **kwargs)\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 34, in __init__\r\n    self._session = session or self._get_default_session()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_interface\/base.py\", line 101, in _get_default_session\r\n    InterfaceBase._default_session = Session(\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 198, in __init__\r\n    self.refresh_token()\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/token_manager.py\", line 104, in refresh_token\r\n    self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec))\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 713, in _do_refresh_token\r\n    six.reraise(*sys.exc_info())\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_VMn9E-py3.8\/lib\/python3.8\/site-packages\/clearml\/backend_api\/session\/session.py\", line 699, in _do_refresh_token\r\n    raise LoginError(\r\nclearml.backend_api.session.session.LoginError: Failed getting token (error 401 from https:\/\/api.pro.clear.ml): Unauthorized (invalid credentials) (failed to locate provided credentials)\r\ndavid@pop-os:~\/silnlp$ \r\n```\r\n\r\n\r\n",
        "Tool":"ClearML",
        "Platform":"Github",
        "Issue_original_content":"Title: translate is trying to use even though it was not requested. preventing translation on local machine.; Content: i tried to translate with the following command line and trace. the command is meant to run locally, but there is an error about credentials. the argument was not set in the command line. ``` python -m silnlp.nmt.translate --checkpoint 6000 --src-project gela3_2021_11_22 --book ot --trg-iso en nlg-en-4 2021-11-22 12:53:27.859063: i tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] successfully opened dynamic library libcudart.so.11.0 2021-11-22 12:53:30,996 - silnlp.common.environment - info - using workspace: \/home\/david\/gutenberg_new as per environment variable sil_nlp_data_path. 2021-11-22 12:53:31,372 - silnlp.common.utils - info - git commit: 12aca87cab traceback (most recent call last): file \"\/usr\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main return _run_code(code, main_globals, none, file \"\/usr\/lib\/python3.8\/runpy.py\", line 87, in _run_code exec(code, run_globals) file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 181, in main() file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 169, in main translator.translate_book( file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 82, in translate_book self.init_translation_task(experiment_suffix=f\"_{self.checkpoint}_{book}\") file \"\/home\/david\/silnlp\/silnlp\/nmt\/translate.py\", line 57, in init_translation_task self. = sil( file \"\", line 8, in __init__ file \"\/home\/david\/silnlp\/silnlp\/common\/.py\", line 27, in __post_init__ self.task = task.init( file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/task.py\", line 491, in init task = cls._create_dev_task( file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/task.py\", line 2554, in _create_dev_task task = cls( file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/task.py\", line 164, in __init__ super(task, self).__init__(**kwargs) file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/task\/task.py\", line 151, in __init__ super(task, self).__init__(id=task_id, session=session, log=log) file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/base.py\", line 131, in __init__ super(idobjectbase, self).__init__(session, log, **kwargs) file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/base.py\", line 34, in __init__ self._session = session or self._get_default_session() file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_interface\/base.py\", line 101, in _get_default_session interfacebase._default_session = session( file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/session.py\", line 198, in __init__ self.refresh_token() file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/token_manager.py\", line 104, in refresh_token self._set_token(self._do_refresh_token(self.__token, exp=self.req_token_expiration_sec)) file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/session.py\", line 713, in _do_refresh_token six.reraise(*sys.exc_info()) file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/six.py\", line 703, in reraise raise value file \"\/home\/david\/.cache\/pypoetry\/virtualenvs\/silnlp-gt_vmn9e-py3.8\/lib\/python3.8\/site-packages\/\/backend_api\/session\/session.py\", line 699, in _do_refresh_token raise loginerror( .backend_api.session.session.loginerror: failed getting token (error 401 from https:\/\/api.pro.clear.ml): unauthorized (invalid credentials) (failed to locate provided credentials) david@pop-os:~\/silnlp$ ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to translate locally, as an error about credentials was raised due to the lack of the argument being set in the command line.",
        "Issue_preprocessed_content":"Title: translate is trying to use even though it was not requested. preventing translation on local machine.; Content: i tried to translate with the following command line and trace. the command is meant to run locally, but there is an error about credentials. the argument was not set in the command line."
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17691",
        "Issue_title":"\"comet-ml not installed\" error in Trainer (despite comet-ml being installed)",
        "Issue_creation_time":1655132901000,
        "Issue_closed_time":1662130932000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"### System Info\n\n```shell\n- `transformers` version: 4.19.4\r\n- Platform: Linux-4.19.0-17-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.4.0\r\n- PyTorch version (GPU?): 1.11.0+cu102 (False)\r\n- Tensorflow version (GPU?): 2.4.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.4.0 (cpu)\r\n- Jax version: 0.3.4\r\n- JaxLib version: 0.3.2\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n```\n\n\n### Who can help?\n\n@sg\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n1. Install comet-ml (in my case comet-ml==3.31.3)\r\n2. Create TrainingArguments with `report-to='comet_ml'\r\n3. Try to instantiate Trainer\r\n\r\n\r\nThis can be reproduced by adding `report_to='comet_ml'` to training arguments in this notebook:\r\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/BERT\/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\r\n\r\nFollowing error happens when creating the Trainer:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_5296\/3132099784.py in <module>\r\n----> 1 trainer = Trainer(\r\n      2     model,\r\n      3     args,\r\n      4     train_dataset=encoded_dataset[\"train\"],\r\n      5     eval_dataset=encoded_dataset[\"validation\"],\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\r\n    444         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\r\n    445         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\r\n--> 446         self.callback_handler = CallbackHandler(\r\n    447             callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\r\n    448         )\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\r\n    288         self.callbacks = []\r\n    289         for cb in callbacks:\r\n--> 290             self.add_callback(cb)\r\n    291         self.model = model\r\n    292         self.tokenizer = tokenizer\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback)\r\n    305 \r\n    306     def add_callback(self, callback):\r\n--> 307         cb = callback() if isinstance(callback, type) else callback\r\n    308         cb_class = callback if isinstance(callback, type) else callback.__class__\r\n    309         if cb_class in [c.__class__ for c in self.callbacks]:\r\n\r\n\/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self)\r\n    667     def __init__(self):\r\n    668         if not _has_comet:\r\n--> 669             raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\r\n    670         self._initialized = False\r\n    671         self._log_assets = False\r\n\r\nRuntimeError: CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\r\n```\n\n### Expected behavior\n\n```shell\nA Trainer is successfully created with cometml callback enabled.\n```\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: \"-ml not installed\" error in trainer (despite -ml being installed); Content: ### system info ```shell - `transformers` version: 4.19.4 - platform: linux-4.19.0-17-amd64-x86_64-with-glibc2.31 - python version: 3.9.6 - huggingface_hub version: 0.4.0 - pytorch version (gpu?): 1.11.0+cu102 (false) - tensorflow version (gpu?): 2.4.1 (false) - flax version (cpu?\/gpu?\/tpu?): 0.4.0 (cpu) - jax version: 0.3.4 - jaxlib version: 0.3.2 - using gpu in script?: no - using distributed or parallel set-up in script?: no ``` ### who can help? @sg ### information - [ ] the official example scripts - [x] my own modified scripts ### tasks - [x] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction 1. install -ml (in my case -ml==3.31.3) 2. create trainingarguments with `report-to='_ml' 3. try to instantiate trainer this can be reproduced by adding `report_to='_ml'` to training arguments in this notebook: https:\/\/github.com\/nielsrogge\/transformers-tutorials\/blob\/master\/bert\/fine_tuning_bert_(and_friends)_for_multi_label_text_classification.ipynb following error happens when creating the trainer: ``` --------------------------------------------------------------------------- runtimeerror traceback (most recent call last) \/tmp\/ipykernel_5296\/3132099784.py in ----> 1 trainer = trainer( 2 model, 3 args, 4 train_dataset=encoded_dataset[\"train\"], 5 eval_dataset=encoded_dataset[\"validation\"], \/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer.py in __init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics) 444 default_callbacks = default_callbacks + get_reporting_integration_callbacks(self.args.report_to) 445 callbacks = default_callbacks if callbacks is none else default_callbacks + callbacks --> 446 self.callback_handler = callbackhandler( 447 callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler 448 ) \/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler) 288 self.callbacks = [] 289 for cb in callbacks: --> 290 self.add_callback(cb) 291 self.model = model 292 self.tokenizer = tokenizer \/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py in add_callback(self, callback) 305 306 def add_callback(self, callback): --> 307 cb = callback() if isinstance(callback, type) else callback 308 cb_class = callback if isinstance(callback, type) else callback.__class__ 309 if cb_class in [c.__class__ for c in self.callbacks]: \/opt\/conda\/lib\/python3.9\/site-packages\/transformers\/integrations.py in __init__(self) 667 def __init__(self): 668 if not _has_: --> 669 raise runtimeerror(\"callback requires -ml to be installed. run `pip install -ml`.\") 670 self._initialized = false 671 self._log_assets = false runtimeerror: callback requires -ml to be installed. run `pip install -ml`. ``` ### expected behavior ```shell a trainer is successfully created with ml callback enabled. ```",
        "Issue_original_content_gpt_summary":"The user encountered an error when trying to instantiate a trainer with the ml callback enabled, despite -ml being installed, resulting in a \"runtimeerror: callback requires -ml to be installed. run `pip install -ml`.\" error.",
        "Issue_preprocessed_content":"Title: not installed error in trainer ; Content: system info who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction . install . create trainingarguments with to training arguments in this notebook following error happens when creating the trainer expected behavior"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/12734",
        "Issue_title":"Unable to create comet logger when using pytorch lightning cli.",
        "Issue_creation_time":1649790124000,
        "Issue_closed_time":1650063297000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nUnable to create comet logger when using pytorch lightning cli.\r\n\r\n### To Reproduce\r\nhttps:\/\/colab.research.google.com\/drive\/1cvEyYHceKjunKpcGY39oFrinWnIVydJV?usp=sharing\r\n\r\n### Expected behavior\r\nRun model.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           11.1\r\n* Packages:\r\n\t- numpy:             1.21.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.10.0+cu111\r\n\t- pytorch-lightning: 1.6.0\r\n\t- tqdm:              4.63.0\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.13\r\n\t- version:           1 SMP Tue Dec 7 09:58:10 PST 2021\r\n\r\n### Additional context\r\n\r\nError message:\r\n```\r\nEpoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]Traceback (most recent call last):\r\n  File \"main.py\", line 48, in <module>\r\n    cli = LightningCLI(BoringModel, LitDataset, save_config_callback=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__\r\n    self._run_subcommand(self.subcommand)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand\r\n    fn(**fn_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit\r\n    self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run\r\n    results = self._run_stage()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage\r\n    return self._run_train()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance\r\n    self.trainer._logger_connector.update_train_step_metrics()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics\r\n    self.log_metrics(self.metrics[\"log\"])\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 252, in log_metrics\r\n    self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment\r\n    return fn(self)\r\n  File \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/comet.py\", line 223, in experiment\r\n    offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs\r\nTypeError: __init__() got an unexpected keyword argument 'agg_key_funcs'\r\n```\r\nFor some reason, `self._kwargs` there has `{'agg_key_funcs': None, 'agg_default_func': None}`.\n\ncc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @Raalsky @Blaizzy",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to create logger when using pytorch lightning cli.; Content: ## bug unable to create logger when using pytorch lightning cli. ### to reproduce https:\/\/colab.research.google.com\/drive\/1cveyyhcekjunkpcgy39ofrinwnivydjv?usp=sharing ### expected behavior run model. ### environment * cuda: - gpu: - tesla t4 - available: true - version: 11.1 * packages: - numpy: 1.21.5 - pytorch_debug: false - pytorch_version: 1.10.0+cu111 - pytorch-lightning: 1.6.0 - tqdm: 4.63.0 * system: - os: linux - architecture: - 64bit - - processor: x86_64 - python: 3.7.13 - version: 1 smp tue dec 7 09:58:10 pst 2021 ### additional context error message: ``` epoch 1: 100% 32\/32 [00:00<00:00, 300.70it\/s, loss=-15.4, v_num=ff79]traceback (most recent call last): file \"main.py\", line 48, in cli = lightningcli(boringmodel, litdataset, save_config_callback=none) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 564, in __init__ self._run_subcommand(self.subcommand) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/cli.py\", line 835, in _run_subcommand fn(**fn_kwargs) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 772, in fit self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 724, in _call_and_handle_interrupt return trainer_fn(*args, **kwargs) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 812, in _fit_impl results = self._run(model, ckpt_path=self.ckpt_path) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1237, in _run results = self._run_stage() file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1324, in _run_stage return self._run_train() file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1354, in _run_train self.fit_loop.run() file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run self.advance(*args, **kwargs) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 269, in advance self._outputs = self.epoch_loop.run(self._data_fetcher) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/base.py\", line 204, in run self.advance(*args, **kwargs) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 246, in advance self.trainer._logger_connector.update_train_step_metrics() file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 202, in update_train_step_metrics self.log_metrics(self.metrics[\"log\"]) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 130, in log_metrics logger.log_metrics(metrics=scalar_metrics, step=step) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn return fn(*args, **kwargs) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/.py\", line 252, in log_metrics self.experiment.log_metrics(metrics_without_epoch, step=step, epoch=epoch) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 41, in experiment return get_experiment() or dummyexperiment() file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn return fn(*args, **kwargs) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/base.py\", line 39, in get_experiment return fn(self) file \"\/usr\/local\/lib\/python3.7\/dist-packages\/pytorch_lightning\/loggers\/.py\", line 223, in experiment offline_directory=self.save_dir, project_name=self._project_name, **self._kwargs typeerror: __init__() got an unexpected keyword argument 'agg_key_funcs' ``` for some reason, `self._kwargs` there has `{'agg_key_funcs': none, 'agg_default_func': none}`. cc @awaelchli @edward-io @borda @ananthsub @rohitgr7 @kamil-kaczmarek @raalsky @blaizzy",
        "Issue_original_content_gpt_summary":"The user encountered an issue when attempting to create a logger when using pytorch lightning cli, resulting in a TypeError due to an unexpected keyword argument.",
        "Issue_preprocessed_content":"Title: unable to create logger when using pytorch lightning cli.; Content: bug unable to create logger when using pytorch lightning cli. to reproduce expected behavior run model. environment cuda gpu tesla t available true version packages numpy false tqdm system os linux architecture bit processor python version smp tue dec pst additional context error message for some reason, there has . cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/11043",
        "Issue_title":"RichProgressBar doesn't display progress bar when using Comet logger.",
        "Issue_creation_time":1639406686000,
        "Issue_closed_time":1666742778000,
        "Issue_upvote_count":6,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nRichProgressBar doesn't display progress bar when using Comet logger.\r\nI verified it works correctly with tensorboard and wandb.\r\n\r\n\r\n### To Reproduce\r\n```python\r\nimport comet_ml\r\nimport os\r\n\r\nimport torch\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom pytorch_lightning.loggers import CometLogger\r\nfrom pytorch_lightning.callbacks import RichProgressBar\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size: int, length: int):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def loss(self, batch, prediction):\r\n        # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls\r\n        return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))\r\n\r\n    def step(self, x):\r\n        x = self(x)\r\n        out = torch.nn.functional.mse_loss(x, torch.ones_like(x))\r\n        return out\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"loss\": loss}\r\n\r\n    def training_step_end(self, training_step_outputs):\r\n        return training_step_outputs\r\n\r\n    def training_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"loss\"] for x in outputs]).mean()\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"x\": loss}\r\n\r\n    def validation_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"x\"] for x in outputs]).mean()\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        output = self(batch)\r\n        loss = self.loss(batch, output)\r\n        return {\"y\": loss}\r\n\r\n    def test_epoch_end(self, outputs) -> None:\r\n        torch.stack([x[\"y\"] for x in outputs]).mean()\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\r\n        return [optimizer], [lr_scheduler]\r\n\r\n    def train_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def val_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def test_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n    def predict_dataloader(self):\r\n        return DataLoader(RandomDataset(32, 64))\r\n\r\n\r\nmodel = BoringModel()\r\n\r\nlogger = CometLogger(api_key=os.environ.get(\"COMET_API_TOKEN\"))\r\n\r\ntrainer = Trainer(logger=logger, max_epochs=100, callbacks=[RichProgressBar()])\r\n# trainer = Trainer(logger=logger, max_epochs=100)\r\n\r\ntrainer.fit(model=model)\r\n```\r\n\r\n### Environment\r\n- PyTorch Lightning Version 1.5.5\r\n- PyTorch Version 1.10.0\r\n- Python version 3.8\r\n- OS Ubuntu 20.04\n\ncc @kaushikb11 @rohitgr7 @SeanNaren",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: richprogressbar doesn't display progress bar when using logger.; Content: ## bug richprogressbar doesn't display progress bar when using logger. i verified it works correctly with tensorboard and . ### to reproduce ```python import _ml import os import torch from pytorch_lightning import lightningmodule, trainer from torch.utils.data import dataloader, dataset from pytorch_lightning.loggers import logger from pytorch_lightning.callbacks import richprogressbar class randomdataset(dataset): def __init__(self, size: int, length: int): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len class boringmodel(lightningmodule): def __init__(self): super().__init__() self.layer = torch.nn.linear(32, 2) def forward(self, x): return self.layer(x) def loss(self, batch, prediction): # an arbitrary loss to have a loss that updates the model weights during `trainer.fit` calls return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction)) def step(self, x): x = self(x) out = torch.nn.functional.mse_loss(x, torch.ones_like(x)) return out def training_step(self, batch, batch_idx): output = self(batch) loss = self.loss(batch, output) return {\"loss\": loss} def training_step_end(self, training_step_outputs): return training_step_outputs def training_epoch_end(self, outputs) -> none: torch.stack([x[\"loss\"] for x in outputs]).mean() def validation_step(self, batch, batch_idx): output = self(batch) loss = self.loss(batch, output) return {\"x\": loss} def validation_epoch_end(self, outputs) -> none: torch.stack([x[\"x\"] for x in outputs]).mean() def test_step(self, batch, batch_idx): output = self(batch) loss = self.loss(batch, output) return {\"y\": loss} def test_epoch_end(self, outputs) -> none: torch.stack([x[\"y\"] for x in outputs]).mean() def configure_optimizers(self): optimizer = torch.optim.sgd(self.layer.parameters(), lr=0.1) lr_scheduler = torch.optim.lr_scheduler.steplr(optimizer, step_size=1) return [optimizer], [lr_scheduler] def train_dataloader(self): return dataloader(randomdataset(32, 64)) def val_dataloader(self): return dataloader(randomdataset(32, 64)) def test_dataloader(self): return dataloader(randomdataset(32, 64)) def predict_dataloader(self): return dataloader(randomdataset(32, 64)) model = boringmodel() logger = logger(api_key=os.environ.get(\"_api_token\")) trainer = trainer(logger=logger, max_epochs=100, callbacks=[richprogressbar()]) # trainer = trainer(logger=logger, max_epochs=100) trainer.fit(model=model) ``` ### environment - pytorch lightning version 1.5.5 - pytorch version 1.10.0 - python version 3.8 - os ubuntu 20.04 cc @kaushikb11 @rohitgr7 @seannaren",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the richprogressbar does not display a progress bar when using a logger in pytorch lightning.",
        "Issue_preprocessed_content":"Title: richprogressbar doesn't display progress bar when using logger.; Content: bug richprogressbar doesn't display progress bar when using logger. i verified it works correctly with tensorboard and . to reproduce environment pytorch lightning version pytorch version python version os ubuntu cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9879",
        "Issue_title":"\"dumps computation\" at the start of validation loop when using wandb\/comet.ml logger during multi-core tpu training",
        "Issue_creation_time":1633792312000,
        "Issue_closed_time":1642181493000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":11.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nI am training a resnet model on multi core tpus on kaggle. I get this error:\r\n```\r\nDumping Computation:\r\n2021-10-08 23:57:50.220206: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92108 = s32[] constant(0)\r\n2021-10-08 23:57:50.220217: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92110 = pred[] compare(s32[] %constant.92102, s32[] %constant.92108), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220227: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92109 = f32[] constant(1)\r\n2021-10-08 23:57:50.220238: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92111 = f32[] convert(s32[] %constant.92102)\r\n2021-10-08 23:57:50.220248: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92112 = f32[] divide(f32[] %constant.92109, f32[] %convert.92111)\r\n2021-10-08 23:57:50.220260: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92113 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220271: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92114 = f32[] select(pred[] %compare.92110, f32[] %divide.92112, f32[] %constant.92113)\r\n2021-10-08 23:57:50.220281: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92115 = f32[] multiply(f32[] %reduce.92107, f32[] %select.92114)\r\n2021-10-08 23:57:50.220292: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92116 = f32[] convert(f32[] %multiply.92115)\r\n2021-10-08 23:57:50.220302: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134449 = f32[1]{0} reshape(f32[] %convert.92116)\r\n2021-10-08 23:57:50.220312: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92081 = f32[1]{0} reshape(f32[] %p3148.47101)\r\n2021-10-08 23:57:50.220323: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92082 = f32[1]{0} concatenate(f32[1]{0} %reshape.92081), dimensions={0}\r\n2021-10-08 23:57:50.220333: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92083 = f32[] constant(0)\r\n2021-10-08 23:57:50.220343: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92089 = f32[] reduce(f32[1]{0} %concatenate.92082, f32[] %constant.92083), dimensions={0}, to_apply=%AddComputation.92085\r\n2021-10-08 23:57:50.220353: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92084 = s32[] constant(1)\r\n2021-10-08 23:57:50.220364: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92090 = s32[] constant(0)\r\n2021-10-08 23:57:50.220375: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92092 = pred[] compare(s32[] %constant.92084, s32[] %constant.92090), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220387: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92091 = f32[] constant(1)\r\n2021-10-08 23:57:50.220397: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92093 = f32[] convert(s32[] %constant.92084)\r\n2021-10-08 23:57:50.220408: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92094 = f32[] divide(f32[] %constant.92091, f32[] %convert.92093)\r\n2021-10-08 23:57:50.220418: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92095 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220465: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92096 = f32[] select(pred[] %compare.92092, f32[] %divide.92094, f32[] %constant.92095)\r\n2021-10-08 23:57:50.220482: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92097 = f32[] multiply(f32[] %reduce.92089, f32[] %select.92096)\r\n2021-10-08 23:57:50.220494: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92098 = f32[] convert(f32[] %multiply.92097)\r\n2021-10-08 23:57:50.220504: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134450 = f32[1]{0} reshape(f32[] %convert.92098)\r\n2021-10-08 23:57:50.220515: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92063 = f32[1]{0} reshape(f32[] %p3147.47082)\r\n2021-10-08 23:57:50.220525: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92064 = f32[1]{0} concatenate(f32[1]{0} %reshape.92063), dimensions={0}\r\n2021-10-08 23:57:50.220535: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92065 = f32[] constant(0)\r\n2021-10-08 23:57:50.220545: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92071 = f32[] reduce(f32[1]{0} %concatenate.92064, f32[] %constant.92065), dimensions={0}, to_apply=%AddComputation.92067\r\n2021-10-08 23:57:50.220556: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92066 = s32[] constant(1)\r\n2021-10-08 23:57:50.220566: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92072 = s32[] constant(0)\r\n2021-10-08 23:57:50.220576: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92074 = pred[] compare(s32[] %constant.92066, s32[] %constant.92072), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220587: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92073 = f32[] constant(1)\r\n2021-10-08 23:57:50.220598: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92075 = f32[] convert(s32[] %constant.92066)\r\n2021-10-08 23:57:50.220608: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92076 = f32[] divide(f32[] %constant.92073, f32[] %convert.92075)\r\n2021-10-08 23:57:50.220618: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92077 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220629: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92078 = f32[] select(pred[] %compare.92074, f32[] %divide.92076, f32[] %constant.92077)\r\n2021-10-08 23:57:50.220640: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92079 = f32[] multiply(f32[] %reduce.92071, f32[] %select.92078)\r\n2021-10-08 23:57:50.220650: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92080 = f32[] convert(f32[] %multiply.92079)\r\n2021-10-08 23:57:50.220660: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134451 = f32[1]{0} reshape(f32[] %convert.92080)\r\n2021-10-08 23:57:50.220670: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92045 = f32[1]{0} reshape(f32[] %p3146.47063)\r\n2021-10-08 23:57:50.220680: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92046 = f32[1]{0} concatenate(f32[1]{0} %reshape.92045), dimensions={0}\r\n2021-10-08 23:57:50.220691: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92047 = f32[] constant(0)\r\n2021-10-08 23:57:50.220701: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92053 = f32[] reduce(f32[1]{0} %concatenate.92046, f32[] %constant.92047), dimensions={0}, to_apply=%AddComputation.92049\r\n2021-10-08 23:57:50.220711: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92048 = s32[] constant(1)\r\n2021-10-08 23:57:50.220722: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92054 = s32[] constant(0)\r\n2021-10-08 23:57:50.220733: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92056 = pred[] compare(s32[] %constant.92048, s32[] %constant.92054), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220759: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92055 = f32[] constant(1)\r\n2021-10-08 23:57:50.220770: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92057 = f32[] convert(s32[] %constant.92048)\r\n2021-10-08 23:57:50.220781: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92058 = f32[] divide(f32[] %constant.92055, f32[] %convert.92057)\r\n2021-10-08 23:57:50.220792: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92059 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220803: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92060 = f32[] select(pred[] %compare.92056, f32[] %divide.92058, f32[] %constant.92059)\r\n2021-10-08 23:57:50.220813: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92061 = f32[] multiply(f32[] %reduce.92053, f32[] %select.92060)\r\n2021-10-08 23:57:50.220823: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92062 = f32[] convert(f32[] %multiply.92061)\r\n2021-10-08 23:57:50.220833: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.134452 = f32[1]{0} reshape(f32[] %convert.92062)\r\n2021-10-08 23:57:50.220843: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reshape.92027 = f32[1]{0} reshape(f32[] %p3145.47044)\r\n2021-10-08 23:57:50.220854: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %concatenate.92028 = f32[1]{0} concatenate(f32[1]{0} %reshape.92027), dimensions={0}\r\n2021-10-08 23:57:50.220865: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92029 = f32[] constant(0)\r\n2021-10-08 23:57:50.220876: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %reduce.92035 = f32[] reduce(f32[1]{0} %concatenate.92028, f32[] %constant.92029), dimensions={0}, to_apply=%AddComputation.92031\r\n2021-10-08 23:57:50.220888: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92030 = s32[] constant(1)\r\n2021-10-08 23:57:50.220899: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92036 = s32[] constant(0)\r\n2021-10-08 23:57:50.220910: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %compare.92038 = pred[] compare(s32[] %constant.92030, s32[] %constant.92036), direction=NE, type=UNSIGNED\r\n2021-10-08 23:57:50.220921: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92037 = f32[] constant(1)\r\n2021-10-08 23:57:50.220932: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92039 = f32[] convert(s32[] %constant.92030)\r\n2021-10-08 23:57:50.220942: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %divide.92040 = f32[] divide(f32[] %constant.92037, f32[] %convert.92039)\r\n2021-10-08 23:57:50.220953: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %constant.92041 = f32[] constant(nan)\r\n2021-10-08 23:57:50.220964: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %select.92042 = f32[] select(pred[] %compare.92038, f32[] %divide.92040, f32[] %constant.92041)\r\n2021-10-08 23:57:50.220975: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %multiply.92043 = f32[] multiply(f32[] %reduce.92035, f32[] %select.92042)\r\n2021-10-08 23:57:50.220986: E tensorflow\/compiler\/xla\/xla_client\/xla_util.cc:76] %convert.92044 = f32[] convert(f32[] %multiply.92043)\r\n```\r\nThis text goes on and on for several pages.\r\n\r\nThe first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output.\r\n\r\nNote that this only happens when using a logger (wandb or comet.ml) and everything works fine when I do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/).\r\n\r\n> I have also tried adding very small batch sizes so this probably isn't a memory issue\r\n\r\n### To Reproduce\r\n\r\nSee this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses wandb and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-comet-ml) with comet.ml.\r\n\r\n### Expected behavior\r\n\r\nTraining should run normally with no issues and logging should work.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1+cpu\r\n\t- pytorch-lightning: 1.4.4\r\n\t- tqdm:              4.62.1\r\n\t- pytorch-xla  1.7\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\r\n### Additional context\r\nNone\r\n\n\ncc @kaushikb11 @rohitgr7 @awaelchli @morganmcg1 @AyushExel @borisdayma @scottire",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: \"dumps computation\" at the start of validation loop when using \/.ml logger during multi-core tpu training; Content: ## bug i am training a resnet model on multi core tpus on kaggle. i get this error: ``` dumping computation: ``` this text goes on and on for several pages. the first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output. note that this only happens when using a logger ( or .ml) and everything works fine when i do `self.print` or normal `print` as evident in this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-no-logging\/). > i have also tried adding very small batch sizes so this probably isn't a memory issue ### to reproduce see this [notebook](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu-resnet200d) that uses and [this](https:\/\/www.kaggle.com\/rustyelectron\/documentclassification-pytorch-tpu--ml) with .ml. ### expected behavior training should run normally with no issues and logging should work.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the training of a ResNet model on multi-core TPUs crashed and printed an error message of \"dumping computation\" at the start of the validation loop when using a logger (Wandb or .ml).",
        "Issue_preprocessed_content":"Title: dumps computation at the start of validation loop when using logger during tpu training; Content: bug i am training a resnet model on multi core tpus on kaggle. i get this error this text goes on and on for several pages. the first epoch runs fine at first and just as the validation loop starts, the training crashes and this text is printed as output. note that this only happens when using a logger and everything works fine when i do or normal as evident in this . i have also tried adding very small batch sizes so this probably isn't a memory issue to reproduce see this that uses and with expected behavior training should run normally with no issues and logging should work. environment cuda gpu available false version none packages numpy false tqdm system os linux architecture bit processor python additional context none cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7880",
        "Issue_title":"Comet Logger doesn't seem to log with tpu_cores=8",
        "Issue_creation_time":1623138830000,
        "Issue_closed_time":1636988013000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nUse following [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing) and post here\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @tchaton",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logger doesn't seem to log with tpu_cores=8; Content: ## bug ## please reproduce using the boringmodel ### to reproduce use following [**boringmodel**](https:\/\/colab.research.google.com\/drive\/1hvwvvtk8j2nj52qu4q4ycyzom0_alqf3?usp=sharing) and post here ### expected behavior ### environment **note**: `bugs with code` are solved faster ! `colab notebook` should be made `public` ! * `ide`: please, use our python [bug_report_model.py](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py ) template. * `colab notebook`: please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually). you can get the script and run it with: ``` wget https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py # for security purposes, please check the contents of collect_env_details.py before running it. python collect_env_details.py ``` - pytorch version (e.g., 1.0): - os (e.g., linux): - how you installed pytorch (`conda`, `pip`, source): - build command you used (if compiling from source): - python version: - cuda\/cudnn version: - gpu models and configuration: - any other relevant information: ### additional context cc @tchaton",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger does not seem to log with tpu_cores=8, and provided a BoringModel Colab link to reproduce the issue, as well as their environment details.",
        "Issue_preprocessed_content":"Title: logger doesn't seem to log with ; Content: bug a clear and concise description of what the bug is. please reproduce using the boringmodel please paste your boringmodel colab link here. to reproduce use following and post here if you could not reproduce using the boringmodel and still think there's a bug, please post here expected behavior fill in environment note are solved faster ! should be made ! please, use our python template. please copy and paste the output from our . you can get the script and run it with pytorch version os how you installed pytorch build command you used python version version gpu models and configuration any other relevant information additional context add any other context about the problem here. cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7599",
        "Issue_title":"Upgrading from 1.2.4 to 1.3.1 causes the pytorch comet logger to produce multiple experiments.",
        "Issue_creation_time":1621374020000,
        "Issue_closed_time":1631600832000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nWhen running a ddp multi-gpu experiment on a SLURM cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple comet experiments, one for each GPU. Only one of them logs any metrics, the others just sit. \r\n\r\n<img width=\"748\" alt=\"Screen Shot 2021-05-18 at 2 00 40 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725668-1903b800-b7e5-11eb-84a5-096fa79fe332.png\">\r\n\r\n<img width=\"1477\" alt=\"Screen Shot 2021-05-18 at 1 59 26 PM\" src=\"https:\/\/user-images.githubusercontent.com\/1208492\/118725654-143f0400-b7e5-11eb-949b-4eb8de527502.png\">\r\n  \r\nHere is an experiment from the 'main' GPU, the one that actually logs the metrics.\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/view\/SYQJplzX3SBwVfG27moJV0b8p\r\n\r\nHere is the same run, a gpu that just announces itself and does not log anything else:\r\nhttps:\/\/www.comet.ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n### To Reproduce\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\nI do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a comet authentication, which I cannot paste here.\r\n\r\n### Expected behavior\r\n\r\nA single comet experiment for a single call to trainer.fit(). This was the behavior in lightning 1.2.4.\r\n\r\n### Environment\r\n\r\n**Note**: `Bugs with code` are solved faster ! `Colab Notebook` should be made `public` !\r\n\r\n* `IDE`: Please, use our python [bug_report_model.py](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n) template.\r\n\r\n* `Colab Notebook`: Please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/tests\/collect_env_details.py\r\n# For security purposes, please check the contents of collect_env_details.py before running it.\r\npython collect_env_details.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): \r\n torch==1.8.1\r\n pytorch-lightning==1.3.1\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: Python 3.8.8\r\n - CUDA\/cuDNN version: 10\r\n - GPU models and configuration: GeForce 2080Ti\r\n\r\n--\r\n\r\n<br class=\"Apple-interchange-newline\">\r\n - Any other relevant information:\r\n SLURM HPC Cluster, single node.\r\n\r\n### Additional context\r\nProblem appears after upgrading to 1.3.1 from 1.2.4. I believe it is related to the thought behind this SO post:\r\n\r\nhttps:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: upgrading from 1.2.4 to 1.3.1 causes the pytorch logger to produce multiple experiments.; Content: ## bug when running a ddp multi-gpu experiment on a slurm cluster, pytorch-lightning==1.3.1, but not 1.2.4, creates multiple experiments, one for each gpu. only one of them logs any metrics, the others just sit. here is an experiment from the 'main' gpu, the one that actually logs the metrics. https:\/\/www..ml\/bw4sz\/everglades\/view\/syqjplzx3sbwvfg27mojv0b8p here is the same run, a gpu that just announces itself and does not log anything else: https:\/\/www..ml\/bw4sz\/everglades\/4d1b0d55601444ffbea00bd87b456c1e ## please reproduce using the boringmodel ### to reproduce i do not know how to make a reproducible example, since you cannot do multi-gpu ddp in colab and would need a authentication, which i cannot paste here. ### expected behavior a single experiment for a single call to trainer.fit(). this was the behavior in lightning 1.2.4. ### environment **note**: `bugs with code` are solved faster ! `colab notebook` should be made `public` ! * `ide`: please, use our python [bug_report_model.py](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py ) template. * `colab notebook`: please copy and paste the output from our [environment collection script](https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py) (or fill out the checklist below manually). you can get the script and run it with: ``` wget https:\/\/raw.githubusercontent.com\/pytorchlightning\/pytorch-lightning\/master\/tests\/collect_env_details.py # for security purposes, please check the contents of collect_env_details.py before running it. python collect_env_details.py ``` - pytorch version (e.g., 1.0): torch==1.8.1 pytorch-lightning==1.3.1 - os (e.g., linux): linux - how you installed pytorch (`conda`, `pip`, source): pip - build command you used (if compiling from source): - python version: python 3.8.8 - cuda\/cudnn version: 10 - gpu models and configuration: geforce 2080ti -- - any other relevant information: slurm hpc cluster, single node. ### additional context problem appears after upgrading to 1.3.1 from 1.2.4. i believe it is related to the thought behind this so post: https:\/\/stackoverflow.com\/questions\/66854148\/proper-way-to-log-things-when-using-pytorch-lightning-ddp",
        "Issue_original_content_gpt_summary":"The user encountered a bug when upgrading from PyTorch Lightning 1.2.4 to 1.3.1, causing the pytorch logger to produce multiple experiments when running a DDP multi-GPU experiment on a Slurm cluster.",
        "Issue_preprocessed_content":"Title: upgrading from to causes the pytorch logger to produce multiple experiments.; Content: bug a clear and concise description of what the bug is. when running a ddp experiment on a slurm cluster, but not creates multiple experiments, one for each gpu. only one of them logs any metrics, the others just sit. img width alt screen shot at pm img width alt screen shot at pm here is an experiment from the 'main' gpu, the one that actually logs the metrics. here is the same run, a gpu that just announces itself and does not log anything else please reproduce using the boringmodel to reproduce if you could not reproduce using the boringmodel and still think there's a bug, please post here i do not know how to make a reproducible example, since you cannot do ddp in colab and would need a authentication, which i cannot paste here. expected behavior a single experiment for a single call to this was the behavior in lightning environment note are solved faster ! should be made ! please, use our python template. please copy and paste the output from our . you can get the script and run it with pytorch version os linux how you installed pytorch pip build command you used python version python version gpu models and configuration geforce ti br any other relevant information slurm hpc cluster, single node. additional context problem appears after upgrading to from i believe it is related to the thought behind this so post"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/7021",
        "Issue_title":"CometLogger can modify logged metrics in-place ",
        "Issue_creation_time":1618430187000,
        "Issue_closed_time":1630398077000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"When `logger.log_metrics(metrics)` is called with a `CometLogger`, `metrics` may be modified in-place. This can lead to confusing errors. E.g. if the user does\r\n\r\n```python\r\ndef training_step(self, batch, batch_idx):\r\n    losses = self._get_losses(batch)\r\n    self.logger.log_metrics(losses)\r\n    return losses\r\n```\r\n\r\nthen `losses` will have all the tensors moved to the CPU and their gradients detached, leading to an error like `RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted.\r\n\r\nNone of the other loggers change `metrics` in-place when `log_metrics` is called. All of them except neptune say that they just accept `metrics: Dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.Tensor`s or other types as well.\r\n\r\nThe `CSVLogger` uses the following for handling tensors:\r\n```python\r\ndef _handle_value(value):\r\n    if isinstance(value, torch.Tensor):\r\n        return value.item()\r\n    return value\r\n...\r\nmetrics = {k: _handle_value(v) for k, v in metrics_dict.items()}\r\n```\r\n\r\nThe `TensorBoardLogger` similarly has\r\n\r\n```python\r\nfor k, v in metrics.items():\r\n    if isinstance(v, torch.Tensor):\r\n        v = v.item()\r\n    ...\r\n    self.experiment.add_scalar(k, v, step)\r\n```\r\n\r\nIn the `CometLogger`, the current tensor conversion code is\r\n\r\n```python\r\nfor key, val in metrics.items():\r\n  if is_tensor(val):\r\n    metrics[key] = val.cpu().detach()\r\n```\r\n\r\nbut then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything.\r\n\r\nI'm happy to submit a PR to fix this so that the `CometLogger` doesn't modify the original `metrics` dictionary. I just wanted to ask for a couple of opinions before changing things:\r\n\r\n1. Should I keep the current tensor conversion behavior for `CometLogger` (`val.cpu().detach()`) or switch to using `val.item()`? My preference would be the latter, though this does change the behavior (see at the end).\r\n2. Should I update the other loggers to all accept `metrics: Dict[str, Union[float, torch.Tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `Dict[str, float]`?\r\n3. * I don't know the other loggers, so I'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code\r\n\r\n---\r\n\r\n`val.cpu().detach()` vs `val.item()`\r\n* Comet sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. However, I don't think anybody would be using this behavior on purpose. If you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get `COMET WARNING: Cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. The metric itself doesn't even appear in the web interface for CometML, so I assume you can only access it if you query for it directly through their API.\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logger can modify logged metrics in-place ; Content: when `logger.log_metrics(metrics)` is called with a `logger`, `metrics` may be modified in-place. this can lead to confusing errors. e.g. if the user does ```python def training_step(self, batch, batch_idx): losses = self._get_losses(batch) self.logger.log_metrics(losses) return losses ``` then `losses` will have all the tensors moved to the cpu and their gradients detached, leading to an error like `runtimeerror: element 0 of tensors does not require grad and does not have a grad_fn` when backprop is attempted. none of the other loggers change `metrics` in-place when `log_metrics` is called. all of them except say that they just accept `metrics: dict[str, float]`, though some others (e.g. the tensorboard logger) have code to handle `torch.tensor`s or other types as well. the `csvlogger` uses the following for handling tensors: ```python def _handle_value(value): if isinstance(value, torch.tensor): return value.item() return value ... metrics = {k: _handle_value(v) for k, v in metrics_dict.items()} ``` the `tensorboardlogger` similarly has ```python for k, v in metrics.items(): if isinstance(v, torch.tensor): v = v.item() ... self.experiment.add_scalar(k, v, step) ``` in the `logger`, the current tensor conversion code is ```python for key, val in metrics.items(): if is_tensor(val): metrics[key] = val.cpu().detach() ``` but then the entire `metrics` dictionary is copied later in the function anyway, so it doesn't really make sense to do in-place modification then copy everything. i'm happy to submit a pr to fix this so that the `logger` doesn't modify the original `metrics` dictionary. i just wanted to ask for a couple of opinions before changing things: 1. should i keep the current tensor conversion behavior for `logger` (`val.cpu().detach()`) or switch to using `val.item()`? my preference would be the latter, though this does change the behavior (see at the end). 2. should i update the other loggers to all accept `metrics: dict[str, union[float, torch.tensor]]` and have them all use the same method (probably imported from `loggers\/base.py`) to convert to a `dict[str, float]`? 3. * i don't know the other loggers, so i'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in third-party code --- `val.cpu().detach()` vs `val.item()` * sort of has support for tensors with >1 element, so using the first method will make logging such tensors valid while the second method would throw an error. however, i don't think anybody would be using this behavior on purpose. if you do `logger.log_metrics({\"test\": torch.tensor([1.0, 10.0])})`, you get ` warning: cannot safely convert array([ 1., 10.], dtype=float32) object to a scalar value, using its string representation for logging`. the metric itself doesn't even appear in the web interface for ml, so i assume you can only access it if you query for it directly through their api.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logger can modify logged metrics in-place, leading to confusing errors.",
        "Issue_preprocessed_content":"Title: logger can modify logged metrics ; Content: when is called with a , may be modified this can lead to confusing errors. if the user does then will have all the tensors moved to the cpu and their gradients detached, leading to an error like when backprop is attempted. none of the other loggers change when is called. all of them except say that they just accept , though some others have code to handle s or other types as well. the uses the following for handling tensors the similarly has in the , the current tensor conversion code is but then the entire dictionary is copied later in the function anyway, so it doesn't really make sense to do modification then copy everything. i'm happy to submit a pr to fix this so that the doesn't modify the original dictionary. i just wanted to ask for a couple of opinions before changing things . should i keep the current tensor conversion behavior for .detach my preference would be the latter, though this does change the behavior . . should i update the other loggers to all accept and have them all use the same method to convert to a ? . i don't know the other loggers, so i'm not sure if tensors are actually not supported or if the type annotation isn't precise and the conversion is happening in code vs sort of has support for tensors with element, so using the first method will make logging such tensors valid while the second method would throw an error. however, i don't think anybody would be using this behavior on purpose. if you do , you get . the metric itself doesn't even appear in the web interface for ml, so i assume you can only access it if you query for it directly through their api."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5829",
        "Issue_title":"Must manually import `comet_ml` before `CometLogger` to avoid import error",
        "Issue_creation_time":1612502289000,
        "Issue_closed_time":1615221269000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nA few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `CometLogger`. However, comet requires for `comet_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. If not, you get the following error:\r\n```\r\nImportError: You must import Comet before these modules: torch, tensorboard\r\n```\r\n\r\nBefore the imports reordering, comet's import requirements could be met by importing `CometLogger` before torch and tensorboard. However, since the refactoring, torch is now imported before comet in `loggers\/comet.py` itself. This forces users to manually add an unused import for `comet_ml` before importing `CometLogger` to avoid the above `ImportError`.\r\n\r\n### To Reproduce\r\nThis [**BoringModel**](https:\/\/colab.research.google.com\/drive\/1u7vE02v40RCebEXg1515KMuCxvelAcNF?usp=sharing) example reproduces the `ImportError`.\r\n\r\n### Expected behavior\r\nUsers should not have to manually import `comet_ml` before `CometLogger` to avoid triggering the `ImportError`. The `comet_ml` import inside `loggers\/comet.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: must manually import `_ml` before `logger` to avoid import error; Content: ## bug a few weeks ago, a [refactoring of logger imports](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/commit\/ec0fb7a3ec709699243c76dae04ee1e4ce2406a0#diff-7a041199139ffcca72689f9a15f47657330ff9d3206a46103e7a061a5fe2bc09) changed the ordering of imports for the `logger`. however, requires for `_ml` to be imported before some other dependencies, i.e. torch and tensorboard, to work properly. if not, you get the following error: ``` importerror: you must import before these modules: torch, tensorboard ``` before the imports reordering, 's import requirements could be met by importing `logger` before torch and tensorboard. however, since the refactoring, torch is now imported before in `loggers\/.py` itself. this forces users to manually add an unused import for `_ml` before importing `logger` to avoid the above `importerror`. ### to reproduce this [**boringmodel**](https:\/\/colab.research.google.com\/drive\/1u7ve02v40rcebexg1515kmucxvelacnf?usp=sharing) example reproduces the `importerror`. ### expected behavior users should not have to manually import `_ml` before `logger` to avoid triggering the `importerror`. the `_ml` import inside `loggers\/.py` should exceptionally come before the `torch` import, even if it violates usual import ordering.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where they must manually import `_ml` before `logger` to avoid an import error when using PyTorch Lightning.",
        "Issue_preprocessed_content":"Title: must manually import before to avoid import error; Content: bug a few weeks ago, a changed the ordering of imports for the . however, requires for to be imported before some other dependencies, torch and tensorboard, to work properly. if not, you get the following error before the imports reordering, 's import requirements could be met by importing before torch and tensorboard. however, since the refactoring, torch is now imported before in itself. this forces users to manually add an unused import for before importing to avoid the above . to reproduce this example reproduces the . expected behavior users should not have to manually import before to avoid triggering the . the import inside should exceptionally come before the import, even if it violates usual import ordering."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4229",
        "Issue_title":"Comet logger overrides COMET_EXPERIMENT_KEY env variable",
        "Issue_creation_time":1603104554000,
        "Issue_closed_time":1603809056000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"After https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/pull\/2553  there is a changed logger behavior. It starts using `COMET_EXPERIMENT_KEY`. But it doesn't respect it if it is set already.\r\nSo the bug is in the following.\r\nI already set this variable \r\nThen logger overwrites my value here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L189\r\nThen it deletes this variable at all here https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L215\r\nThis way it ignores my variable and deletes it at all later\r\nMoreover in version function it also ignores my set variable\r\nI will create a pull request to fix it ",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logger overrides _experiment_key env variable; Content: after https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/pull\/2553 there is a changed logger behavior. it starts using `_experiment_key`. but it doesn't respect it if it is set already. so the bug is in the following. i already set this variable then logger overwrites my value here https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/.py#l189 then it deletes this variable at all here https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/.py#l215 this way it ignores my variable and deletes it at all later moreover in version function it also ignores my set variable i will create a pull request to fix it",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the PyTorch Lightning logger which overrides the _experiment_key environment variable and deletes it, ignoring the user's set variable.",
        "Issue_preprocessed_content":"Title: logger overrides env variable; Content: after there is a changed logger behavior. it starts using . but it doesn't respect it if it is set already. so the bug is in the following. i already set this variable then logger overwrites my value here then it deletes this variable at all here this way it ignores my variable and deletes it at all later moreover in version function it also ignores my set variable i will create a pull request to fix it"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3417",
        "Issue_title":"CometLogger failing without save_dir",
        "Issue_creation_time":1599655027000,
        "Issue_closed_time":1599658056000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nCometmllogger with api key and  without save dir results in error.\r\nThis happens due to this if https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/comet.py#L135\r\n_save_dir is not set and later train loop tries to read it and fails.\r\nThis can be fixed by setting _save_dir to None. I will supply PR in a moment\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```\r\n    model = LightningModel({})\r\n    comet_logger = CometLogger(\r\n        api_key=KEY,\r\n        workspace=\"workspace\"\r\n    )\r\n\r\n    trainer = Trainer(logger=comet_logger)\r\n    trainer.fit(model)\r\n```\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n\r\n\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n\r\nTraceback (most recent call last):\r\ntrainer.fit(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn\r\nresult = fn(self, *args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit\r\nresults = self.accelerator_backend.train(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train\r\nresults = self.trainer.run_pretrain_routine(model)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine\r\nself.train()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train\r\nself.on_train_start()\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start\r\ncallback.on_train_start(self, self.get_model())\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn\r\nreturn fn(*args, **kwargs)\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start\r\nsave_dir = trainer.logger.save_dir or trainer.default_root_dir\r\nFile \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/comet.py\", line 253, in save_dir\r\nreturn self._save_dir\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logger failing without save_dir; Content: ## bug mllogger with api key and without save dir results in error. this happens due to this if https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/master\/pytorch_lightning\/loggers\/.py#l135 _save_dir is not set and later train loop tries to read it and fails. this can be fixed by setting _save_dir to none. i will supply pr in a moment ### to reproduce steps to reproduce the behavior: ``` model = lightningmodel({}) _logger = logger( api_key=key, workspace=\"workspace\" ) trainer = trainer(logger=_logger) trainer.fit(model) ``` traceback (most recent call last): trainer.fit(model) file \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/states.py\", line 48, in wrapped_fn result = fn(self, *args, **kwargs) file \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1073, in fit results = self.accelerator_backend.train(model) file \"\/python3.8\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py\", line 51, in train results = self.trainer.run_pretrain_routine(model) file \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1239, in run_pretrain_routine self.train() file \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 363, in train self.on_train_start() file \"\/python3.8\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 111, in on_train_start callback.on_train_start(self, self.get_model()) file \"\/python3.8\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 27, in wrapped_fn return fn(*args, **kwargs) file \"\/python3.8\/site-packages\/pytorch_lightning\/callbacks\/model_checkpoint.py\", line 296, in on_train_start save_dir = trainer.logger.save_dir or trainer.default_root_dir file \"\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py\", line 253, in save_dir return self._save_dir ### additional context",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the mllogger with an API key and without a save directory resulted in an error due to the lack of a save directory.",
        "Issue_preprocessed_content":"Title: logger failing without ; Content: bug mllogger with api key and without save dir results in error. this happens due to this if is not set and later train loop tries to read it and fails. this can be fixed by setting to none. i will supply pr in a moment to reproduce steps to reproduce the behavior if you have a code sample, error messages, stack traces, please provide it here as well a clear and concise description of what you expected to happen. traceback file line , in result fn file line , in fit results file line , in train results file line , in file line , in train file line , in file line , in return fn file line , in or file line , in return additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1704",
        "Issue_title":"Error running on ddp (can't pickle local object 'SummaryTopic) with comet logger",
        "Issue_creation_time":1588434434000,
        "Issue_closed_time":1591023634000,
        "Issue_upvote_count":6,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I have the following problem running on ddp mode with cometlogger.\r\nWhen I detach the logger from the trainer (i.e deleting`logger=comet_logger`) the code runs.\r\n```\r\nException has occurred: AttributeError\r\nCan't pickle local object 'SummaryTopic.__init__.<locals>.default'\r\n  File \"\/path\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/path\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/path\/multiprocessing\/process.py\", line 112, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))\r\n  File \"\/repo_path\/train.py\", line 158, in main_train\r\n    trainer.fit(model)\r\n  File \"\/repo_path\/train.py\", line 72, in main\r\n    main_train(model_class_pointer, hyperparams, logger)\r\n  File \"\/repo_path\/train.py\", line 167, in <module>\r\n    main()\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/path\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/path\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/path\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n```",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: error running on ddp (can't pickle local object 'summarytopic) with logger; Content: i have the following problem running on ddp mode with logger. when i detach the logger from the trainer (i.e deleting`logger=_logger`) the code runs. ``` exception has occurred: attributeerror can't pickle local object 'summarytopic.__init__..default' file \"\/path\/multiprocessing\/reduction.py\", line 60, in dump forkingpickler(file, protocol).dump(obj) file \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch reduction.dump(process_obj, fp) file \"\/path\/multiprocessing\/popen_fork.py\", line 20, in __init__ self._launch(process_obj) file \"\/path\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__ super().__init__(process_obj) file \"\/path\/multiprocessing\/context.py\", line 284, in _popen return popen(process_obj) file \"\/path\/multiprocessing\/process.py\", line 112, in start self._popen = self._popen(self) file \"\/path\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn process.start() file \"\/path\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 751, in fit mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,)) file \"\/repo_path\/train.py\", line 158, in main_train trainer.fit(model) file \"\/repo_path\/train.py\", line 72, in main main_train(model_class_pointer, hyperparams, logger) file \"\/repo_path\/train.py\", line 167, in main() file \"\/path\/runpy.py\", line 85, in _run_code exec(code, run_globals) file \"\/path\/runpy.py\", line 96, in _run_module_code mod_name, mod_spec, pkg_name, script_name) file \"\/path\/runpy.py\", line 263, in run_path pkg_name=pkg_name, script_name=fname) file \"\/path\/runpy.py\", line 85, in _run_code exec(code, run_globals) file \"\/path\/runpy.py\", line 193, in _run_module_as_main \"__main__\", mod_spec) ```",
        "Issue_original_content_gpt_summary":"The user encountered an error running on ddp mode with a logger, where an AttributeError occurred due to an inability to pickle a local object.",
        "Issue_preprocessed_content":"Title: error running on ddp with logger; Content: i have the following problem running on ddp mode with logger. when i detach the logger from the trainer the code runs."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1682",
        "Issue_title":"Comet logger cannot be pickled after creating an experiment",
        "Issue_creation_time":1588303817000,
        "Issue_closed_time":1591023635000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"## \ud83d\udc1b Bug \r\n\r\nThe Comet logger cannot be pickled after an experiment (at least an OfflineExperiment) has been created.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\ninitialize the logger object (works fine)\r\n```\r\nfrom pytorch_lightning.loggers import CometLogger\r\nimport tests.base.utils as tutils\r\nfrom pytorch_lightning import Trainer\r\nimport pickle\r\n\r\nmodel, _ = tutils.get_default_model()\r\nlogger = CometLogger(save_dir='test')\r\npickle.dumps(logger)\r\n```\r\n\r\ninitialize a Trainer object with the logger (works fine)\r\n```\r\ntrainer = Trainer(\r\n    max_epochs=1,\r\n    logger=logger\r\n)\r\npickle.dumps(logger)\r\npickle.dumps(trainer)\r\n```\r\n\r\naccess the `experiment` attribute which creates the OfflineExperiment object (fails)\r\n```\r\nlogger.experiment\r\npickle.dumps(logger)\r\n>> TypeError: can't pickle _thread.lock objects\r\n```\r\n\r\n### Expected behavior\r\n\r\nWe should be able to pickle loggers for distributed training.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           None\r\n* Packages:\r\n        - numpy:             1.18.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.4.0\r\n        - pytorch-lightning: 0.7.5\r\n        - tensorboard:       2.1.0\r\n        - tqdm:              4.42.0\r\n* System:\r\n        - OS:                Darwin\r\n        - architecture:\r\n                - 64bit\r\n                - \r\n        - processor:         i386\r\n        - python:            3.7.6\r\n        - version:           Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1\/RELEASE_X86_64\r\n\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logger cannot be pickled after creating an experiment; ## bug the logger cannot be pickled after an experiment (at least an offlineexperiment) has been created. ### to reproduce steps to reproduce the behavior: initialize the logger object (works fine) ``` from pytorch_lightning.loggers import logger import tests.base.utils as tutils from pytorch_lightning import trainer import pickle model, _ = tutils.get_default_model() logger = logger(save_dir='test') pickle.dumps(logger) ``` initialize a trainer object with the logger (works fine) ``` trainer = trainer( max_epochs=1, logger=logger ) pickle.dumps(logger) pickle.dumps(trainer) ``` access the `experiment` attribute which creates the offlineexperiment object (fails) ``` logger.experiment pickle.dumps(logger) >> typeerror: can't pickle _thread.lock objects ``` ### expected behavior we should be able to pickle loggers for distributed training. ### environment * cuda: - gpu: - available: false - version: none * packages: - numpy: 1.18.1 - pytorch_debug: false - pytorch_version: 1.4.0 - pytorch-lightning: 0.7.5 - tensorboard: 2.1.0 - tqdm: 4.42.0 * system: - os: darwin - architecture: - 64bit - - processor: i386 - python: 3.7.6 - version: darwin kernel version 19.3.0: thu jan 9 20:58:23 pst 2020; Content: root:xnu-6153.81.5~1\/release_x86_64",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logger object cannot be pickled after an experiment has been created, resulting in a TypeError.",
        "Issue_preprocessed_content":"Title: logger cannot be pickled after creating an experiment; Content: bug the logger cannot be pickled after an experiment has been created. to reproduce steps to reproduce the behavior initialize the logger object initialize a trainer object with the logger access the attribute which creates the offlineexperiment object expected behavior we should be able to pickle loggers for distributed training. environment cuda gpu available false version none packages numpy false tensorboard tqdm system os darwin architecture bit processor i python version darwin kernel version thu jan pst ;"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/1460",
        "Issue_title":"Test metrics are no longer pushed to Comet.ML (and perhaps others)",
        "Issue_creation_time":1586647457000,
        "Issue_closed_time":1586910754000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nPyTorch Lightning 0.7.2 used to publish test metrics to Comet.ML.  Commit https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun fast-run of training and observe test metrics not being submitted to Comet.ML (and possibly other logging destinations).\r\n\r\n### Environment\r\n\r\n```\r\ncuda:\r\n        GPU:\r\n                Tesla T4\r\n        available:           True\r\n        version:             10.1\r\npackages:\r\n        numpy:               1.17.2\r\n        pyTorch_debug:       False\r\n        pyTorch_version:     1.4.0\r\n        pytorch-lightning:   0.7.4-dev\r\n        tensorboard:         2.2.0\r\n        tqdm:                4.45.0\r\nsystem:\r\n        OS:                  Linux\r\n        architecture:\r\n                64bit\r\n\r\n        processor:           x86_64\r\n        python:              3.6.8\r\n        version:             #69-Ubuntu SMP Thu Mar 26 02:17:29 UTC 2020\r\n```\r\n\r\ncc @alexeykarnachev",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: test metrics are no longer pushed to .ml (and perhaps others); Content: ## bug pytorch lightning 0.7.2 used to publish test metrics to .ml. commit https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/commit\/ddbf7de6dc97924de07331f1575ee0b37cb7f7aa has broken this functionality. ### to reproduce steps to reproduce the behavior: run fast-run of training and observe test metrics not being submitted to .ml (and possibly other logging destinations). ### environment ``` cuda: gpu: tesla t4 available: true version: 10.1 packages: numpy: 1.17.2 pytorch_debug: false pytorch_version: 1.4.0 pytorch-lightning: 0.7.4-dev tensorboard: 2.2.0 tqdm: 4.45.0 system: os: linux architecture: 64bit processor: x86_64 python: 3.6.8 version: #69-ubuntu smp thu mar 26 02:17:29 utc 2020 ``` cc @alexeykarnachev",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where test metrics are no longer pushed to .ml (and perhaps others) when using pytorch lightning 0.7.2, and provided steps to reproduce the behavior and their environment.",
        "Issue_preprocessed_content":"Title: test metrics are no longer pushed to ; Content: bug pytorch lightning used to publish test metrics to commit has broken this functionality. to reproduce steps to reproduce the behavior run of training and observe test metrics not being submitted to . environment cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/760",
        "Issue_title":"Test metrics not logging to Comet after training",
        "Issue_creation_time":1580225794000,
        "Issue_closed_time":1582760093000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":10.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nWhen testing a model with `Trainer.test` metrics are not logged to Comet if the model was previously trained using `Trainer.fit`. While training metrics are logged correctly.\r\n\r\n\r\n#### Code sample\r\n```\r\n    comet_logger = CometLogger()\r\n    trainer = Trainer(logger=comet_logger)\r\n    model = get_model()\r\n\r\n    trainer.fit(model) # Metrics are logged to Comet\r\n    trainer.test(model) # No metrics are logged to Comet\r\n```\r\n\r\n### Expected behavior\r\n\r\nTest metrics should also be logged in to Comet.\r\n\r\n### Environment\r\n\r\n```\r\n- PyTorch version: 1.3.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.67\r\ncuDNN version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.6.0\r\n[pip3] torch==1.3.0\r\n[pip3] torchvision==0.4.1\r\n[conda] Could not collect\r\n```\r\n\r\n### Additional context\r\n\r\nI believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#L366), `logger.finalize(\"success\")` is called. This in turn calls `experiment.end()` inside the logger and the `Experiment` object doesn't expect to send more information after this.\r\n\r\nAn alternative is to create another `Trainer` object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the `ExistingExperiment` object form the Comet SDK, but the solution seems a little hacky and the `CometLogger` currently doesn't support this kind of experiment.\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: test metrics not logging to after training; Content: ## bug when testing a model with `trainer.test` metrics are not logged to if the model was previously trained using `trainer.fit`. while training metrics are logged correctly. #### code sample ``` _logger = logger() trainer = trainer(logger=_logger) model = get_model() trainer.fit(model) # metrics are logged to trainer.test(model) # no metrics are logged to ``` ### expected behavior test metrics should also be logged in to . ### environment ``` - pytorch version: 1.3.0 is debug build: no cuda used to build pytorch: 10.1.243 os: ubuntu 18.04.3 lts gcc version: (ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0 cmake version: version 3.10.2 python version: 3.7 is cuda available: yes cuda runtime version: 10.1.168 gpu models and configuration: gpu 0: geforce gtx 1080 ti gpu 1: geforce gtx 1080 ti gpu 2: geforce gtx 1080 ti gpu 3: geforce gtx 1080 ti gpu 4: geforce gtx 1080 ti gpu 5: geforce gtx 1080 ti gpu 6: geforce gtx 1080 ti gpu 7: geforce gtx 1080 ti nvidia driver version: 418.67 cudnn version: \/usr\/local\/cuda-10.1\/targets\/x86_64-linux\/lib\/libcudnn.so.7.6.1 versions of relevant libraries: [pip3] numpy==1.16.4 [pip3] pytorch-lightning==0.6.0 [pip3] torch==1.3.0 [pip3] torchvision==0.4.1 [conda] could not collect ``` ### additional context i believe the issue is caused because at the [end of the training routine](https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/deffbaba7ffb16ff57b56fe65f62df761f25fbd6\/pytorch_lightning\/trainer\/training_loop.py#l366), `logger.finalize(\"success\")` is called. this in turn calls `experiment.end()` inside the logger and the `experiment` object doesn't expect to send more information after this. an alternative is to create another `trainer` object, with another logger but this means that the metrics will be logged into a different experiment from the original. this issue can be solved using the `existingexperiment` object form the sdk, but the solution seems a little hacky and the `logger` currently doesn't support this kind of experiment.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where metrics were not logging to after training a model with `trainer.test` if the model was previously trained using `trainer.fit`.",
        "Issue_preprocessed_content":"Title: test metrics not logging to after training; Content: bug when testing a model with metrics are not logged to if the model was previously trained using . while training metrics are logged correctly. code sample expected behavior test metrics should also be logged in to . environment additional context i believe the issue is caused because at the , is called. this in turn calls inside the logger and the object doesn't expect to send more information after this. an alternative is to create another object, with another logger but this means that the metrics will be logged into a different experiment from the original. this issue can be solved using the object form the sdk, but the solution seems a little hacky and the currently doesn't support this kind of experiment."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/618",
        "Issue_title":"Comet PAPI Depreciated",
        "Issue_creation_time":1575967432000,
        "Issue_closed_time":1576023863000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.\r\n\r\nExample:\r\n`COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https:\/\/www.comet.ml\/docs\/python-sdk\/releases\/#release-300`",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: papi depreciated; use of the api logger reports an unecessary depreciation warning relating to the use of _ml.papi, rather than the newer _ml.api. example: ` warning: you have imported _ml.papi; Content: this interface is deprecated. please use _ml.api instead. for more information, see: https:\/\/www..ml\/docs\/python-sdk\/releases\/#release-300`",
        "Issue_original_content_gpt_summary":"The user encountered a depreciation warning when using the API logger, which indicated that the older _ml.papi interface should be replaced with the newer _ml.api interface.",
        "Issue_preprocessed_content":"Title: papi depreciated; Content: use of the api logger reports an unecessary depreciation warning relating to the use of rather than the newer example"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/470",
        "Issue_title":"CometLogger does not implement name() and version() class methods",
        "Issue_creation_time":1573092782000,
        "Issue_closed_time":1573531232000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Explicitly creating a CometLogger instance and passing it to Trainer using trainer(logger=my_comet_logger) raises a NotImplementedError because CometLogger does not implement the name() and version() class methods.\r\n\r\nBelow is the traceback:\r\n`\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 126, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train\r\n    self.run_training_epoch()\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch\r\n    output = self.run_training_batch(batch, batch_nb)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch\r\n    self.main_progress_bar.set_postfix(**self.training_tqdm_dict)\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict\r\n    if self.logger is not None and self.logger.version is not None:\r\n  File \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version\r\n    raise NotImplementedError(\"Sub-classes must provide a version property\")\r\n`\r\n\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logger does not implement name() and version() class methods; Content: explicitly creating a logger instance and passing it to trainer using trainer(logger=my__logger) raises a notimplementederror because logger does not implement the name() and version() class methods. below is the traceback: ` traceback (most recent call last): file \"main.py\", line 126, in trainer.fit(model) file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 351, in fit self.single_gpu_train(model) file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/dp_mixin.py\", line 77, in single_gpu_train self.run_pretrain_routine(model) file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 471, in run_pretrain_routine self.train() file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 60, in train self.run_training_epoch() file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 99, in run_training_epoch output = self.run_training_batch(batch, batch_nb) file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/train_loop_mixin.py\", line 255, in run_training_batch self.main_progress_bar.set_postfix(**self.training_tqdm_dict) file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 309, in training_tqdm_dict if self.logger is not none and self.logger.version is not none: file \"\/home\/ryan\/miniconda3\/envs\/compling\/lib\/python3.7\/site-packages\/pytorch_lightning\/logging\/base.py\", line 76, in version raise notimplementederror(\"sub-classes must provide a version property\") `",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where explicitly creating a logger instance and passing it to trainer using trainer(logger=my__logger) raised a NotImplementedError because the logger did not implement the name() and version() class methods.",
        "Issue_preprocessed_content":"Title: logger does not implement name and version class methods; Content: explicitly creating a logger instance and passing it to trainer using raises a notimplementederror because logger does not implement the name and version class methods. below is the traceback"
    },
    {
        "Issue_link":"https:\/\/github.com\/ultralytics\/yolov5\/issues\/10301",
        "Issue_title":"Comet Bug: Unable to train on Window 11",
        "Issue_creation_time":1669476859000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":14.0,
        "Issue_body":"### Search before asking\n\n- [X] I have searched the YOLOv5 [issues](https:\/\/github.com\/ultralytics\/yolov5\/issues) and [discussions](https:\/\/github.com\/ultralytics\/yolov5\/discussions) and found no similar questions.\n\n\n### Question\n\nI am unable to train alway the same error:\r\n\r\npython train.py --img 640 --batch 16 --epochs 5 --data dataset.yaml --weights yolov5s.pt\r\ntrain: weights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\ngithub: skipping check (not a git repository), for updates see https:\/\/github.com\/ultralytics\/yolov5\r\nYOLOv5  2022-11-26 Python-3.9.13 torch-1.13.0+cpu CPU\r\n\r\nhyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\nClearML: run 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\r\nTensorBoard: Start with 'tensorboard --logdir runs\\train', view at http:\/\/localhost:6006\/\r\nCOMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Using 'C:\\\\Users\\\\telem\\\\Desktop\\\\Yolo\\\\.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\r\nCOMET WARNING: Native output logging mode is not available, falling back to basic output logging\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 633, in <module>\r\n    main(opt)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 527, in main\r\n    train(opt.hyp, opt, device, callbacks)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\train.py\", line 95, in train\r\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\__init__.py\", line 132, in __init__\r\n    self.comet_logger = CometLogger(self.opt, self.hyp)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 97, in __init__\r\n    self.data_dict = self.check_dataset(self.opt.data)\r\n  File \"C:\\Users\\telem\\Desktop\\Yolo\\utils\\loggers\\comet\\__init__.py\", line 234, in check_dataset\r\n    if data_config['path'].startswith(COMET_PREFIX):\r\nKeyError: 'path'\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO: Comet.ml OfflineExperiment Summary\r\nCOMET INFO: ----------------------------------\r\nCOMET INFO:   Data:\r\nCOMET INFO:     display_summary_level : 1\r\nCOMET INFO:     url                   : [OfflineExperiment will get URL after upload]\r\nCOMET INFO:   Others:\r\nCOMET INFO:     offline_experiment : True\r\nCOMET INFO:   Uploads:\r\nCOMET INFO:     environment details : 1\r\nCOMET INFO:     installed packages  : 1\r\nCOMET INFO: ----------------------------------\r\nCOMET WARNING: Experiment Name is generated at upload time for Offline Experiments unless set explicitly with Experiment.set_name\r\nCOMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: tensorboard, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\r\nCOMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\r\nCOMET INFO: Starting saving the offline archive\r\nCOMET INFO: To upload this offline experiment, run:\r\n    comet upload C:\\Users\\telem\\Desktop\\Yolo\\.cometml-runs\\5f05924ec89f489db0356c7c3201ce0f.zip\r\n\r\nI have tested many dataset and alway the same error any advice ?\r\n\n\n### Additional\n\n_No response_",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: bug: unable to train on window 11; Content: ### search before asking - [x] i have searched the yolov5 [issues](https:\/\/github.com\/ultralytics\/yolov5\/issues) and [discussions](https:\/\/github.com\/ultralytics\/yolov5\/discussions) and found no similar questions. ### question i am unable to train alway the same error: python train.py --img 640 --batch 16 --epochs 5 --data dataset.yaml --weights yolov5s.pt train: weights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=false, resume=false, nosave=false, noval=false, noautoanchor=false, noplots=false, evolve=none, bucket=, cache=none, image_weights=false, device=, multi_scale=false, single_cls=false, optimizer=sgd, sync_bn=false, workers=8, project=runs\\train, name=exp, exist_ok=false, quad=false, cos_lr=false, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=none, upload_dataset=false, bbox_interval=-1, artifact_alias=latest github: skipping check (not a git repository), for updates see https:\/\/github.com\/ultralytics\/yolov5 yolov5 2022-11-26 python-3.9.13 torch-1.13.0+cpu cpu hyperparameters: lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0 : run 'pip install ' to automatically track, visualize and remotely train yolov5 in tensorboard: start with 'tensorboard --logdir runs\\train', view at http:\/\/localhost:6006\/ warning: credentials have not been set. will default to offline logging. please set your credentials to enable online logging. warning: has disabled auto-logging functionality as it has been imported after the following ml modules: tensorboard, torch. metrics and hyperparameters can still be logged using _ml.log_metrics() and _ml.log_parameters() info: using 'c:\\\\users\\\\telem\\\\desktop\\\\yolo\\\\.ml-runs' path as offline directory. pass 'offline_directory' parameter into constructor or set the '_offline_directory' environment variable to manually choose where to store offline experiment archives. warning: native output logging mode is not available, falling back to basic output logging traceback (most recent call last): file \"c:\\users\\telem\\desktop\\yolo\\train.py\", line 633, in main(opt) file \"c:\\users\\telem\\desktop\\yolo\\train.py\", line 527, in main train(opt.hyp, opt, device, callbacks) file \"c:\\users\\telem\\desktop\\yolo\\train.py\", line 95, in train loggers = loggers(save_dir, weights, opt, hyp, logger) # loggers instance file \"c:\\users\\telem\\desktop\\yolo\\utils\\loggers\\__init__.py\", line 132, in __init__ self._logger = logger(self.opt, self.hyp) file \"c:\\users\\telem\\desktop\\yolo\\utils\\loggers\\\\__init__.py\", line 97, in __init__ self.data_dict = self.check_dataset(self.opt.data) file \"c:\\users\\telem\\desktop\\yolo\\utils\\loggers\\\\__init__.py\", line 234, in check_dataset if data_config['path'].startswith(_prefix): keyerror: 'path' info: ---------------------------------- info: .ml offlineexperiment summary info: ---------------------------------- info: data: info: display_summary_level : 1 info: url : [offlineexperiment will get url after upload] info: others: info: offline_experiment : true info: uploads: info: environment details : 1 info: installed packages : 1 info: ---------------------------------- warning: experiment name is generated at upload time for offline experiments unless set explicitly with experiment.set_name warning: has disabled auto-logging functionality as it has been imported after the following ml modules: tensorboard, torch. metrics and hyperparameters can still be logged using _ml.log_metrics() and _ml.log_parameters() info: still saving offline stats to messages file before program termination (may take up to 120 seconds) info: starting saving the offline archive info: to upload this offline experiment, run: upload c:\\users\\telem\\desktop\\yolo\\.ml-runs\\5f05924ec89f489db0356c7c3201ce0f.zip i have tested many dataset and alway the same error any advice ? ### additional _no response_",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with training on Windows 11, where they received a KeyError and were unable to find a similar issue in the Yolov5 issues and discussions.",
        "Issue_preprocessed_content":"Title: bug unable to train on window ; Content: search before asking i have searched the yolov and and found no similar questions. question i am unable to train alway the same error python train cfg , epochs , imgsz , rect false, resume false, nosave false, noval false, noautoanchor false, noplots false, evolve none, bucket , cache none, device , optimizer sgd, workers , name exp, quad false, patience , freeze , seed , entity none, github skipping check , for updates see yolov cpu hyperparameters run 'pip install ' to automatically track, visualize and remotely train yolov in tensorboard start with 'tensorboard view at warning credentials have not been set. will default to offline logging. please set your credentials to enable online logging. warning has disabled functionality as it has been imported after the following ml modules tensorboard, torch. metrics and hyperparameters can still be logged using and info using path as offline directory. pass parameter into constructor or set the environment variable to manually choose where to store offline experiment archives. warning native output logging mode is not available, falling back to basic output logging traceback file line , in main file line , in main opt, device, callbacks file line , in train loggers weights, opt, hyp, logger loggers instance file line , in file line , in file line , in if keyerror 'path' info info offlineexperiment summary info info data info info url info others info true info uploads info environment details info installed packages info warning experiment name is generated at upload time for offline experiments unless set explicitly with warning has disabled functionality as it has been imported after the following ml modules tensorboard, torch. metrics and hyperparameters can still be logged using and info still saving offline stats to messages file before program termination info starting saving the offline archive info to upload this offline experiment, run upload i have tested many dataset and alway the same error any advice ? additional"
    },
    {
        "Issue_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/733",
        "Issue_title":"The learning rate plot in Comet is not the expected one",
        "Issue_creation_time":1591889615000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"Hi! I've been trying the comet.ml integration and I must say this has been a great addition to the framework. \ud83d\ude4c\r\n\r\nI wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that I expected, especially when trying the learning_rate_warmup_epochs option, which I set to 6 as suggested. The learning rate that is plot on comet is the one set in learning_rate, and it's constant for the first epochs.\r\n\r\nCould this be related to this error?\r\n\r\n`COMET ERROR: Failed to extract parameters from Optimizer.init()\r\n`\r\n\r\n**To Reproduce**\r\n1. Setup comet\r\n2. Set  learning_rate_warmup_epochs option to 6\r\n\r\n**Expected behavior**\r\nI expected to see the lr increase in the first 6 epochs, reach the lr set in learning_rate, and eventually decrease, as I set also reduce_learning_rate_on_plateau .\r\n\r\n**Actual behavior**\r\nThe lr is equal to the set learning_rate in the first epochs, and eventually decreases due to reduce_learning_rate_on_plateau .\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: the learning rate plot in is not the expected one; Content: hi! i've been trying the .ml integration and i must say this has been a great addition to the framework. i wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that i expected, especially when trying the learning_rate_warmup_epochs option, which i set to 6 as suggested. the learning rate that is plot on is the one set in learning_rate, and it's constant for the first epochs. could this be related to this error? ` error: failed to extract parameters from optimizer.init() ` **to reproduce** 1. setup 2. set learning_rate_warmup_epochs option to 6 **expected behavior** i expected to see the lr increase in the first 6 epochs, reach the lr set in learning_rate, and eventually decrease, as i set also reduce_learning_rate_on_plateau . **actual behavior** the lr is equal to the set learning_rate in the first epochs, and eventually decreases due to reduce_learning_rate_on_plateau .",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the learning rate plot in TensorBoard was not the expected one, despite setting the learning_rate_warmup_epochs option to 6.",
        "Issue_preprocessed_content":"Title: the learning rate plot in is not the expected one; Content: hi! i've been trying the integration and i must say this has been a great addition to the framework. i wanted to exploit it to keep track of the learning rate updates, but the lr being plot is not the one that i expected, especially when trying the option, which i set to as suggested. the learning rate that is plot on is the one set in and it's constant for the first epochs. could this be related to this error? to reproduce . setup . set option to expected behavior i expected to see the lr increase in the first epochs, reach the lr set in and eventually decrease, as i set also . actual behavior the lr is equal to the set in the first epochs, and eventually decreases due to ."
    },
    {
        "Issue_link":"https:\/\/github.com\/ludwig-ai\/ludwig\/issues\/340",
        "Issue_title":"Logging issue when activating Comet contrib",
        "Issue_creation_time":1557825272000,
        "Issue_closed_time":1559077203000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"**Describe the bug**\r\n\r\nWhen activating the Comet contrib, most of Ludwig log message disappears.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\nLaunch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --comet`\r\n\r\nYou won't see the following output:\r\n```\r\n _         _        _      \r\n| |_  _ __| |_ __ _(_)__ _ \r\n| | || \/ _` \\ V  V \/ \/ _` |\r\n|_|\\_,_\\__,_|\\_\/\\_\/|_\\__, |\r\n                     |___\/ \r\nludwig v0.1.2 - Experiment\r\n\r\nExperiment name: experiment\r\nModel name: run\r\nOutput path: results\/experiment_run_43\r\n\r\n\r\nludwig_version: '0.1.2'\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThe log messages should be displayed when the Comet contrib is activated.\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Fedora\r\n - Version 28\r\n- Python version: 3.6.8\r\n- Ludwig version: 0.1.2\r\n\r\n**Additional context**\r\n\r\nI think the issue is that ludwig is using the root-level logger configured through `logging.basicConfig`. The comet contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/comet.py#L56.\r\n\r\nThose calls happen before any `basicConfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#L461.\r\n\r\nThe issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicConfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/Lib\/logging\/__init__.py#L2065. The direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a StreamHandler pointing to `\/dev\/stderr`.\r\n\r\nThe unfortunate side-effect is that calling `basicConfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device.\r\n\r\nI would recommend moving from using the root logger and configure the logger through `basicConfig` to using a `ludwig` logger and configure it manually, it's not that more complex. I can help if wanted.\r\n\r\nOne last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. That includes requests and is polluting the output. Using a separate logger would also solve this issue.\r\n",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: logging issue when activating contrib; Content: **describe the bug** when activating the contrib, most of ludwig log message disappears. **to reproduce** steps to reproduce the behavior: launch: `ludwig experiment --data_csv reuters-allcats.csv --model_definition_file model_definition.yaml -l info --` you won't see the following output: ``` _ _ _ | |_ _ __| |_ __ _(_)__ _ | | || \/ _` \\ v v \/ \/ _` | |_|\\_,_\\__,_|\\_\/\\_\/|_\\__, | |___\/ ludwig v0.1.2 - experiment experiment name: experiment model name: run output path: results\/experiment_run_43 ludwig_version: '0.1.2' ``` **expected behavior** the log messages should be displayed when the contrib is activated. **environment (please complete the following information):** - os: fedora - version 28 - python version: 3.6.8 - ludwig version: 0.1.2 **additional context** i think the issue is that ludwig is using the root-level logger configured through `logging.basicconfig`. the contrib integration contains some logging calls, for example, https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/contribs\/.py#l56. those calls happen before any `basicconfig` call https:\/\/github.com\/uber\/ludwig\/blob\/master\/ludwig\/experiment.py#l461. the issue with calling the root-level `logging.info`, `logging.error` and so on is that they will call `logging.basicconfig` on their own if the root logger is not configured yet https:\/\/github.com\/python\/cpython\/blob\/master\/lib\/logging\/__init__.py#l2065. the direct effect is that the first call to `logging.info` will configure the root logger with no configuration which will create a streamhandler pointing to `\/dev\/stderr`. the unfortunate side-effect is that calling `basicconfig` will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device. i would recommend moving from using the root logger and configure the logger through `basicconfig` to using a `ludwig` logger and configure it manually, it's not that more complex. i can help if wanted. one last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. that includes requests and is polluting the output. using a separate logger would also solve this issue.",
        "Issue_original_content_gpt_summary":"The user encountered a logging issue when activating the contrib, where most of Ludwig's log messages disappeared due to a misconfiguration of the root logger.",
        "Issue_preprocessed_content":"Title: logging issue when activating contrib; Content: describe the bug when activating the contrib, most of ludwig log message disappears. to reproduce steps to reproduce the behavior launch you won't see the following output expected behavior the log messages should be displayed when the contrib is activated. environment os fedora version python version ludwig version additional context i think the issue is that ludwig is using the logger configured through . the contrib integration contains some logging calls, for example, those calls happen before any call the issue with calling the , and so on is that they will call on their own if the root logger is not configured yet the direct effect is that the first call to will configure the root logger with no configuration which will create a streamhandler pointing to . the unfortunate is that calling will do nothing as the root handler as already a handler so the root logger will not be set to the right log level and the stream handler will not point to the right device. i would recommend moving from using the root logger and configure the logger through to using a logger and configure it manually, it's not that more complex. i can help if wanted. one last issue with using the root logger is when configuring the root logger to the debug level, all libraries which are logging will start displaying their log messages. that includes requests and is polluting the output. using a separate logger would also solve this issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/cc-ai\/climategan\/issues\/116",
        "Issue_title":"Comet \"Reproduce\" feature doesn't work",
        "Issue_creation_time":1595861398000,
        "Issue_closed_time":1624956881000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"It fails at the \"apply patch\" stage",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: \"reproduce\" feature doesn't work; Content: it fails at the \"apply patch\" stage",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the \"reproduce\" feature was not working, as it was failing at the \"apply patch\" stage.",
        "Issue_preprocessed_content":"Title: reproduce feature doesn't work; Content: it fails at the apply patch stage"
    },
    {
        "Issue_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/132",
        "Issue_title":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.",
        "Issue_creation_time":1600305917000,
        "Issue_closed_time":1600309841000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"When using nn.DataParallel, the name of the model saved in comet.ml will be DataParallel.\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: 0.7.0\r\n- Python version: 3.6.6\r\n- OS: Ubuntu 18.04\r\n- (Optional) Other libraries and their versions:\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\n# python code\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: when using nn.dataparallel, the name of the model saved in .ml will be dataparallel.; Content: when using nn.dataparallel, the name of the model saved in .ml will be dataparallel. ## expected behavior ## environment - enchanter version: 0.7.0 - python version: 3.6.6 - os: ubuntu 18.04 - (optional) other libraries and their versions: ## error messages, stack traces, or logs ``` # error messages, stack traces, or logs ``` ## steps to reproduce 1. 2. 3. ## reproducible examples (optional) ```python # python code ``` ## additional context (optional)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using nn.dataparallel, where the name of the model saved in .ml was not as expected.",
        "Issue_preprocessed_content":"Title: when using the name of the model saved in will be dataparallel.; Content: when using the name of the model saved in will be dataparallel. expected behavior please write a clear and concise description of what you expected to happen. environment enchanter version python version os ubuntu other libraries and their versions error messages, stack traces, or logs steps to reproduce . . . reproducible examples additional context please add any other context or screenshots about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/khirotaka\/enchanter\/issues\/129",
        "Issue_title":"COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)",
        "Issue_creation_time":1600151670000,
        "Issue_closed_time":1600153381000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Enchanter v0.7.0 raise `COMET WARNING: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...)` when using Context API\r\n\r\n## Expected behavior\r\n\r\n<!-- Please write a clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n- Enchanter version: v0.7.0\r\n- Python version: ?\r\n- OS: Linux\r\n- (Optional) Other libraries and their versions: Google Colab with GPU\r\n\r\n## Error messages, stack traces, or logs\r\n\r\n```\r\n# error messages, stack traces, or logs\r\n```\r\n\r\n## Steps to reproduce\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n## Reproducible examples (optional)\r\n\r\n```python\r\nrunner = ClassificationRunner(\r\n    net, optimizer, criterion, Experiment()\r\n)\r\n\r\nwith runner:\r\n    runner.scaler = torch.cuda.amp.GradScaler()\r\n\r\n    runner.add_loader(\"train\", trainloader)\r\n    runner.add_loader(\"test\", testloader)\r\n    runner.train_config(epochs=20)\r\n\r\n    runner.run()\r\n```\r\n\r\n## Additional context (optional)\r\n\r\n<!-- Please add any other context or screenshots about the problem here. -->",
        "Tool":"Comet",
        "Platform":"Github",
        "Issue_original_content":"Title: warning: log_asset_data(..., file_name=...) is deprecated; use log_asset_data(..., name=...); enchanter v0.7.0 raise ` warning: log_asset_data(..., file_name=...) is deprecated; Content: use log_asset_data(..., name=...)` when using context api ## expected behavior ## environment - enchanter version: v0.7.0 - python version: ? - os: linux - (optional) other libraries and their versions: google colab with gpu ## error messages, stack traces, or logs ``` # error messages, stack traces, or logs ``` ## steps to reproduce 1. 2. 3. ## reproducible examples (optional) ```python runner = classificationrunner( net, optimizer, criterion, experiment() ) with runner: runner.scaler = torch.cuda.amp.gradscaler() runner.add_loader(\"train\", trainloader) runner.add_loader(\"test\", testloader) runner.train_config(epochs=20) runner.run() ``` ## additional context (optional)",
        "Issue_original_content_gpt_summary":"The user encountered a warning when using the context API in enchanter v0.7.0, which stated that log_asset_data(..., file_name=...) is deprecated and should be replaced with log_asset_data(..., name=...).",
        "Issue_preprocessed_content":"Title: warning is deprecated; use ; Content: enchanter raise when using context api expected behavior please write a clear and concise description of what you expected to happen. environment enchanter version python version ? os linux other libraries and their versions google colab with gpu error messages, stack traces, or logs steps to reproduce . . . reproducible examples additional context please add any other context or screenshots about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/396",
        "Issue_title":"Fix the definition of pipelines\/sentence_embedding\/dvc.yaml",
        "Issue_creation_time":1625148874000,
        "Issue_closed_time":1626683431000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## \ud83d\udc1b Bug description\r\n\r\nThe pipeline `sentence_embedding\/dvc.yaml` is not correctly defined for `evaluation:deps`.\r\n\r\nThis creates the following issues:\r\n  - The evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`.\r\n  - The training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`.\r\n\r\n## To reproduce\r\n\r\n```\r\ngit checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972\r\nexport PIPELINE=data_and_models\/pipelines\/sentence_embedding\/dvc.yaml\r\ndvc pull -d $PIPELINE\r\ndvc repro -f $PIPELINE\r\n```\r\n\r\nThis will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@biobert_nli_sts_cord19_v1':\r\n...\r\nAttributeError: Path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found\r\n```\r\n\r\nAfter manually pulling `biobert_nli_sts_cord19_v1`, this will give the error:\r\n```\r\nRunning stage 'data_and_models\/pipelines\/sentence_embedding\/dvc.yaml:evaluation@tf_idf':\r\n...\r\nFileNotFoundError: [Errno 2] No such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl'\r\n```\r\n\r\n## Expected behavior\r\n\r\n`dvc pull -d` and `dvc repro -f` should run without errors about missing files.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: fix the definition of pipelines\/sentence_embedding\/.yaml; Content: ## bug description the pipeline `sentence_embedding\/.yaml` is not correctly defined for `evaluation:deps`. this creates the following issues: - the evaluation stage does not know how to pull the model `biobert_nli_sts_cord19_v1\/`. - the training stage does not know it has to run before the evaluation stage for the models `tf_idf\/` and `count\/`. ## to reproduce ``` git checkout 12988ef564dd4e6373a7455f5ee30c0608e2e972 export pipeline=data_and_models\/pipelines\/sentence_embedding\/.yaml pull -d $pipeline repro -f $pipeline ``` this will give the error: ``` running stage 'data_and_models\/pipelines\/sentence_embedding\/.yaml:evaluation@biobert_nli_sts_cord19_v1': ... attributeerror: path ..\/..\/models\/sentence_embedding\/biobert_nli_sts_cord19_v1\/ not found ``` after manually pulling `biobert_nli_sts_cord19_v1`, this will give the error: ``` running stage 'data_and_models\/pipelines\/sentence_embedding\/.yaml:evaluation@tf_idf': ... filenotfounderror: [errno 2] no such file or directory: '..\/..\/models\/sentence_embedding\/tf_idf\/model.pkl' ``` ## expected behavior ` pull -d` and ` repro -f` should run without errors about missing files.",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the definition of the pipeline `sentence_embedding\/.yaml` which caused errors when running `pull -d` and `repro -f` commands.",
        "Issue_preprocessed_content":"Title: fix the definition of ; Content: bug description the pipeline is not correctly defined for . this creates the following issues the evaluation stage does not know how to pull the model . the training stage does not know it has to run before the evaluation stage for the models and . to reproduce this will give the error after manually pulling , this will give the error expected behavior and should run without errors about missing files."
    },
    {
        "Issue_link":"https:\/\/github.com\/BlueBrain\/Search\/issues\/361",
        "Issue_title":"DVC eval crashes \"int64 not JSON serializable\"",
        "Issue_creation_time":1620385977000,
        "Issue_closed_time":1620393602000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The DVC evaluation is crashing. After investigation, the bug was introduced by #348.\r\n\r\nThe bug:\r\n```\r\nTraceback (most recent call last):\r\n  File \"eval.py\", line 111, in <module>\r\n    main()\r\n  File \"eval.py\", line 107, in main\r\n    json.dump(all_metrics_dict, f)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump\r\n    for chunk in iterable:\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode\r\n    yield from _iterencode_dict(o, _current_indent_level)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict\r\n    yield from chunks\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode\r\n    o = _default(o)\r\n  File \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type int64 is not JSON serializable\r\n```\r\n\r\nTo reproduce:\r\n\r\n```\r\n# For the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c.\r\n# For the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03.\r\nexport COMMIT=...\r\n\r\ngit clone https:\/\/github.com\/BlueBrain\/Search\r\ncd Search\/\r\n\r\n# Change <image> and <container>.\r\ndocker build -f data_and_models\/pipelines\/ner\/Dockerfile --build-arg BBS_REVISION=$COMMIT -t <image> .\r\ndocker run -it --rm -v \/raid:\/raid --name <container> <image>\r\n\r\ngit checkout $COMMIT\r\ngit checkout -- data_and_models\/pipelines\/ner\/dvc.lock\r\n\r\ncd data_and_models\/pipelines\/ner\/\r\ndvc pull --with-deps evaluation@organism\r\ndvc repro -fs evaluation@organism\r\n```\r\n\r\n_Originally posted by @pafonta in https:\/\/github.com\/BlueBrain\/Search\/issues\/335#issuecomment-833506692_",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: eval crashes \"int64 not json serializable\"; Content: the evaluation is crashing. after investigation, the bug was introduced by #348. the bug: ``` traceback (most recent call last): file \"eval.py\", line 111, in main() file \"eval.py\", line 107, in main json.dump(all_metrics_dict, f) file \"\/opt\/conda\/lib\/python3.8\/json\/__init__.py\", line 179, in dump for chunk in iterable: file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 431, in _iterencode yield from _iterencode_dict(o, _current_indent_level) file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 405, in _iterencode_dict yield from chunks file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 438, in _iterencode o = _default(o) file \"\/opt\/conda\/lib\/python3.8\/json\/encoder.py\", line 179, in default raise typeerror(f'object of type {o.__class__.__name__} ' typeerror: object of type int64 is not json serializable ``` to reproduce: ``` # for the bug introduced by #348, use 0bb500551b1b7c6f5bb9228335aa4df30a654e9c. # for the working code __before__ #348, use b9c886966ca4d893b41457a17262e198e3ba7f03. export commit=... git clone https:\/\/github.com\/bluebrain\/search cd search\/ # change and . docker build -f data_and_models\/pipelines\/ner\/dockerfile --build-arg bbs_revision=$commit -t . docker run -it --rm -v \/raid:\/raid --name git checkout $commit git checkout -- data_and_models\/pipelines\/ner\/.lock cd data_and_models\/pipelines\/ner\/ pull --with-deps evaluation@organism repro -fs evaluation@organism ``` _originally posted by @pafonta in https:\/\/github.com\/bluebrain\/search\/issues\/335#issuecomment-833506692_",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the evaluation process which caused it to crash due to an int64 not being json serializable, which was introduced by a commit in the repository.",
        "Issue_preprocessed_content":"Title: eval crashes int not json serializable ; Content: the evaluation is crashing. after investigation, the bug was introduced by . the bug to reproduce posted by in"
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/435",
        "Issue_title":"Combine `zn.params` and `dvc.params` might not work",
        "Issue_creation_time":1668011905000,
        "Issue_closed_time":1668012691000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":null,
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: combine `zn.params` and `.params` might not work; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to combine `zn.params` and `.params` which did not work.",
        "Issue_preprocessed_content":"Title: combine and might not work; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/348",
        "Issue_title":"znNodes not working with `dvc.<...>`",
        "Issue_creation_time":1658850596000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"- [ ] fix docstring\r\n- [ ] test with a Node that has `dvc.params` and `dvc.outs`\r\n\r\nhttps:\/\/github.com\/zincware\/ZnTrack\/blob\/cd2c4f05ad5abf2b23da80fe56558cef6c73e636\/zntrack\/zn\/nodes.py#L11-L28",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: znnodes not working with `.<...>`; Content: - [ ] fix docstring - [ ] test with a node that has `.params` and `.outs` https:\/\/github.com\/zincware\/zntrack\/blob\/cd2c4f05ad5abf2b23da80fe56558cef6c73e636\/zntrack\/zn\/nodes.py#l11-l28",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with znnodes not working with `.<...>` and had to fix the docstring and test with a node that has `.params` and `.outs`.",
        "Issue_preprocessed_content":"Title: znnodes not working with ; Content: fix docstring test with a node that has and"
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/211",
        "Issue_title":"zn.Method does not add params to `dvc.yaml`",
        "Issue_creation_time":1643228171000,
        "Issue_closed_time":1643235530000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When only `zn.Method` without `zn.params` is used in a Node the `dvc.yaml` will not depend on the `params.yaml`.\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: zn.method does not add params to `.yaml`; Content: when only `zn.method` without `zn.params` is used in a node the `.yaml` will not depend on the `params.yaml`.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where using only zn.method without zn.params in a node caused the .yaml file to not depend on the params.yaml.",
        "Issue_preprocessed_content":"Title: does not add params to ; Content: when only without is used in a node the will not depend on the ."
    },
    {
        "Issue_link":"https:\/\/github.com\/zincware\/ZnTrack\/issues\/76",
        "Issue_title":"raise Error if pre-initialized DVC option is being changed",
        "Issue_creation_time":1633013182000,
        "Issue_closed_time":1634716886000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"One can either define a DVC option with default values in the init, which could be considered a constant, or change a DVC option that has no default values in the call method.\r\n\r\nIf a pre-intialized DVC option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: raise error if pre-initialized option is being changed; Content: one can either define a option with default values in the init, which could be considered a constant, or change a option that has no default values in the call method. if a pre-intialized option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of raising an error if a pre-initialized option is being changed, as this could lead to issues and should be logged or an exception should be raised.",
        "Issue_preprocessed_content":"Title: raise error if option is being changed; Content: one can either define a option with default values in the init, which could be considered a constant, or change a option that has no default values in the call method. if a option is being changed within the call that can lead to issues and should either raise an exception or at least log that it can lead to not supported problems"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/255",
        "Issue_title":"ERROR: 'dvc.lock' is git-ignored.",
        "Issue_creation_time":1619681675000,
        "Issue_closed_time":1621495987000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"```\r\n$ dvc repro run_benchmarks\r\nERROR: 'dvc.lock' is git-ignored.\r\n```\r\n\r\n`.dvc.lock` in `.gitignore` causes Exceptions at running benchmark. Delete this line solves this problem. And because of #168 maybe we need some better ways to deal with `dvc.lock`.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: error: '.lock' is git-ignored.; Content: ``` $ repro run_benchmarks error: '.lock' is git-ignored. ``` `..lock` in `.gitignore` causes exceptions at running benchmark. delete this line solves this problem. and because of #168 maybe we need some better ways to deal with `.lock`.",
        "Issue_original_content_gpt_summary":"The user encountered an error when running a benchmark due to a '.lock' file being git-ignored, and needed to delete the line from the .gitignore file to solve the problem.",
        "Issue_preprocessed_content":"Title: error is ; Content: in causes exceptions at running benchmark. delete this line solves this problem. and because of maybe we need some better ways to deal with ."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/244",
        "Issue_title":"requirements: update dvc",
        "Issue_creation_time":1616672577000,
        "Issue_closed_time":1628758546000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"After https:\/\/github.com\/iterative\/dvc\/pull\/5265\r\nWe do not allow ignoring lockfile. `dvc-bench` is running currently on some older version of `dvc`, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: requirements: update ; Content: after https:\/\/github.com\/iterative\/\/pull\/5265 we do not allow ignoring lockfile. `-bench` is running currently on some older version of ``, though it would be good to adjust it so that it works with `>2.0.0`.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of updating the requirements to ensure that the '-bench' command works with versions greater than 2.0.0, while still not allowing the ignoring of the lockfile.",
        "Issue_preprocessed_content":"Title: requirements update ; Content: after we do not allow ignoring lockfile. is running currently on some older version of , though it would be good to adjust it so that it works with ."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-bench\/issues\/149",
        "Issue_title":"dvc tries to launch updater using asv script",
        "Issue_creation_time":1593808834000,
        "Issue_closed_time":1594041670000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"In every run you can see:\r\n```\r\n               2020-07-03 23:24:19,549 DEBUG: Trying to spawn '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\r\n\/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               2020-07-03 23:24:19,550 DEBUG: Spawned '['\/home\/efiop\/git\/dvc-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef\r\niop\/.pyenv\/versions\/3.8.3\/envs\/dvc-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']'\r\n               Unknown mode daemon\r\n```\r\nwe clearly need to take more care on dvc-side, but a good enough workaround is to set CI or DVC_TEST env var to make dvc skip launching the updater.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: tries to launch updater using asv script; Content: in every run you can see: ``` 2020-07-03 23:24:19,549 debug: trying to spawn '['\/home\/efiop\/git\/-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', ' \/home\/efiop\/.pyenv\/versions\/3.8.3\/envs\/-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']' 2020-07-03 23:24:19,550 debug: spawned '['\/home\/efiop\/git\/-bench\/envs\/76391772e92136ec87b9940d70226329\/bin\/python', '\/home\/ef iop\/.pyenv\/versions\/3.8.3\/envs\/-3.8.3\/lib\/python3.8\/site-packages\/asv\/benchmark.py', 'daemon', '-q', 'updater']' unknown mode daemon ``` we clearly need to take more care on -side, but a good enough workaround is to set ci or _test env var to make skip launching the updater.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to launch an updater using an asv script, requiring them to take more care on the side and set a ci or _test env var to make it skip launching the updater.",
        "Issue_preprocessed_content":"Title: tries to launch updater using asv script; Content: in every run you can see we clearly need to take more care on but a good enough workaround is to set ci or env var to make skip launching the updater."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/98",
        "Issue_title":"Various issues in `example-dvc-experiments`",
        "Issue_creation_time":1638880026000,
        "Issue_closed_time":1642605521000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":11.0,
        "Issue_body":"> These are reported by @tapadipti (thanks). I'm moving here to discuss and follow: \r\n\r\nI was running experiments by following the docs (https:\/\/dvc.org\/doc\/start\/experiments) and encountered the following issues. Sharing here for any required action.\r\n1. dvc is not installed by `pip install -r requirements.txt`. So, if someone is trying to use a new virtual env, they need to install dvc separately. Would be good to include `dvc` in `requirements.txt`.\r\n2. `dvc pull` gave this error:\r\n   ```\r\n   ERROR: failed to pull data from the cloud - Checkout failed for following targets:\r\n   models\/model.h5\r\n   metrics\r\n   Is your cache up to date?\r\n   <https:\/\/error.dvc.org\/missing-files>\r\n   ```\r\n\r\n3. `dvc exp run` lists all the image when running the `extract` stage. Would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data`\r\n4. `If you used dvc repro before` section in the doc is a little unclear. Does `dvc exp run` replace `dvc repro`? If yes, can we state this clearly? Also would be great to change this statement `We use dvc repro to run the pipeline...` to `dvc repro runs the pipeline...`",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: various issues in `example--experiments`; Content: > these are reported by @tapadipti (thanks). i'm moving here to discuss and follow: i was running experiments by following the docs (https:\/\/.org\/doc\/start\/experiments) and encountered the following issues. sharing here for any required action. 1. is not installed by `pip install -r requirements.txt`. so, if someone is trying to use a new virtual env, they need to install separately. would be good to include `` in `requirements.txt`. 2. ` pull` gave this error: ``` error: failed to pull data from the cloud - checkout failed for following targets: models\/model.h5 metrics is your cache up to date? ``` 3. ` exp run` lists all the image when running the `extract` stage. would be good to remove `-v` from `tar -xvzf data\/images.tar.gz --directory data` 4. `if you used repro before` section in the doc is a little unclear. does ` exp run` replace ` repro`? if yes, can we state this clearly? also would be great to change this statement `we use repro to run the pipeline...` to ` repro runs the pipeline...`",
        "Issue_original_content_gpt_summary":"The user encountered various issues while running experiments by following the documentation, such as not being able to install a required package, errors when pulling data from the cloud, listing all images when running the extract stage, and confusion about the use of repro.",
        "Issue_preprocessed_content":"Title: various issues in ; Content: these are reported by . i'm moving here to discuss and follow i was running experiments by following the docs and encountered the following issues. sharing here for any required action. . is not installed by . so, if someone is trying to use a new virtual env, they need to install separately. would be good to include in . . gave this error . lists all the image when running the stage. would be good to remove from . section in the doc is a little unclear. does replace ? if yes, can we state this clearly? also would be great to change this statement to"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/17",
        "Issue_title":"example-get-started is broken with latest DVC",
        "Issue_creation_time":1606072868000,
        "Issue_closed_time":1606074573000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"> From https:\/\/github.com\/iterative\/dvc.org\/issues\/1743#issuecomment-730726776\r\n\r\n```console\r\n$ git@github.com:iterative\/example-get-started.git\r\n...\r\n$ cd example-get-started\r\n$ dvc fetch\r\nERROR: failed to fetch data from the cloud - Lockfile 'dvc.lock' is corrupted.\r\n```",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: example-get-started is broken with latest ; Content: > from https:\/\/github.com\/iterative\/.org\/issues\/1743#issuecomment-730726776 ```console $ git@github.com:iterative\/example-get-started.git ... $ cd example-get-started $ fetch error: failed to fetch data from the cloud - lockfile '.lock' is corrupted. ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the example-get-started repository, where an error was thrown when attempting to fetch data from the cloud due to a corrupted lockfile.",
        "Issue_preprocessed_content":"Title: is broken with latest ; Content: from"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/example-repos-dev\/issues\/12",
        "Issue_title":"need to rebuild get-started with the latest DVC version",
        "Issue_creation_time":1582914224000,
        "Issue_closed_time":1588739140000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Experience is broken since every DVC command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: need to rebuild get-started with the latest version; Content: experience is broken since every command changes `.gitignore` now - makes it very annoying to jump between branches.",
        "Issue_original_content_gpt_summary":"The user experienced broken functionality due to changes in the `.gitignore` file when jumping between branches, making it difficult to rebuild the get-started process with the latest version.",
        "Issue_preprocessed_content":"Title: need to rebuild with the latest version; Content: experience is broken since every command changes now makes it very annoying to jump between branches."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/121",
        "Issue_title":"fds fails to pull dvc on windows",
        "Issue_creation_time":1647838452000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"When running pull command on DagsHub remote. I receive dvc pull failure, so I have to manually pull dvc again. \n\nThis issue permanent issue on windows. \n\n```bash\nfds clone <remote> \n\n```\n\nIt is not urgent issue, but in annoyance category. ",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: fds fails to pull on windows; Content: when running pull command on dagshub remote. i receive pull failure, so i have to manually pull again. this issue permanent issue on windows. ```bash fds clone ``` it is not urgent issue, but in annoyance category.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running the 'pull' command on Dagshub Remote resulted in a pull failure, requiring them to manually pull again, which is a permanent issue on Windows.",
        "Issue_preprocessed_content":"Title: fds fails to pull on windows; Content: when running pull command on dagshub remote. i receive pull failure, so i have to manually pull again. this issue permanent issue on windows. it is not urgent issue, but in annoyance category."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/92",
        "Issue_title":"DVC and Git services don't correctly detect the repo root directory",
        "Issue_creation_time":1629832721000,
        "Issue_closed_time":1630227884000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"It seems they both assume that the current working dir is where they can find the `.git` and `.dvc` dirs.\r\nWe should correctly detect those paths, as it affects all our logic to e.g. automatically dvc init on behalf of the user.\r\n\r\nRelevant resources:\r\n1. https:\/\/stackoverflow.com\/a\/957978\r\n2. https:\/\/dvc.org\/doc\/command-reference\/root",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: and git services don't correctly detect the repo root directory; Content: it seems they both assume that the current working dir is where they can find the `.git` and `.` dirs. we should correctly detect those paths, as it affects all our logic to e.g. automatically init on behalf of the user. relevant resources: 1. https:\/\/stackoverflow.com\/a\/957978 2. https:\/\/.org\/doc\/command-reference\/root",
        "Issue_original_content_gpt_summary":"The user encountered challenges with git and git services not correctly detecting the repository root directory, which affects the logic to automatically initialize on behalf of the user.",
        "Issue_preprocessed_content":"Title: and git services don't correctly detect the repo root directory; Content: it seems they both assume that the current working dir is where they can find the and dirs. we should correctly detect those paths, as it affects all our logic to automatically init on behalf of the user. relevant resources . ."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/87",
        "Issue_title":"fsd clone for non-DVC repos throws an error",
        "Issue_creation_time":1628503684000,
        "Issue_closed_time":1630576282000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When using `fds clone` for non-DVC repo it throws the following error:\r\n\r\n`ERROR: you are not inside of a DVC repository (checked up to mount point '\/')`\r\n\r\nCloning a non-DVC repo using FDS can be a common use case, e.g., cloning a DAGsHub repo containing many files, but none of them are tracked by DVC nur the repo contains DVC config files. \r\n\r\nI suggest that after cloning the Git server, FDS will check if the repo contains DVC files. \r\n\r\nif it contains DVC files:\r\n  - echo 'Starting DVC Clone...`\r\n  - FDS will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?)\r\n  - FDS will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote)\r\n \r\nIt doesn't contain DVC files:\r\n  - FDS will initialize DVC\r\n  \r\n    if the Git server URL is DAGsHub's:\r\n      - FDS will set DAGsHub storage as the remote using the Git URL (replacing`.git` with `.dvc`).\r\n      - FDS will start a wizard to set the remote user name, password, and name.\r\n      \r\n    else:\r\n       - FDS will start a wizard asking do you want to set a DVC remote\r\n       if yes:\r\n           - With the wizard, the user will set the remote URL, name, username, and password.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: fsd clone for non- repos throws an error; Content: when using `fds clone` for non- repo it throws the following error: `error: you are not inside of a repository (checked up to mount point '\/')` cloning a non- repo using fds can be a common use case, e.g., cloning a dagshub repo containing many files, but none of them are tracked by nur the repo contains config files. i suggest that after cloning the git server, fds will check if the repo contains files. if it contains files: - echo 'starting clone...` - fds will start a wizard to set the user name and password for each remote storage in the local config. (consider checking if they are set in the global config file first?) - fds will pull all the files from the remotes and show a progress bar (might be reasonable to ask if the user wants to pull the files from each remote) it doesn't contain files: - fds will initialize if the git server url is dagshub's: - fds will set dagshub storage as the remote using the git url (replacing`.git` with `.`). - fds will start a wizard to set the remote user name, password, and name. else: - fds will start a wizard asking do you want to set a remote if yes: - with the wizard, the user will set the remote url, name, username, and password.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using `fds clone` for non- repo, as it throws an error, and suggests a solution to check if the repo contains files and set the user name and password for each remote storage in the local config.",
        "Issue_preprocessed_content":"Title: fsd clone for repos throws an error; Content: when using for repo it throws the following error cloning a repo using fds can be a common use case, cloning a dagshub repo containing many files, but none of them are tracked by nur the repo contains config files. i suggest that after cloning the git server, fds will check if the repo contains files. if it contains files echo 'starting fds will start a wizard to set the user name and password for each remote storage in the local config. fds will pull all the files from the remotes and show a progress bar it doesn't contain files fds will initialize if the git server url is dagshub's fds will set dagshub storage as the remote using the git url . fds will start a wizard to set the remote user name, password, and name. else fds will start a wizard asking do you want to set a remote if yes with the wizard, the user will set the remote url, name, username, and password."
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/39",
        "Issue_title":"Fails to add files to DVC tracking",
        "Issue_creation_time":1622120972000,
        "Issue_closed_time":1622139051000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When running the `fds add` command for data files it tries to add them to DVC tracking but fails.\r\n\r\nIn my case I tried to add the raw-data directory that contains the following image files:\r\n```\r\n$ tree data\/raw-data\r\ndata\/raw-data\r\n\u251c\u2500\u2500 IM-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0003-0001.jpeg\r\n\u251c\u2500\u2500 IM-0005-0001.jpeg\r\n\u251c\u2500\u2500 IM-0006-0001.jpeg\r\n\u251c\u2500\u2500 IM-0007-0001.jpeg\r\n\u251c\u2500\u2500 IM-0009-0001.jpeg\r\n\u251c\u2500\u2500 IM-0010-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0001.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001-0002.jpeg\r\n\u251c\u2500\u2500 IM-0011-0001.jpeg\r\n\u251c\u2500\u2500 IM-0013-0001.jpeg\r\n\u251c\u2500\u2500 IM-0015-0001.jpeg\r\n\u251c\u2500\u2500 IM-0016-0001.jpeg\r\n\u251c\u2500\u2500 IM-0017-0001.jpeg\r\n....\r\n```\r\nBut fds failed to execute the add command:\r\n```\r\n$ fds add data\/raw-data\r\n========== Make your selection, Press \"h\" for help ==========\r\n\r\nDVC add failed to execute\r\n```",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: fails to add files to tracking; Content: when running the `fds add` command for data files it tries to add them to tracking but fails. in my case i tried to add the raw-data directory that contains the following image files: ``` $ tree data\/raw-data data\/raw-data im-0001-0001.jpeg im-0003-0001.jpeg im-0005-0001.jpeg im-0006-0001.jpeg im-0007-0001.jpeg im-0009-0001.jpeg im-0010-0001.jpeg im-0011-0001-0001.jpeg im-0011-0001-0002.jpeg im-0011-0001.jpeg im-0013-0001.jpeg im-0015-0001.jpeg im-0016-0001.jpeg im-0017-0001.jpeg .... ``` but fds failed to execute the add command: ``` $ fds add data\/raw-data ========== make your selection, press \"h\" for help ========== add failed to execute ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running the `fds add` command for data files, which failed to add them to tracking.",
        "Issue_preprocessed_content":"Title: fails to add files to tracking; Content: when running the command for data files it tries to add them to tracking but fails. in my case i tried to add the directory that contains the following image files but fds failed to execute the add command"
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/37",
        "Issue_title":"Only display the DVC add prompt if there is anything to add",
        "Issue_creation_time":1622117855000,
        "Issue_closed_time":1622551859000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Currently, it will display always display\r\n`========== Make your selection, Press \"h\" for help ==========`\r\neven if there is no selection to make since the list of files is empty\r\n\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/dvc_service.py#L119",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: only display the add prompt if there is anything to add; Content: currently, it will display always display `========== make your selection, press \"h\" for help ==========` even if there is no selection to make since the list of files is empty https:\/\/github.com\/dagshub\/fds\/blob\/a8fea54f59131d3ddea4df5184adeee3ecc9998f\/fds\/services\/_service.py#l119",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the prompt to add a file was always displayed, even when the list of files was empty.",
        "Issue_preprocessed_content":"Title: only display the add prompt if there is anything to add; Content: currently, it will display always display even if there is no selection to make since the list of files is empty"
    },
    {
        "Issue_link":"https:\/\/github.com\/DagsHub\/fds\/issues\/13",
        "Issue_title":"Markdown in dvc install prompt isn't rendered as markdown",
        "Issue_creation_time":1621771875000,
        "Issue_closed_time":1621785253000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"`Should we install dvc[https:\/\/dvc.org\/] (`pip install dvc <3`) for you right now?`\r\nhttps:\/\/github.com\/DAGsHub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#L27",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: markdown in install prompt isn't rendered as markdown; Content: `should we install [https:\/\/.org\/] (`pip install <3`) for you right now?` https:\/\/github.com\/dagshub\/fds\/blob\/6e93c2b3259a7601f392c09604a60fc0ff360ad8\/fds\/run.py#l27",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the markdown in an install prompt was not rendered as markdown.",
        "Issue_preprocessed_content":"Title: markdown in install prompt isn't rendered as markdown; Content: pip install"
    },
    {
        "Issue_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/79",
        "Issue_title":"Use DVC remove instead of just removing the base image file",
        "Issue_creation_time":1643114302000,
        "Issue_closed_time":1643645434000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The problem described in this issue es very similar to #77 .\r\n\r\nCurrently, the \"delete\" action just removes the base image file. This is not correct for some reasons:\r\n\r\n- The base images are under control by DVC. The right way to remove a file that has been previously added to DVC is using its remove command, which removes the file pointer.\r\n- The deletion of the base image is not needed because it is not actually in the repository: it is pushed to the DVC remote storage during the base image generation and does not persist after this finishes. In case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow.\r\n\r\nTo summarize: the right way to do the deletion would be using DVC _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: use remove instead of just removing the base image file; Content: the problem described in this issue es very similar to #77 . currently, the \"delete\" action just removes the base image file. this is not correct for some reasons: - the base images are under control by . the right way to remove a file that has been previously added to is using its remove command, which removes the file pointer. - the deletion of the base image is not needed because it is not actually in the repository: it is pushed to the remote storage during the base image generation and does not persist after this finishes. in case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow. to summarize: the right way to do the deletion would be using _remove_ command, which is already available in the wrapper, and is how it must be implemented in the action.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the \"delete\" action was not correctly removing the base image file, and instead the remove command from the wrapper should be used to properly delete the file.",
        "Issue_preprocessed_content":"Title: use remove instead of just removing the base image file; Content: the problem described in this issue es very similar to . currently, the delete action just removes the base image file. this is not correct for some reasons the base images are under control by . the right way to remove a file that has been previously added to is using its remove command, which removes the file pointer. the deletion of the base image is not needed because it is not actually in the repository it is pushed to the remote storage during the base image generation and does not persist after this finishes. in case that the file were in the working tree because it was pulled at the beginning of some workflow execution, we can remove it just for good practices, but it would be removed at the end of the execution anyhow. to summarize the right way to do the deletion would be using command, which is already available in the wrapper, and is how it must be implemented in the action."
    },
    {
        "Issue_link":"https:\/\/github.com\/Nautilus-Cyberneering\/nautilus-librarian\/issues\/77",
        "Issue_title":"Use DVC move instead of system's mv in rename action",
        "Issue_creation_time":1642662749000,
        "Issue_closed_time":1643122571000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The current implementation of the rename action (to be triggered when the \"rename\" section of the DVC diff contains elements) includes the actual rename of the base image using the shutils' mv command. \r\n\r\n```python\r\nguard_that_base_image_exists(base_filename_old)\r\ncreate_output_folder(base_filename_new)\r\nmove(f\"{base_filename_old}\", f\"{base_filename_new}\")\r\n```\r\n\r\nThis rename is to be committed afterwards so that the rename of the base image is applied to the main branch.\r\n\r\nHowever, this approach is invalid:\r\n\r\n- If only the actual file is renamed, when a DVC pull is performed, the file with the previous name will be pulled. We will get two identical files with different names.\r\n- Nor can we just rename the pointer (.dvc file), as the pointer file name is irrelevant to DVC. The _path_ property inside the pointer is what determines the filename of the pulled file.\r\n\r\nThe right, convenient way to implement the file rename action is using the **dvc rename** command that performs all these actions:\r\n\r\n- Rename the actual file\r\n- Rename the pointer\r\n- Update the _path_ property\r\n\r\nFor consistency, we should use our DVC wrapper. If the move command is not wrapped there, we can do it as part of this issue.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: use move instead of system's mv in rename action; Content: the current implementation of the rename action (to be triggered when the \"rename\" section of the diff contains elements) includes the actual rename of the base image using the shutils' mv command. ```python guard_that_base_image_exists(base_filename_old) create_output_folder(base_filename_new) move(f\"{base_filename_old}\", f\"{base_filename_new}\") ``` this rename is to be committed afterwards so that the rename of the base image is applied to the main branch. however, this approach is invalid: - if only the actual file is renamed, when a pull is performed, the file with the previous name will be pulled. we will get two identical files with different names. - nor can we just rename the pointer (. file), as the pointer file name is irrelevant to . the _path_ property inside the pointer is what determines the filename of the pulled file. the right, convenient way to implement the file rename action is using the ** rename** command that performs all these actions: - rename the actual file - rename the pointer - update the _path_ property for consistency, we should use our wrapper. if the move command is not wrapped there, we can do it as part of this issue.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to rename a base image using the shutils' mv command, as this approach was invalid due to potential conflicts with the pull command, and the right way to implement the file rename action was to use the rename command.",
        "Issue_preprocessed_content":"Title: use move instead of system's mv in rename action; Content: the current implementation of the rename action includes the actual rename of the base image using the shutils' mv command. this rename is to be committed afterwards so that the rename of the base image is applied to the main branch. however, this approach is invalid if only the actual file is renamed, when a pull is performed, the file with the previous name will be pulled. we will get two identical files with different names. nor can we just rename the pointer , as the pointer file name is irrelevant to . the property inside the pointer is what determines the filename of the pulled file. the right, convenient way to implement the file rename action is using the rename command that performs all these actions rename the actual file rename the pointer update the property for consistency, we should use our wrapper. if the move command is not wrapped there, we can do it as part of this issue."
    },
    {
        "Issue_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/11",
        "Issue_title":"data loading bug with dvc",
        "Issue_creation_time":1641729327000,
        "Issue_closed_time":1642070875000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"load_dataset function from hugging face can't access the dvc tracked data directory \r\n--> OSError: [Errno 30] Read-only file system: '\/data'",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: data loading bug with ; Content: load_dataset function from hugging face can't access the tracked data directory --> oserror: [errno 30] read-only file system: '\/data'",
        "Issue_original_content_gpt_summary":"The user encountered a data loading bug with the load_dataset function from hugging face, which resulted in an OSError due to a read-only file system in the tracked data directory.",
        "Issue_preprocessed_content":"Title: data loading bug with ; Content: function from hugging face can't access the tracked data directory oserror file system"
    },
    {
        "Issue_link":"https:\/\/github.com\/se4ai2122-cs-uniba\/CT-COVID\/issues\/30",
        "Issue_title":"Missing params field for evaluate stage in dvc.yaml",
        "Issue_creation_time":1638704752000,
        "Issue_closed_time":1638706309000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":null,
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: missing params field for evaluate stage in .yaml; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the evaluate stage in the .yaml file was missing the params field.",
        "Issue_preprocessed_content":"Title: missing params field for evaluate stage in ; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/csia-pme\/csia-pme\/issues\/39",
        "Issue_title":"Resolve DVC Bad request with Minio",
        "Issue_creation_time":1670317034000,
        "Issue_closed_time":1670919923000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: resolve bad request with minio; Content: ![10a2a57d-e765-4359-915e-a60163bd6ec8](https:\/\/user-images.githubusercontent.com\/58698728\/205865653-bf35fb85-19cb-4e95-958e-619d13015db0.jpg)",
        "Issue_original_content_gpt_summary":"The user encountered a \"Bad Request\" error when attempting to use Minio, and needed to find a solution to resolve the issue.",
        "Issue_preprocessed_content":"Title: resolve bad request with minio; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/28",
        "Issue_title":"\"dvc-cc init\" just take three letters for the dvc folder name?",
        "Issue_creation_time":1584006633000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"CALL DVC-CC INIT just takes the first three letters of the repo name???\r\n\r\nHere you can enter the folder where you want to store the DVC files on the DVC Storage Server.\r\n\tThe remote DVC folder that you want use (default: ~\/*****\/***\/TES): \r\nThe username with that you can access the DVC storage server \"dt1.f4.htw-berlin.de\".\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: \"-cc init\" just take three letters for the folder name?; Content: call -cc init just takes the first three letters of the repo name??? here you can enter the folder where you want to store the files on the storage server. the remote folder that you want use (default: ~\/*****\/***\/tes): the username with that you can access the storage server \"dt1.f4.htw-berlin.de\".",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the \"-cc init\" command only took the first three letters of the repository name when attempting to store files on a remote storage server.",
        "Issue_preprocessed_content":"Title: init just take three letters for the folder name?; Content: call init just takes the first three letters of the repo name??? here you can enter the folder where you want to store the files on the storage server. the remote folder that you want use the username with that you can access the storage server"
    },
    {
        "Issue_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/27",
        "Issue_title":"\"dvc pull\" does not work in the result branch if a sshfs connection is mounted",
        "Issue_creation_time":1583006053000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n> Entsprechend dem Tutorial habe ich mit sshfs den data Ordner\r\n> gemountet, um externe Daten verwenden zu k\u00f6nnen, was soweit auch\r\n> funktioniert.\r\n> Wenn ich dann aber nach erfolgreich abgeschlossenem Job die Ergebnisse\r\n> ansehen will (git pull, git checkout rcc_00XX_ergebnis_branch, dvc\r\n> pull), bekomme ich eine Fehlermeldung:\r\n> \r\n> rmdir: data: Das Ger\u00e4t oder die Ressource ist belegt\r\n> \r\n> Wenn ich vorher mit fusermount -u data den Dataordner wieder unmounte,\r\n> funktioniert alles wie erwartet. Ist das das zu erwartende Verhalten?\r\n> Muss ich also \"data\" unmounten, um die Ergebnisse ansehen zu k\u00f6nnen?\r\n> Und dann erneut mounten, um einen neuen Job zu starten?\r\n\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: \" pull\" does not work in the result branch if a sshfs connection is mounted; Content: **describe the bug** > entsprechend dem tutorial habe ich mit sshfs den data ordner > gemountet, um externe daten verwenden zu knnen, was soweit auch > funktioniert. > wenn ich dann aber nach erfolgreich abgeschlossenem job die ergebnisse > ansehen will (git pull, git checkout rcc_00xx_ergebnis_branch, > pull), bekomme ich eine fehlermeldung: > > rmdir: data: das gert oder die ressource ist belegt > > wenn ich vorher mit fusermount -u data den dataordner wieder unmounte, > funktioniert alles wie erwartet. ist das das zu erwartende verhalten? > muss ich also \"data\" unmounten, um die ergebnisse ansehen zu knnen? > und dann erneut mounten, um einen neuen job zu starten? > -v 0.87.0 > faice -v 9.1.0 > -cc -v 0.8.66",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where \"pull\" does not work in the result branch if a sshfs connection is mounted.",
        "Issue_preprocessed_content":"Title: pull does not work in the result branch if a sshfs connection is mounted; Content: describe the bug entsprechend dem tutorial habe ich mit sshfs den data ordner gemountet, um externe daten verwenden zu knnen, was soweit auch funktioniert. wenn ich dann aber nach erfolgreich abgeschlossenem job die ergebnisse ansehen will , bekomme ich eine fehlermeldung rmdir data das gert oder die ressource ist belegt wenn ich vorher mit fusermount u data den dataordner wieder unmounte, funktioniert alles wie erwartet. ist das das zu erwartende verhalten? muss ich also data unmounten, um die ergebnisse ansehen zu knnen? und dann erneut mounten, um einen neuen job zu starten? v faice v v"
    },
    {
        "Issue_link":"https:\/\/github.com\/deep-projects\/dvc-cc\/issues\/26",
        "Issue_title":"dvc servername and url not found by calling \"dvc-cc run\"",
        "Issue_creation_time":1583005907000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nIf the dvc\/config file is created with whitespaces dvc-cc cann't read the config file.\r\n\r\n**To Reproduce**\r\nCreate a dvc\/config file like this:\r\n`[core]\r\n    remote = dvc_connection\r\n['remote \"dvc_connection\"']\r\n    url = ...............\r\n    ask_password = true\r\n\r\n**Additional context**\r\n> dvc -V 0.87.0\r\n> faice -v 9.1.0\r\n> dvc-cc -v 0.8.66\r\n",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: servername and url not found by calling \"-cc run\"; Content: **describe the bug** if the \/config file is created with whitespaces -cc cann't read the config file. **to reproduce** create a \/config file like this: `[core] remote = _connection ['remote \"_connection\"'] url = ............... ask_password = true **additional context** > -v 0.87.0 > faice -v 9.1.0 > -cc -v 0.8.66",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the servername and url were not found when calling \"-cc run\" due to whitespaces in the \/config file.",
        "Issue_preprocessed_content":"Title: servername and url not found by calling run ; Content: describe the bug if the file is created with whitespaces cann't read the config file. to reproduce create a file like this ` remote 'remote url true additional context v faice v v"
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20",
        "Issue_title":"DVC View and Plots don't load in `vscode-dvc`",
        "Issue_creation_time":1655687938000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":11.0,
        "Issue_body":"UPDATE: Summary in https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/issues\/20#issuecomment-1164570090\r\n\r\nI cloned https:\/\/github.com\/iterative\/dvc-checkpoints-mnist. I setup the IDE workspace so the extension is active.\r\n\r\nI haven't run any experiments:\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174509065-ac8f2c97-0d7f-4b1f-b6c4-e36603406c50.png)\r\n\r\nI check out the [`make_checkpoint`](https:\/\/github.com\/iterative\/dvc-checkpoints-mnist\/tree\/make_checkpoint) branch. The DVC view and Plots Dashboard never load.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/1477535\/174508899-c1e5788a-2ead-446d-bab6-0239cbc27519.png)\r\n\r\nThe Experiments Table says \"No Experiments to Display.\"\r\n\r\nOther components do load.\r\n\r\nDVC virtual env is loaded via MS Python extension.\r\n\r\n```console\r\n$ dvc version\r\nDVC version: 2.11.0 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.13 on macOS-12.4-arm64-arm-64bit\r\nSupports:\r\n        webhdfs (fsspec = 2022.5.0),\r\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\r\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6)\r\nCache types: <https:\/\/error.dvc.org\/no-dvc-cache>\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk3s1s1\r\nRepo: dvc, git\r\n```\r\n\r\n---\r\n\r\n~~p.s. the same happens in the included `demo\/` project if I set up the extension with `\"dvc.dvcPath\": \"demo\/.env\/bin\/dvc\"` in .vscode\/settings.json (no MS Python extension).~~",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: view and plots don't load in `vscode-`; Content: update: summary in https:\/\/github.com\/iterative\/-checkpoints-mnist\/issues\/20#issuecomment-1164570090 i cloned https:\/\/github.com\/iterative\/-checkpoints-mnist. i setup the ide workspace so the extension is active. i haven't run any experiments: ![image](https:\/\/user-images.githubusercontent.com\/1477535\/174509065-ac8f2c97-0d7f-4b1f-b6c4-e36603406c50.png) i check out the [`make_checkpoint`](https:\/\/github.com\/iterative\/-checkpoints-mnist\/tree\/make_checkpoint) branch. the view and plots dashboard never load. ![image](https:\/\/user-images.githubusercontent.com\/1477535\/174508899-c1e5788a-2ead-446d-bab6-0239cbc27519.png) the experiments table says \"no experiments to display.\" other components do load. virtual env is loaded via ms python extension. ```console $ version version: 2.11.0 (pip) --------------------------------- platform: python 3.9.13 on macos-12.4-arm64-arm-64bit supports: webhdfs (fsspec = 2022.5.0), http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6), https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6) cache types: caches: local remotes: none workspace directory: apfs on \/dev\/disk3s1s1 repo: , git ``` --- ~~p.s. the same happens in the included `demo\/` project if i set up the extension with `\".path\": \"demo\/.env\/bin\/\"` in .vscode\/settings.json (no ms python extension).~~",
        "Issue_original_content_gpt_summary":"The user encountered challenges with loading view and plots in the `vscode-` extension while cloning the `iterative\/-checkpoints-mnist` repository, despite other components loading correctly.",
        "Issue_preprocessed_content":"Title: view and plots don't load in ; Content: update summary in i cloned i setup the ide workspace so the extension is active. i haven't run any experiments i check out the branch. the view and plots dashboard never load. the experiments table says no experiments to other components do load. virtual env is loaded via ms python extension. the same happens in the included project if i set up the extension with in ."
    },
    {
        "Issue_link":"https:\/\/github.com\/MantisAI\/Rasa-MLOPs\/issues\/5",
        "Issue_title":"Remote storage is not publicly accessible (dvc pull fails)",
        "Issue_creation_time":1634634148000,
        "Issue_closed_time":1668173479000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"`ERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden`\r\n\r\n`dvc pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: remote storage is not publicly accessible ( pull fails); Content: `error: unexpected error - forbidden: an error occurred (403) when calling the headobject operation: forbidden` ` pull` needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the remote storage was not publicly accessible, causing a `pull` to fail, and requiring the bucket to be made public and read-only in order to allow readers to access the content.",
        "Issue_preprocessed_content":"Title: remote storage is not publicly accessible ; Content: needs mantis creds so a reader will not be able to follow. we need to make the bucket public and read only."
    },
    {
        "Issue_link":"https:\/\/github.com\/adamtupper\/cookiecutter-lvsn-workflow\/issues\/9",
        "Issue_title":"Post-gen hook shouldn't configure a DVC remote if no name is provided",
        "Issue_creation_time":1623947751000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"If the DVC remote name is left blank, the post-gen hook shouldn't try to set one. Currently this raises a (non-fatal) error.",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: post-gen hook shouldn't configure a remote if no name is provided; Content: if the remote name is left blank, the post-gen hook shouldn't try to set one. currently this raises a (non-fatal) error.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the post-gen hook was attempting to configure a remote even when the remote name was left blank, resulting in a non-fatal error.",
        "Issue_preprocessed_content":"Title: hook shouldn't configure a remote if no name is provided; Content: if the remote name is left blank, the hook shouldn't try to set one. currently this raises a error."
    },
    {
        "Issue_link":"https:\/\/github.com\/iterative\/checkpoints-tutorial\/issues\/1",
        "Issue_title":"AttributeError: module 'dvclive' has no attribute 'log'",
        "Issue_creation_time":1633694389000,
        "Issue_closed_time":1633697794000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I try to follow this Checkpoints tutorial and documentation page https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/checkpoints \r\n\r\nHowever, after adding `dvclive` in the train.py file with this code: \r\n\r\n import the dvclive package with the other imports:\r\n\r\n```python\r\nimport dvclive\r\n...\r\n    ...\r\n    for k, v in metrics.items():\r\n        print('Epoch %s: %s=%s'%(i, k, v))\r\n        dvclive.log(k, v)\r\n    dvclive.next_step()\r\n```\r\nI got an error: \r\n```bash \r\n\u276f dvc exp run\r\nModified checkpoint experiment based on 'exp-defaa' will be created   \r\nRunning stage 'train':                                                                                                                                                                                                                                               \r\n> python train.py\r\n...\r\nEpoch 1: loss=0.1541447937488556\r\nTraceback (most recent call last):\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 125, in <module>\r\n    main()\r\n  File \"[USER-PATH]\/checkpoints-tutorial\/train.py\", line 118, in main\r\n    dvclive.log(name=k, val=v)\r\nAttributeError: module 'dvclive' has no attribute 'log'\r\n\r\nfile:\/\/\/[USER-PATH]\/checkpoints-tutorial\/dvclive.html\r\nERROR: failed to reproduce 'dvc.yaml': failed to run: python train.py, exited with 1\r\n``` \r\n\r\nI only could run the example with the following trick: \r\n```python\r\nfrom dvclive import Live \r\ndvclive = Live()\r\n```\r\nAre there any updated in `dvclive` API? \r\n\r\nSystem info\r\n```bash \r\n\u276f dvc doctor\r\nDVC version: 2.6.4 (pip)\r\n---------------------------------\r\nPlatform: Python 3.9.4 on macOS-11.6-x86_64-i386-64bit\r\nSupports:\r\n        hdfs (pyarrow = 5.0.0),\r\n        http (requests = 2.26.0),\r\n        https (requests = 2.26.0)\r\nCache types: reflink, hardlink, symlink\r\nCache directory: apfs on \/dev\/disk1s1s1\r\nCaches: local\r\nRemotes: None\r\nWorkspace directory: apfs on \/dev\/disk1s1s1\r\nRepo: dvc, git\r\n```\r\n\r\nFIY @flippedcoder @daavoo ",
        "Tool":"DVC",
        "Platform":"Github",
        "Issue_original_content":"Title: attributeerror: module 'live' has no attribute 'log'; Content: i try to follow this checkpoints tutorial and documentation page https:\/\/.org\/doc\/user-guide\/experiment-management\/checkpoints however, after adding `live` in the train.py file with this code: import the live package with the other imports: ```python import live ... ... for k, v in metrics.items(): print('epoch %s: %s=%s'%(i, k, v)) live.log(k, v) live.next_step() ``` i got an error: ```bash exp run modified checkpoint experiment based on 'exp-defaa' will be created running stage 'train': > python train.py ... epoch 1: loss=0.1541447937488556 traceback (most recent call last): file \"[user-path]\/checkpoints-tutorial\/train.py\", line 125, in main() file \"[user-path]\/checkpoints-tutorial\/train.py\", line 118, in main live.log(name=k, val=v) attributeerror: module 'live' has no attribute 'log' file:\/\/\/[user-path]\/checkpoints-tutorial\/live.html error: failed to reproduce '.yaml': failed to run: python train.py, exited with 1 ``` i only could run the example with the following trick: ```python from live import live live = live() ``` are there any updated in `live` api? system info ```bash doctor version: 2.6.4 (pip) --------------------------------- platform: python 3.9.4 on macos-11.6-x86_64-i386-64bit supports: hdfs (pyarrow = 5.0.0), http (requests = 2.26.0), https (requests = 2.26.0) cache types: reflink, hardlink, symlink cache directory: apfs on \/dev\/disk1s1s1 caches: local remotes: none workspace directory: apfs on \/dev\/disk1s1s1 repo: , git ``` fiy @flippedcoder @daavoo",
        "Issue_original_content_gpt_summary":"The user encountered an attributeerror when attempting to use the live package in their train.py file while following a Checkpoints tutorial and documentation page, and was able to resolve the issue with a workaround, but was curious if there had been any updates to the live API.",
        "Issue_preprocessed_content":"Title: attributeerror module 'live' has no attribute 'log'; Content: i try to follow this checkpoints tutorial and documentation page however, after adding in the file with this code import the live package with the other imports i got an error i only could run the example with the following trick are there any updated in api? system info fiy"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Issue_title":"kedro mlflow ui gets a FileNotFoundError",
        "Issue_creation_time":1664539296000,
        "Issue_closed_time":1664786016000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: ui gets a filenotfounderror; firstly i'd like to apologize if this is a dummy question. i'm following the tutorial to get introduced to ,; Content: after running the command \" init\" i tried to run the command \" mlflofw ui\" but i get an error: info the '_tracking_uri' key in .yml is relative ('server._tracking_uri = s'). it is converted to a valid uri: 'file:\/\/\/c:\/users\/e107338\/pycharmprojects\/\/--example\/s' __config.py:202 after the traceback i get an error: filenotfounderrror",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to run the command \" ui\" after running the command \" init\" while following a tutorial to get introduced to Kedro.",
        "Issue_preprocessed_content":"Title: ui gets a filenotfounderror; Content: firstly i'd like to apologize if this is a dummy question. i'm following the tutorial to get introduced to ,; after running the command init i tried to run the command mlflofw ui but i get an error info the key in is relative . it is converted to a valid uri after the traceback i get an error filenotfounderrror"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Issue_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Issue_creation_time":1656532075000,
        "Issue_closed_time":1657139268000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: init displays a wrong sucess message when the env folder does not exist; Content: ## description when running `` init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. we should move this code : https:\/\/github.com\/galileo-galilei\/-\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/_\/framework\/cli\/cli.py#l116-l122 inside the \"try\" block above.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running \" init --env=xxx\" displays a success message even if the env \"xxx\" folder does not exist, instead of an error message.",
        "Issue_preprocessed_content":"Title: init displays a wrong sucess message when the env folder does not exist; Content: description when running , a success message is displayed even if the env xxx folder does not exist, instead of an error message. we should move this code inside the try block above."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Issue_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Issue_creation_time":1652380533000,
        "Issue_closed_time":1652640252000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: - is broken with ==0.18.1; Content: ## description the plugin does not work with projects created with ``==0.18.1`` ## context try to launch `` run`` in a project with ``==0.18.1`` and - installed. ## steps to reproduce ```python conda create -n temp python=3.8 -y conda activate temp pip install ==0.18.1 -==0.9.0 new --starter=pandas-iris cd pandas-iris init run ``` ## expected result this should run the pipeleine and log the parameters. ## actual result this raises the following error: ```bash attributeerror: module '.framework.session.session' has no attribute '_active_session' ``` ## your environment include as many relevant details about the environment in which you experienced the bug: * `` and `-` version used (`pip show ` and `pip show -`): ``==0.18.1`` and ``-<=0.9.0`` * python version used (`python -v`): all * operating system and version: all ## does the bug also happen with the last version on master? yes ## solution currently, - uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/galileo-galilei\/-\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/_\/config\/__config.py#l233-l247) inside a hook. with ==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. retrieving the configuration and set it up should be moved to this new hook: https:\/\/github.com\/galileo-galilei\/-\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/_\/framework\/hooks\/pipeline_hook.py#l108-l109",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with - not working with projects created with ``==0.18.1``, which was solved by moving the configuration setup to the ``after_context_created`` hook.",
        "Issue_preprocessed_content":"Title: is broken with ; Content: description the plugin does not work with projects created with context try to launch in a project with and installed. steps to reproduce expected result this should run the pipeleine and log the parameters. actual result this raises the following error your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used all operating system and version all does the bug also happen with the last version on master? yes solution currently, uses inside a hook. with this private attribute was removed and the new recommandation is to use the hook. retrieving the configuration and set it up should be moved to this new hook"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/273",
        "Issue_title":"KedroPipelineModel requires unnecessary pipeline input dependencies to be executed",
        "Issue_creation_time":1640015142000,
        "Issue_closed_time":1644791409000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi @Galileo-Galilei\r\n\r\n## Description\r\nthe KedroPipelineModel has a `initial_catalog` property which causes some problems. This `initial_catalog` can contain some Kedro Datasets but it's not necessary to log them when you train your model. because of this property I can't load my model anymore. I have to train it again.\r\n\r\nI explain : when I trained my model I used a kedro home-made plugin to load a specific dataset (which has no impact for my model). After that, I updated this plugin independently of my ML project. Today, I want to load my model but I can't because the load function uses the old Kedro Catalog with my old plugin version which is not in my environnement anymore. \r\n\r\n## Context\r\nIt would be great if we can update the kedro-catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models.\r\n\r\n## Possible Implementation\r\nLog in Mlflow what is only necessary.\r\n\r\nI hope my issue is clear.\r\n\r\nthank you",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: pipelinemodel requires unnecessary pipeline input dependencies to be executed; Content: hi @galileo-galilei ## description the pipelinemodel has a `initial_catalog` property which causes some problems. this `initial_catalog` can contain some datasets but it's not necessary to log them when you train your model. because of this property i can't load my model anymore. i have to train it again. i explain : when i trained my model i used a home-made plugin to load a specific dataset (which has no impact for my model). after that, i updated this plugin independently of my ml project. today, i want to load my model but i can't because the load function uses the old catalog with my old plugin version which is not in my environnement anymore. ## context it would be great if we can update the -catalog (only dataset and not the artifacts for the model of course !) without having to retrain our models. ## possible implementation log in what is only necessary. i hope my issue is clear. thank you",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the pipelinemodel requiring unnecessary pipeline input dependencies to be executed in order to load the model, preventing them from updating the catalog without having to retrain the model.",
        "Issue_preprocessed_content":"Title: pipelinemodel requires unnecessary pipeline input dependencies to be executed; Content: hi description the pipelinemodel has a property which causes some problems. this can contain some datasets but it's not necessary to log them when you train your model. because of this property i can't load my model anymore. i have to train it again. i explain when i trained my model i used a plugin to load a specific dataset . after that, i updated this plugin independently of my ml project. today, i want to load my model but i can't because the load function uses the old catalog with my old plugin version which is not in my environnement anymore. context it would be great if we can update the without having to retrain our models. possible implementation log in what is only necessary. i hope my issue is clear. thank you"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Issue_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Issue_creation_time":1619193727000,
        "Issue_closed_time":1619987466000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: - cli is unavailable inside a project; ## description i try to reproduce the minimal example from the docs: a project using the starter `pandas-iris` using the `-` functinality. i do not arrive at initializing the - project, since the cli commands are not available. ## context it is unclear to me if this is connected to #157 i wanted to start looking into -, but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. ## steps to reproduce ``` conda create -n _ python=3.8 conda activate _ pip install - -h new --starter=pandas-iris cd _test\/ -h > error \"no such command ''\" ``` ## expected result ` ` is available in a project directory, i.e. ` -h` gives the same output inside the folder as before ## actual result inside the project folder the `` command is unknown to ``` ...\/miniconda3\/envs\/_\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: deprecationwarning: use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release. return get_provider(package_or_requirement).get_resource_filename( ....\/miniconda3\/envs\/_\/lib\/python3.8\/site-packages\/\/types\/schema.py:49: deprecationwarning: `np.object` is a deprecated alias for the builtin `object`. to silence this warning, use `object` by itself. doing this will not modify any behavior and is safe. deprecated in numpy 1.20; Content: for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations binary = (7, np.dtype(\"bytes\"), \"binarytype\", np.object) 2021-04-23 17:49:52,197 - root - info - registered hooks from 2 installed plugin(s): --0.7.1 usage: [options] command [args]... try ' -h' for help. error: no such command ''. ``` ## your environment ubuntu 18.04.5 - 0.17.3 - - 0.7.1 - python 3.8.8. - 1.15.0 ## does the bug also happen with the last version on master? yes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the - CLI commands were unavailable inside a project, despite the user having installed the necessary packages and following the minimal example from the documentation.",
        "Issue_preprocessed_content":"Title: cli is unavailable inside a project; Content: description i try to reproduce the minimal example from the docs a project using the starter using the functinality. i do not arrive at initializing the project, since the cli commands are not available. context it is unclear to me if this is connected to i wanted to start looking into but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. steps to reproduce expected result is available in a project directory, gives the same output inside the folder as before actual result inside the project folder the command is unknown to your environment ubuntu python does the bug also happen with the last version on master? yes"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Issue_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Issue_creation_time":1617627646000,
        "Issue_closed_time":1618006798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: ui does not use arguments from .yml; Content: ## description as described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in--yml-and-run---ui-but-host-and-port), the `ui` command does not use the options ## context & steps to reproduce - create a project - call ` init` - modify the port in `.yml` to 5001 - launch ` ui` ## expected result the ui should open in port 5001. ## actual result it opens on port 5000 (the default). ## your environment include as many relevant details about the environment in which you experienced the bug: * `` version: 0.17.0 * `-` version: 0.6.0 * python version used (`python -v`): 3.6.8 * operating system and version: windows ## does the bug also happen with the last version on master? yes ## solution we should pass the arguments in the command: https:\/\/github.com\/galileo-galilei\/-\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/_\/framework\/cli\/cli.py#l149-l151",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the `ui` command in Kedro does not use the options specified in the `.yml` file, despite the expected result being that it should open in the port specified in the `.yml` file.",
        "Issue_preprocessed_content":"Title: ui does not use arguments from ; Content: description as described in , the command does not use the options context & steps to reproduce create a project call modify the port in to launch expected result the ui should open in port . actual result it opens on port . your environment include as many relevant details about the environment in which you experienced the bug version version python version used operating system and version windows does the bug also happen with the last version on master? yes solution we should pass the arguments in the command"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Issue_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Issue_creation_time":1610404594000,
        "Issue_closed_time":1615716614000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: cli is broken if configuration is declared in pyproject.toml; Content: ## description enable to declare configuration either in ``..yml`` or in ``pyproject.toml`` (in the ``[tool.]`` section). we cl to support both, but the cli commands are not accessible if the project contains only a ``pyproject.toml file``. ## steps to reproduce call `` init`` inside a project with no ``..yml`` file but only a ``pyproject.toml``. ## expected result the cli commands should be available (``init``) ## actual result only the ``new`` command is available. this is not considered as a project. ``` -- separate them if you have more than one. ``` ## your environment * `` and `-` version used (`pip show ` and `pip show -`): ==16.6, -==0.4.1 * python version used (`python -v`): 3.7.9 * operating system and version: windows 7 ## does the bug also happen with the last version on develop? yes ## solution the error comes from the ``is__project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``..yml``.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the cli commands were not accessible if the project only contained a pyproject.toml file, and the solution was to update the is__project function to consider a folder as the root of a project even if it does not contain a ..yml file.",
        "Issue_preprocessed_content":"Title: cli is broken if configuration is declared in ; Content: description enable to declare configuration either in or in . we cl to support both, but the cli commands are not accessible if the project contains only a . steps to reproduce call inside a project with no file but only a . expected result the cli commands should be available actual result only the command is available. this is not considered as a project. your environment and version used python version used operating system and version windows does the bug also happen with the last version on develop? yes solution the error comes from the function which does not consider that a folder is the root of a kdro project if it does not contain a ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Issue_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Issue_creation_time":1605983313000,
        "Issue_closed_time":1606599848000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: a pipelinemodel cannot be loaded from if its catalog contains non deepcopy-able datasets; Content: ## description i tried to load a pipelinemodel from , and i got a \"cannot pickle context artifacts\" error, which is due do the ## context i cannot load a previously saved pipelinemodel generated by pipeline_ml_factory. ## steps to reproduce save a pipelinemodel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer) ## expected result the model should be loaded ## actual result an error is raised ## your environment include as many relevant details about the environment in which you experienced the bug: * `` and `-` version used: 0.16.5 and 0.4.0 * python version used (`python -v`): 3.6.8 * windows 10 & centos were tested ## does the bug also happen with the last version on develop? yes # potential solution the faulty line is: https:\/\/github.com\/galileo-galilei\/-\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/_\/\/_pipeline_model.py#l45",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to load a previously saved KedroPipelineModel generated by pipeline_ml_factory due to the catalog containing an object which cannot be deepcopied.",
        "Issue_preprocessed_content":"Title: a pipelinemodel cannot be loaded from if its catalog contains non datasets; Content: description i tried to load a pipelinemodel from , and i got a cannot pickle context artifacts error, which is due do the context i cannot load a previously saved pipelinemodel generated by steps to reproduce save a pipelinemodel with a dataset that contains an object which cannot be deepcopied expected result the model should be loaded actual result an error is raised your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used windows & centos were tested does the bug also happen with the last version on develop? yes potential solution the faulty line is"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Issue_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Issue_creation_time":1605982845000,
        "Issue_closed_time":1606515096000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: runstatus of run is \"finished\" instead of \"failed\" when the run fails; Content: ## description when i launch ` run` and the run fails, the `on_pipeline_error` closes all the runs (to avoid interactions with further runs) ## context i cannot distinguish failed runs from sucessful ones in the ui. ## steps to reproduce launch a failing pipeline with run. ## expected result the ui should display the run with a red cross ## actual result the ui displays the run with a green tick ## does the bug also happen with the last version on develop? yes. ## potential solution: replace these lines: `https:\/\/github.com\/galileo-galilei\/-\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/_\/framework\/hooks\/pipeline_hook.py#l193-l194` with ```python while .active_run(): .end_run(.entities.runstatus.failed) ``` or even better, retrieve current run status from ?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the runstatus of a run is \"finished\" instead of \"failed\" when the run fails, preventing them from distinguishing failed runs from successful ones in the UI.",
        "Issue_preprocessed_content":"Title: runstatus of run is finished instead of failed when the run fails; Content: description when i launch and the run fails, the closes all the runs context i cannot distinguish failed runs from sucessful ones in the ui. steps to reproduce launch a failing pipeline with run. expected result the ui should display the run with a red cross actual result the ui displays the run with a green tick does the bug also happen with the last version on develop? yes. potential solution replace these lines with or even better, retrieve current run status from ?"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/119",
        "Issue_title":"PipelineML objects in `hooks.py` breaks all kedro-viz versions with kedro template>=0.16.5",
        "Issue_creation_time":1605718283000,
        "Issue_closed_time":1605720463000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nIf I create a PipelineML objects  and I return it in the `hooks.py`:\r\n\r\n\r\n```python\r\nclass ProjectHooks:\r\n    @hook_impl\r\n    def register_pipelines(self) -> Dict[str, Pipeline]:\r\n        \"\"\"Register the project's pipeline.\r\n        Returns:\r\n            A mapping from a pipeline name to a ``Pipeline`` object.\r\n        \"\"\"\r\n       ml_pipeline=create_ml_pipeline()\r\n        training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\")\r\n\r\n        return {\r\n            \"training\": training_pipeline,\r\n            \"__default__\": other_pipeline\r\n        }\r\n````\r\n\r\n`kedro run` command works fine, but `kedro viz` and `kedro pipeline list` fail.\r\n\r\n## Context\r\n\r\nI was trying to visualise a pipeline with kedro-viz==3.7.0 (I also tried 3.4.0 and 3.0.0), and kedro==0.16.6\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a PipelineMl object with pipeline_ml_factory in `hooks;py`\r\n2. Launch `kedro viz` in terminal\r\n\r\n## Expected Result\r\nKedro viz should be launched on localhost:5000\r\n\r\n## Actual Result\r\nTell us what happens instead.\r\n\r\n```\r\n-- If you received an error, place it here.\r\n```\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`):\r\n* Python version used (`python -V`):\r\n* Operating system and version:\r\n\r\n*Note: everything works fine with the older template (`kedro<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`*\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Potential solution: \r\n\r\nIt seems the `__add__` method of the `PipelineML` class must be implemented.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: pipelineml objects in `hooks.py` breaks all -viz versions with template>=0.16.5; Content: ## description if i create a pipelineml objects and i return it in the `hooks.py`: ```python class projecthooks: @hook_impl def register_pipelines(self) -> dict[str, pipeline]: \"\"\"register the project's pipeline. returns: a mapping from a pipeline name to a ``pipeline`` object. \"\"\" ml_pipeline=create_ml_pipeline() training_pipeline = pipeline_ml_factory(training=ml_pipeline.only_nodes_with_tags(\"training\"), inference=ml_pipeline.only_nodes_with_tags(\"inference\"), input_name=\"instances\") return { \"training\": training_pipeline, \"__default__\": other_pipeline } ```` ` run` command works fine, but ` viz` and ` pipeline list` fail. ## context i was trying to visualise a pipeline with -viz==3.7.0 (i also tried 3.4.0 and 3.0.0), and ==0.16.6 ## steps to reproduce 1. create a pipelineml object with pipeline_ml_factory in `hooks;py` 2. launch ` viz` in terminal ## expected result viz should be launched on localhost:5000 ## actual result tell us what happens instead. ``` -- if you received an error, place it here. ``` ``` -- separate them if you have more than one. ``` ## your environment include as many relevant details about the environment in which you experienced the bug: * `` and `-` version used (`pip show ` and `pip show -`): * python version used (`python -v`): * operating system and version: *note: everything works fine with the older template (`<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`* ## does the bug also happen with the last version on develop? yes ## potential solution: it seems the `__add__` method of the `pipelineml` class must be implemented.",
        "Issue_original_content_gpt_summary":"The user encountered a bug when attempting to visualise a pipeline with template>=0.16.5, where creating a pipelineml object in `hooks.py` and launching `viz` in the terminal resulted in an error, but the bug did not occur with the older template (`<=0.16.4`) and the `pipeline.py` file instead of `hooks.py`, and the potential solution was to implement the `__add__` method of the `pipelineml` class.",
        "Issue_preprocessed_content":"Title: pipelineml objects in breaks all versions with ; Content: description if i create a pipelineml objects and i return it in the run viz pipeline list hooks;py viz version used python version used operating system and version note everything works fine with the older template and the file instead of does the bug also happen with the last version on develop? yes potential solution it seems the method of the class must be implemented."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/78",
        "Issue_title":"TypeError in _generate_kedro_command when debugging run in VSCode",
        "Issue_creation_time":1601890192000,
        "Issue_closed_time":1601893558000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"`TypeError: object of type 'NoneType' has no len()` happens when suggested [VSCode configuration for kedro](https:\/\/kedro.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. The error is due to commandline arguments being `None` when running pipeline directly through `run.py`.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main\r\n    run()\r\n  File \"\/Users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonFiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in <module>\r\n    run_package()\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package\r\n    project_context.run()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 725, in run\r\n    run_params=record_data, pipeline=filtered_pipeline, catalog=catalog\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall\r\n    return outcome.get_result()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run\r\n    pipeline_name=run_params[\"pipeline_name\"],\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate_kedro_command\r\n    if len(from_inputs) > 0:\r\nTypeError: object of type 'NoneType' has no len()\r\n```",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: typeerror in _generate__command when debugging run in vscode; Content: `typeerror: object of type 'nonetype' has no len()` happens when suggested [vscode configuration for ](https:\/\/.readthedocs.io\/en\/stable\/09_development\/01_set_up_vscode.html) is used for debugging. the error is due to commandline arguments being `none` when running pipeline directly through `run.py`. ``` traceback (most recent call last): file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 193, in _run_module_as_main \"__main__\", mod_spec) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code exec(code, run_globals) file \"\/users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonfiles\/lib\/python\/debugpy\/__main__.py\", line 45, in cli.main() file \"\/users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonfiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 430, in main run() file \"\/users\/olszewk2\/.vscode\/extensions\/ms-python.python-2020.8.105369\/pythonfiles\/lib\/python\/debugpy\/..\/debugpy\/server\/cli.py\", line 267, in run_file runpy.run_path(options.target, run_name=compat.force_str(\"__main__\")) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 263, in run_path pkg_name=pkg_name, script_name=fname) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code mod_name, mod_spec, pkg_name, script_name) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/runpy.py\", line 85, in _run_code exec(code, run_globals) file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 75, in run_package() file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/pyzypad_example\/run.py\", line 71, in run_package project_context.run() file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/framework\/context\/context.py\", line 725, in run run_params=record_data, pipeline=filtered_pipeline, catalog=catalog file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/hooks.py\", line 286, in __call__ return self._hookexec(self, self.get_hookimpls(), kwargs) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 93, in _hookexec return self._inner_hookexec(hook, methods, kwargs) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/manager.py\", line 87, in firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else false, file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 208, in _multicall return outcome.get_result() file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 80, in get_result raise ex[1].with_traceback(ex[2]) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/pluggy\/callers.py\", line 187, in _multicall res = hook_impl.function(*args) file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/-\/_\/framework\/hooks\/pipeline_hook.py\", line 85, in before_pipeline_run pipeline_name=run_params[\"pipeline_name\"], file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/-\/_\/framework\/hooks\/pipeline_hook.py\", line 136, in _generate__command if len(from_inputs) > 0: typeerror: object of type 'nonetype' has no len() ```",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when attempting to debug a run in VSCode using the suggested configuration, due to commandline arguments being `None` when running the pipeline directly through `run.py`.",
        "Issue_preprocessed_content":"Title: typeerror in when debugging run in vscode; Content: happens when suggested is used for debugging. the error is due to commandline arguments being when running pipeline directly through ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Issue_title":"Warning message appears when calling ``kedro mlflow init``",
        "Issue_creation_time":1593379921000,
        "Issue_closed_time":1600718139000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: warning message appears when calling `` init``; Content: the warning cls that the project is not initialised yet, and that you must call `` init`` before calling any command while you are calling `` init``. it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command.",
        "Issue_original_content_gpt_summary":"The user encountered a warning message when calling `` init``, which can be safely ignored as the command works as intended, due to a bug caused by the dynamic creation of command.",
        "Issue_preprocessed_content":"Title: warning message appears when calling ; Content: the warning cls that the project is not initialised yet, and that you must call before calling any command while you are calling . it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command."
    },
    {
        "Issue_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/160",
        "Issue_title":"Add support for kedro namespaces in data catalog",
        "Issue_creation_time":1658927398000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Since kedro 0.17.7(?) there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled.\r\n\r\nUnless the function to create kfp artifacts is disabled in `kubeflow.yaml` config:\r\n```yaml\r\nstore_kedro_outputs_as_kfp_artifacts: False\r\n```\r\nIt causes issues like:\r\n```\r\nValueError: Only letters, numbers, spaces, \"_\", and \"-\" are allowed in name. Must begin with a letter. Got name: data_science.active_modelling_pipeline.X_train\r\n```\r\nwhen trying to run or update the pipeline.\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: add support for namespaces in data catalog; Content: since 0.17.7(?) there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled. unless the function to create kfp artifacts is disabled in `kubeflow.yaml` config: ```yaml store__outputs_as_kfp_artifacts: false ``` it causes issues like: ``` valueerror: only letters, numbers, spaces, \"_\", and \"-\" are allowed in name. must begin with a letter. got name: data_science.active_modelling_pipeline.x_train ``` when trying to run or update the pipeline.",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to add support for namespaces in the data catalog, as the function to create KFP artifacts was not properly handled and caused issues unless disabled in the 'kubeflow.yaml' config.",
        "Issue_preprocessed_content":"Title: add support for namespaces in data catalog; Content: since there have been introduced namespaces which cause issues in kfp artifacts, as they are not properly handled. unless the function to create kfp artifacts is disabled in config it causes issues like when trying to run or update the pipeline."
    },
    {
        "Issue_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/102",
        "Issue_title":"Plugin only compatible with kedro-mlflow<0.8.0",
        "Issue_creation_time":1643989114000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # NOQA\r\n        from kedro_mlflow.framework.context import get_mlflow_config  # NOQA\r\n        return True\r\n    except ImportError:\r\n        return False\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: plugin only compatible with -<0.8.0; Content: ```python def is__enabled() -> bool: try: import # noqa from _.framework.context import get__config # noqa return true except importerror: return false ``` alway throws exception since `context` package has been moved or refactored",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where a plugin was only compatible with versions lower than 0.8.0, and the `context` package had been moved or refactored, causing the plugin to always throw an exception.",
        "Issue_preprocessed_content":"Title: plugin only compatible with ; Content: alway throws exception since package has been moved or refactored"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/83",
        "Issue_title":"Kedro Telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists",
        "Issue_creation_time":1669645759000,
        "Issue_closed_time":1670415546000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\nKedro Telemetry installed alongside a packaged and installed Kedro project breaks the project by assuming that the `pyproject.toml` file exists. The `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases.\r\n\r\nThe problem was introduced with https:\/\/github.com\/kedro-org\/kedro-plugins\/pull\/62\r\n\r\n## Context\r\nWhen deploying Kedro projects and if you have installed Kedro Telemetry, it breaks your project.\r\n\r\n## Steps to Reproduce\r\n1. Create a Kedro project\r\n2. Add a dependency on kedro-telemetry\r\n3. Package it through `kedro package`\r\n4. Install it in a different environment\r\n5. Run the project through `.\/<project>` in a folder where only the `conf\/` is\r\n\r\n## Expected Result\r\nThe project should run.\r\n\r\n## Actual Result\r\nAn exception is thrown.\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.18.x\r\n* Kedro plugin and kedro plugin version used (`pip show kedro-telemetry`): 0.2.2 \r\n* Python version used (`python -V`): Not relevant\r\n* Operating system and version: Not relevant\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: telemetry breaks packaged projects due to wrongly assuming `pyproject.toml` exists; Content: ## description telemetry installed alongside a packaged and installed project breaks the project by assuming that the `pyproject.toml` file exists. the `pyproject.toml` is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases. the problem was introduced with https:\/\/github.com\/-org\/-plugins\/pull\/62 ## context when deploying projects and if you have installed telemetry, it breaks your project. ## steps to reproduce 1. create a project 2. add a dependency on -telemetry 3. package it through ` package` 4. install it in a different environment 5. run the project through `.\/` in a folder where only the `conf\/` is ## expected result the project should run. ## actual result an exception is thrown. ## your environment include as many relevant details about the environment in which you experienced the bug: * version used (`pip show ` or ` -v`): 0.18.x * plugin and plugin version used (`pip show -telemetry`): 0.2.2 * python version used (`python -v`): not relevant * operating system and version: not relevant",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where telemetry installed alongside a packaged and installed project breaks the project by assuming that the `pyproject.toml` file exists.",
        "Issue_preprocessed_content":"Title: telemetry breaks packaged projects due to wrongly assuming exists; Content: description telemetry installed alongside a packaged and installed project breaks the project by assuming that the file exists. the is only a recipe for building the project and should not be assumed to be existing in the current folder in all cases. the problem was introduced with context when deploying projects and if you have installed telemetry, it breaks your project. steps to reproduce . create a project . add a dependency on . package it through . install it in a different environment . run the project through in a folder where only the is expected result the project should run. actual result an exception is thrown. your environment include as many relevant details about the environment in which you experienced the bug version used plugin and plugin version used python version used not relevant operating system and version not relevant"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/64",
        "Issue_title":"pip installing kedro-datasets[option] causes different dependencies to installing kedro[option]",
        "Issue_creation_time":1666866011000,
        "Issue_closed_time":1667929584000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\nInstalling `kedro-datasets[option]` installs a different set of dependencies than `kedro[option]`. It appears that `kedro-datasets[option]` is installing the superset of requirements for all datasets.\n\n## Context\nThis is currently blocking https:\/\/github.com\/kedro-org\/kedro\/issues\/1495\n\n## Steps to Reproduce\n1. `pip install \"kedro[pandas.CSVDataSet]\"; pip freeze > requirements-kedro.txt`\n2. `pip install \"kedro-datasets[pandas.CSVDataSet]\"; pip freeze > requirements-kedro-datasets.txt`\n3. Compare the requirements\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: pip installing -datasets[option] causes different dependencies to installing [option]; ## description installing `-datasets[option]` installs a different set of dependencies than `[option]`. it appears that `-datasets[option]` is installing the superset of requirements for all datasets. ## context this is currently blocking https:\/\/github.com\/-org\/\/issues\/1495 ## steps to reproduce 1. `pip install \"[pandas.csvdataset]\"; pip freeze > requirements-.txt` 2. `pip install \"-datasets[pandas.csvdataset]\"; Content: pip freeze > requirements--datasets.txt` 3. compare the requirements",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where installing `-datasets[option]` installs a different set of dependencies than `[option]`, blocking a GitHub issue.",
        "Issue_preprocessed_content":"Title: pip installing causes different dependencies to installing ; Content: description installing installs a different set of dependencies than . it appears that is installing the superset of requirements for all datasets. context this is currently blocking steps to reproduce . . . compare the requirements"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/75",
        "Issue_title":"kedro airflow plugins: ValueError Pipeline input(s) not found in the DataCatalog",
        "Issue_creation_time":1664420413000,
        "Issue_closed_time":1668830449000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"\r\nwhen I run the Airflow Job\r\nHave this problem\r\n```\r\nValueError: Pipeline input(s) {'X_test', 'y_train', 'X_train'} not found in the DataCatalog\r\n```\r\n\r\n```python\r\nimport sys\r\nfrom collections import defaultdict\r\nfrom datetime import datetime, timedelta\r\nfrom pathlib import Path\r\n\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.version import version\r\nfrom kedro.framework.project import configure_project\r\nfrom kedro.framework.session import KedroSession\r\n\r\n\r\nsys.path.append(\"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\/src\")\r\n\r\n\r\n\r\n\r\nclass KedroOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, package_name: str, pipeline_name: str, node_name: str,\r\n                 project_path: str, env: str, *args, **kwargs) -> None:\r\n        super().__init__(*args, **kwargs)\r\n        self.package_name = package_name\r\n        self.pipeline_name = pipeline_name\r\n        self.node_name = node_name\r\n        self.project_path = project_path\r\n        self.env = env\r\n\r\n    def execute(self, context):\r\n        configure_project(self.package_name)\r\n        with KedroSession.create(self.package_name,\r\n                                 self.project_path,\r\n                                 env=self.env) as session:\r\n            session.run(self.pipeline_name, node_names=[self.node_name])\r\n\r\n\r\n# Kedro settings required to run your pipeline\r\nenv = \"local\"\r\npipeline_name = \"__default__\"\r\n#project_path = Path.cwd()\r\nproject_path = \"\/Users\/mahao\/airflow\/dags\/pandas_iris_01\"\r\nprint(project_path)\r\n\r\npackage_name = \"pandas_iris_01\"\r\n\r\n# Default settings applied to all tasks\r\ndefault_args = {\r\n    'owner': 'airflow',\r\n    'depends_on_past': False,\r\n    'email_on_failure': False,\r\n    'email_on_retry': False,\r\n    'retries': 1,\r\n    'retry_delay': timedelta(minutes=5)\r\n}\r\n\r\n# Using a DAG context manager, you don't have to specify the dag property of each task\r\nwith DAG(\r\n        \"pandas-iris-01\",\r\n        start_date=datetime(2019, 1, 1),\r\n        max_active_runs=3,\r\n        schedule_interval=timedelta(\r\n            minutes=30\r\n        ),  # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs\r\n        default_args=default_args,\r\n        catchup=False  # enable if you don't want historical dag runs to run\r\n) as dag:\r\n\r\n    tasks = {}\r\n\r\n    tasks[\"split\"] = KedroOperator(\r\n        task_id=\"split\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"split\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"make-predictions\"] = KedroOperator(\r\n        task_id=\"make-predictions\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"make_predictions\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"report-accuracy\"] = KedroOperator(\r\n        task_id=\"report-accuracy\",\r\n        package_name=package_name,\r\n        pipeline_name=pipeline_name,\r\n        node_name=\"report_accuracy\",\r\n        project_path=project_path,\r\n        env=env,\r\n    )\r\n\r\n    tasks[\"split\"] >> tasks[\"make-predictions\"]\r\n\r\n    tasks[\"split\"] >> tasks[\"report-accuracy\"]\r\n\r\n    tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"]\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: airflow plugins: valueerror pipeline input(s) not found in the datacatalog; Content: when i run the airflow job have this problem ``` valueerror: pipeline input(s) {'x_test', 'y_train', 'x_train'} not found in the datacatalog ``` ```python import sys from collections import defaultdict from datetime import datetime, timedelta from pathlib import path from airflow import dag from airflow.models import baseoperator from airflow.utils.decorators import apply_defaults from airflow.version import version from .framework.project import configure_project from .framework.session import session sys.path.append(\"\/users\/mahao\/airflow\/dags\/pandas_iris_01\/src\") class operator(baseoperator): @apply_defaults def __init__(self, package_name: str, pipeline_name: str, node_name: str, project_path: str, env: str, *args, **kwargs) -> none: super().__init__(*args, **kwargs) self.package_name = package_name self.pipeline_name = pipeline_name self.node_name = node_name self.project_path = project_path self.env = env def execute(self, context): configure_project(self.package_name) with session.create(self.package_name, self.project_path, env=self.env) as session: session.run(self.pipeline_name, node_names=[self.node_name]) # settings required to run your pipeline env = \"local\" pipeline_name = \"__default__\" #project_path = path.cwd() project_path = \"\/users\/mahao\/airflow\/dags\/pandas_iris_01\" print(project_path) package_name = \"pandas_iris_01\" # default settings applied to all tasks default_args = { 'owner': 'airflow', 'depends_on_past': false, 'email_on_failure': false, 'email_on_retry': false, 'retries': 1, 'retry_delay': timedelta(minutes=5) } # using a dag context manager, you don't have to specify the dag property of each task with dag( \"pandas-iris-01\", start_date=datetime(2019, 1, 1), max_active_runs=3, schedule_interval=timedelta( minutes=30 ), # https:\/\/airflow.apache.org\/docs\/stable\/scheduler.html#dag-runs default_args=default_args, catchup=false # enable if you don't want historical dag runs to run ) as dag: tasks = {} tasks[\"split\"] = operator( task_id=\"split\", package_name=package_name, pipeline_name=pipeline_name, node_name=\"split\", project_path=project_path, env=env, ) tasks[\"make-predictions\"] = operator( task_id=\"make-predictions\", package_name=package_name, pipeline_name=pipeline_name, node_name=\"make_predictions\", project_path=project_path, env=env, ) tasks[\"report-accuracy\"] = operator( task_id=\"report-accuracy\", package_name=package_name, pipeline_name=pipeline_name, node_name=\"report_accuracy\", project_path=project_path, env=env, ) tasks[\"split\"] >> tasks[\"make-predictions\"] tasks[\"split\"] >> tasks[\"report-accuracy\"] tasks[\"make-predictions\"] >> tasks[\"report-accuracy\"] ```",
        "Issue_original_content_gpt_summary":"The user encountered a ValueError when running an Airflow job, indicating that pipeline input(s) were not found in the DataCatalog.",
        "Issue_preprocessed_content":"Title: airflow plugins valueerror pipeline input not found in the datacatalog; Content: when i run the airflow job have this problem"
    },
    {
        "Issue_link":"https:\/\/github.com\/kedro-org\/kedro-plugins\/issues\/13",
        "Issue_title":"Kedro-Airflow not working with Astrocloud",
        "Issue_creation_time":1648473272000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":12.0,
        "Issue_body":"Raised by @jweiss-ocurate:\r\n\r\n## Description\r\nI am trying to run a simple spaceflights example with Astrocloud. I wasn't sure if anyone has been able to get it to work. \r\n\r\nHere is the DockerFile:\r\nFROM quay.io\/astronomer\/astro-runtime:4.1.0\r\n\r\nRUN pip install --user new_kedro_project-0.1-py3-none-any.whl --ignore-requires-python\r\n\r\n## Context\r\nI am trying to use kedro-airflow with astrocloud.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Follow directions here https:\/\/kedro.readthedocs.io\/en\/latest\/10_deployment\/11_airflow_astronomer.html\r\n2. Replace the DockerFile with the above mentioned image.\r\n\r\n## Expected Result\r\nComplete Kedro Run on local Airflow image.\r\n\r\n## Actual Result\r\nFailure in local Airflow image.\r\n[2022-02-26, 16:43:26 UTC] {store.py:32} INFO - `read()` not implemented for `BaseSessionStore`. Assuming empty store.\r\n[2022-02-26, 16:43:26 UTC] {session.py:78} WARNING - Unable to git describe \/usr\/local\/airflow\r\n[2022-02-26, 16:43:29 UTC] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment you experienced the bug in:\r\n\r\n* Kedro-Airflow plugin version used (get it by running `pip show kedro-airflow`): 0.4.1\r\n* Airflow version (`airflow --version`):\r\n* Kedro version used (`pip show kedro` or `kedro -V`): 0.17.7\r\n* Python version used (`python -V`): > 2.0.0\r\n* Operating system and version: Ubuntu Linux 20.04",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: -airflow not working with astrocloud; Content: raised by @jweiss-ocurate: ## description i am trying to run a simple spaceflights example with astrocloud. i wasn't sure if anyone has been able to get it to work. here is the dockerfile: from quay.io\/astronomer\/astro-runtime:4.1.0 run pip install --user new__project-0.1-py3-none-any.whl --ignore-requires-python ## context i am trying to use -airflow with astrocloud. ## steps to reproduce 1. follow directions here https:\/\/.readthedocs.io\/en\/latest\/10_deployment\/11_airflow_astronomer.html 2. replace the dockerfile with the above mentioned image. ## expected result complete run on local airflow image. ## actual result failure in local airflow image. [2022-02-26, 16:43:26 utc] {store.py:32} info - `read()` not implemented for `basesessionstore`. assuming empty store. [2022-02-26, 16:43:26 utc] {session.py:78} warning - unable to git describe \/usr\/local\/airflow [2022-02-26, 16:43:29 utc] {local_task_job.py:154} info - task exited with return code negsignal.sigkill ## your environment include as many relevant details about the environment you experienced the bug in: * -airflow plugin version used (get it by running `pip show -airflow`): 0.4.1 * airflow version (`airflow --version`): * version used (`pip show ` or ` -v`): 0.17.7 * python version used (`python -v`): > 2.0.0 * operating system and version: ubuntu linux 20.04",
        "Issue_original_content_gpt_summary":"The user @jweiss-ocurate encountered challenges while trying to run a simple spaceflights example with astrocloud, resulting in failure in the local airflow image.",
        "Issue_preprocessed_content":"Title: not working with astrocloud; Content: raised by description i am trying to run a simple spaceflights example with astrocloud. i wasn't sure if anyone has been able to get it to work. here is the dockerfile from run pip install context i am trying to use with astrocloud. steps to reproduce . follow directions here . replace the dockerfile with the above mentioned image. expected result complete run on local airflow image. actual result failure in local airflow image. , utc info not implemented for . assuming empty store. , utc warning unable to git describe , utc info task exited with return code your environment include as many relevant details about the environment you experienced the bug in plugin version used airflow version version used python version used operating system and version ubuntu linux"
    },
    {
        "Issue_link":"https:\/\/github.com\/quaseldoku\/QuaselDoku\/issues\/1",
        "Issue_title":"Running kedro-pipeline \"dp\" results in \"character maps to <undefined>\"-error",
        "Issue_creation_time":1654682698000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Description\r\nRunning the kedro-pipeline \"dp\" via kedro-cli with \"kedro run --pipeline dp\" results in the following error:\r\n```cmd\r\nkedro.io.core.DataSetError: Failed while loading data from data set TextDataSet(filepath=C:\/EEAA\/Repos\/QuaselDoku\/data\/01_raw\/Doku_v1\/Bedienung\/EasyInsert.html, protocol=file).\r\n'charmap' codec can't decode byte 0x81 in position 5899: character maps to <undefined>\r\n```\r\n\r\n### Steps to reproduce\r\ncatalog.yml:\r\n```yml\r\necu_test_doku:\r\n  type: PartitionedDataSet\r\n  path: data\/01_raw\/Doku_v1\r\n  dataset: text.TextDataSet\r\n  filename_suffix: html\r\n```\r\n\r\npython-function to parse documentation-data:\r\n```python\r\ndef filter_doku(partitioned_input: Dict[str, Callable[[], Any]], params: Dict) -> Dict[str, Callable[[], Any]]:\r\n    \"\"\"\r\n    flatten input where html files can occur on multiple levels, as well as filter out files that match certain string.\r\n    Return new Dictionary with filenames and load functions from which a PartioniedDataset can be created and persisted.\r\n\r\n    Args:\r\n        partitioned_input: A dictionary with partition ids (file path) as keys and load functions as values.\r\n\r\n    Returns:\r\n        Dictionary with the partitions to create.\r\n    \"\"\"\r\n\r\n    result = {}\r\n\r\n    print(\"filtering out relevant html files from doku ...\")\r\n    for partition_key, partition_load_func in tqdm(sorted(partitioned_input.items())):\r\n        \r\n        exclude = False\r\n        for string in params['exclude_docs']:\r\n            \r\n            if string in partition_key:\r\n                \r\n                exclude = True\r\n                break\r\n\r\n        if exclude:\r\n            continue\r\n\r\n        filename = partition_key.replace('\/', ' ')\r\n        filename += 'html'\r\n\r\n        # append new filename with load function to results dictionary\r\n        result[filename] = partition_load_func\r\n\r\n    return result\r\n```\r\n\r\nkedro  0.18.0\r\n\r\nThanks! :)\r\n",
        "Tool":"Kedro",
        "Platform":"Github",
        "Issue_original_content":"Title: running -pipeline \"dp\" results in \"character maps to \"-error; Content: ### description running the -pipeline \"dp\" via -cli with \" run --pipeline dp\" results in the following error: ```cmd .io.core.dataseterror: failed while loading data from data set textdataset(filepath=c:\/eeaa\/repos\/quaseldoku\/data\/01_raw\/doku_v1\/bedienung\/easyinsert.html, protocol=file). 'charmap' codec can't decode byte 0x81 in position 5899: character maps to ``` ### steps to reproduce catalog.yml: ```yml ecu_test_doku: type: partitioneddataset path: data\/01_raw\/doku_v1 dataset: text.textdataset filename_suffix: html ``` python-function to parse documentation-data: ```python def filter_doku(partitioned_input: dict[str, callable[[], any]], params: dict) -> dict[str, callable[[], any]]: \"\"\" flatten input where html files can occur on multiple levels, as well as filter out files that match certain string. return new dictionary with filenames and load functions from which a partionieddataset can be created and persisted. args: partitioned_input: a dictionary with partition ids (file path) as keys and load functions as values. returns: dictionary with the partitions to create. \"\"\" result = {} print(\"filtering out relevant html files from doku ...\") for partition_key, partition_load_func in tqdm(sorted(partitioned_input.items())): exclude = false for string in params['exclude_docs']: if string in partition_key: exclude = true break if exclude: continue filename = partition_key.replace('\/', ' ') filename += 'html' # append new filename with load function to results dictionary result[filename] = partition_load_func return result ``` 0.18.0 thanks! :)",
        "Issue_original_content_gpt_summary":"The user encountered an error when running the -pipeline \"dp\" via -cli with \"run --pipeline dp\" which resulted in a 'charmap' codec error.",
        "Issue_preprocessed_content":"Title: running dp results in character maps to ; Content: description running the dp via with run dp results in the following error steps to reproduce to parse thanks!"
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/480",
        "Issue_title":"using mlflow without an mlflow writer configured appears to fail silently",
        "Issue_creation_time":1647628309000,
        "Issue_closed_time":1655127386000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Summary\r\n\r\nProfiling with mlflow and without an mlflow writer fails silently. \r\n\r\n### Steps to Reproduce it\r\n\r\nuse mlflow with get_or_create_session and no files are written.\r\n\r\n### Example\r\n\r\nThere are examples of how to configure mlflow writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_mlflow.yaml\r\n\r\nwhylogs should mention the missing mlflow writer in a warning. Maybe we can automatically add the mlflow writer (with a warning), so that it works and draws attention to where the behavior can be modified.\r\n\r\n## What is the current *bug* behavior?\r\n\r\nlogging with mlflow and default configuration appears to fail silently.\r\n\r\n### What is the expected *correct* behavior?\r\n\r\nmlflow integration should write to mlflow by default and warn if missing or inconsistent config is set.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: using without an writer configured appears to fail silently; Content: ### summary profiling with and without an writer fails silently. ### steps to reproduce it use with get_or_create_session and no files are written. ### example there are examples of how to configure writer config here: https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/.whylogs_.yaml whylogs should mention the missing writer in a warning. maybe we can automatically add the writer (with a warning), so that it works and draws attention to where the behavior can be modified. ## what is the current *bug* behavior? logging with and default configuration appears to fail silently. ### what is the expected *correct* behavior? integration should write to by default and warn if missing or inconsistent config is set.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where profiling with WhyLogs and without an output writer configured fails silently, with no warning or indication of the missing configuration.",
        "Issue_preprocessed_content":"Title: using without an writer configured appears to fail silently; Content: summary profiling with and without an writer fails silently. steps to reproduce it use with and no files are written. example there are examples of how to configure writer config here whylogs should mention the missing writer in a warning. maybe we can automatically add the writer , so that it works and draws attention to where the behavior can be modified. what is the current bug behavior? logging with and default configuration appears to fail silently. what is the expected correct behavior? integration should write to by default and warn if missing or inconsistent config is set."
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/458",
        "Issue_title":"Support writing out dataset profiles as json format with mlflow",
        "Issue_creation_time":1646156442000,
        "Issue_closed_time":1655127391000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### Summary\r\n\r\nyou can call mlflow.log_artifact directly and save the profile JSON:\r\n```\r\nsummary = profile.to_summary()\r\nopen(\"local_path\", \"wt\", transport_params=transport_params) as f:\r\n    f.write(message_to_json(summary))\r\nmlflow.log_artifact(\"local_path\", your\/path\")\r\n```\r\n\r\nbut if you pass a format config to mlflow writer specifying 'json' it isn't supported and instead uses the protobuf bin format.\r\n\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: support writing out dataset profiles as json format with ; Content: ### summary you can call .log_artifact directly and save the profile json: ``` summary = profile.to_summary() open(\"local_path\", \"wt\", transport_params=transport_params) as f: f.write(message_to_json(summary)) .log_artifact(\"local_path\", your\/path\") ``` but if you pass a format config to writer specifying 'json' it isn't supported and instead uses the protobuf bin format.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to write out dataset profiles as json format with the .log_artifact method, as the format config specifying 'json' was not supported and instead used the protobuf bin format.",
        "Issue_preprocessed_content":"Title: support writing out dataset profiles as json format with ; Content: summary you can call directly and save the profile json but if you pass a format config to writer specifying 'json' it isn't supported and instead uses the protobuf bin format."
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/411",
        "Issue_title":"MLflow example: close session error",
        "Issue_creation_time":1641941774000,
        "Issue_closed_time":1655127397000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"### Summary\r\n\r\n[<!-- Summarize the bug encountered concisely -->\r\n](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/MLFlow%20Integration%20Example.ipynb)\r\n### Steps to Reproduce it\r\n\r\nUsed Binder to run the above notebook\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\/tmp\/ipykernel_157\/4031979109.py in <module>\r\n     12 \r\n     13         # use whylogs to log data quality metrics for the current batch\r\n---> 14         mlflow.whylogs.log_pandas(batch)\r\n     15 \r\n     16     # wait a second between runs to create a time series of prediction results\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp)\r\n     71         :param dataset_name: the name of the dataset (Optional). If not specified, the experiment name is used\r\n     72         \"\"\"\r\n---> 73         ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n     74 \r\n     75         if ylogs is None:\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp)\r\n    103         ylogs = self._loggers.get(dataset_name)\r\n    104         if ylogs is None:\r\n--> 105             ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp)\r\n    106             self._loggers[dataset_name] = ylogs\r\n    107         return ylogs\r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/mlflow\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp)\r\n     57             tags,\r\n     58         )\r\n---> 59         logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags)\r\n     60         return logger_\r\n     61 \r\n\r\n\/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints)\r\n    172         \"\"\"\r\n    173         if not self._active:\r\n--> 174             raise RuntimeError(\"Session is already closed. Cannot create more loggers\")\r\n    175 \r\n    176         # Explicitly set the default timezone to utc if none was provided. Helps with equality testing\r\n\r\nRuntimeError: Session is already closed. Cannot create more loggers\r\n```\r\n### Example\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: example: close session error; Content: ### summary [ ](https:\/\/github.com\/whylabs\/whylogs-examples\/blob\/mainline\/python\/%20integration%20example.ipynb) ### steps to reproduce it used binder to run the above notebook ``` --------------------------------------------------------------------------- runtimeerror traceback (most recent call last) \/tmp\/ipykernel_157\/4031979109.py in 12 13 # use whylogs to log data quality metrics for the current batch ---> 14 .whylogs.log_pandas(batch) 15 16 # wait a second between runs to create a time series of prediction results \/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/\/patcher.py in log_pandas(self, df, dataset_name, dataset_timestamp) 71 :param dataset_name: the name of the dataset (optional). if not specified, the experiment name is used 72 \"\"\" ---> 73 ylogs = self._get_or_create_logger(dataset_name, dataset_timestamp=dataset_timestamp) 74 75 if ylogs is none: \/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/\/patcher.py in _get_or_create_logger(self, dataset_name, dataset_timestamp) 103 ylogs = self._loggers.get(dataset_name) 104 if ylogs is none: --> 105 ylogs = self._create_logger(dataset_name, dataset_timestamp=dataset_timestamp) 106 self._loggers[dataset_name] = ylogs 107 return ylogs \/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/\/patcher.py in _create_logger(self, dataset_name, dataset_timestamp) 57 tags, 58 ) ---> 59 logger_ = self._session.logger(run_info.run_id, session_timestamp=session_timestamp, dataset_timestamp=dataset_timestamp, tags=tags) 60 return logger_ 61 \/srv\/conda\/envs\/notebook\/lib\/python3.7\/site-packages\/whylogs\/app\/session.py in logger(self, dataset_name, dataset_timestamp, session_timestamp, tags, metadata, segments, profile_full_dataset, with_rotation_time, cache_size, constraints) 172 \"\"\" 173 if not self._active: --> 174 raise runtimeerror(\"session is already closed. cannot create more loggers\") 175 176 # explicitly set the default timezone to utc if none was provided. helps with equality testing runtimeerror: session is already closed. cannot create more loggers ``` ### example",
        "Issue_original_content_gpt_summary":"The user encountered a runtime error when attempting to create a logger in a closed session.",
        "Issue_preprocessed_content":"Title: example close session error; Content: summary summarize the bug encountered concisely steps to reproduce it used binder to run the above notebook example"
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/338",
        "Issue_title":"[Bug\/Feature Request] Support mlflow 1.19",
        "Issue_creation_time":1634141491000,
        "Issue_closed_time":1634667593000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Users are reporting issues with mlflow 1.19\r\n\r\nCreating an issue here to track. Details will be added as we investigate further",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug\/feature request] support 1.19; Content: users are reporting issues with 1.19 creating an issue here to track. details will be added as we investigate further",
        "Issue_original_content_gpt_summary":"The user is encountering issues with 1.19 and is creating a bug\/feature request to track the details as they investigate further.",
        "Issue_preprocessed_content":"Title: support ; Content: users are reporting issues with creating an issue here to track. details will be added as we investigate further"
    },
    {
        "Issue_link":"https:\/\/github.com\/whylabs\/whylogs\/issues\/72",
        "Issue_title":"MLFlow NameError",
        "Issue_creation_time":1603138068000,
        "Issue_closed_time":1603222865000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When I don't have the optional MLFlow dependency installed I get the following exception the first time I try to import the `numbertracker`.  The second time I run the import, everything works just fine.\r\n\r\n```python\r\nfrom whylogs.core.statistics import numbertracker\r\n\r\n\r\n\r\nFailed to import MLFLow\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-1-3964e19b3cb4> in <module>\r\n----> 1 from whylogs.core.statistics import numbertracker\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/__init__.py in <module>\r\n      4 from .app.session import get_or_create_session\r\n      5 from .app.session import reset_default_session\r\n----> 6 from .mlflow import enable_mlflow\r\n      7 \r\n      8 __all__ = [\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/__init__.py in <module>\r\n----> 1 from .patcher import enable_mlflow\r\n      2 \r\n      3 __all__ = [\"enable_mlflow\"]\r\n\r\n~\/src\/whylogs-github\/src\/whylogs\/mlflow\/patcher.py in <module>\r\n    145 \r\n    146 _active_whylogs = []\r\n--> 147 _original_end_run = mlflow.tracking.fluent.end_run\r\n    148 \r\n    149 \r\n\r\nNameError: name 'mlflow' is not defined\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: nameerror; Content: when i don't have the optional dependency installed i get the following exception the first time i try to import the `numbertracker`. the second time i run the import, everything works just fine. ```python from whylogs.core.statistics import numbertracker failed to import --------------------------------------------------------------------------- nameerror traceback (most recent call last) in ----> 1 from whylogs.core.statistics import numbertracker ~\/src\/whylogs-github\/src\/whylogs\/__init__.py in 4 from .app.session import get_or_create_session 5 from .app.session import reset_default_session ----> 6 from . import enable_ 7 8 __all__ = [ ~\/src\/whylogs-github\/src\/whylogs\/\/__init__.py in ----> 1 from .patcher import enable_ 2 3 __all__ = [\"enable_\"] ~\/src\/whylogs-github\/src\/whylogs\/\/patcher.py in 145 146 _active_whylogs = [] --> 147 _original_end_run = .tracking.fluent.end_run 148 149 nameerror: name '' is not defined ```",
        "Issue_original_content_gpt_summary":"The user encountered a NameError when attempting to import the numbertracker module from the whylogs library, which was resolved by running the import a second time.",
        "Issue_preprocessed_content":"Title: nameerror; Content: when i don't have the optional dependency installed i get the following exception the first time i try to import the . the second time i run the import, everything works just fine."
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/810",
        "Issue_title":"Project template mlflow secret bad name",
        "Issue_creation_time":1650980403000,
        "Issue_closed_time":1664791991000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Currently the `.drone.yaml` is referencing the k8s secret `mlflow-server-secret` which doesn't exist by default.\r\n\r\nWe have noticed that `{{ .ProjectID }}-mlflow-secret` secret is created when a `kdlProject` resource is created.\r\n\r\nTo solve this issue the name of the `mlflow-server-secret` must be changed into `{{ .ProjectID }}-mlflow-secret`",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: project template secret bad name; Content: currently the `.drone.yaml` is referencing the k8s secret `-server-secret` which doesn't exist by default. we have noticed that `{{ .projectid }}--secret` secret is created when a `kdlproject` resource is created. to solve this issue the name of the `-server-secret` must be changed into `{{ .projectid }}--secret`",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the `.drone.yaml` was referencing a k8s secret `-server-secret` which didn't exist by default, and the solution was to change the name of the `-server-secret` to `{{ .projectid }}--secret`.",
        "Issue_preprocessed_content":"Title: project template secret bad name; Content: currently the is referencing the k s secret which doesn't exist by default. we have noticed that secret is created when a resource is created. to solve this issue the name of the must be changed into"
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/623",
        "Issue_title":"Project operator mlflow image tag is set to \"latest\"",
        "Issue_creation_time":1635429600000,
        "Issue_closed_time":1635871931000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"On chart release v0.13.2 the default value for projectOperator.mlflow.image.tag is set to latest when it should be set to v0.13.2.\r\n\r\nCheck values.yml:\r\n\r\n```yaml\r\nprojectOperator:\r\n  image:\r\n    repository: konstellation\/project-operator\r\n    tag: v0.13.2\r\n    pullPolicy: IfNotPresent\r\n  mlflow:\r\n    image:\r\n      repository: konstellation\/mlflow\r\n      tag: latest\r\n      pullPolicy: IfNotPresent\r\n    volume:\r\n      storageClassName: standard\r\n      size: 1Gi\r\n  filebrowser:\r\n    image:\r\n      repository: filebrowser\/filebrowser\r\n      tag: v2\r\n      pullPolicy: IfNotPresent\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: project operator image tag is set to \"latest\"; Content: on chart release v0.13.2 the default value for projectoperator..image.tag is set to latest when it should be set to v0.13.2. check values.yml: ```yaml projectoperator: image: repository: konstellation\/project-operator tag: v0.13.2 pullpolicy: ifnotpresent : image: repository: konstellation\/ tag: latest pullpolicy: ifnotpresent volume: storageclassname: standard size: 1gi filebrowser: image: repository: filebrowser\/filebrowser tag: v2 pullpolicy: ifnotpresent ```",
        "Issue_original_content_gpt_summary":"The encountered challenge was that the default value for projectoperator.image.tag was set to \"latest\" when it should have been set to \"v0.13.2\" in the chart release v0.13.2.",
        "Issue_preprocessed_content":"Title: project operator image tag is set to latest ; Content: on chart release the default value for is set to latest when it should be set to check"
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/404",
        "Issue_title":"Users can access to any MLflow project",
        "Issue_creation_time":1619772559000,
        "Issue_closed_time":1620648494000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Only allow access to project members for the given MLflow.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: users can access to any project; Content: only allow access to project members for the given .",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of allowing access to project members only for the given project, while still allowing access to any project.",
        "Issue_preprocessed_content":"Title: users can access to any project; Content: only allow access to project members for the given ."
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/380",
        "Issue_title":"Bad MLflow artifact folder by default",
        "Issue_creation_time":1619181542000,
        "Issue_closed_time":1623230636000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"The artifact folder by default is not reemplacing the `$ARTIFACTS_BUCKET` env var",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: bad artifact folder by default; Content: the artifact folder by default is not reemplacing the `$artifacts_bucket` env var",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the artifact folder by default was not replacing the `$artifacts_bucket` environment variable.",
        "Issue_preprocessed_content":"Title: bad artifact folder by default; Content: the artifact folder by default is not reemplacing the env var"
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/379",
        "Issue_title":"All MLflow experiments are visible for any user",
        "Issue_creation_time":1619181390000,
        "Issue_closed_time":1623230615000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"We should create a instance of MLflow for each project in order to see the experiments related to the current project.\r\n\r\n- [x] Create project operator to deploy a MLFlow instance for each project.\r\n- [x] Update KDL APP API to create the KDLProject custom resource in k8s.\r\n- [x] Update kdlctl.sh adding project-operator docker image building.\r\n- [x] Add project-operator to KDL server helm chart.\r\n- [x] Add github workflows to publish the project-operator in docker hub.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: all experiments are visible for any user; Content: we should create a instance of for each project in order to see the experiments related to the current project. - [x] create project operator to deploy a instance for each project. - [x] update kdl app api to create the kdlproject custom resource in k8s. - [x] update kdlctl.sh adding project-operator docker image building. - [x] add project-operator to kdl server helm chart. - [x] add github workflows to publish the project-operator in docker hub.",
        "Issue_original_content_gpt_summary":"The user encountered challenges in creating a project operator to deploy a instance for each project, updating the KDL app API to create the KDLProject custom resource in K8s, updating the KDLctl.sh adding project-operator docker image building, adding project-operator to KDL server helm chart, and adding GitHub workflows to publish the project-operator in Docker Hub.",
        "Issue_preprocessed_content":"Title: all experiments are visible for any user; Content: we should create a instance of for each project in order to see the experiments related to the current project. create project operator to deploy a instance for each project. update kdl app api to create the kdlproject custom resource in k s. update adding docker image building. add to kdl server helm chart. add github workflows to publish the in docker hub."
    },
    {
        "Issue_link":"https:\/\/github.com\/konstellation-io\/kdl-server\/issues\/150",
        "Issue_title":"Error loading MLFlow",
        "Issue_creation_time":1614760376000,
        "Issue_closed_time":1646221957000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: error loading",
        "Issue_original_content_gpt_summary":"The user encountered challenges when trying to load an error, resulting in a lack of access to the desired content.",
        "Issue_preprocessed_content":"Title: error loading ; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/deepset-ai\/FARM\/issues\/217",
        "Issue_title":"MLFlowLogger: \"Connection aborted.\" - RemoteDisconnected Error",
        "Issue_creation_time":1580136891000,
        "Issue_closed_time":1580393757000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\nI try to do multi-label classification with \"doc_classification_multilabel.py\". It worked at first. However when it came to `\"Train epoch 1\/1:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 17251\/26668 [10:19:41<4:04:28,  1.56s\/it]\"`, it stopped and report:\r\n\r\n```\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen\r\n    chunked=chunked,\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse\r\n    response.begin()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 297, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n......\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\r\n```\r\n\r\n  I have checked that the Internet connection was ok. So I was confused why this error occured ?\r\n  \r\n\r\n**Error message**\r\nError that was thrown (if available)\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here, like type of downstream task, part of  etc.. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior\r\n\r\n**System:**\r\n - OS: \r\n - GPU\/CPU:\r\n - FARM version:\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logger: \"connection aborted.\" - remotedisconnected error; Content: **describe the bug** i try to do multi-label classification with \"doc_classification_multilabel.py\". it worked at first. however when it came to `\"train epoch 1\/1: 65%| | 17251\/26668 [10:19:41<4:04:28, 1.56s\/it]\"`, it stopped and report: ``` file \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 672, in urlopen chunked=chunked, file \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 421, in _make_request six.raise_from(e, none) file \"\", line 3, in raise_from file \"\/home\/python3.6\/site-packages\/urllib3\/connectionpool.py\", line 416, in _make_request httplib_response = conn.getresponse() file \"\/home\/python3.6\/http\/client.py\", line 1331, in getresponse response.begin() file \"\/home\/python3.6\/http\/client.py\", line 297, in begin version, status, reason = self._read_status() file \"\/home\/python3.6\/http\/client.py\", line 266, in _read_status raise remotedisconnected(\"remote end closed connection without\" http.client.remotedisconnected: remote end closed connection without response ...... urllib3.exceptions.protocolerror: ('connection aborted.', remotedisconnected('remote end closed connection without response',)) ``` i have checked that the internet connection was ok. so i was confused why this error occured ? **error message** error that was thrown (if available) **expected behavior** a clear and concise description of what you expected to happen. **additional context** add any other context about the problem here, like type of downstream task, part of etc.. **to reproduce** steps to reproduce the behavior **system:** - os: - gpu\/cpu: - farm version:",
        "Issue_original_content_gpt_summary":"The user encountered a remotedisconnected error when attempting to do multi-label classification with \"doc_classification_multilabel.py\", despite having a working internet connection.",
        "Issue_preprocessed_content":"Title: logger connection remotedisconnected error; Content: describe the bug i try to do classification with it worked at first. however when it came to , it stopped and report i have checked that the internet connection was ok. so i was confused why this error occured ? error message error that was thrown expected behavior a clear and concise description of what you expected to happen. additional context add any other context about the problem here, like type of downstream task, part of to reproduce steps to reproduce the behavior system os farm version"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/499",
        "Issue_title":"Permission denied when log models to mlflow on Mac",
        "Issue_creation_time":1642469367000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"```\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py\", line 188, in log\r\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 584, in log_artifacts\r\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 977, in log_artifacts\r\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 334, in log_artifacts\r\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py\", line 57, in log_artifacts\r\n    mkdir(artifact_dir)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/utils\/file_utils.py\", line 113, in mkdir\r\n    raise e\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/mlflow\/utils\/file_utils.py\", line 110, in mkdir\r\n    os.makedirs(target)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  [Previous line repeated 2 more times]\r\n  File \"\/Users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 225, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/var\/lib\/mlflow'\r\n```\r\n\r\nEnvironment:\r\n* mlflow==1.2\r\n* macOS 12.1",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: permission denied when log models to on mac; Content: ``` file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/models\/model.py\", line 188, in log .tracking.fluent.log_artifacts(local_path, artifact_path) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/tracking\/fluent.py\", line 584, in log_artifacts client().log_artifacts(run_id, local_dir, artifact_path) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/tracking\/client.py\", line 977, in log_artifacts self._tracking_client.log_artifacts(run_id, local_dir, artifact_path) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 334, in log_artifacts self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/store\/artifact\/local_artifact_repo.py\", line 57, in log_artifacts mkdir(artifact_dir) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/utils\/file_utils.py\", line 113, in mkdir raise e file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/site-packages\/\/utils\/file_utils.py\", line 110, in mkdir os.makedirs(target) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs makedirs(head, exist_ok=exist_ok) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs makedirs(head, exist_ok=exist_ok) file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 215, in makedirs makedirs(head, exist_ok=exist_ok) [previous line repeated 2 more times] file \"\/users\/lei\/miniforge3\/envs\/rikai\/lib\/python3.9\/os.py\", line 225, in makedirs mkdir(name, mode) permissionerror: [errno 13] permission denied: '\/var\/lib\/' ``` environment: * ==1.2 * macos 12.1",
        "Issue_original_content_gpt_summary":"The user encountered a PermissionError when attempting to log models on MacOS, due to insufficient permissions to create a directory in the '\/var\/lib\/' directory.",
        "Issue_preprocessed_content":"Title: permission denied when log models to on mac; Content: environment macos"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/493",
        "Issue_title":"Can not create model in MLflowCatalog",
        "Issue_creation_time":1642187856000,
        "Issue_closed_time":1642206718000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> CREATE MODEL ssd1 using \"mlflow:\/model\/ssd\"\r\n. . . . . . . . . . . . . . . . . . . .> ;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.mlflow.tracking.MlflowHttpException: statusCode=404 reasonPhrase=[NOT FOUND] bodyMessage=[{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Registered Model with name=ssd1 not found\"}]\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperti\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: can not create model in catalog; Content: ``` 0: jdbc:hive2:\/\/localhost:10001\/default> create model ssd1 using \":\/model\/ssd\" . . . . . . . . . . . . . . . . . . . .> ; error: org.apache.hive.service.cli.hivesqlexception: error running query: org..tracking.httpexception: statuscode=404 reasonphrase=[not found] bodymessage=[{\"error_code\": \"resource_does_not_exist\", \"message\": \"registered model with name=ssd1 not found\"}] at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation.org$apache$spark$sql$hive$thriftserver$sparkexecutestatementoperation$$execute(sparkexecutestatementoperation.scala:361) at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.$anonfun$run$2(sparkexecutestatementoperation.scala:263) at scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:23) at org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperties(sparkoperation.scala:78) at org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperti ```",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to create a model in the catalog, resulting in an org.apache.hive.service.cli.hivesqlexception.",
        "Issue_preprocessed_content":"Title: can not create model in catalog; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/492",
        "Issue_title":"MlflowCatalog anonymous function is not registered ",
        "Issue_creation_time":1642186768000,
        "Issue_closed_time":1642203169000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Problem:\r\n```\r\n0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ML_predict(ssd, image) FROM coco limit 1;\r\nError: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Undefined function: '<anonymous>'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 17\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:361)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:263)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:263)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:258)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:272)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Register models into mlflow\r\n2. Start Spark thrift server\r\n3. Use `beeline` to connec to the thrift server:  `beeline -u jdbc:hive2:\/\/localhost:10001\/default`\r\n4. run `SELECT ML_PREDICT(ssd, image) FROM coco`",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: catalog anonymous function is not registered ; problem: ``` 0: jdbc:hive2:\/\/localhost:10001\/default> select image_id, ml_predict(ssd, image) from coco limit 1; error: org.apache.hive.service.cli.hivesqlexception: error running query: org.apache.spark.sql.analysisexception: undefined function: ''. this function is neither a registered temporary function nor a permanent function registered in the database 'default'.; Content: line 1 pos 17 at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation.org$apache$spark$sql$hive$thriftserver$sparkexecutestatementoperation$$execute(sparkexecutestatementoperation.scala:361) at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.$anonfun$run$2(sparkexecutestatementoperation.scala:263) at scala.runtime.java8.jfunction0$mcv$sp.apply(jfunction0$mcv$sp.java:23) at org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperties(sparkoperation.scala:78) at org.apache.spark.sql.hive.thriftserver.sparkoperation.withlocalproperties$(sparkoperation.scala:62) at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation.withlocalproperties(sparkexecutestatementoperation.scala:43) at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.run(sparkexecutestatementoperation.scala:263) at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2$$anon$3.run(sparkexecutestatementoperation.scala:258) at java.security.accesscontroller.doprivileged(native method) at javax.security.auth.subject.doas(subject.java:422) at org.apache.hadoop.security.usergroupinformation.doas(usergroupinformation.java:1730) at org.apache.spark.sql.hive.thriftserver.sparkexecutestatementoperation$$anon$2.run(sparkexecutestatementoperation.scala:272) at java.util.concurrent.executors$runnableadapter.call(executors.java:511) at java.util.concurrent.futuretask.run(futuretask.java:266) at java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1149) ``` steps to reproduce: 1. register models into 2. start spark thrift server 3. use `beeline` to connec to the thrift server: `beeline -u jdbc:hive2:\/\/localhost:10001\/default` 4. run `select ml_predict(ssd, image) from coco`",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to run an anonymous function in a Hive query, due to the function not being registered in the database.",
        "Issue_preprocessed_content":"Title: catalog anonymous function is not registered ; Content: problem steps to reproduce . register models into . start spark thrift server . use to connec to the thrift server . run"
    },
    {
        "Issue_link":"https:\/\/github.com\/eto-ai\/rikai\/issues\/207",
        "Issue_title":"Leaking mlflow dependency",
        "Issue_creation_time":1617993087000,
        "Issue_closed_time":1617994925000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"```\r\ntests\/conftest.py:4: in <module>\r\n    from rikai.spark.sql import init\r\n..\/rikai\/python\/rikai\/__init__.py:19: in <module>\r\n    from rikai.spark.sql.codegen import mlflow_logger as mlflow\r\n..\/rikai\/python\/rikai\/spark\/sql\/codegen\/mlflow_logger.py:19: in <module>\r\n    import mlflow\r\nE   ModuleNotFoundError: No module named 'mlflow'\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: leaking dependency; Content: ``` tests\/conftest.py:4: in from rikai.spark.sql import init ..\/rikai\/python\/rikai\/__init__.py:19: in from rikai.spark.sql.codegen import _logger as ..\/rikai\/python\/rikai\/spark\/sql\/codegen\/_logger.py:19: in import e modulenotfounderror: no module named '' ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with a leaking dependency causing a ModuleNotFoundError when importing a module.",
        "Issue_preprocessed_content":"Title: leaking dependency; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/270",
        "Issue_title":"Pytorch Lightning 1.2.0 requires new MLflow version",
        "Issue_creation_time":1613838919000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Keep it in mind before mindlessly updating\r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/4118",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: pytorch lightning 1.2.0 requires new version; Content: keep it in mind before mindlessly updating https:\/\/github.com\/\/\/pull\/4118",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where updating to the latest version of PyTorch Lightning 1.2.0 required a new version of PyTorch, and they needed to keep this in mind before mindlessly updating.",
        "Issue_preprocessed_content":"Title: pytorch lightning requires new version; Content: keep it in mind before mindlessly updating"
    },
    {
        "Issue_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/229",
        "Issue_title":"Warning when training mlflow-pytorch 2.0.0",
        "Issue_creation_time":1612379283000,
        "Issue_closed_time":1613342987000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"`2021\/02\/03 19:07:05 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: float() argument must be a string or a number, not 'Accuracy'`\r\n\r\nprinted after every epoch!",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: warning when training -pytorch 2.0.0; Content: `2021\/02\/03 19:07:05 warning .utils.autologging_utils: encountered unexpected error during autologging: float() argument must be a string or a number, not 'accuracy'` printed after every epoch!",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected error during autologging when training with PyTorch 2.0.0, which printed a warning after every epoch.",
        "Issue_preprocessed_content":"Title: warning when training ; Content: printed after every epoch!"
    },
    {
        "Issue_link":"https:\/\/github.com\/mlf-core\/mlf-core\/issues\/171",
        "Issue_title":"subprocess.call and mlflow.log_artifact checks inconsistent in linter",
        "Issue_creation_time":1608150046000,
        "Issue_closed_time":1613430703000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n* ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',`\r\n* ` f'mlflow.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'`\r\n\r\nThose two linting functions caused the template create WFs (and sometimes even local) to fail\r\n\r\n\r\n**Expected behavior**\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nThey should pass. We should discuss why they fail and how to fix!\r\nSo currently they are outcommented!\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: subprocess.call and .log_artifact checks inconsistent in linter; Content: **describe the bug** * ` f'subprocess.call([\\'conda\\', \\'env\\', \\'export\\', \\'--name\\', \\'{self.project_slug_no_hyphen}\\'], stdout=conda_env_filehandler)',` * ` f'.log_artifact(f\\'{{reports_output_dir}}\/{self.project_slug_no_hyphen}_conda_environment.yml\\', artifact_path=\\'reports\\')'` those two linting functions caused the template create wfs (and sometimes even local) to fail **expected behavior** they should pass. we should discuss why they fail and how to fix! so currently they are outcommented!",
        "Issue_original_content_gpt_summary":"The user encountered a bug where two linting functions, `subprocess.call` and `.log_artifact`, caused the template create WFS (and sometimes even local) to fail, instead of passing as expected.",
        "Issue_preprocessed_content":"Title: and checks inconsistent in linter; Content: describe the bug a clear and concise description of what the bug is. those two linting functions caused the template create wfs to fail expected behavior a clear and concise description of what you expected to happen. they should pass. we should discuss why they fail and how to fix! so currently they are outcommented!"
    },
    {
        "Issue_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/548",
        "Issue_title":"MLFlow Error 409 when deploying --assets-only",
        "Issue_creation_time":1667484803000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"## Expected Behavior\r\nDeploy Jobs with --assets-only option\r\n## Current Behavior\r\nMLFlow API Request 409 Conflict \r\n## Steps to Reproduce (for bugs)\r\n[dbx][2022-11-03 12:30:40.370] \ud83d\udd0e Deployment file is not provided, searching in the conf directory\r\n[dbx][2022-11-03 12:30:40.375] \ud83d\udca1 Auto-discovery found deployment file conf\/deployment.json\r\n[dbx][2022-11-03 12:30:40.376] \ud83c\udd97 Deployment file conf\/deployment.json exists and will be used for deployment\r\n[dbx][2022-11-03 12:30:40.377] Starting new deployment for environment dev\r\n[dbx][2022-11-03 12:30:40.378] Using profile provided from the project file\r\n[dbx][2022-11-03 12:30:40.378] Found auth config from provider EnvironmentVariableConfigProvider, verifying it\r\n[dbx][2022-11-03 12:30:40.379] Found auth config from provider EnvironmentVariableConfigProvider, verification successful\r\n[dbx][2022-11-03 12:30:44.897] \r\n                Since v0.7.0 environment configurations should be nested under environments section.\r\n\r\n                Please nest environment configurations under this section to avoid potential issues while using \"build\"\r\n                configuration directive.\r\n            \r\n[dbx][2022-11-03 12:30:44.899] No build logic defined in the deployment file. Default pip-based build logic will be used.\r\n[dbx][2022-11-03 12:30:44.903] Usage of jobs keyword in deployment file is deprecated. Please use workflows instead (simply rename this section to workflows).\r\n[dbx][2022-11-03 12:30:44.906] Workflows ['add-on-chanel-AT', 'add-on-chanel-PL', 'add-on-PL'] were selected for further operations\r\n[dbx][2022-11-03 12:30:44.907] Following the provided build logic\r\n[dbx][2022-11-03 12:30:44.908] \ud83d\udc0d Building a Python-based project\r\n[dbx][2022-11-03 12:30:46.262] \u2705 Python-based project build finished\r\n[dbx][2022-11-03 12:30:46.264] Locating package file\r\n[dbx][2022-11-03 12:30:46.265] Package file located in: dist\/ds_recommenders-1.2.9-py3-none-any.whl\r\n[dbx][2022-11-03 12:30:47.221] Starting the traversal process\r\n[dbx][2022-11-03 12:30:47.222] Processing libraries for workflow add-on-chanel-AT\r\n[dbx][2022-11-03 12:30:47.223] \u2705 Processing libraries for workflow add-on-chanel-AT - done\r\n[dbx][2022-11-03 12:30:47.224] Processing libraries for workflow add-on-chanel-PL\r\n[dbx][2022-11-03 12:30:47.225] \u2705 Processing libraries for workflow add-on-chanel-PL - done\r\n[dbx][2022-11-03 12:30:47.225] Processing libraries for workflow add-on-PL\r\n[dbx][2022-11-03 12:30:47.226] \u2705 Processing libraries for workflow add-on-PL - done\r\n[dbx][2022-11-03 12:30:47.227] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.412] \u2705 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n[dbx][2022-11-03 12:30:50.414] \u2b06 Uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/comma \u2502\r\n\u2502 nds\/deploy.py:157 in deploy                                                  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, ele \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502   156 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 157 \u2502   \u2502   \u2502   adjuster.traverse(deployable_workflows)                    \u2502\r\n\u2502   158 \u2502   \u2502   \u2502   if not _assets_only:                                       \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   wf_manager = WorkflowDeploymentManager(api_client, dep \u2502\r\n\u2502   [16](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:17)0 \u2502   \u2502   \u2502   \u2502   wf_manager.apply()                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:185 in traverse                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   182 \u2502   def traverse(self, workflows: Union[WorkflowList, List[str]]):     \u2502\r\n\u2502   183 \u2502   \u2502   dbx_echo(\"Starting the traversal process\")                     \u2502\r\n\u2502   184 \u2502   \u2502   self.property_adjuster.library_traverse(workflows, self.additi \u2502\r\n\u2502 \u2771 185 \u2502   \u2502   self.property_adjuster.file_traverse(workflows, self.file_adju \u2502\r\n\u2502   186 \u2502   \u2502   self.property_adjuster.property_traverse(workflows)            \u2502\r\n\u2502   187 \u2502   \u2502   self.property_adjuster.cluster_policy_traverse(workflows)      \u2502\r\n\u2502   188 \u2502   \u2502   dbx_echo(\"Traversal process finished, all provided references  \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/adjuster.py:168 in file_traverse                                     \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   165 \u2502   \u2502   for element, parent, index in self.traverse(workflows):        \u2502\r\n\u2502   166 \u2502   \u2502   \u2502   if isinstance(element, str):                               \u2502\r\n\u2502   167 \u2502   \u2502   \u2502   \u2502   if element.startswith(\"file:\/\/\") or element.startswith \u2502\r\n\u2502 \u2771 168 \u2502   \u2502   \u2502   \u2502   \u2502   file_adjuster.adjust_file_ref(element, parent, ind \u2502\r\n\u2502   169                                                                        \u2502\r\n\u2502   [17](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:18)0                                                                        \u2502\r\n\u2502   171 class Adjuster:                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a \u2502\r\n\u2502 djuster\/mixins\/file_reference.py:12 in adjust_file_ref                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    9 \u2502   \u2502   self._uploader = file_uploader                                  \u2502\r\n\u2502   10 \u2502                                                                       \u2502\r\n\u2502   11 \u2502   def adjust_file_ref(self, element: str, parent: Any, index: Any):   \u2502\r\n\u2502 \u2771 12 \u2502   \u2502   _uploaded = self._uploader.upload_and_provide_path(element)     \u2502\r\n\u2502   13 \u2502   \u2502   self.set_element_at_parent(_uploaded, parent, index)            \u2502\r\n\u2502   14                                                                         \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/utils \u2502\r\n\u2502 \/file_uploader.py:59 in upload_and_provide_path                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   56 \u2502   \u2502   \u2502   self._verify_fuse_support()                                 \u2502\r\n\u2502   57 \u2502   \u2502                                                                   \u2502\r\n\u2502   58 \u2502   \u2502   dbx_echo(f\":arrow_up: Uploading local file {local_file_path}\")  \u2502\r\n\u2502 \u2771 59 \u2502   \u2502   self._upload_file(local_file_path)                              \u2502\r\n\u2502   60 \u2502   \u2502   dbx_echo(f\":white_check_mark: Uploading local file {local_file_ \u2502\r\n\u2502   61 \u2502   \u2502   return self._postprocess_path(local_file_path, as_fuse)         \u2502\r\n\u2502   62                                                                         \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[19](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:20)9 in http_request_safe                                   \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   196 \u2502   Wrapper around ``http_request`` that also verifies that the reques \u2502\r\n\u2502   197 \u2502   \"\"\"                                                                \u2502\r\n\u2502   198 \u2502   response = http_request(host_creds=host_creds, endpoint=endpoint,  \u2502\r\n\u2502 \u2771 199 \u2502   return verify_rest_response(response, endpoint)                    \u2502\r\n\u2502   [20](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:21)0                                                                        \u2502\r\n\u2502   201                                                                        \u2502\r\n\u2502   202 def verify_rest_response(response, endpoint):                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/opt\/hostedtoolcache\/Python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/mlflow\/ut \u2502\r\n\u2502 ils\/rest_utils.py:[21](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:22)2 in verify_rest_response                                \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   209 \u2502   \u2502   \u2502   \u2502   endpoint,                                              \u2502\r\n\u2502   210 \u2502   \u2502   \u2502   \u2502   response.status_code,                                  \u2502\r\n\u2502   211 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2502 \u2771 212 \u2502   \u2502   \u2502   raise MlflowException(                                     \u2502\r\n\u2502   213 \u2502   \u2502   \u2502   \u2502   \"%s. Response body: '%s'\" % (base_msg, response.text), \u2502\r\n\u2502   214 \u2502   \u2502   \u2502   \u2502   error_code=get_error_code(response.status_code),       \u2502\r\n\u2502   215 \u2502   \u2502   \u2502   )                                                          \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nMlflowException: API request to endpoint \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f9[22](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:23)5de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n failed with error code 409 != 200. Response body: '<html>\r\n<head>\r\n<meta http-equiv=\"Content-Type\" content=\"text\/html;charset=ISO-8859-1\"\/>\r\n<title>Error 409 <\/title>\r\n<\/head>\r\n<body>\r\n<h2>HTTP ERROR: 409<\/h2>\r\n<p>Problem accessing \r\n\/dbfs\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742\r\n8b97e6371f92[25](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:26)de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.py\r\n. Reason:\r\n<pre>    File already exists, cannot overwrite: \r\n&apos;\/Shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c908874\r\n[28](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:29)b97e6[37](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:38)1f[92](https:\/\/github.com\/Flaconi\/DS_Recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:93)25de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_AT.p\r\ny&apos;<\/pre><\/p>\r\n<hr \/>\r\n<\/body>\r\n<\/html>\r\n'\r\nError: Process completed with exit code 1.\r\n\r\n## Context\r\nUpdated few jobs today using the latest dbx version, and at the jobless deployment cicd step I get the error above.\r\nMLFlow is only used to define a specific experiment path. No path related updates or changes here!\r\n## Your Environment\r\n\r\n* dbx version used: 0.8.x\r\n* Databricks Runtime version:  10.4 LTS (standard or ML)\r\n* Python version: 3.8.11",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: error 409 when deploying --assets-only; Content: ## expected behavior deploy jobs with --assets-only option ## current behavior api request 409 conflict ## steps to reproduce (for bugs) [dbx][2022-11-03 12:30:40.370] deployment file is not provided, searching in the conf directory [dbx][2022-11-03 12:30:40.375] auto-discovery found deployment file conf\/deployment.json [dbx][2022-11-03 12:30:40.376] deployment file conf\/deployment.json exists and will be used for deployment [dbx][2022-11-03 12:30:40.377] starting new deployment for environment dev [dbx][2022-11-03 12:30:40.378] using profile provided from the project file [dbx][2022-11-03 12:30:40.378] found auth config from provider environmentvariableconfigprovider, verifying it [dbx][2022-11-03 12:30:40.379] found auth config from provider environmentvariableconfigprovider, verification successful [dbx][2022-11-03 12:30:44.897] since v0.7.0 environment configurations should be nested under environments section. please nest environment configurations under this section to avoid potential issues while using \"build\" configuration directive. [dbx][2022-11-03 12:30:44.899] no build logic defined in the deployment file. default pip-based build logic will be used. [dbx][2022-11-03 12:30:44.903] usage of jobs keyword in deployment file is deprecated. please use workflows instead (simply rename this section to workflows). [dbx][2022-11-03 12:30:44.906] workflows ['add-on-chanel-at', 'add-on-chanel-pl', 'add-on-pl'] were selected for further operations [dbx][2022-11-03 12:30:44.907] following the provided build logic [dbx][2022-11-03 12:30:44.908] building a python-based project [dbx][2022-11-03 12:30:46.262] python-based project build finished [dbx][2022-11-03 12:30:46.264] locating package file [dbx][2022-11-03 12:30:46.265] package file located in: dist\/ds_recommenders-1.2.9-py3-none-any.whl [dbx][2022-11-03 12:30:47.221] starting the traversal process [dbx][2022-11-03 12:30:47.222] processing libraries for workflow add-on-chanel-at [dbx][2022-11-03 12:30:47.223] processing libraries for workflow add-on-chanel-at - done [dbx][2022-11-03 12:30:47.224] processing libraries for workflow add-on-chanel-pl [dbx][2022-11-03 12:30:47.225] processing libraries for workflow add-on-chanel-pl - done [dbx][2022-11-03 12:30:47.225] processing libraries for workflow add-on-pl [dbx][2022-11-03 12:30:47.226] processing libraries for workflow add-on-pl - done [dbx][2022-11-03 12:30:47.227] uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py [dbx][2022-11-03 12:30:50.412] uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py [dbx][2022-11-03 12:30:50.414] uploading local file src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py traceback (most recent call last) \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/comma nds\/deploy.py:157 in deploy 154 wf_manager = workflowdeploymentmanager(api_client, ele 155 wf_manager.apply() 156 else: 157 adjuster.traverse(deployable_workflows) 158 if not _assets_only: 159 wf_manager = workflowdeploymentmanager(api_client, dep [16](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:17)0 wf_manager.apply() \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a djuster\/adjuster.py:185 in traverse 182 def traverse(self, workflows: union[workflowlist, list[str]]): 183 dbx_echo(\"starting the traversal process\") 184 self.property_adjuster.library_traverse(workflows, self.additi 185 self.property_adjuster.file_traverse(workflows, self.file_adju 186 self.property_adjuster.property_traverse(workflows) 187 self.property_adjuster.cluster_policy_traverse(workflows) 188 dbx_echo(\"traversal process finished, all provided references \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a djuster\/adjuster.py:168 in file_traverse 165 for element, parent, index in self.traverse(workflows): 166 if isinstance(element, str): 167 if element.startswith(\"file:\/\/\") or element.startswith 168 file_adjuster.adjust_file_ref(element, parent, ind 169 [17](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:18)0 171 class adjuster: \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/api\/a djuster\/mixins\/file_reference.py:12 in adjust_file_ref 9 self._uploader = file_uploader 10 11 def adjust_file_ref(self, element: str, parent: any, index: any): 12 _uploaded = self._uploader.upload_and_provide_path(element) 13 self.set_element_at_parent(_uploaded, parent, index) 14 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/dbx\/utils \/file_uploader.py:59 in upload_and_provide_path 56 self._verify_fuse_support() 57 58 dbx_echo(f\":arrow_up: uploading local file {local_file_path}\") 59 self._upload_file(local_file_path) 60 dbx_echo(f\":white_check_mark: uploading local file {local_file_ 61 return self._postprocess_path(local_file_path, as_fuse) 62 \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/\/ut ils\/rest_utils.py:[19](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:20)9 in http_request_safe 196 wrapper around ``http_request`` that also verifies that the reques 197 \"\"\" 198 response = http_request(host_creds=host_creds, endpoint=endpoint, 199 return verify_rest_response(response, endpoint) [20](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:21)0 201 202 def verify_rest_response(response, endpoint): \/opt\/hostedtoolcache\/python\/3.8.11\/x64\/lib\/python3.8\/site-packages\/\/ut ils\/rest_utils.py:[21](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:22)2 in verify_rest_response 209 endpoint, 210 response.status_code, 211 ) 212 raise exception( 213 \"%s. response body: '%s'\" % (base_msg, response.text), 214 error_code=get_error_code(response.status_code), 215 ) exception: api request to endpoint \/dbfs\/shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742 8b97e6371f9[22](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:23)5de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py failed with error code 409 != 200. response body: ' error 409 http error: 409 problem accessing \/dbfs\/shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c9088742 8b97e6371f92[25](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:26)de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.py . reason: file already exists, cannot overwrite: '\/shared\/ds_recommenders\/projects\/ds_recommenders_experiments\/8cfd06c908874 [28](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:29)b97e6[37](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:38)1f[92](https:\/\/github.com\/flaconi\/ds_recommenders\/actions\/runs\/3385675925\/jobs\/5624102075#step:11:93)25de5a\/artifacts\/src\/jobs\/add_on_products\/add_on_chanel\/chanel_at.p y' ' error: process completed with exit code 1. ## context updated few jobs today using the latest dbx version, and at the jobless deployment cicd step i get the error above. is only used to define a specific experiment path. no path related updates or changes here! ## your environment * dbx version used: 0.8.x * databricks runtime version: 10.4 lts (standard or ml) * python version: 3.8.11",
        "Issue_original_content_gpt_summary":"The user encountered an error 409 conflict when deploying with the --assets-only option, due to a file already existing in the specified path.",
        "Issue_preprocessed_content":"Title: error when deploying ; Content: expected behavior deploy jobs with option current behavior api request conflict steps to reproduce deployment file is not provided, searching in the conf directory found deployment file deployment file exists and will be used for deployment starting new deployment for environment dev using profile provided from the project file found auth config from provider environmentvariableconfigprovider, verifying it found auth config from provider environmentvariableconfigprovider, verification successful since environment configurations should be nested under environments section. please nest environment configurations under this section to avoid potential issues while using build configuration directive. no build logic defined in the deployment file. default build logic will be used. usage of jobs keyword in deployment file is deprecated. please use workflows instead . workflows were selected for further operations following the provided build logic building a project project build finished locating package file package file located in starting the traversal process processing libraries for workflow processing libraries for workflow done processing libraries for workflow processing libraries for workflow done processing libraries for workflow processing libraries for workflow done uploading local file uploading local file uploading local file traceback in deploy ele else if not dep in traverse def traverse the traversal process process finished, all provided references in for element, parent, index in if isinstance if or parent, ind class adjuster in def element str, parent any, index any parent, index in uploading local file uploading local file return in wrapper around that also verifies that the reques response endpoint endpoint, return endpoint def endpoint in endpoint, raise exception , exception api request to endpoint failed with error code ! . response body ' head meta title error body h http error p problem accessing . reason pre file already exists, cannot overwrite hr ' error process completed with exit code . context updated few jobs today using the latest dbx version, and at the jobless deployment cicd step i get the error above. is only used to define a specific experiment path. no path related updates or changes here! your environment dbx version used databricks runtime version lts python version"
    },
    {
        "Issue_link":"https:\/\/github.com\/databrickslabs\/dbx\/issues\/385",
        "Issue_title":"dbx deploy fails due to mlflow experiment not found",
        "Issue_creation_time":1660398516000,
        "Issue_closed_time":1661539227000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"## Expected Behavior\r\n`dbx deploy --environment=default` succeeds\r\n\r\n## Current Behavior\r\nThe command returns \r\n`mlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Experiment with id '2170254243754186' does not exist.`\r\n\r\n## Steps to Reproduce (for bugs)\r\nFollow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx\r\n\r\n## Context\r\nTrying to set up dbx for the first time.\r\n\r\n## Your Environment\r\nmac os m1 2021 with macos Monterey 12.5\r\n\r\n* dbx version used: DataBricks eXtensions aka dbx, version ~> 0.6.11\r\n* Databricks Runtime version: Version 0.17.1",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: dbx deploy fails due to experiment not found; Content: ## expected behavior `dbx deploy --environment=default` succeeds ## current behavior the command returns `.exceptions.restexception: invalid_parameter_value: experiment with id '2170254243754186' does not exist.` ## steps to reproduce (for bugs) follow the instructions at https:\/\/docs.gcp.databricks.com\/dev-tools\/ide-how-to.html#run-with-dbx ## context trying to set up dbx for the first time. ## your environment mac os m1 2021 with macos monterey 12.5 * dbx version used: databricks extensions aka dbx, version ~> 0.6.11 * databricks runtime version: version 0.17.1",
        "Issue_original_content_gpt_summary":"The user encountered an issue when trying to deploy with dbx, where the command returned an error stating that the experiment with the specified ID did not exist.",
        "Issue_preprocessed_content":"Title: dbx deploy fails due to experiment not found; Content: expected behavior succeeds current behavior the command returns steps to reproduce follow the instructions at context trying to set up dbx for the first time. your environment mac os m with macos monterey dbx version used databricks extensions aka dbx, version databricks runtime version version"
    },
    {
        "Issue_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/576",
        "Issue_title":"[BUG]: Helm fetch command for ai-engine,sdk-helper and mlflow includes the 22.09 release instead of 22.11",
        "Issue_creation_time":1671535506000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Version\r\n\r\n22.11\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\n_No response_\r\n\r\n### Describe the bug.\r\n\r\nai-engine fetch command at the 22.11 guide:\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-ai-engine-**22.09**.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-sdk-client-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\nhelm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-mlflow-22.09.tgz --username='$oauthtoken' --password=$API_KEY --untar\r\n\r\n### Minimum reproducible example\r\n\r\n_No response_\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Full env printout\r\n\r\n_No response_\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: helm fetch command for ai-engine,sdk-helper and includes the 22.09 release instead of 22.11; Content: ### version 22.11 ### which installation method(s) does this occur on? _no response_ ### describe the bug. ai-engine fetch command at the 22.11 guide: helm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-ai-engine-**22.09**.tgz --username='$oauthtoken' --password=$api_key --untar helm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus-sdk-client-22.09.tgz --username='$oauthtoken' --password=$api_key --untar helm fetch https:\/\/helm.ngc.nvidia.com\/nvidia\/morpheus\/charts\/morpheus--22.09.tgz --username='$oauthtoken' --password=$api_key --untar ### minimum reproducible example _no response_ ### relevant log output _no response_ ### full env printout _no response_ ### other\/misc. _no response_ ### code of conduct - [x] i agree to follow morpheus' code of conduct - [x] i have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/morpheus\/issues?q=is%3aopen+is%3aissue+label%3abug) and have found no duplicates for this bug report",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the helm fetch command for ai-engine, sdk-helper and includes the 22.09 release instead of 22.11.",
        "Issue_preprocessed_content":"Title: helm fetch command for and includes the release instead of ; Content: version . which installation method does this occur on? describe the bug. fetch command at the guide helm fetch helm fetch helm fetch minimum reproducible example relevant log output full env printout code of conduct i agree to follow morpheus' code of conduct i have searched the and have found no duplicates for this bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/512",
        "Issue_title":"[BUG]: Unable to Start DFP Production MLFlow Server",
        "Issue_creation_time":1669916494000,
        "Issue_closed_time":1669922495000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### Version\r\n\r\n23.01\r\n\r\n### Which installation method(s) does this occur on?\r\n\r\nDocker\r\n\r\n### Describe the bug.\r\n\r\nUnable to start the mlflow server when using `branch-22.11` but it works fine with `branch-22.09`\r\n\r\nDowngrading  mlflow version to `<1.29.0` works fine.\r\n\r\n\r\n### Minimum reproducible example\r\n\r\n```shell\r\n$ cd ~\/Morpheus\/examples\/digital_fingerprinting\/production\r\n\r\n$ docker-compose up mlflow\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n[+] Running 3\/3                                                                                                                                               \r\n \u283f Network production_backend      Created                                                                                                               0.0s \r\n \u283f Network production_frontend     Created                                                                                                               0.0s\r\n \u283f Container mlflow_server  Created                                                                                                               0.1s\r\nAttaching to mlflow_server\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Error initializing backend store\r\nmlflow_server  | 2022\/12\/01 17:30:28 ERROR mlflow.cli: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server  | Traceback (most recent call last):\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 392, in server\r\nmlflow_server  |     initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 265, in initialize_backend_stores\r\nmlflow_server  |     _get_tracking_store(backend_store_uri, default_artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 244, in _get_tracking_store\r\nmlflow_server  |     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 39, in get_store\r\nmlflow_server  |     return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri\r\nmlflow_server  |     return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 112, in _get_sqlalchemy_store\r\nmlflow_server  |     return SqlAlchemyStore(store_uri, artifact_uri)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__\r\nmlflow_server  |     mlflow.store.db.utils._verify_schema(self.engine)\r\nmlflow_server  |   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 71, in _verify_schema\r\nmlflow_server  |     raise MlflowException(\r\nmlflow_server  | mlflow.exceptions.MlflowException: Detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\nmlflow_server exited with code 1\r\n```\r\n\r\n\r\n### Full env printout\r\n\r\n```shell\r\n<details><summary>Click here to see environment details<\/summary><pre>\r\n     \r\n     **git***\r\n     commit 9619c0e3a5ddbdd476aba9331f288aac855da7cd (HEAD -> dfp-pipeline-module, origin\/dfp-pipeline-module)\r\n     Author: bsuryadevara <bhargavsuryadevara@gmail.com>\r\n     Date:   Wed Nov 30 17:13:05 2022 -0600\r\n     \r\n     used dill to persist source and preprocess schema\r\n     **git submodules***\r\n     -27efc4fd1c984332920db2a2d6ab1f84d3cb55cd external\/morpheus-visualizations\r\n     \r\n     ***OS Information***\r\n     DGX_NAME=\"DGX Server\"\r\n     DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\r\n     DGX_SWBUILD_DATE=\"2020-03-04\"\r\n     DGX_SWBUILD_VERSION=\"4.4.0\"\r\n     DGX_COMMIT_ID=\"ee09ebc\"\r\n     DGX_PLATFORM=\"DGX Server for DGX-1\"\r\n     DGX_SERIAL_NUMBER=\"QTFCOU7140058-R1\"\r\n     DISTRIB_ID=Ubuntu\r\n     DISTRIB_RELEASE=18.04\r\n     DISTRIB_CODENAME=bionic\r\n     DISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"\r\n     NAME=\"Ubuntu\"\r\n     VERSION=\"18.04.6 LTS (Bionic Beaver)\"\r\n     ID=ubuntu\r\n     ID_LIKE=debian\r\n     PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\r\n     VERSION_ID=\"18.04\"\r\n     HOME_URL=\"https:\/\/www.ubuntu.com\/\"\r\n     SUPPORT_URL=\"https:\/\/help.ubuntu.com\/\"\r\n     BUG_REPORT_URL=\"https:\/\/bugs.launchpad.net\/ubuntu\/\"\r\n     PRIVACY_POLICY_URL=\"https:\/\/www.ubuntu.com\/legal\/terms-and-policies\/privacy-policy\"\r\n     VERSION_CODENAME=bionic\r\n     UBUNTU_CODENAME=bionic\r\n     Linux dgx04 4.15.0-162-generic #170-Ubuntu SMP Mon Oct 18 11:38:05 UTC 2021 x86_64 x86_64 x86_64 GNU\/Linux\r\n     \r\n     ***GPU Information***\r\n     Thu Dec  1 17:37:05 2022\r\n     +-----------------------------------------------------------------------------+\r\n     | NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n     |-------------------------------+----------------------+----------------------+\r\n     | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n     | Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n     |                               |                      |               MIG M. |\r\n     |===============================+======================+======================|\r\n     |   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    56W \/ 300W |  11763MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    43W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\r\n     | N\/A   28C    P0    41W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\r\n     | N\/A   29C    P0    44W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\r\n     | N\/A   31C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n     | N\/A   32C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     |   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\r\n     | N\/A   30C    P0    42W \/ 300W |      3MiB \/ 32510MiB |      0%      Default |\r\n     |                               |                      |                  N\/A |\r\n     +-------------------------------+----------------------+----------------------+\r\n     \r\n     +-----------------------------------------------------------------------------+\r\n     | Processes:                                                                  |\r\n     |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n     |        ID   ID                                                   Usage      |\r\n     |=============================================================================|\r\n     |    0   N\/A  N\/A     31232      C   ...da\/envs\/rapids\/bin\/python      303MiB |\r\n     |    0   N\/A  N\/A     41206      C   ...da\/envs\/rapids\/bin\/python     7051MiB |\r\n     |    0   N\/A  N\/A     52497      C   ...nda3\/envs\/venv\/bin\/python     3137MiB |\r\n     |    0   N\/A  N\/A     55973      C   tritonserver                     1267MiB |\r\n     +-----------------------------------------------------------------------------+\r\n     \r\n     ***CPU***\r\n     Architecture:        x86_64\r\n     CPU op-mode(s):      32-bit, 64-bit\r\n     Byte Order:          Little Endian\r\n     CPU(s):              80\r\n     On-line CPU(s) list: 0-79\r\n     Thread(s) per core:  2\r\n     Core(s) per socket:  20\r\n     Socket(s):           2\r\n     NUMA node(s):        2\r\n     Vendor ID:           GenuineIntel\r\n     CPU family:          6\r\n     Model:               79\r\n     Model name:          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\r\n     Stepping:            1\r\n     CPU MHz:             3267.078\r\n     CPU max MHz:         3600.0000\r\n     CPU min MHz:         1200.0000\r\n     BogoMIPS:            4390.17\r\n     Virtualization:      VT-x\r\n     L1d cache:           32K\r\n     L1i cache:           32K\r\n     L2 cache:            256K\r\n     L3 cache:            51200K\r\n     NUMA node0 CPU(s):   0-19,40-59\r\n     NUMA node1 CPU(s):   20-39,60-79\r\n     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\n     \r\n     ***CMake***\r\n     \/usr\/bin\/cmake\r\n     cmake version 3.10.2\r\n     \r\n     CMake suite maintained and supported by Kitware (kitware.com\/cmake).\r\n     \r\n     ***g++***\r\n     \/usr\/bin\/g++\r\n     g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n     Copyright (C) 2017 Free Software Foundation, Inc.\r\n     This is free software; see the source for copying conditions.  There is NO\r\n     warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n     \r\n     \r\n     ***nvcc***\r\n     \/usr\/local\/cuda\/bin\/nvcc\r\n     nvcc: NVIDIA (R) Cuda compiler driver\r\n     Copyright (c) 2005-2021 NVIDIA Corporation\r\n     Built on Thu_Nov_18_09:45:30_PST_2021\r\n     Cuda compilation tools, release 11.5, V11.5.119\r\n     Build cuda_11.5.r11.5\/compiler.30672275_0\r\n     \r\n     ***Python***\r\n     \/usr\/bin\/python\r\n     Python 2.7.17\r\n     \r\n     ***Environment Variables***\r\n     PATH                            : \/usr\/local\/cuda\/bin:\/opt\/bin\/:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/snap\/bin:\/home\/nfs\/bsuryadevara:\/home\/nfs\/bsuryadevara\r\n     LD_LIBRARY_PATH                 :\r\n     NUMBAPRO_NVVM                   :\r\n     NUMBAPRO_LIBDEVICE              :\r\n     CONDA_PREFIX                    :\r\n     PYTHON_PATH                     :\r\n     \r\n     conda not found\r\n     ***pip packages***\r\n     \/usr\/bin\/pip\r\n\/usr\/lib\/python2.7\/dist-packages\/OpenSSL\/crypto.py:12: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in the next release.\r\n  from cryptography import x509\r\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\r\n     ansible (2.9.9)\r\n     asn1crypto (0.24.0)\r\n     backports.functools-lru-cache (1.6.4)\r\n     backports.shutil-get-terminal-size (1.0.0)\r\n     bcrypt (3.1.7)\r\n     beautifulsoup4 (4.9.3)\r\n     boto3 (1.17.112)\r\n     botocore (1.20.112)\r\n     bs4 (0.0.1)\r\n     certifi (2018.1.18)\r\n     cffi (1.11.5)\r\n     chardet (3.0.4)\r\n     click (7.1.2)\r\n     configparser (4.0.2)\r\n     contextlib2 (0.6.0.post1)\r\n     cryptography (3.3.2)\r\n     decorator (4.1.2)\r\n     defusedxml (0.6.0)\r\n     distro (1.6.0)\r\n     dnspython (1.15.0)\r\n     docker (4.4.4)\r\n     docopt (0.6.2)\r\n     enum34 (1.1.10)\r\n     fastrlock (0.8)\r\n     flake8 (3.9.2)\r\n     functools32 (3.2.3.post2)\r\n     futures (3.3.0)\r\n     gssapi (1.4.1)\r\n     gyp (0.1)\r\n     html-to-json (2.0.0)\r\n     html2text (2019.8.11)\r\n     html5lib (0.999999999)\r\n     http (0.2)\r\n     httplib2 (0.14.0)\r\n     httpserver (1.1.0)\r\n     idna (2.6)\r\n     importlib-metadata (2.1.3)\r\n     ipaclient (4.6.90rc1+git20180411)\r\n     ipaddress (1.0.17)\r\n     ipalib (4.6.90rc1+git20180411)\r\n     ipaplatform (4.6.90rc1+git20180411)\r\n     ipapython (4.6.90rc1+git20180411)\r\n     Jinja2 (2.10)\r\n     jmespath (0.10.0)\r\n     lxml (4.2.1)\r\n     MarkupSafe (1.0)\r\n     mccabe (0.6.1)\r\n     netaddr (0.7.19)\r\n     netifaces (0.10.4)\r\n     numpy (1.16.6)\r\n     ofed-le-utils (1.0.3)\r\n     olefile (0.45.1)\r\n     pandas (0.24.2)\r\n     paramiko (2.11.0)\r\n     pathlib2 (2.3.7.post1)\r\n     Pillow (5.1.0)\r\n     pip (9.0.1)\r\n     ply (3.11)\r\n     pyasn1 (0.4.2)\r\n     pyasn1-modules (0.2.1)\r\n     pycodestyle (2.7.0)\r\n     pycparser (2.18)\r\n     pycrypto (2.6.1)\r\n     pyflakes (2.3.1)\r\n     pygobject (3.26.1)\r\n     PyNaCl (1.4.0)\r\n     pyOpenSSL (17.5.0)\r\n     python-apt (1.6.5+ubuntu0.7)\r\n     python-augeas (0.5.0)\r\n     python-dateutil (2.8.2)\r\n     python-dotenv (0.18.0)\r\n     python-ldap (3.0.0)\r\n     python-yubico (1.3.2)\r\n     pytz (2022.4)\r\n     pyusb (1.0.0)\r\n     PyYAML (5.4.1)\r\n     qrcode (5.3)\r\n     requests (2.27.1)\r\n     s3fs (0.2.2)\r\n     s3transfer (0.4.2)\r\n     scandir (1.10.0)\r\n     setuptools (39.0.1)\r\n     six (1.16.0)\r\n     soupsieve (1.9.6)\r\n     splunk-sdk (1.7.2)\r\n     subprocess32 (3.5.4)\r\n     tqdm (4.60.0)\r\n     typing (3.10.0.0)\r\n     urllib3 (1.26.12)\r\n     webencodings (0.5)\r\n     yapf (0.32.0)\r\n     zipp (1.2.0)\r\n     \r\n<\/pre><\/details>\r\n```\r\n\r\n\r\n### Other\/Misc.\r\n\r\n_No response_\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow Morpheus' Code of Conduct\r\n- [X] I have searched the [open bugs](https:\/\/github.com\/nv-morpheus\/Morpheus\/issues?q=is%3Aopen+is%3Aissue+label%3Abug) and have found no duplicates for this bug report",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: unable to start dfp production server; Content: ### describe the bug. unable to start the server when using `branch-22.11` but it works fine with `branch-22.09` downgrading version to `<1.29.0` works fine. ### minimum reproducible example ```shell $ cd ~\/morpheus\/examples\/digital_fingerprinting\/production $ docker-compose up ``` ### relevant log output ```shell [+] running 3\/3 network production_backend created 0.0s network production_frontend created 0.0s container _server created 0.1s attaching to _server _server | 2022\/12\/01 17:30:28 error .cli: error initializing backend store _server | 2022\/12\/01 17:30:28 error .cli: detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). take a backup of your database, then run ' db upgrade ' to migrate your database to the latest schema. note: schema migration may result in database downtime - please consult your database's documentation for more detail. _server | traceback (most recent call last): _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/cli.py\", line 392, in server _server | initialize_backend_stores(backend_store_uri, registry_store_uri, default_artifact_root) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/server\/handlers.py\", line 265, in initialize_backend_stores _server | _get_tracking_store(backend_store_uri, default_artifact_root) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/server\/handlers.py\", line 244, in _get_tracking_store _server | _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/registry.py\", line 39, in get_store _server | return self._get_store_with_resolved_uri(resolved_store_uri, artifact_uri) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/registry.py\", line 49, in _get_store_with_resolved_uri _server | return builder(store_uri=resolved_store_uri, artifact_uri=artifact_uri) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/server\/handlers.py\", line 112, in _get_sqlalchemy_store _server | return sqlalchemystore(store_uri, artifact_uri) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/store\/tracking\/sqlalchemy_store.py\", line 150, in __init__ _server | .store.db.utils._verify_schema(self.engine) _server | file \"\/usr\/local\/lib\/python3.8\/site-packages\/\/store\/db\/utils.py\", line 71, in _verify_schema _server | raise exception( _server | .exceptions.exception: detected out-of-date database schema (found version cc1f77228345, but expected 97727af70f4d). take a backup of your database, then run ' db upgrade ' to migrate your database to the latest schema. note: schema migration may result in database downtime - please consult your database's documentation for more detail. _server exited with code 1 ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to start the Digital Fingerprinting Production Server, as the database schema was out-of-date and needed to be migrated in order to work properly.",
        "Issue_preprocessed_content":"Title: unable to start dfp production server; Content: version . which installation method does this occur on? docker describe the bug. unable to start the server when using but it works fine with downgrading version to works fine. minimum reproducible example relevant log output full env printout code of conduct i agree to follow morpheus' code of conduct i have searched the and have found no duplicates for this bug report"
    },
    {
        "Issue_link":"https:\/\/github.com\/nv-morpheus\/Morpheus\/issues\/125",
        "Issue_title":"[BUG] mlflow deployments create can fail (k8s\/Helm)",
        "Issue_creation_time":1653424629000,
        "Issue_closed_time":1654018977000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nFor some reason, `mlflow deployment create ...` can fail unexpectedly. \r\n\r\n```\r\nmlflow deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -C \"version=1\"\r\nCopied \/mlflow\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx\r\nSaved mlflow-meta.json to \/common\/triton-model-repo\/sid-minibert-onnx\r\nTraceback (most recent call last):\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow_triton\/deployments.py\", line 109, in create_deployment\r\n    self.triton_client.load_model(name)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model\r\n    _raise_if_error(response)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error\r\n    raise error\r\ntritonclient.utils.InferenceServerException: failed to load 'sid-minibert-onnx', no version is available\r\n```\r\n\r\nFix is to delete the mlflow pod and start over.\r\n\r\n**Steps\/Code to reproduce bug**\r\nFollow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Expected behavior**\r\nSuccessful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment\r\n\r\n**Environment overview (please complete the following information)**\r\n - Environment location: LaunchPad\r\n - Method of Morpheus install: Kubernetes\r\n\r\n**Environment details**\r\nLaunchPad Helm deployment on A30. Unfortunately, unable to capture the print_env.sh output from ipykernel there.\r\n\r\n**Additional context**\r\nMLflow sqlite db likely gets corrupted or otherwise \"confused\". Possibly an issue in tritonclient?\r\nTriton logging complains about unable to read config.pbtxt\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] deployments create can fail (k8s\/helm); Content: **describe the bug** for some reason, ` deployment create ...` can fail unexpectedly. ``` deployments create -t triton --flavor triton --name sid-minibert-onnx -m models:\/sid-minibert-onnx\/1 -c \"version=1\" copied \/\/artifacts\/0\/41f4069628e5429eb5c75728486a247a\/artifacts\/triton\/sid-minibert-onnx to \/common\/triton-model-repo\/sid-minibert-onnx saved -meta.json to \/common\/triton-model-repo\/sid-minibert-onnx traceback (most recent call last): file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/_triton\/deployments.py\", line 109, in create_deployment self.triton_client.load_model(name) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 622, in load_model _raise_if_error(response) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/tritonclient\/http\/__init__.py\", line 64, in _raise_if_error raise error tritonclient.utils.inferenceserverexception: failed to load 'sid-minibert-onnx', no version is available ``` fix is to delete the pod and start over. **steps\/code to reproduce bug** follow steps in docs\/source\/morpheus_quickstart_guide.md#model-deployment **expected behavior** successful deployment as described at docs\/source\/morpheus_quickstart_guide.md#model-deployment **environment overview (please complete the following information)** - environment location: launchpad - method of morpheus install: kubernetes **environment details** launchpad helm deployment on a30. unfortunately, unable to capture the print_env.sh output from ipykernel there. **additional context** sqlite db likely gets corrupted or otherwise \"confused\". possibly an issue in tritonclient? triton logging complains about unable to read config.pbtxt",
        "Issue_original_content_gpt_summary":"The user encountered a bug where deployments created with k8s\/helm can fail unexpectedly, requiring the user to delete the pod and start over.",
        "Issue_preprocessed_content":"Title: deployments create can fail ; Content: describe the bug for some reason, can fail unexpectedly. fix is to delete the pod and start over. to reproduce bug follow steps in expected behavior successful deployment as described at environment overview environment location launchpad method of morpheus install kubernetes environment details launchpad helm deployment on a . unfortunately, unable to capture the output from ipykernel there. additional context sqlite db likely gets corrupted or otherwise confused . possibly an issue in tritonclient? triton logging complains about unable to read"
    },
    {
        "Issue_link":"https:\/\/github.com\/equinor\/flownet\/issues\/408",
        "Issue_title":"MLFlow expecting mlruns folder",
        "Issue_creation_time":1621607539000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When running hyperparameter tuning, MLflow expects an mlruns folder - which we don't create. If we stick with the standard we can ommit having to run `mlflow ui` with the backend store argument.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: expecting s folder; Content: when running hyperparameter tuning, expects an s folder - which we don't create. if we stick with the standard we can ommit having to run ` ui` with the backend store argument.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running hyperparameter tuning, as it expects an s folder which was not created, requiring the user to run the 'ui' command with the backend store argument.",
        "Issue_preprocessed_content":"Title: expecting s folder; Content: when running hyperparameter tuning, expects an s folder which we don't create. if we stick with the standard we can ommit having to run with the backend store argument."
    },
    {
        "Issue_link":"https:\/\/github.com\/equinor\/flownet\/issues\/269",
        "Issue_title":"Failed ERT runs are not registered correctly in mlflow",
        "Issue_creation_time":1606471214000,
        "Issue_closed_time":1606475795000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"If an ERT subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. This will then lead to a \"successful\" run in mlflow, whereas it should be registered as a failed run.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: failed ert runs are not registered correctly in ; Content: if an ert subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than 0 is ignored. this will then lead to a \"successful\" run in , whereas it should be registered as a failed run.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where failed ERT runs were not being registered correctly in , leading to a \"successful\" run being registered when it should have been registered as a failed run.",
        "Issue_preprocessed_content":"Title: failed ert runs are not registered correctly in ; Content: if an ert subprocess has failed for any other reason than what is hard coded in the subprocess call, a returncode larger than is ignored. this will then lead to a successful run in , whereas it should be registered as a failed run."
    },
    {
        "Issue_link":"https:\/\/github.com\/NRCan\/geo-deep-learning\/issues\/440",
        "Issue_title":"Fix logging of parameters on Mlflow",
        "Issue_creation_time":1671594369000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Logging of parameters on Mlflow works as expected with default parameters set with Hydra; However hydra allows modification of parameters per experiment run, but modified parameters are not logged on Mlflow.  ",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: fix logging of parameters on ; logging of parameters on works as expected with default parameters set with hydra; Content: however hydra allows modification of parameters per experiment run, but modified parameters are not logged on .",
        "Issue_original_content_gpt_summary":"The user encountered challenges with logging of parameters on , as modified parameters were not logged when using Hydra to modify parameters per experiment run.",
        "Issue_preprocessed_content":"Title: fix logging of parameters on ; Content: logging of parameters on works as expected with default parameters set with hydra; however hydra allows modification of parameters per experiment run, but modified parameters are not logged on ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/362",
        "Issue_title":"MlflowArtifactDataset.load() fails if artifact_path is not None and run_id is specified",
        "Issue_creation_time":1664829089000,
        "Issue_closed_time":1665079955000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nWhen you try to specify an artifact path and a run_id in an ``MlflowArtifactDataSet``, you get an error. \r\n\r\nThis works:\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=None,\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\n\r\nwhile this :\r\n```python\r\nmlflow_csv_dataset = MlflowArtifactDataSet(\r\n    data_set=dict(type=CSVDataSet, filepath=\"path\/to\/df.csv\"),\r\n    artifact_path=\"folder\", # this is the difference\r\n    run_id=\"1234\",\r\n)\r\nmlflow_csv_dataset .load()\r\n```\r\nraises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: artifactdataset.load() fails if artifact_path is not none and run_id is specified; Content: ## description when you try to specify an artifact path and a run_id in an ``artifactdataset``, you get an error. this works: ```python _csv_dataset = artifactdataset( data_set=dict(type=csvdataset, filepath=\"path\/to\/df.csv\"), artifact_path=none, run_id=\"1234\", ) _csv_dataset .load() ``` while this : ```python _csv_dataset = artifactdataset( data_set=dict(type=csvdataset, filepath=\"path\/to\/df.csv\"), artifact_path=\"folder\", # this is the difference run_id=\"1234\", ) _csv_dataset .load() ``` raises the following error: ``unsupported operand type(s) for \/: 'str' and 'str'``:",
        "Issue_original_content_gpt_summary":"The user encountered an error when trying to specify an artifact path and a run_id in an ``artifactdataset``.",
        "Issue_preprocessed_content":"Title: fails if is not none and is specified; Content: description when you try to specify an artifact path and a in an , you get an error. this works while this raises the following error"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/361",
        "Issue_title":"kedro mlflow ui gets a FileNotFoundError",
        "Issue_creation_time":1664539296000,
        "Issue_closed_time":1664786016000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Firstly I'd like to apologize if this is a dummy question.\r\nI'm following the tutorial to get introduced to kedro mlflow,; after running the command \"kedro mlflow init\" I tried to run the command \"kedro mlflofw ui\" but I get an error:\r\n\r\nINFO     The 'mlflow_tracking_uri' key in mlflow.yml is relative ('server.mlflow_tracking_uri = mlruns'). It is converted to a valid uri: 'file:\/\/\/C:\/Users\/e107338\/PycharmProjects\/mlflow\/kedro-mlflow-example\/mlruns'                                                   kedro_mlflow_config.py:202\r\n\r\nAfter the Traceback I get an error: FileNotFoundErrror\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: ui gets a filenotfounderror; firstly i'd like to apologize if this is a dummy question. i'm following the tutorial to get introduced to ,; Content: after running the command \" init\" i tried to run the command \" mlflofw ui\" but i get an error: info the '_tracking_uri' key in .yml is relative ('server._tracking_uri = s'). it is converted to a valid uri: 'file:\/\/\/c:\/users\/e107338\/pycharmprojects\/\/--example\/s' __config.py:202 after the traceback i get an error: filenotfounderrror",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to run the command \" ui\" after running the command \" init\" while following a tutorial to get introduced to Kedro.",
        "Issue_preprocessed_content":"Title: ui gets a filenotfounderror; Content: firstly i'd like to apologize if this is a dummy question. i'm following the tutorial to get introduced to ,; after running the command init i tried to run the command mlflofw ui but i get an error info the key in is relative . it is converted to a valid uri after the traceback i get an error filenotfounderrror"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/346",
        "Issue_title":"MlflowMetricsDataSet logs invalid metric which breaks mlflow UI",
        "Issue_creation_time":1660281331000,
        "Issue_closed_time":1662408460000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"## Description\r\n\r\nThis happens when i tried to configure my own metric functions. \r\n\r\n## Context\r\n\r\nI am trying to create a custom metric indicator, to be logged after each experimentation. When i run `kedro mlflow ui`, this is what I'm getting on the UI.\r\n![image](https:\/\/user-images.githubusercontent.com\/54475793\/184276876-57872dd2-3fb3-41c9-b3a3-edd6a4396aca.png)\r\n\r\n\r\n## Steps to Reproduce\r\n\r\nThis is my nodes.py\r\n```\r\ndef pnl_metrics(df:pd.DataFrame): \r\n    avg_pnl = {}\r\n    avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()}\r\n    avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()}\r\n    return avg_pnl\r\n```\r\n\r\n\r\n## Expected Result\r\n\r\nHow do i get the metric to be displayed when i use the Mlflow ui? Are there specific keywords that mlflow is tracking to be logged as metric?\r\n\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): **0.10.0**\r\n* Python version used (`python -V`):  **3.9.0** \r\n* Operating system and version: Windows 10\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: metricsdataset logs invalid metric which breaks ui; Content: ## description this happens when i tried to configure my own metric functions. ## context i am trying to create a custom metric indicator, to be logged after each experimentation. when i run ` ui`, this is what i'm getting on the ui. ![image](https:\/\/user-images.githubusercontent.com\/54475793\/184276876-57872dd2-3fb3-41c9-b3a3-edd6a4396aca.png) ## steps to reproduce this is my nodes.py ``` def pnl_metrics(df:pd.dataframe): avg_pnl = {} avg_pnl[f'{avg_metric}'] = {'trader1': df.pnl.mean()} avg_pnl[f'{total_metric}'] = {'trader1': df.pnl.sum(), 'trader2': df.pnl.sum()} return avg_pnl ``` ## expected result how do i get the metric to be displayed when i use the ui? are there specific keywords that is tracking to be logged as metric? ## your environment include as many relevant details about the environment in which you experienced the bug: * `` and `-` version used (`pip show ` and `pip show -`): **0.10.0** * python version used (`python -v`): **3.9.0** * operating system and version: windows 10",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge where metricsdataset logs an invalid metric which breaks the UI.",
        "Issue_preprocessed_content":"Title: metricsdataset logs invalid metric which breaks ui; Content: description this happens when i tried to configure my own metric functions. context i am trying to create a custom metric indicator, to be logged after each experimentation. when i run , this is what i'm getting on the ui. steps to reproduce this is my expected result how do i get the metric to be displayed when i use the ui? are there specific keywords that is tracking to be logged as metric? your environment include as many relevant details about the environment in which you experienced the bug and version used python version used operating system and version windows"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/336",
        "Issue_title":"kedro mlflow init displays a wrong sucess message when the env folder does not exist",
        "Issue_creation_time":1656532075000,
        "Issue_closed_time":1657139268000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nWhen running ``kedro mlflow init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. We should move this code : \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/kedro_mlflow\/framework\/cli\/cli.py#L116-L122\r\n\r\ninside the \"try\" block above.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: init displays a wrong sucess message when the env folder does not exist; Content: ## description when running `` init --env=xxx``, a success message is displayed even if the env \"xxx\" folder does not exist, instead of an error message. we should move this code : https:\/\/github.com\/galileo-galilei\/-\/blob\/d31820a7d4ea808d0a4460d41966b762a404b5a5\/_\/framework\/cli\/cli.py#l116-l122 inside the \"try\" block above.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running \" init --env=xxx\" displays a success message even if the env \"xxx\" folder does not exist, instead of an error message.",
        "Issue_preprocessed_content":"Title: init displays a wrong sucess message when the env folder does not exist; Content: description when running , a success message is displayed even if the env xxx folder does not exist, instead of an error message. we should move this code inside the try block above."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/309",
        "Issue_title":"kedro-mlflow is broken with kedro==0.18.1",
        "Issue_creation_time":1652380533000,
        "Issue_closed_time":1652640252000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nThe plugin does not work with projects created with ``kedro==0.18.1``\r\n\r\n## Context\r\n\r\nTry to launch ``kedro run`` in a project with ``kedro==0.18.1`` and kedro-mlflow installed.\r\n\r\n\r\n## Steps to Reproduce\r\n\r\n```python\r\nconda create -n temp python=3.8 -y\r\nconda activate temp\r\npip install kedro==0.18.1 kedro-mlflow==0.9.0\r\nkedro new --starter=pandas-iris\r\ncd pandas-iris\r\nkedro mlflow init\r\nkedro run\r\n```\r\n\r\n## Expected Result\r\n\r\nThis should run the pipeleine and log the parameters.\r\n\r\n## Actual Result\r\n\r\nThis raises the following error:\r\n\r\n```bash\r\nAttributeError: module 'kedro.framework.session.session' has no attribute '_active_session'\r\n```\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): ``kedro==0.18.1`` and ``kedro-mlflow<=0.9.0``\r\n* Python version used (`python -V`): All\r\n* Operating system and version: All\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nCurrently, kedro-mlflow uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/kedro_mlflow\/config\/kedro_mlflow_config.py#L233-L247) inside a hook. \r\n\r\nWith kedro==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. \r\n\r\nRetrieving the configuration and set it up should be moved to this new hook:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L108-L109",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: - is broken with ==0.18.1; Content: ## description the plugin does not work with projects created with ``==0.18.1`` ## context try to launch `` run`` in a project with ``==0.18.1`` and - installed. ## steps to reproduce ```python conda create -n temp python=3.8 -y conda activate temp pip install ==0.18.1 -==0.9.0 new --starter=pandas-iris cd pandas-iris init run ``` ## expected result this should run the pipeleine and log the parameters. ## actual result this raises the following error: ```bash attributeerror: module '.framework.session.session' has no attribute '_active_session' ``` ## your environment include as many relevant details about the environment in which you experienced the bug: * `` and `-` version used (`pip show ` and `pip show -`): ``==0.18.1`` and ``-<=0.9.0`` * python version used (`python -v`): all * operating system and version: all ## does the bug also happen with the last version on master? yes ## solution currently, - uses [the private ``_active_session`` global variable to access the configuration](https:\/\/github.com\/galileo-galilei\/-\/blob\/e855f59faa76c881b32616880608d41c064c23a0\/_\/config\/__config.py#l233-l247) inside a hook. with ==0.18.1, this private attribute was removed and the new recommandation is to use the ``after_context_created`` hook. retrieving the configuration and set it up should be moved to this new hook: https:\/\/github.com\/galileo-galilei\/-\/blob\/963c338d6259dd118232c45801abe0a2b0a463df\/_\/framework\/hooks\/pipeline_hook.py#l108-l109",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with - not working with projects created with ``==0.18.1``, which was solved by moving the configuration setup to the ``after_context_created`` hook.",
        "Issue_preprocessed_content":"Title: is broken with ; Content: description the plugin does not work with projects created with context try to launch in a project with and installed. steps to reproduce expected result this should run the pipeleine and log the parameters. actual result this raises the following error your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used all operating system and version all does the bug also happen with the last version on master? yes solution currently, uses inside a hook. with this private attribute was removed and the new recommandation is to use the hook. retrieving the configuration and set it up should be moved to this new hook"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/258",
        "Issue_title":"MlflowArtifactDataSet does not work with PartitionedDataSet",
        "Issue_creation_time":1636062318000,
        "Issue_closed_time":1644674290000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nIt is not possible to store a ``PartitionedDataSet`` as an mlflow artifact with the ``MlflowArtifactDataSet``.\r\n\r\n## Context\r\n\r\nI had a use case where I need to save a dict with many small result tables to mlflow, and I tried to use ``PartitionedDataSet`` for this.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# catalog.yml\r\n\r\nmy_dataset:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: PartitionedDataSet  # or any valid kedro DataSet\r\n        path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"!\r\n        dataset: \"pandas.CSVDataSet\"\r\n```\r\n\r\nthen save a dict using this dataset:\r\n\r\n```\r\ncatalog.save(\"my_dataset\", dict(\"a\": pd.DataFrame(data=[1,2,3], columns=[\"a\"], \"b\": pd.DataFrame(data=[1,2,3], columns=[\"b\"])\r\n```\r\n## Expected Result\r\n\r\nThe 2 Dataframes should be logged as artifacts in the current mlflow run.\r\n\r\n## Actual Result\r\n\r\nAn error ``dataset has not attribute \"_filepath\"`` is raised.\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe error comes from this line:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py#L53\r\n\r\nmaybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: artifactdataset does not work with partitioneddataset; Content: ## description it is not possible to store a ``partitioneddataset`` as an artifact with the ``artifactdataset``. ## context i had a use case where i need to save a dict with many small result tables to , and i tried to use ``partitioneddataset`` for this. ## steps to reproduce ```yaml # catalog.yml my_dataset: type: _.io.artifacts.artifactdataset data_set: type: partitioneddataset # or any valid dataset path: \/path\/to\/a\/local\/folder # the attribute is \"path\", and not \"filepath\"! dataset: \"pandas.csvdataset\" ``` then save a dict using this dataset: ``` catalog.save(\"my_dataset\", dict(\"a\": pd.dataframe(data=[1,2,3], columns=[\"a\"], \"b\": pd.dataframe(data=[1,2,3], columns=[\"b\"]) ``` ## expected result the 2 dataframes should be logged as artifacts in the current run. ## actual result an error ``dataset has not attribute \"_filepath\"`` is raised. ## does the bug also happen with the last version on master? yes ## potential solution the error comes from this line: https:\/\/github.com\/galileo-galilei\/-\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/_\/io\/artifacts\/_artifact_dataset.py#l53 maybe we can add a better condition here to default to \"path\" if there is no \"filepath\" attribute.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where it was not possible to store a partitioneddataset as an artifact with the artifactdataset.",
        "Issue_preprocessed_content":"Title: artifactdataset does not work with partitioneddataset; Content: description it is not possible to store a as an artifact with the . context i had a use case where i need to save a dict with many small result tables to , and i tried to use for this. steps to reproduce then save a dict using this dataset expected result the dataframes should be logged as artifacts in the current run. actual result an error is raised. does the bug also happen with the last version on master? yes potential solution the error comes from this line maybe we can add a better condition here to default to path if there is no filepath attribute."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/256",
        "Issue_title":"Setting the mlflow experiment does not work in interactive mode",
        "Issue_creation_time":1636045277000,
        "Issue_closed_time":1636318265000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nIf I specify an experiment in `mlflow.yml`, and the set up the mlflow configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in mlflow \"Default\" (0) experiment. This works when running \"kedro run\" through the CLI.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\n# mlflow.yml\r\nexperiment:\r\n  name: my_awesome_experiment\r\n  create: True  # if the specified `name` does not exists, should it be created?\r\n```\r\n\r\n```python\r\n# test.py\r\n\r\nfrom kedro.framework.session import KedroSession\r\nfrom kedro.framework.startup import bootstrap_project\r\nfrom kedro_mlflow.config import get_mlflow_config\r\n\r\nbootstrap_project(r\"path\/to\/project\")\r\nwith KedroSession.create(project_path=r\"path\/to\/project\"):\r\n    config=get_mlflow_config()\r\n    config.setup()\r\n    \r\n    mlflow.log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"Default\".\r\n\r\n```\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Potential solution\r\n\r\nThe faulty line is: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L100\r\n\r\n[We should use mlflow ``mlflow.set_experiment`` method](https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.set_experiment), but it does not restore deleted experiment. This wil replace part of the logic here: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/kedro_mlflow\/config\/kedro_mlflow_config.py#L124-L132",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: setting the experiment does not work in interactive mode; Content: ## description if i specify an experiment in `.yml`, and the set up the configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in \"default\" (0) experiment. this works when running \" run\" through the cli. ## steps to reproduce ```yaml # .yml experiment: name: my_awesome_experiment create: true # if the specified `name` does not exists, should it be created? ``` ```python # test.py from .framework.session import session from .framework.startup import bootstrap_project from _.config import get__config bootstrap_project(r\"path\/to\/project\") with session.create(project_path=r\"path\/to\/project\"): config=get__config() config.setup() .log_param(\"test_param\",1) # this should be logged in \"my_awesome_experiment\" but is logged in \"default\". ``` ## does the bug also happen with the last version on master? yes ## potential solution the faulty line is: https:\/\/github.com\/galileo-galilei\/-\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/_\/config\/__config.py#l100 [we should use ``.set_experiment`` method](https:\/\/www..org\/docs\/latest\/python_api\/.html#.set_experiment), but it does not restore deleted experiment. this wil replace part of the logic here: https:\/\/github.com\/galileo-galilei\/-\/blob\/904207ad505b71391d78d8088aaed151ca6a011d\/_\/config\/__config.py#l124-l132",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where specifying an experiment in a .yml file and then setting up the configuration interactively caused all runs to be stored in the \"default\" (0) experiment instead of the specified experiment.",
        "Issue_preprocessed_content":"Title: setting the experiment does not work in interactive mode; Content: description if i specify an experiment in , and the set up the configuration interactively, all runs should be stored by default in this experiment while they are currently sotred in default experiment. this works when running run through the cli. steps to reproduce does the bug also happen with the last version on master? yes potential solution the faulty line is we should use but it does not restore deleted experiment. this wil replace part of the logic here"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/193",
        "Issue_title":"kedro-mlflow CLI is unavailable inside a Kedro project",
        "Issue_creation_time":1619193727000,
        "Issue_closed_time":1619987466000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## Description\r\n\r\nI try to reproduce the minimal example from the Docs: a Kedro project using the starter `pandas-iris` using the `kedro-mlflow` functinality. I do not arrive at initializing the kedro-mlflow project, since the cli commands are not available.\r\n\r\n## Context\r\n\r\nIt is unclear to me if this is connected to #157 \r\nI wanted to start looking into kedro-mlflow, but got immediatle blocked by the initialization of the project. Therefore any advice on where to look to fix this would also be appreciated. \r\n\r\n## Steps to Reproduce\r\n\r\n```\r\nconda create -n kedro_mlflow python=3.8\r\nconda activate kedro_mlflow\r\npip install kedro-mlflow\r\nkedro mlflow -h\r\nkedro new --starter=pandas-iris\r\ncd mlflow_test\/\r\nkedro mlflow -h\r\n> ERROR \"No such command 'mlflow'\"\r\n```\r\n\r\n## Expected Result\r\n\r\n`kedro mlflow` is available in a project directory, i.e. `kedro mlflow -h` gives the same output inside the folder as before\r\n\r\n## Actual Result\r\n\r\ninside the project folder the `mlflow` command is unknown to Kedro\r\n\r\n```\r\n...\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\r\n  return get_provider(package_or_requirement).get_resource_filename(\r\n....\/miniconda3\/envs\/kedro_mlflow\/lib\/python3.8\/site-packages\/mlflow\/types\/schema.py:49: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\r\n  binary = (7, np.dtype(\"bytes\"), \"BinaryType\", np.object)\r\n2021-04-23 17:49:52,197 - root - INFO - Registered hooks from 2 installed plugin(s): kedro-mlflow-0.7.1\r\nUsage: kedro [OPTIONS] COMMAND [ARGS]...\r\nTry 'kedro -h' for help.\r\n\r\nError: No such command 'mlflow'.\r\n\r\n```\r\n\r\n## Your Environment\r\n\r\nUbuntu 18.04.5\r\n\r\n- Kedro 0.17.3\r\n- kedro-mlflow 0.7.1\r\n- python 3.8.8.\r\n- mlflow 1.15.0\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nyes",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: - cli is unavailable inside a project; ## description i try to reproduce the minimal example from the docs: a project using the starter `pandas-iris` using the `-` functinality. i do not arrive at initializing the - project, since the cli commands are not available. ## context it is unclear to me if this is connected to #157 i wanted to start looking into -, but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. ## steps to reproduce ``` conda create -n _ python=3.8 conda activate _ pip install - -h new --starter=pandas-iris cd _test\/ -h > error \"no such command ''\" ``` ## expected result ` ` is available in a project directory, i.e. ` -h` gives the same output inside the folder as before ## actual result inside the project folder the `` command is unknown to ``` ...\/miniconda3\/envs\/_\/lib\/python3.8\/site-packages\/pkg_resources\/__init__.py:1130: deprecationwarning: use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release. return get_provider(package_or_requirement).get_resource_filename( ....\/miniconda3\/envs\/_\/lib\/python3.8\/site-packages\/\/types\/schema.py:49: deprecationwarning: `np.object` is a deprecated alias for the builtin `object`. to silence this warning, use `object` by itself. doing this will not modify any behavior and is safe. deprecated in numpy 1.20; Content: for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations binary = (7, np.dtype(\"bytes\"), \"binarytype\", np.object) 2021-04-23 17:49:52,197 - root - info - registered hooks from 2 installed plugin(s): --0.7.1 usage: [options] command [args]... try ' -h' for help. error: no such command ''. ``` ## your environment ubuntu 18.04.5 - 0.17.3 - - 0.7.1 - python 3.8.8. - 1.15.0 ## does the bug also happen with the last version on master? yes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the - CLI commands were unavailable inside a project, despite the user having installed the necessary packages and following the minimal example from the documentation.",
        "Issue_preprocessed_content":"Title: cli is unavailable inside a project; Content: description i try to reproduce the minimal example from the docs a project using the starter using the functinality. i do not arrive at initializing the project, since the cli commands are not available. context it is unclear to me if this is connected to i wanted to start looking into but got immediatle blocked by the initialization of the project. therefore any advice on where to look to fix this would also be appreciated. steps to reproduce expected result is available in a project directory, gives the same output inside the folder as before actual result inside the project folder the command is unknown to your environment ubuntu python does the bug also happen with the last version on master? yes"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/187",
        "Issue_title":"kedro mlflow ui does not use arguments from mlflow.yml",
        "Issue_creation_time":1617627646000,
        "Issue_closed_time":1618006798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nAs described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in-mlflow-yml-and-run-kedro-mlflow-ui-but-host-and-port), the `ui` command does not use the options\r\n\r\n## Context & Steps to Reproduce\r\n\r\n- Create a kedro project\r\n- Call `kedro mlflow init`\r\n- Modify the port in `mlflow.yml` to 5001\r\n- Launch `kedro mlflow ui`\r\n\r\n## Expected Result\r\n\r\nThe mlflow UI should open in port 5001.\r\n\r\n## Actual Result\r\n\r\nIt opens on port 5000 (the default).\r\n\r\n## Your Environment\r\n\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` version: 0.17.0\r\n* `kedro-mlflow` version: 0.6.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Operating system and version: Windows\r\n\r\n## Does the bug also happen with the last version on master?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nWe should pass the arguments in the command: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/kedro_mlflow\/framework\/cli\/cli.py#L149-L151",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: ui does not use arguments from .yml; Content: ## description as described in [this stackoverflow question](https:\/\/stackoverflow.com\/questions\/66917129\/specify-host-and-port-in--yml-and-run---ui-but-host-and-port), the `ui` command does not use the options ## context & steps to reproduce - create a project - call ` init` - modify the port in `.yml` to 5001 - launch ` ui` ## expected result the ui should open in port 5001. ## actual result it opens on port 5000 (the default). ## your environment include as many relevant details about the environment in which you experienced the bug: * `` version: 0.17.0 * `-` version: 0.6.0 * python version used (`python -v`): 3.6.8 * operating system and version: windows ## does the bug also happen with the last version on master? yes ## solution we should pass the arguments in the command: https:\/\/github.com\/galileo-galilei\/-\/blob\/477147f6aa2dbf59c67f916b2002dea2de74d1fd\/_\/framework\/cli\/cli.py#l149-l151",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the `ui` command in Kedro does not use the options specified in the `.yml` file, despite the expected result being that it should open in the port specified in the `.yml` file.",
        "Issue_preprocessed_content":"Title: ui does not use arguments from ; Content: description as described in , the command does not use the options context & steps to reproduce create a project call modify the port in to launch expected result the ui should open in port . actual result it opens on port . your environment include as many relevant details about the environment in which you experienced the bug version version python version used operating system and version windows does the bug also happen with the last version on master? yes solution we should pass the arguments in the command"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/157",
        "Issue_title":"kedro mlflow cli is broken if configuration is declared in pyproject.toml",
        "Issue_creation_time":1610404594000,
        "Issue_closed_time":1615716614000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Description\r\n\r\nKedro enable to declare configuration either in ``.kedro.yml`` or in ``pyproject.toml`` (in the ``[tool.kedro]`` section). We claim to support both, but the CLI commands are not accessible if the project contains only a ``pyproject.toml file``.\r\n\r\n## Steps to Reproduce\r\n\r\nCall ``kedro mlflow init`` inside a project with no ``.kedro.yml`` file but only a ``pyproject.toml``.\r\n\r\n## Expected Result\r\n\r\nThe cli commands should be available (``init``)\r\n\r\n## Actual Result\r\nOnly the ``new`` command is available. This is not considered as a kedro project.\r\n\r\n```\r\n-- Separate them if you have more than one.\r\n```\r\n\r\n## Your Environment\r\n\r\n* `kedro` and `kedro-mlflow` version used (`pip show kedro` and `pip show kedro-mlflow`): kedro==16.6, kedro-mlflow==0.4.1\r\n* Python version used (`python -V`): 3.7.9\r\n* Operating system and version: Windows 7\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n## Solution\r\n\r\nThe error comes from the ``is_kedro_project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``.kedro.yml``.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: cli is broken if configuration is declared in pyproject.toml; Content: ## description enable to declare configuration either in ``..yml`` or in ``pyproject.toml`` (in the ``[tool.]`` section). we cl to support both, but the cli commands are not accessible if the project contains only a ``pyproject.toml file``. ## steps to reproduce call `` init`` inside a project with no ``..yml`` file but only a ``pyproject.toml``. ## expected result the cli commands should be available (``init``) ## actual result only the ``new`` command is available. this is not considered as a project. ``` -- separate them if you have more than one. ``` ## your environment * `` and `-` version used (`pip show ` and `pip show -`): ==16.6, -==0.4.1 * python version used (`python -v`): 3.7.9 * operating system and version: windows 7 ## does the bug also happen with the last version on develop? yes ## solution the error comes from the ``is__project`` function which does not consider that a folder is the root of a kdro project if it does not contain a ``..yml``.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the cli commands were not accessible if the project only contained a pyproject.toml file, and the solution was to update the is__project function to consider a folder as the root of a project even if it does not contain a ..yml file.",
        "Issue_preprocessed_content":"Title: cli is broken if configuration is declared in ; Content: description enable to declare configuration either in or in . we cl to support both, but the cli commands are not accessible if the project contains only a . steps to reproduce call inside a project with no file but only a . expected result the cli commands should be available actual result only the command is available. this is not considered as a project. your environment and version used python version used operating system and version windows does the bug also happen with the last version on develop? yes solution the error comes from the function which does not consider that a folder is the root of a kdro project if it does not contain a ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/122",
        "Issue_title":"A KedroPipelineModel cannot be loaded from mlflow if its catalog contains non deepcopy-able DataSets",
        "Issue_creation_time":1605983313000,
        "Issue_closed_time":1606599848000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nI tried to load a KedroPipelineModel from mlflow, and I got a \"cannot pickle context artifacts\" error, which is due do the \r\n\r\n## Context\r\n\r\nI cannot load a previously saved KedroPipelineModel generated by pipeline_ml_factory.\r\n\r\n## Steps to Reproduce\r\n\r\nSave A KedroPipelineModel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer)\r\n\r\n## Expected Result\r\n\r\nThe model should be loaded\r\n\r\n## Actual Result\r\n\r\nAn error is raised\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* `kedro` and `kedro-mlflow` version used: 0.16.5 and 0.4.0\r\n* Python version used (`python -V`): 3.6.8\r\n* Windows 10 & CentOS were tested\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes\r\n\r\n# Potential solution\r\n\r\nThe faulty line is:\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/mlflow\/kedro_pipeline_model.py#L45",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: a pipelinemodel cannot be loaded from if its catalog contains non deepcopy-able datasets; Content: ## description i tried to load a pipelinemodel from , and i got a \"cannot pickle context artifacts\" error, which is due do the ## context i cannot load a previously saved pipelinemodel generated by pipeline_ml_factory. ## steps to reproduce save a pipelinemodel with a dataset that contains an object which cannot be deepcopied (for me, a keras tokenizer) ## expected result the model should be loaded ## actual result an error is raised ## your environment include as many relevant details about the environment in which you experienced the bug: * `` and `-` version used: 0.16.5 and 0.4.0 * python version used (`python -v`): 3.6.8 * windows 10 & centos were tested ## does the bug also happen with the last version on develop? yes # potential solution the faulty line is: https:\/\/github.com\/galileo-galilei\/-\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/_\/\/_pipeline_model.py#l45",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to load a previously saved KedroPipelineModel generated by pipeline_ml_factory due to the catalog containing an object which cannot be deepcopied.",
        "Issue_preprocessed_content":"Title: a pipelinemodel cannot be loaded from if its catalog contains non datasets; Content: description i tried to load a pipelinemodel from , and i got a cannot pickle context artifacts error, which is due do the context i cannot load a previously saved pipelinemodel generated by steps to reproduce save a pipelinemodel with a dataset that contains an object which cannot be deepcopied expected result the model should be loaded actual result an error is raised your environment include as many relevant details about the environment in which you experienced the bug and version used and python version used windows & centos were tested does the bug also happen with the last version on develop? yes potential solution the faulty line is"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/121",
        "Issue_title":"RunStatus of mlflow run is \"FINISHED\" instead of \"FAILED\" when the kedro run fails",
        "Issue_creation_time":1605982845000,
        "Issue_closed_time":1606515096000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nWhen I launch `kedro run` and the run fails, the `on_pipeline_error` closes all the mlflow runs (to avoid interactions with further runs)\r\n\r\n## Context\r\n\r\nI cannot distinguish failed runs from sucessful ones in the mlflow ui.\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch a failing pipeline with kedro run.\r\n\r\n## Expected Result\r\n\r\nThe mlflow ui should display the run with a red cross\r\n\r\n## Actual Result\r\n\r\nThe mlflow ui displays the run with a green tick\r\n\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.\r\n\r\n## Potential solution: \r\n\r\nReplace these lines:\r\n\r\n`https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/kedro_mlflow\/framework\/hooks\/pipeline_hook.py#L193-L194`\r\n\r\nwith \r\n\r\n```python\r\nwhile mlflow.active_run():\r\n    mlflow.end_run(mlflow.entities.RunStatus.FAILED)\r\n```\r\nor even better, retrieve current run status from mlflow?\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: runstatus of run is \"finished\" instead of \"failed\" when the run fails; Content: ## description when i launch ` run` and the run fails, the `on_pipeline_error` closes all the runs (to avoid interactions with further runs) ## context i cannot distinguish failed runs from sucessful ones in the ui. ## steps to reproduce launch a failing pipeline with run. ## expected result the ui should display the run with a red cross ## actual result the ui displays the run with a green tick ## does the bug also happen with the last version on develop? yes. ## potential solution: replace these lines: `https:\/\/github.com\/galileo-galilei\/-\/blob\/63dcd501bfe98bebc81f25f70020ff4141c1e91c\/_\/framework\/hooks\/pipeline_hook.py#l193-l194` with ```python while .active_run(): .end_run(.entities.runstatus.failed) ``` or even better, retrieve current run status from ?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the runstatus of a run is \"finished\" instead of \"failed\" when the run fails, preventing them from distinguishing failed runs from successful ones in the UI.",
        "Issue_preprocessed_content":"Title: runstatus of run is finished instead of failed when the run fails; Content: description when i launch and the run fails, the closes all the runs context i cannot distinguish failed runs from sucessful ones in the ui. steps to reproduce launch a failing pipeline with run. expected result the ui should display the run with a red cross actual result the ui displays the run with a green tick does the bug also happen with the last version on develop? yes. potential solution replace these lines with or even better, retrieve current run status from ?"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/116",
        "Issue_title":"TypeError: unsupported operand type(s) for \/: 'str' and 'str' when using MlflowArtifactDataSet with MlflowModelSaverDataSet",
        "Issue_creation_time":1604666166000,
        "Issue_closed_time":1605715301000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\n`TypeError: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `MlflowArtifactDataSet` is used with `MlflowModelSaverDataSet`.\r\n\r\n## Context\r\n\r\nLogging locally and to MLflow in one step.\r\n\r\n## Steps to Reproduce\r\n\r\n```yaml\r\nsklearn_model:\r\n    type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet\r\n    data_set:\r\n        type: kedro_mlflow.io.models.MlflowModelSaverDataSet\r\n        flavor: mlflow.sklearn\r\n        filepath: data\/06_models\/sklearn_model\r\n        versioned: true\r\n```\r\n\r\n## Expected Result\r\n\r\nThe model should be saved locally and in MLflow run at the same time.\r\n\r\n## Actual Result\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 240, in save\r\n    self._save(data)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/src\/kedro-mlflow\/kedro_mlflow\/io\/artifacts\/mlflow_artifact_dataset.py\", line 40, in _save\r\n    if hasattr(self, \"_version\")\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 605, in _get_save_path\r\n    versioned_path = self._get_versioned_path(save_version)  # type: ignore\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 616, in _get_versioned_path\r\n    return self._filepath \/ version \/ self._filepath.name\r\nTypeError: unsupported operand type(s) for \/: 'str' and 'str'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/kedro\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py\", line 725, in main\r\n    cli_collection()\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/Users\/olszewk2\/dev\/pyzypad-example\/kedro_cli.py\", line 230, in run\r\n    pipeline_name=pipeline,\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 767, in run\r\n    raise exc\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py\", line 759, in run\r\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 101, in run\r\n    self._run(pipeline, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py\", line 90, in _run\r\n    run_node(node, catalog, self._is_async, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 213, in run_node\r\n    node = _run_node_sequential(node, catalog, run_id)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py\", line 249, in _run_node_sequential\r\n    catalog.save(name, data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py\", line 448, in save\r\n    func(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 625, in save\r\n    super().save(data)\r\n  File \"\/Users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/kedro\/io\/core.py\", line 247, in save\r\n    raise DataSetError(message) from exc\r\nkedro.io.core.DataSetError: Failed while saving data to data set MlflowMlflowModelSaverDataSet(filepath=\/Users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=mlflow.sklearn, load_args={}, save_args={}, version=Version(load=None, save='2020-11-06T12.28.57.593Z')).\r\nunsupported operand type(s) for \/: 'str' and 'str'\r\n```\r\n\r\n## Your Environment\r\nInclude as many relevant details about the environment in which you experienced the bug:\r\n\r\n* kedro 0.16.6\r\n* kedro-mlflow 0.4.0\r\n* Python 3.7.7\r\n* MacOS Catalina\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: typeerror: unsupported operand type(s) for \/: 'str' and 'str' when using artifactdataset with modelsaverdataset; Content: ## description `typeerror: unsupported operand type(s) for \/: 'str' and 'str'` occurs when `artifactdataset` is used with `modelsaverdataset`. ## context logging locally and to in one step. ## steps to reproduce ```yaml sklearn_model: type: _.io.artifacts.artifactdataset data_set: type: _.io.models.modelsaverdataset flavor: .sklearn filepath: data\/06_models\/sklearn_model versioned: true ``` ## expected result the model should be saved locally and in run at the same time. ## actual result ``` traceback (most recent call last): file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/io\/core.py\", line 240, in save self._save(data) file \"\/users\/olszewk2\/dev\/pyzypad-example\/src\/-\/_\/io\/artifacts\/_artifact_dataset.py\", line 40, in _save if hasattr(self, \"_version\") file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/io\/core.py\", line 605, in _get_save_path versioned_path = self._get_versioned_path(save_version) # type: ignore file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/io\/core.py\", line 616, in _get_versioned_path return self._filepath \/ version \/ self._filepath.name typeerror: unsupported operand type(s) for \/: 'str' and 'str' the above exception was the direct cause of the following exception: traceback (most recent call last): file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/bin\/\", line 8, in sys.exit(main()) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/framework\/cli\/cli.py\", line 725, in main cli_collection() file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__ return self.main(*args, **kwargs) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main rv = self.invoke(ctx) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke return callback(*args, **kwargs) file \"\/users\/olszewk2\/dev\/pyzypad-example\/_cli.py\", line 230, in run pipeline_name=pipeline, file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/framework\/context\/context.py\", line 767, in run raise exc file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/framework\/context\/context.py\", line 759, in run run_result = runner.run(filtered_pipeline, catalog, run_id) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 101, in run self._run(pipeline, catalog, run_id) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/runner\/sequential_runner.py\", line 90, in _run run_node(node, catalog, self._is_async, run_id) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 213, in run_node node = _run_node_sequential(node, catalog, run_id) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 249, in _run_node_sequential catalog.save(name, data) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/io\/data_catalog.py\", line 448, in save func(data) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/io\/core.py\", line 625, in save super().save(data) file \"\/users\/olszewk2\/miniconda3\/envs\/pyzypad-example-env\/lib\/python3.7\/site-packages\/\/io\/core.py\", line 247, in save raise dataseterror(message) from exc .io.core.dataseterror: failed while saving data to data set modelsaverdataset(filepath=\/users\/olszewk2\/dev\/pyzypad-example\/data\/06_models\/pclass_encoder, flavor=.sklearn, load_args={}, save_args={}, version=version(load=none, save='2020-11-06t12.28.57.593z')). unsupported operand type(s) for \/: 'str' and 'str' ``` ## your environment include as many relevant details about the environment in which you experienced the bug: * 0.16.6 * - 0.4.0 * python 3.7.7 * macos catalina ## does the bug also happen with the last version on develop? yes.",
        "Issue_original_content_gpt_summary":"The user encountered a `typeerror: unsupported operand type(s) for \/: 'str' and 'str'` when using `artifactdataset` with `modelsaverdataset` in Kedro 0.16.6 and Kedro- 0.4.0 with Python 3.7.7 on macOS Catalina, which also happened with the last version on develop.",
        "Issue_preprocessed_content":"Title: typeerror unsupported operand type for 'str' and 'str' when using artifactdataset with modelsaverdataset; Content: description occurs when is used with . context logging locally and to in one step. steps to reproduce expected result the model should be saved locally and in run at the same time. actual result your environment include as many relevant details about the environment in which you experienced the bug python macos catalina does the bug also happen with the last version on develop? yes."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/102",
        "Issue_title":"MlflowMetricsDataSet ignores run_id when prefix is not specified",
        "Issue_creation_time":1603488238000,
        "Issue_closed_time":1603665805000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\nWhen `MlflowMetricsDataset` has no \"prefix\" specified, the name in the catalog is used instead. However, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set.\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create a mlflow run interactively: \r\n```python\r\nmlflow.start_run()\r\nmlflow.end_run()\r\n```\r\nAnd browse the ui to retrieve the run_id\r\n\r\n2. Declare a `MlflowMetricsDataset` in the `catalog.yml`: with no prefix and an existing run_id.\r\n```python\r\nmy_metrics:\r\n    type: kedro_mlflow.io.MlflowMetricsDataSet\r\n    run_id: 123456789 # existing run_id\r\n```\r\n\r\n3. Launch the pipeline which saves this catalog: `kedro run`\r\n\r\n## Expected Result\r\n\r\nA metric should be loggedin run \"1346579\".\r\n\r\n## Actual Result\r\n\r\nThe metric is logged is a new run.\r\n\r\n## Does the bug also happen with the last version on develop?\r\n\r\nYes",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: metricsdataset ignores run_id when prefix is not specified; Content: ## description when `metricsdataset` has no \"prefix\" specified, the name in the catalog is used instead. however, when the run_id is specified, it is overriden by the current run id when the prefix is automatically set. ## steps to reproduce 1. create a run interactively: ```python .start_run() .end_run() ``` and browse the ui to retrieve the run_id 2. declare a `metricsdataset` in the `catalog.yml`: with no prefix and an existing run_id. ```python my_metrics: type: _.io.metricsdataset run_id: 123456789 # existing run_id ``` 3. launch the pipeline which saves this catalog: ` run` ## expected result a metric should be loggedin run \"1346579\". ## actual result the metric is logged is a new run. ## does the bug also happen with the last version on develop? yes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where metricsdataset ignores the specified run_id when no prefix is specified, resulting in the metric being logged in a new run.",
        "Issue_preprocessed_content":"Title: metricsdataset ignores when prefix is not specified; Content: description when has no prefix specified, the name in the catalog is used instead. however, when the is specified, it is overriden by the current run id when the prefix is automatically set. steps to reproduce . create a run interactively and browse the ui to retrieve the . declare a in the with no prefix and an existing . launch the pipeline which saves this catalog expected result a metric should be loggedin run . actual result the metric is logged is a new run. does the bug also happen with the last version on develop? yes"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/96",
        "Issue_title":"Make mlflow init work when configuration is in pyproject.toml",
        "Issue_creation_time":1603011348000,
        "Issue_closed_time":1603658563000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## Description\r\n\r\nSince 0.16.5, kedro project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/439) instead of a `.kedro.yml` at the root of the projects. This breaks the `kedro mlflow init` command which is only compatible with `.kedro.yml` configuration file.\r\n\r\n## Context\r\nWe should remove the `_get_project_globals` util function in kedromlflow and use `kedro.framework.context import get_static_project_data` as suggested in #86. **Beware: this will break retrocompatibilty and work only with kedro>=0.16.5**\r\n\r\n## Steps to Reproduce\r\n\r\nLaunch `kedro mlflow init` with no `.kedro.yml` config file in your project but a valid `pyproject.toml`.\r\n\r\n## Expected Result\r\nThe mlflow.yml file should be created\r\n\r\n## Actual Result\r\nAn error is raised.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: make init work when configuration is in pyproject.toml; Content: ## description since 0.16.5, project can [now be configured with a `pyproject.toml` config file](https:\/\/github.com\/quantumblacklabs\/\/issues\/439) instead of a `..yml` at the root of the projects. this breaks the ` init` command which is only compatible with `..yml` configuration file. ## context we should remove the `_get_project_globals` util function in and use `.framework.context import get_static_project_data` as suggested in #86. **beware: this will break retrocompatibilty and work only with >=0.16.5** ## steps to reproduce launch ` init` with no `..yml` config file in your project but a valid `pyproject.toml`. ## expected result the .yml file should be created ## actual result an error is raised.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the ' init' command was not compatible with the 'pyproject.toml' configuration file, resulting in an error being raised instead of the expected .yml file being created.",
        "Issue_preprocessed_content":"Title: make init work when configuration is in ; Content: description since project can instead of a at the root of the projects. this breaks the command which is only compatible with configuration file. context we should remove the util function in and use as suggested in . beware this will break retrocompatibilty and work only with steps to reproduce launch with no config file in your project but a valid . expected result the file should be created actual result an error is raised."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/74",
        "Issue_title":"MlflowDataSet fails to log on remote storage when underlying dataset filepath is converted as a PurePosixPath",
        "Issue_creation_time":1601476316000,
        "Issue_closed_time":1602278580000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When I register a dataset in the catalog.yml\r\n\r\n```yaml\r\nmy_dataset:\r\n  type : kedro_mlflow.io.MlflowDataSet \r\n  data_set : \r\n    type: pickle.PickleDataSet\r\n    filepath: data\/02_intermediate\/my_dataset.pkl\r\n```\r\n\r\nand I run `kedro run` I got a `expected string or bytes-like object` when **the local path is linux AND the `mlflow_tracking_uri` is an Azure blob storage (it works locally)**. I don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: \r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L51\r\n\r\nhttps:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/kedro_mlflow\/io\/mlflow_dataset.py#L55\r\n\r\n@kaemo @akruszewski did you experience some issues with S3 too?\r\n\r\n**EDIT**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/kedro-mlflow\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: dataset fails to log on remote storage when underlying dataset filepath is converted as a pureposixpath; Content: when i register a dataset in the catalog.yml ```yaml my_dataset: type : _.io.dataset data_set : type: pickle.pickledataset filepath: data\/02_intermediate\/my_dataset.pkl ``` and i run ` run` i got a `expected string or bytes-like object` when **the local path is linux and the `_tracking_uri` is an azure blob storage (it works locally)**. i don't know really why this append, but it can be fied by replacing `self._filepath` by `self._filepath.as_posix()` in these 2 locations: https:\/\/github.com\/galileo-galilei\/-\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/_\/io\/_dataset.py#l51 https:\/\/github.com\/galileo-galilei\/-\/blob\/94bae3df9a054c85dfc0bf13de8db876363de475\/_\/io\/_dataset.py#l55 @kaemo @akruszewski did you experience some issues with s3 too? **edit**: @akruszewski it is [the very same issue you encountered here](https:\/\/github.com\/akruszewski\/-\/commit\/41e9e3fdd2c54a774cca69e1cb52e26cadf50b1e)",
        "Issue_original_content_gpt_summary":"The user encountered an issue when registering a dataset in the catalog.yml, where the local path was Linux and the _tracking_uri was an Azure Blob Storage, resulting in an \"expected string or bytes-like object\" error when running ` run`.",
        "Issue_preprocessed_content":"Title: dataset fails to log on remote storage when underlying dataset filepath is converted as a pureposixpath; Content: when i register a dataset in the and i run i got a when the local path is linux and the is an azure blob storage . i don't know really why this append, but it can be fied by replacing by in these locations did you experience some issues with s too? edit it is"
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/72",
        "Issue_title":"mlflow.yml is not parsed properly when using TemplatedConfigLoader",
        "Issue_creation_time":1601411953000,
        "Issue_closed_time":1602948810000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When you have a global variable in the mlflow.yml file (e.g `mlruns: ${USER}\/mlruns`), the global variable is not replaced by its value even if the user has [registered a TemplatedConfigLoader](https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.config.TemplatedConfigLoader.html) in his project. This is due to `get_mlflow_config()` to manually recreate the default ConfigLoader.\r\n\r\nThis is part of the numerous issues that will  be fixed by #66.\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: .yml is not parsed properly when using templatedconfigloader; Content: when you have a global variable in the .yml file (e.g `s: ${user}\/s`), the global variable is not replaced by its value even if the user has [registered a templatedconfigloader](https:\/\/.readthedocs.io\/en\/stable\/.config.templatedconfigloader.html) in his project. this is due to `get__config()` to manually recreate the default configloader. this is part of the numerous issues that will be fixed by #66.",
        "Issue_original_content_gpt_summary":"The user encountered an issue with templatedconfigloader not properly parsing .yml files when a global variable is present, which is due to the manual recreation of the default configloader in `get__config()` and will be fixed by #66.",
        "Issue_preprocessed_content":"Title: is not parsed properly when using templatedconfigloader; Content: when you have a global variable in the file , the global variable is not replaced by its value even if the user has in his project. this is due to to manually recreate the default configloader. this is part of the numerous issues that will be fixed by ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/30",
        "Issue_title":"get_mlflow_config use the working directory instead of given path when called within load_context",
        "Issue_creation_time":1595365457000,
        "Issue_closed_time":1602948810000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"This may lead to strange behaviour when called in interactive mode in another place thant the kedro project root.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: get__config use the working directory instead of given path when called within load_context; Content: this may lead to strange behaviour when called in interactive mode in another place thant the project root.",
        "Issue_original_content_gpt_summary":"The user may experience strange behaviour when calling the get_config function within the load_context in interactive mode from a different directory than the Kedro project root.",
        "Issue_preprocessed_content":"Title: use the working directory instead of given path when called within ; Content: this may lead to strange behaviour when called in interactive mode in another place thant the project root."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/14",
        "Issue_title":"Warning message appears when calling ``kedro mlflow init``",
        "Issue_creation_time":1593379921000,
        "Issue_closed_time":1600718139000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The warning claims that the project is not initialised yet, and that you must call ``kedro mlflow init`` before calling any command while you are calling ``kedro mlflow init``. It can be safely ignored because the command works as intended. This bug is due to the dynamic creation of command.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: warning message appears when calling `` init``; Content: the warning cls that the project is not initialised yet, and that you must call `` init`` before calling any command while you are calling `` init``. it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command.",
        "Issue_original_content_gpt_summary":"The user encountered a warning message when calling `` init``, which can be safely ignored as the command works as intended, due to a bug caused by the dynamic creation of command.",
        "Issue_preprocessed_content":"Title: warning message appears when calling ; Content: the warning cls that the project is not initialised yet, and that you must call before calling any command while you are calling . it can be safely ignored because the command works as intended. this bug is due to the dynamic creation of command."
    },
    {
        "Issue_link":"https:\/\/github.com\/Galileo-Galilei\/kedro-mlflow\/issues\/10",
        "Issue_title":"Close mlflow run when a pipeline fails in interactive mode",
        "Issue_creation_time":1591562037000,
        "Issue_closed_time":1598336871000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"# Context\r\nToday, you can execute a kedro pipeline interactively. The logic would be to load the context, and then to run the pipeline.\r\n\r\n```python\r\nfrom kedro.context import load_context\r\nlocal_context = load_context(\".\")\r\nlocal_context.run(pipeline=local_context.pipelines[PIPELINE_NAME],\r\n                             catalog=local_context.catalog)\r\n```\r\n\r\n# Description\r\nIf the execution fails for some reason (bug in the pipeline), the mlflow run is not closed. This creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy.\r\n\r\nThis bug does not occur when running from the command line since the mlflow run is automatically closed when exiting.\r\n\r\n# Possible Implementation \r\nImplement a [``on_pipeline_error`` kedro ``Hook``](https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the mlflow run when the pipeline fails.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: close run when a pipeline fails in interactive mode; Content: # context today, you can execute a pipeline interactively. the logic would be to load the context, and then to run the pipeline. ```python from .context import load_context local_context = load_context(\".\") local_context.run(pipeline=local_context.pipelines[pipeline_name], catalog=local_context.catalog) ``` # description if the execution fails for some reason (bug in the pipeline), the run is not closed. this creates unintended side effects: for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy. this bug does not occur when running from the command line since the run is automatically closed when exiting. # possible implementation implement a [``on_pipeline_error`` ``hook``](https:\/\/.readthedocs.io\/en\/stable\/04_user_guide\/15_hooks.html?highlight=on_pipeline_error#hook-specification) to close the run when the pipeline fails.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the run was not closed when a pipeline failed in interactive mode, resulting in unintended side effects and a messy Mlflow database.",
        "Issue_preprocessed_content":"Title: close run when a pipeline fails in interactive mode; Content: context today, you can execute a pipeline interactively. the logic would be to load the context, and then to run the pipeline. description if the execution fails for some reason , the run is not closed. this creates unintended side effects for instance, if you rerun the pipeline, the new run will be nested in the failing runs and the mllflow database will become very messy. this bug does not occur when running from the command line since the run is automatically closed when exiting. possible implementation implement a to close the run when the pipeline fails."
    },
    {
        "Issue_link":"https:\/\/github.com\/omegaml\/omegaml\/issues\/258",
        "Issue_title":"running mlflow>1.28 projects causes mlflow not found error",
        "Issue_creation_time":1663358103000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"*Currently*\n\n* since mlflow 1.28, running mlflow projects form remote sources causes\n  *mlflow: not found* issue on starting the project\n\n*Reproduce*\n\n* run TestMLFlowProjects.test_mlflow_gitproject_remote_https\n\n*Expected*\n\n* running remote-sourced mlflow project is supported as previously",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: running >1.28 projects causes not found error; Content: *currently* * since 1.28, running projects form remote sources causes *: not found* issue on starting the project *reproduce* * run testprojects.test__gitproject_remote_https *expected* * running remote-sourced project is supported as previously",
        "Issue_original_content_gpt_summary":"The user encountered an issue when running projects from remote sources since version 1.28, resulting in a \"not found\" error when starting the project.",
        "Issue_preprocessed_content":"Title: running projects causes not found error; Content: currently since running projects form remote sources causes not found issue on starting the project reproduce run expected running project is supported as previously"
    },
    {
        "Issue_link":"https:\/\/github.com\/ugr-sail\/sinergym\/issues\/101",
        "Issue_title":"Experiments with mlflow don't work if MLFLOW_TRACKING_URI container variable specify an incorrect ip address",
        "Issue_creation_time":1639047613000,
        "Issue_closed_time":1639743100000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Execution get stuck if this case happens. It is necessary to manage this exception properly.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: experiments with don't work if _tracking_uri container variable specify an incorrect ip address; Content: execution get stuck if this case happens. it is necessary to manage this exception properly.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where experiments with don't work if the _tracking_uri container variable specifies an incorrect IP address, causing execution to get stuck and requiring proper exception management.",
        "Issue_preprocessed_content":"Title: experiments with don't work if container variable specify an incorrect ip address; Content: execution get stuck if this case happens. it is necessary to manage this exception properly."
    },
    {
        "Issue_link":"https:\/\/github.com\/prinz-nussknacker\/prinz\/issues\/78",
        "Issue_title":"Error when starting new experiment in mlflow",
        "Issue_creation_time":1608388229000,
        "Issue_closed_time":1610230183000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Error in pipeline in GithubActions\r\n`14:25:43.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDOUT: Starting Mlflow UI on port 5000\r\n14:25:46.430 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.453 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\r\n14:25:46.468 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: Error initializing backend store\r\n14:25:46.480 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.483 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.484 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.485 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.487 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.489 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.491 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.493 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.495 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.496 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.497 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.500 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.505 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.509 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.510 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.511 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: The above exception was the direct cause of the following exception:\r\n14:25:46.516 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _tracking_store = _tracking_store_registry.get_store(store_uri, artifact_root)\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1618, in _run_visitor\r\n14:25:46.517 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     conn._run_visitor(visitorcallable, element, **kwargs)\r\n14:25:46.518 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: (Background on this error at: http:\/\/sqlalche.me\/e\/gkpj)\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: ]\r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.541 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: )\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tFOREIGN KEY(experiment_id) REFERENCES experiments (experiment_id)\r\n14:25:46.542 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT runs_lifecycle_stage CHECK (lifecycle_stage IN ('active', 'deleted')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 INFO mlflow.store.db.utils: Updating database tables\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Will assume transactional DDL.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.\r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT status CHECK (status IN ('SCHEDULED', 'FAILED', 'FINISHED', 'RUNNING')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT source_type CHECK (source_type IN ('NOTEBOOK', 'JOB', 'LOCAL', 'UNKNOWN', 'PROJECT')), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tCONSTRAINT run_pk PRIMARY KEY (run_uuid), \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \texperiment_id INTEGER, \r\n14:25:46.543 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tartifact_uri VARCHAR(200), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tlifecycle_stage VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_version VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tend_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstart_time BIGINT, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tstatus VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tuser_id VARCHAR(256), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tentry_point_name VARCHAR(50), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_name VARCHAR(500), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tsource_type VARCHAR(20), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \tname VARCHAR(250), \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \trun_uuid VARCHAR(32) NOT NULL, \r\n14:25:46.548 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: sqlalchemy.exc.IntegrityError: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.549 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     raise value.with_traceback(tb)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 152, in reraise\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     reraise(type(exception), exception, tb=exc_tb, cause=cause)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/util\/compat.py\", line 398, in raise_from_cause\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     util.raise_from_cause(sqlalchemy_exception, exc_info)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1476, in _handle_dbapi_exception\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self._handle_dbapi_exception(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1249, in _execute_context\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     ret = self._execute_context(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1039, in _execute_ddl\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return connection._execute_ddl(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 72, in _execute_on_connection\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(self, multiparams, params)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 982, in execute\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.connection.execute(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 821, in visit_table\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.traverse_single(\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/ddl.py\", line 777, in visit_metadata\r\n14:25:46.550 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return meth(obj, **kw)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/visitors.py\", line 138, in traverse_single\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 2049, in _run_visitor\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     bind._run_visitor(\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/sql\/schema.py\", line 4315, in create_all\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     InitialBase.metadata.create_all(engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/db\/utils.py\", line 30, in _initialize_tables\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     mlflow.store.db.utils._initialize_tables(self.engine)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/sqlalchemy_store.py\", line 99, in __init__\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return SqlAlchemyStore(store_uri, artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 64, in _get_sqlalchemy_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     return builder(store_uri=store_uri, artifact_uri=artifact_uri)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/registry.py\", line 37, in get_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 91, in _get_tracking_store\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     _get_tracking_store(backend_store_uri, default_artifact_root)\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/server\/handlers.py\", line 105, in initialize_backend_stores\r\n14:25:46.564 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     initialize_backend_stores(backend_store_uri, default_artifact_root)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/mlflow\/cli.py\", line 291, in server\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     cursor.execute(statement, parameters)\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/default.py\", line 588, in do_execute\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:     self.dialect.do_execute(\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR:   File \"\/usr\/local\/lib\/python3.8\/site-packages\/sqlalchemy\/engine\/base.py\", line 1245, in _execute_context\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: Traceback (most recent call last):\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: CREATE TABLE runs (\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: [SQL: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: \r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: DETAIL:  Key (typname, typnamespace)=(runs, 2200) already exists.\r\n14:25:46.565 [ducttape-1] DEBUG org.testcontainers.containers.output.WaitingConsumer - STDERR: 2020\/12\/19 14:25:46 ERROR mlflow.cli: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"`\r\nwhich causes test to not pass",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: error when starting new experiment in ; Content: error in pipeline in githubactions which causes test to not pass",
        "Issue_original_content_gpt_summary":"The user encountered an error in the pipeline in GitHub Actions which caused their tests to not pass when starting a new experiment.",
        "Issue_preprocessed_content":"Title: error when starting new experiment in ; Content: error in pipeline in githubactions which causes test to not pass"
    },
    {
        "Issue_link":"https:\/\/github.com\/getindata\/kedro-kubeflow\/issues\/102",
        "Issue_title":"Plugin only compatible with kedro-mlflow<0.8.0",
        "Issue_creation_time":1643989114000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```python\r\ndef is_mlflow_enabled() -> bool:\r\n    try:\r\n        import mlflow  # NOQA\r\n        from kedro_mlflow.framework.context import get_mlflow_config  # NOQA\r\n        return True\r\n    except ImportError:\r\n        return False\r\n```\r\nalway throws exception since `context` package has been moved or refactored",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: plugin only compatible with -<0.8.0; Content: ```python def is__enabled() -> bool: try: import # noqa from _.framework.context import get__config # noqa return true except importerror: return false ``` alway throws exception since `context` package has been moved or refactored",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where a plugin was only compatible with versions lower than 0.8.0, and the `context` package had been moved or refactored, causing the plugin to always throw an exception.",
        "Issue_preprocessed_content":"Title: plugin only compatible with ; Content: alway throws exception since package has been moved or refactored"
    },
    {
        "Issue_link":"https:\/\/github.com\/nyanp\/nyaggle\/issues\/19",
        "Issue_title":"experiment_gbdt raise errors with long parameters and mlflow",
        "Issue_creation_time":1580251523000,
        "Issue_closed_time":1580353817000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"mlflow raises error if length of key\/value exceeds 250. If the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception.\r\n\r\nPossible option:\r\n- catch and ignore all errors from mlflow\r\n- truncate logging parameters automatically ",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: experiment_gbdt raise errors with long parameters and ; Content: raises error if length of key\/value exceeds 250. if the length of gbdt parameters or cat_columns is long, experiment_gbdt will raise an exception. possible option: - catch and ignore all errors from - truncate logging parameters automatically",
        "Issue_original_content_gpt_summary":"The user encountered challenges when using the experiment_gbdt function, as it raised errors if the length of the key\/value exceeded 250, or if the length of the gbdt parameters or cat_columns was too long.",
        "Issue_preprocessed_content":"Title: raise errors with long parameters and ; Content: raises error if length of exceeds . if the length of gbdt parameters or is long, will raise an exception. possible option catch and ignore all errors from truncate logging parameters automatically"
    },
    {
        "Issue_link":"https:\/\/github.com\/artefactory\/one-click-mlflow\/issues\/79",
        "Issue_title":"make one-click-mlflow not working after make destroy because of undeleted bucket",
        "Issue_creation_time":1633706491000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\nProblem encountered by @ucsky. Running ```make one-click-mlflow``` is not working after ```make destroy``` because of the artifacts' bucket which still exists.\r\nGot the following error:\r\n````\r\n\r\nSetting up your GCP project...\r\n\u2577\r\n\u2502 Error: googleapi: Error 409: You already own this bucket. Please select another name., conflict\r\n\u2502 \r\n\u2502   with module.bucket_backend.google_storage_bucket.this,\r\n\u2502   on ..\/modules\/mlflow\/artifacts\/main.tf line 18, in resource \"google_storage_bucket\" \"this\":\r\n\u2502   18: resource \"google_storage_bucket\" \"this\" {\r\n\r\n````\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. run ```make one-click-mlflow``` and finish it\r\n2. run ```make destroy```\r\n3. run ```make one-click-mlflow```\r\n4. See error\r\n\r\n**Expected behavior**\r\nThe second command ```make one-click-mlflow``` should work \r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: make one-click- not working after make destroy because of undeleted bucket; Content: **describe the bug** problem encountered by @ucsky. running ```make one-click-``` is not working after ```make destroy``` because of the artifacts' bucket which still exists. got the following error: ```` setting up your gcp project... error: googleapi: error 409: you already own this bucket. please select another name., conflict with module.bucket_backend.google_storage_bucket.this, on ..\/modules\/\/artifacts\/main.tf line 18, in resource \"google_storage_bucket\" \"this\": 18: resource \"google_storage_bucket\" \"this\" { ```` **to reproduce** steps to reproduce the behavior: 1. run ```make one-click-``` and finish it 2. run ```make destroy``` 3. run ```make one-click-``` 4. see error **expected behavior** the second command ```make one-click-``` should work",
        "Issue_original_content_gpt_summary":"The user @ucsky encountered a challenge where running \"make one-click-\" was not working after \"make destroy\" due to an undeleted bucket.",
        "Issue_preprocessed_content":"Title: make not working after make destroy because of undeleted bucket; Content: describe the bug problem encountered by running is not working after because of the artifacts' bucket which still exists. got the following error ` should work"
    },
    {
        "Issue_link":"https:\/\/github.com\/canonical\/mlflow-operator\/issues\/24",
        "Issue_title":"MLFlow hardcoded bucket name - impossible to use MLFlow with AWS S3",
        "Issue_creation_time":1646132216000,
        "Issue_closed_time":1647350309000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"https:\/\/github.com\/canonical\/mlflow-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/mlflow-server\/src\/charm.py#L20\r\n\r\nThe name of the bucket for MLFlow is hardcoded. This is a big issue because this makes using Minio in Gateway mode + MLFlow impossible on AWS (S3 buckets are globally unique).\r\n\r\nIt's a good first issue :)",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: hardcoded bucket name - impossible to use with aws s3; Content: https:\/\/github.com\/canonical\/-operator\/blob\/c856446074868d4735627c95878960d91555f4da\/charms\/-server\/src\/charm.py#l20 the name of the bucket for is hardcoded. this is a big issue because this makes using minio in gateway mode + impossible on aws (s3 buckets are globally unique). it's a good first issue :)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the bucket name was hardcoded, making it impossible to use with AWS S3, which requires globally unique bucket names.",
        "Issue_preprocessed_content":"Title: hardcoded bucket name impossible to use with aws s ; Content: the name of the bucket for is hardcoded. this is a big issue because this makes using minio in gateway mode + impossible on aws . it's a good first issue"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/35",
        "Issue_title":"[mlflow] Migration Job should run before upgrade",
        "Issue_creation_time":1660748480000,
        "Issue_closed_time":1660908206000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen trying to install mlflow chart I'm trying to migrate from old mlflow version to the new one. I'm using `backendStore.databaseMigration: true` value for that. But mlflow pod failed to start with error:\r\n```\r\nmlflow.exceptions.MlflowException: Detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). Take a backup of your database, then run 'mlflow db upgrade <database_uri>' to migrate your database to the latest schema. NOTE: schema migration may result in database downtime - please consult your database's documentation for more detail.\r\n```\r\n\r\nFrom the looks of things migration Job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but I can be wrong here. \r\n\r\nRunning Job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release.\r\n\r\nThanks!\n\n### What's your helm version?\n\nv3.9.3\n\n### What's your kubectl version?\n\nv1.24.3\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.6.0\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\nDB migration job should run before mlflow pod upgrade. \n\n### How to reproduce it?\n\n1. Install mlflow with old DB schema (1.23.1)\r\n2. Try to upgrade with 0.6.0 helm chart\n\n### Enter the changed values of values.yaml?\n\n```\r\nmlflow:\r\n  nodeSelector:\r\n    redacted: Shared\r\n  \r\n  ingress:\r\n    enabled: true\r\n  \r\n  artifactRoot:\r\n    s3:\r\n      enabled: true\r\n      bucket: \"redacted\"\r\n      awsAccessKeyId: \"\"\r\n      awsSecretAccessKey: \"\"\r\n  \r\n  extraEnvVars:\r\n    AWS_DEFAULT_REGION: eu-central-1\r\n    MLFLOW_S3_ENDPOINT_URL: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com\r\n  \r\n  backendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    mysql:\r\n      enabled: true\r\n      host: \"redacted.eu-central-1.rds.amazonaws.com\"\r\n      database: \"mlflow\"\r\n      user: \"\"\r\n      password: \"\"\r\n```\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services\n\n### Anything else we need to know?\n\nChart was installed as a part of another umbrella chart",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [] migration job should run before upgrade; Content: ### describe the bug a clear and concise description of what the bug is. when trying to install chart i'm trying to migrate from old version to the new one. i'm using `backendstore.databasemigration: true` value for that. but pod failed to start with error: ``` .exceptions.exception: detected out-of-date database schema (found version c48cb773bb87, but expected cc1f77228345). take a backup of your database, then run ' db upgrade ' to migrate your database to the latest schema. note: schema migration may result in database downtime - please consult your database's documentation for more detail. ``` from the looks of things migration job should have `pre-install,pre-upgrade` hooks instead of `post-install,post-upgrade` but i can be wrong here. running job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release. thanks! ### what's your helm version? v3.9.3 ### what's your kubectl version? v1.24.3 ### which chart? ### what's the chart version? 0.6.0 ### what happened? _no response_ ### what you expected to happen? db migration job should run before pod upgrade. ### how to reproduce it? 1. install with old db schema (1.23.1) 2. try to upgrade with 0.6.0 helm chart ### enter the changed values of values.yaml? ``` : nodeselector: redacted: shared ingress: enabled: true artifactroot: s3: enabled: true bucket: \"redacted\" awsaccesskeyid: \"\" awssecretaccesskey: \"\" extraenvvars: aws_default_region: eu-central-1 _s3_endpoint_url: https:\/\/bucket.redacted.s3.eu-central-1.vpce.amazonaws.com backendstore: databasemigration: true databaseconnectioncheck: true mysql: enabled: true host: \"redacted.eu-central-1.rds.amazonaws.com\" database: \"\" user: \"\" password: \"\" ``` ### enter the command that you execute and failing\/misfunctioning. helm upgrade --install --values override.yaml --wait --create-namespace --atomic --timeout 15m0s -f secrets:\/\/secrets.yaml shared-services .\/shared-services ### anything else we need to know? chart was installed as a part of another umbrella chart",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to migrate from an old version to a new one using the `backendstore.databasemigration: true` value, where the pod failed to start due to an out-of-date database schema, and the migration job should have had pre-install and pre-upgrade hooks instead of post-install and post-upgrade.",
        "Issue_preprocessed_content":"Title: migration job should run before upgrade; Content: describe the bug a clear and concise description of what the bug is. when trying to install chart i'm trying to migrate from old version to the new one. i'm using value for that. but pod failed to start with error from the looks of things migration job should have hooks instead of but i can be wrong here. running job from the chart manually with kubectl fixed this issue for me, but it will probably appear with the next release. thanks! what's your helm version? what's your kubectl version? which chart? what's the chart version? what happened? what you expected to happen? db migration job should run before pod upgrade. how to reproduce it? . install with old db schema . try to upgrade with helm chart enter the changed values of enter the command that you execute and helm upgrade m s f anything else we need to know? chart was installed as a part of another umbrella chart"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/32",
        "Issue_title":"[mlflow] model artifacts not saved in remote s3 artifact store",
        "Issue_creation_time":1660152345000,
        "Issue_closed_time":1660345866000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\r\n\r\nI have local minikube cluster. I installed the helm chart with some changed settings. See below for the changed values. Everthing else is same as per default values yaml file. For db backend I am using `bitnami\/postgresql` and for s3 storage minio instance. I also have created a initial bucket named \"mlflow\" in minio. \r\n\r\nAnd then I created a simple k8s pod to run the simple training example from mlflow docs. This pod has env variables set as : `MLFLOW_TRACKING_URI=http:\/\/mlflow.airflow.svc.cluster.local:5000` [Here ](https:\/\/raw.githubusercontent.com\/mlflow\/mlflow\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. I can see the metadata about the model in UI however , artifact section in UI is empty and also the bucket is empty. \r\n\r\n### What's your helm version?\r\n\r\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\r\n\r\n### What's your kubectl version?\r\n\r\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/amd64\"}\r\n\r\n### Which chart?\r\n\r\nmlflow\r\n\r\n### What's the chart version?\r\n\r\nlatest\r\n\r\n### What happened?\r\n\r\n_No response_\r\n\r\n### What you expected to happen?\r\n\r\nI would expect the artifacts in minio bucket.\r\n\r\n### How to reproduce it?\r\n\r\ninstall the helm chart with minio and postgresql config. Run a simple exmple frpom docs. \r\n\r\n### Enter the changed values of values.yaml?\r\n\r\n```\r\nbackendStore:\r\n    databaseMigration: true\r\n    databaseConnectionCheck: true\r\n    postgres:\r\n      enabled: true\r\n      host: mlflow-postgres-postgresql.airflow.svc.cluster.local\r\n      database: mlflow_db\r\n      user: mlflow\r\n      password: mlflow\r\nartifactRoot:\r\n  proxiedArtifactStorage: true\r\n  s3:\r\n    enabled: true\r\n    bucket: mlflow\r\n    awsAccessKeyId: {{ requiredEnv \"MINIO_USERNAME\" }}\r\n    awsSecretAccessKey: {{ requiredEnv \"MINIO_PASSWORD\" }}\r\nextraEnvVars:\r\n  MLFLOW_S3_ENDPOINT_URL: minio.airflow.svc.cluster.local\r\n```\r\n\r\n### Enter the command that you execute and failing\/misfunctioning.\r\n\r\nhelm install mlflow-release community-charts\/mlflow --values values.yaml\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [] model artifacts not saved in remote s3 artifact store; Content: ### describe the bug a clear and concise description of what the bug is. i have local minikube cluster. i installed the helm chart with some changed settings. see below for the changed values. everthing else is same as per default values yaml file. for db backend i am using `bitnami\/postgresql` and for s3 storage minio instance. i also have created a initial bucket named \"\" in minio. and then i created a simple k8s pod to run the simple training example from docs. this pod has env variables set as : `_tracking_uri=http:\/\/.airflow.svc.cluster.local:5000` [here ](https:\/\/raw.githubusercontent.com\/\/\/master\/examples\/sklearn_elasticnet_wine\/train.py) is the link to that code. i can see the metadata about the model in ui however , artifact section in ui is empty and also the bucket is empty. ### what's your helm version? version.buildinfo{version:\"v3.9.0\", gitcommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", gittreestate:\"clean\", goversion:\"go1.17.5\"} ### what's your kubectl version? server version: version.info{major:\"1\", minor:\"23\", gitversion:\"v1.23.3\", gitcommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", gittreestate:\"clean\", builddate:\"2022-01-25t21:19:12z\", goversion:\"go1.17.6\", compiler:\"gc\", platform:\"linux\/amd64\"} ### which chart? ### what's the chart version? latest ### what happened? _no response_ ### what you expected to happen? i would expect the artifacts in minio bucket. ### how to reproduce it? install the helm chart with minio and postgresql config. run a simple exmple frpom docs. ### enter the changed values of values.yaml? ``` backendstore: databasemigration: true databaseconnectioncheck: true postgres: enabled: true host: -postgres-postgresql.airflow.svc.cluster.local database: _db user: password: artifactroot: proxiedartifactstorage: true s3: enabled: true bucket: awsaccesskeyid: {{ requiredenv \"minio_username\" }} awssecretaccesskey: {{ requiredenv \"minio_password\" }} extraenvvars: _s3_endpoint_url: minio.airflow.svc.cluster.local ``` ### enter the command that you execute and failing\/misfunctioning. helm install -release community-charts\/ --values values.yaml ### anything else we need to know? _no response_",
        "Issue_original_content_gpt_summary":"The user encountered challenges with their local minikube cluster when attempting to install a helm chart with changed settings, resulting in model artifacts not being saved in the remote s3 artifact store.",
        "Issue_preprocessed_content":"Title: model artifacts not saved in remote s artifact store; Content: describe the bug a clear and concise description of what the bug is. i have local minikube cluster. i installed the helm chart with some changed settings. see below for the changed values. everthing else is same as per default values yaml file. for db backend i am using and for s storage minio instance. i also have created a initial bucket named in minio. and then i created a simple k s pod to run the simple training example from docs. this pod has env variables set as is the link to that code. i can see the metadata about the model in ui however , artifact section in ui is empty and also the bucket is empty. what's your helm version? gitcommit ceeda c a a d cd f d b a , gittreestate clean , what's your kubectl version? server version minor , gitcommit c ab cff a c eccca f e e d , gittreestate clean , compiler gc , which chart? what's the chart version? latest what happened? what you expected to happen? i would expect the artifacts in minio bucket. how to reproduce it? install the helm chart with minio and postgresql config. run a simple exmple frpom docs. enter the changed values of enter the command that you execute and helm install anything else we need to know?"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/22",
        "Issue_title":"[mlflow] Use port name instead of port number in ServiceMonitor ",
        "Issue_creation_time":1658844949000,
        "Issue_closed_time":1658854769000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nFirst of all, thanks to everyone creating this Helm Chart as it is really good and easy to use.\r\n\r\nHowever, I encountered a problem when choosing to include ServiceMonitor and Prometheus metrics along the Deployment. Generally, the created ServiceMonitor for MLFlow is correct, yet in the current form it does not work for me.\r\nI use the latest Prometheus deployed using the official Helm Chart and the MLFlow metrics did not show up in the Targets, yet it was visible in Service Discovery panel in Prometheus Dashboard, but appeared as `0\/1 active targets`.\r\n\r\nAfter a couple of hours of educated debugging I changed manually the `targetPort: 80` to `port: http` in the deployed ServiceMonitor manifest. It worked straightaway! \r\n\r\n\r\nWhat I propose is a simple fix:\r\nAccording to official Prometheus Troubleshooting docs the port specified in ServiceMonitor should use `name` instead of port number ([Link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/Documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) \r\nSimple fix would be to change `targetPort: 80` to `port: http` in `templates\/servicemonitor.yaml`. Port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name.\r\nI am aware that port number of type Integer should also work...\r\n\n\n### What's your helm version?\n\n3.6.0\n\n### What's your kubectl version?\n\n1.19\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.21\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\n helm install --namespace mlflow mlflow-tracking-server community-charts\/mlflow --set serviceMonitor.enabled=true\n\n### Anything else we need to know?\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [] use port name instead of port number in servicemonitor ; Content: ### describe the bug a clear and concise description of what the bug is. first of all, thanks to everyone creating this helm chart as it is really good and easy to use. however, i encountered a problem when choosing to include servicemonitor and prometheus metrics along the deployment. generally, the created servicemonitor for is correct, yet in the current form it does not work for me. i use the latest prometheus deployed using the official helm chart and the metrics did not show up in the targets, yet it was visible in service discovery panel in prometheus dashboard, but appeared as `0\/1 active targets`. after a couple of hours of educated debugging i changed manually the `targetport: 80` to `port: http` in the deployed servicemonitor manifest. it worked straightaway! what i propose is a simple fix: according to official prometheus troubleshooting docs the port specified in servicemonitor should use `name` instead of port number ([link to docs](https:\/\/github.com\/prometheus-operator\/prometheus-operator\/blob\/main\/documentation\/troubleshooting.md#using-textual-port-number-instead-of-port-name)) simple fix would be to change `targetport: 80` to `port: http` in `templates\/servicemonitor.yaml`. port name `http` is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name. i am aware that port number of type integer should also work... ### what's your helm version? 3.6.0 ### what's your kubectl version? 1.19 ### which chart? ### what's the chart version? 0.2.21 ### what happened? _no response_ ### what you expected to happen? _no response_ ### how to reproduce it? _no response_ ### enter the changed values of values.yaml? _no response_ ### enter the command that you execute and failing\/misfunctioning. helm install --namespace -tracking-server community-charts\/ --set servicemonitor.enabled=true ### anything else we need to know? _no response_",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using the servicemonitor and prometheus metrics along with the helm chart, where the port specified in the servicemonitor was not working correctly, and a fix was proposed to use the port name instead of port number.",
        "Issue_preprocessed_content":"Title: use port name instead of port number in servicemonitor ; Content: describe the bug a clear and concise description of what the bug is. first of all, thanks to everyone creating this helm chart as it is really good and easy to use. however, i encountered a problem when choosing to include servicemonitor and prometheus metrics along the deployment. generally, the created servicemonitor for is correct, yet in the current form it does not work for me. i use the latest prometheus deployed using the official helm chart and the metrics did not show up in the targets, yet it was visible in service discovery panel in prometheus dashboard, but appeared as . after a couple of hours of educated debugging i changed manually the to in the deployed servicemonitor manifest. it worked straightaway! what i propose is a simple fix according to official prometheus troubleshooting docs the port specified in servicemonitor should use instead of port number simple fix would be to change to in . port name is already hardcoded, so can be used directly or new parameter could be introduced to give the freedom to choose port name. i am aware that port number of type integer should also what's your helm version? what's your kubectl version? . which chart? what's the chart version? what happened? what you expected to happen? how to reproduce it? enter the changed values of enter the command that you execute and helm install anything else we need to know?"
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/18",
        "Issue_title":"[mlflow] Extra args broken",
        "Issue_creation_time":1657550069000,
        "Issue_closed_time":1657616964000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nThe new staticPrefix argument being under extraArgs breaks the chart for users that need to use the extraArgs\n\n### What's your helm version?\n\nversion.BuildInfo{Version:\"v3.8.1\", GitCommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", GitTreeState:\"clean\", GoVersion:\"go1.17.8\"}\n\n### What's your kubectl version?\n\nClient Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.5\", GitCommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", GitTreeState:\"clean\", BuildDate:\"2021-12-16T08:38:33Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"darwin\/arm64\"} Server Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.3\", GitCommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T21:19:12Z\", GoVersion:\"go1.17.6\", Compiler:\"gc\", Platform:\"linux\/arm64\"}\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.2.7\n\n### What happened?\n\nThe newly added staticPrefix parameter under extraArgs breaks the chart when used because it tries to add an extra argument to the mlflow server command that doesnt exist.\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nhelm install -f mlflow\/values.yaml mlflow .\/mlflow\/\n\n### Anything else we need to know?\n\nI am just creating a pull request to address this in a bit different way and havent tested it yet. Just wanted to create a request to highlight a solution.\r\n\r\nYou could also handle the staticPrefix as a separate argument in the extraEnv when starting up the mlflow server to make this work smoother for a final user, but this solution should work as well.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [] extra args broken; Content: ### describe the bug a clear and concise description of what the bug is. the new staticprefix argument being under extraargs breaks the chart for users that need to use the extraargs ### what's your helm version? version.buildinfo{version:\"v3.8.1\", gitcommit:\"5cb9af4b1b271d11d7a97a71df3ac337dd94ad37\", gittreestate:\"clean\", goversion:\"go1.17.8\"} ### what's your kubectl version? client version: version.info{major:\"1\", minor:\"22\", gitversion:\"v1.22.5\", gitcommit:\"5c99e2ac2ff9a3c549d9ca665e7bc05a3e18f07e\", gittreestate:\"clean\", builddate:\"2021-12-16t08:38:33z\", goversion:\"go1.16.12\", compiler:\"gc\", platform:\"darwin\/arm64\"} server version: version.info{major:\"1\", minor:\"23\", gitversion:\"v1.23.3\", gitcommit:\"816c97ab8cff8a1c72eccca1026f7820e93e0d25\", gittreestate:\"clean\", builddate:\"2022-01-25t21:19:12z\", goversion:\"go1.17.6\", compiler:\"gc\", platform:\"linux\/arm64\"} ### which chart? ### what's the chart version? 0.2.7 ### what happened? the newly added staticprefix parameter under extraargs breaks the chart when used because it tries to add an extra argument to the server command that doesnt exist. ### what you expected to happen? _no response_ ### how to reproduce it? _no response_ ### enter the changed values of values.yaml? _no response_ ### enter the command that you execute and failing\/misfunctioning. helm install -f \/values.yaml .\/\/ ### anything else we need to know? i am just creating a pull request to address this in a bit different way and havent tested it yet. just wanted to create a request to highlight a solution. you could also handle the staticprefix as a separate argument in the extraenv when starting up the server to make this work smoother for a final user, but this solution should work as well.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the newly added staticprefix parameter under extraargs breaks the chart when used, causing the chart to fail when executing the helm install command.",
        "Issue_preprocessed_content":"Title: extra args broken; Content: describe the bug a clear and concise description of what the bug is. the new staticprefix argument being under extraargs breaks the chart for users that need to use the extraargs what's your helm version? gitcommit cb af b b d d a a df ac dd ad , gittreestate clean , what's your kubectl version? client version minor , gitcommit c e ac ff a c d ca e bc a e f e , gittreestate clean , compiler gc , server version minor , gitcommit c ab cff a c eccca f e e d , gittreestate clean , compiler gc , which chart? what's the chart version? what happened? the newly added staticprefix parameter under extraargs breaks the chart when used because it tries to add an extra argument to the server command that doesnt exist. what you expected to happen? how to reproduce it? enter the changed values of enter the command that you execute and helm install f anything else we need to know? i am just creating a pull request to address this in a bit different way and havent tested it yet. just wanted to create a request to highlight a solution. you could also handle the staticprefix as a separate argument in the extraenv when starting up the server to make this work smoother for a final user, but this solution should work as well."
    },
    {
        "Issue_link":"https:\/\/github.com\/community-charts\/helm-charts\/issues\/2",
        "Issue_title":"[mlflow] Run chart-testing (lint) step returns Error validating maintainer 404 Not Found error",
        "Issue_creation_time":1656578904000,
        "Issue_closed_time":1656583953000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### Describe the bug a clear and concise description of what the bug is.\n\nWhen we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#L60) file getting the following error.\r\n\r\n```\r\nError: Error linting charts: Error processing charts\r\n------------------------------------------------------------------------------------------------------------------------\r\n \u2716\ufe0e mlflow => (version: \"0.1.47\", path: \"charts\/mlflow\") > Error validating maintainer 'Burak Ince': 404 Not Found\r\n------------------------------------------------------------------------------------------------------------------------\r\n```\r\n\r\nBecause of maintainer name for the `ct lint` command must be a GitHub username rather than a real name.\n\n### What's your helm version?\n\nv3.9.0\n\n### What's your kubectl version?\n\nv1.24.2\n\n### Which chart?\n\nmlflow\n\n### What's the chart version?\n\n0.1.47\n\n### What happened?\n\n_No response_\n\n### What you expected to happen?\n\n_No response_\n\n### How to reproduce it?\n\n_No response_\n\n### Enter the changed values of values.yaml?\n\n_No response_\n\n### Enter the command that you execute and failing\/misfunctioning.\n\nct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml\n\n### Anything else we need to know?\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [] run chart-testing (lint) step returns error validating maintainer 404 not found error; Content: ### describe the bug a clear and concise description of what the bug is. when we open a pull request, chart-testing (lint) step in [release.yaml](https:\/\/github.com\/community-charts\/helm-charts\/blob\/main\/.github\/workflows\/release.yml#l60) file getting the following error. ``` error: error linting charts: error processing charts ------------------------------------------------------------------------------------------------------------------------ => (version: \"0.1.47\", path: \"charts\/\") > error validating maintainer 'burak ince': 404 not found ------------------------------------------------------------------------------------------------------------------------ ``` because of maintainer name for the `ct lint` command must be a github username rather than a real name. ### what's your helm version? v3.9.0 ### what's your kubectl version? v1.24.2 ### which chart? ### what's the chart version? 0.1.47 ### what happened? _no response_ ### what you expected to happen? _no response_ ### how to reproduce it? _no response_ ### enter the changed values of values.yaml? _no response_ ### enter the command that you execute and failing\/misfunctioning. ct lint --debug --config .\/.github\/configs\/ct-lint.yaml --lint-conf .\/.github\/configs\/lintconf.yaml ### anything else we need to know? _no response_",
        "Issue_original_content_gpt_summary":"The user encountered an error when running the chart-testing (lint) step in the release.yaml file, resulting in an error validating the maintainer 'burak ince' with a 404 not found error.",
        "Issue_preprocessed_content":"Title: run step returns error validating maintainer not found error; Content: describe the bug a clear and concise description of what the bug is. when we open a pull request, step in file getting the following error. because of maintainer name for the command must be a github username rather than a real name. what's your helm version? what's your kubectl version? which chart? what's the chart version? what happened? what you expected to happen? how to reproduce it? enter the changed values of enter the command that you execute and ct lint anything else we need to know?"
    },
    {
        "Issue_link":"https:\/\/github.com\/PacktPublishing\/Machine-Learning-Engineering-with-MLflow\/issues\/7",
        "Issue_title":"Chapter 7 `mlflow run . --experiement-name=psystock_data_pipelines` fails - BUGFIX",
        "Issue_creation_time":1636550432000,
        "Issue_closed_time":1637063606000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Instructions \r\nPage 133 of chapter 7 requires the reader to navigate to the following directory and enter the commands below:\r\n\r\n`cd Chapter07\/psystock-data-features-main`\r\n `mlflow run . --experiement-name=psystock_data_pipelines`\r\n\r\n## Problem\r\n\r\nThe following error message appears when running line of code specified above:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"feature_set_generation.py\", line 30, in <module>\r\n    raise Exception('x should not exceed 5. The value of x was: {}'.format(x))\r\nNameError: name 'x' is not defined\r\n```\r\n\r\n## Solution\r\nResolve this by deleting line 30 below in `feature_set_generation.py`\r\n\r\n`30         raise Exception('x should not exceed 5. The value of x was: {}'.format(x))`\r\nThe stray `raise` statement is referencing an undefined variable `x`.\r\n\r\nRemoving this line of code removed the reference to this point and lead to the successful deployment of the experiment. I would consider adding such assertions in the `check_verify_data.py` file instead.\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: chapter 7 ` run . --experiement-name=psystock_data_pipelines` fails - bugfix; Content: ## instructions page 133 of chapter 7 requires the reader to navigate to the following directory and enter the commands below: `cd chapter07\/psystock-data-features-main` ` run . --experiement-name=psystock_data_pipelines` ## problem the following error message appears when running line of code specified above: ``` traceback (most recent call last): file \"feature_set_generation.py\", line 30, in raise exception('x should not exceed 5. the value of x was: {}'.format(x)) nameerror: name 'x' is not defined ``` ## solution resolve this by deleting line 30 below in `feature_set_generation.py` `30 raise exception('x should not exceed 5. the value of x was: {}'.format(x))` the stray `raise` statement is referencing an undefined variable `x`. removing this line of code removed the reference to this point and lead to the successful deployment of the experiment. i would consider adding such assertions in the `check_verify_data.py` file instead.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where running the command `run . --experiement-name=psystock_data_pipelines` in the directory `chapter07\/psystock-data-features-main` resulted in an error message due to a stray `raise` statement referencing an undefined variable.",
        "Issue_preprocessed_content":"Title: chapter fails bugfix; Content: instructions page of chapter requires the reader to navigate to the following directory and enter the commands below problem the following error message appears when running line of code specified above solution resolve this by deleting line below in the stray statement is referencing an undefined variable . removing this line of code removed the reference to this point and lead to the successful deployment of the experiment. i would consider adding such assertions in the file instead."
    },
    {
        "Issue_link":"https:\/\/github.com\/sash-a\/es_pytorch\/issues\/8",
        "Issue_title":"Improve mlflow logging for population",
        "Issue_creation_time":1601310463000,
        "Issue_closed_time":1601714116000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Separate each individuals performance into its own graph.\r\n\r\n- [x] graphs for each individual (simply append pop-idx to each graph)\r\n- [x] sub runs on mlflow",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: improve logging for population; Content: separate each individuals performance into its own graph. - [x] graphs for each individual (simply append pop-idx to each graph) - [x] sub runs on",
        "Issue_original_content_gpt_summary":"The user encountered the challenge of improving logging for population by separating each individual's performance into its own graph, which was solved by appending the population index to each graph.",
        "Issue_preprocessed_content":"Title: improve logging for population; Content: separate each individuals performance into its own graph. graphs for each individual sub runs on"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18146",
        "Issue_title":"MLflow fails to log to a tracking server",
        "Issue_creation_time":1657876344000,
        "Issue_closed_time":1662562977000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"### System Info\r\n\r\nPython 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)\r\n\r\nprint(transformers.__version__)\r\n4.20.1\r\n\r\nprint(mlflow.__version__)\r\n1.27.0\r\n\r\n### Who can help?\r\n\r\n_No response_\r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [X] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1. Install mlflow\r\n2. Configure a vanilla training job to use a tracking server (os.environ[\"MLFLOW_TRACKING_URI\"]=\"...\")\r\n3. Run the job\r\n\r\nYou should see an error similar to:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/train.py\", line 45, in <module>\r\n    trainer.train()\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train\r\n    return inner_training_loop(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop\r\n    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin\r\n    return self.call_event(\"on_train_begin\", args, state, control)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event\r\n    result = getattr(callback, event)(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin\r\n    self.setup(args, state, model)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup\r\n    self._ml_flow.log_params(dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH]))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'logging_nan_inf_filter', 'value': 'True'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'None'}, {'key': 'save_on_each_node', 'value': 'False'}, {'key': 'no_cuda', 'value': 'False'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'None'}, {'key': 'jit_mode_eval', 'value': 'False'}, {'key': 'use_ipex', 'value': 'False'}, {'key': 'bf16', 'value': 'False'}, {'key': 'fp16', 'value': 'False'}, {'key': 'fp16_opt_level', 'value': 'O1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'False'}, {'key': 'fp16_full_eval', 'value': 'False'}, {'key': 'tf32', 'value': 'None'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'None'}, {'key': 'tpu_num_cores', 'value': 'None'}, {'key': 'tpu_metrics_debug', 'value': 'False'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'False'}, {'key': 'eval_steps', 'value': 'None'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'False'}, {'key': 'remove_unused_columns', 'value': 'True'}, {'key': 'label_names', 'value': 'None'}, {'key': 'load_best_model_at_end', 'value': 'False'}, {'key': 'metric_for_best_model', 'value': 'None'}, {'key': 'greater_is_better', 'value': 'None'}, {'key': 'ignore_data_skip', 'value': 'False'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'None'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'False'}, {'key': 'group_by_length', 'value': 'False'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['mlflow']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'None'}, {'key': 'ddp_bucket_cap_mb', 'value': 'None'}, {'key': 'dataloader_pin_memory', 'value': 'True'}, {'key': 'skip_memory_metrics', 'value': 'True'}, {'key': 'use_legacy_prediction_loop', 'value': 'False'}, {'key': 'push_to_hub', 'value': 'False'}, {'key': 'resume_from_checkpoint', 'value': 'None'}, {'key': 'hub_model_id', 'value': 'None'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': '<HUB_TOKEN>'}, {'key': 'hub_private_repo', 'value': 'False'}, {'key': 'gradient_checkpointing', 'value': 'False'}, {'key': 'include_inputs_for_metrics', 'value': 'False'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'None'}, {'key': 'push_to_hub_organization', 'value': 'None'}, {'key': 'push_to_hub_token', 'value': '<PUSH_TO_HUB_TOKEN>'}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'False'}, {'key': 'full_determinism', 'value': 'False'}, {'key': 'torchdynamo', 'value': 'None'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\r\n```\r\n\r\nTraining script:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nfrom datasets import load_dataset, load_metric\r\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification\r\n\r\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test'])\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=2)\r\n\r\nmetric = load_metric(\"accuracy\")\r\n\r\ndef compute_metrics(eval_pred):\r\n    logits, labels = eval_pred\r\n    predictions = np.argmax(logits, axis=-1)\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\nos.environ[\"HF_MLFLOW_LOG_ARTIFACTS\"]=\"1\"\r\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"]=\"trainer-mlflow-demo\"\r\nos.environ[\"MLFLOW_FLATTEN_PARAMS\"]=\"1\"\r\n#os.environ[\"MLFLOW_TRACKING_URI\"]=<MY_SERVER IP>\r\n\r\ntraining_args = TrainingArguments(\r\n    num_train_epochs=1,\r\n    output_dir=\".\/output\",\r\n    logging_steps=500,\r\n    save_strategy=\"epoch\",\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    compute_metrics=compute_metrics\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect logging to work :)",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: fails to log to a tracking server; Content: ### system info python 3.9.13 | packaged by conda-forge | (main, may 27 2022, 16:56:21) print(transformers.__version__) 4.20.1 print(.__version__) 1.27.0 ### who can help? _no response_ ### information - [ ] the official example scripts - [x] my own modified scripts ### tasks - [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction 1. install 2. configure a vanilla training job to use a tracking server (os.environ[\"_tracking_uri\"]=\"...\") 3. run the job you should see an error similar to: ``` traceback (most recent call last): file \"\/home\/ubuntu\/train.py\", line 45, in trainer.train() file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1409, in train return inner_training_loop( file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer.py\", line 1580, in _inner_training_loop self.control = self.callback_handler.on_train_begin(args, self.state, self.control) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 347, in on_train_begin return self.call_event(\"on_train_begin\", args, state, control) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/trainer_callback.py\", line 388, in call_event result = getattr(callback, event)( file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 856, in on_train_begin self.setup(args, state, model) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/transformers\/integrations.py\", line 847, in setup self._ml_flow.log_params(dict(combined_dict_items[i : i + self._max_params_tags_per_batch])) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/tracking\/fluent.py\", line 675, in log_params client().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[]) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/tracking\/client.py\", line 918, in log_batch self._tracking_client.log_batch(run_id, metrics, params, tags) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 315, in log_batch self.store.log_batch( file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/store\/tracking\/rest_store.py\", line 309, in log_batch self._call_endpoint(logbatch, req_body) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/utils\/rest_utils.py\", line 256, in call_endpoint response = verify_rest_response(response, endpoint) file \"\/home\/ubuntu\/.local\/lib\/python3.9\/site-packages\/\/utils\/rest_utils.py\", line 185, in verify_rest_response raise restexception(json.loads(response.text)) .exceptions.restexception: invalid_parameter_value: invalid value [{'key': 'logging_nan_inf_filter', 'value': 'true'}, {'key': 'save_strategy', 'value': 'epoch'}, {'key': 'save_steps', 'value': '500'}, {'key': 'save_total_limit', 'value': 'none'}, {'key': 'save_on_each_node', 'value': 'false'}, {'key': 'no_cuda', 'value': 'false'}, {'key': 'seed', 'value': '42'}, {'key': 'data_seed', 'value': 'none'}, {'key': 'jit_mode_eval', 'value': 'false'}, {'key': 'use_ipex', 'value': 'false'}, {'key': 'bf16', 'value': 'false'}, {'key': 'fp16', 'value': 'false'}, {'key': 'fp16_opt_level', 'value': 'o1'}, {'key': 'half_precision_backend', 'value': 'auto'}, {'key': 'bf16_full_eval', 'value': 'false'}, {'key': 'fp16_full_eval', 'value': 'false'}, {'key': 'tf32', 'value': 'none'}, {'key': 'local_rank', 'value': '-1'}, {'key': 'xpu_backend', 'value': 'none'}, {'key': 'tpu_num_cores', 'value': 'none'}, {'key': 'tpu_metrics_debug', 'value': 'false'}, {'key': 'debug', 'value': '[]'}, {'key': 'dataloader_drop_last', 'value': 'false'}, {'key': 'eval_steps', 'value': 'none'}, {'key': 'dataloader_num_workers', 'value': '0'}, {'key': 'past_index', 'value': '-1'}, {'key': 'run_name', 'value': '.\/output'}, {'key': 'disable_tqdm', 'value': 'false'}, {'key': 'remove_unused_columns', 'value': 'true'}, {'key': 'label_names', 'value': 'none'}, {'key': 'load_best_model_at_end', 'value': 'false'}, {'key': 'metric_for_best_model', 'value': 'none'}, {'key': 'greater_is_better', 'value': 'none'}, {'key': 'ignore_data_skip', 'value': 'false'}, {'key': 'sharded_ddp', 'value': '[]'}, {'key': 'fsdp', 'value': '[]'}, {'key': 'fsdp_min_num_params', 'value': '0'}, {'key': 'deepspeed', 'value': 'none'}, {'key': 'label_smoothing_factor', 'value': '0.0'}, {'key': 'optim', 'value': 'adamw_hf'}, {'key': 'adafactor', 'value': 'false'}, {'key': 'group_by_length', 'value': 'false'}, {'key': 'length_column_name', 'value': 'length'}, {'key': 'report_to', 'value': \"['']\"}, {'key': 'ddp_find_unused_parameters', 'value': 'none'}, {'key': 'ddp_bucket_cap_mb', 'value': 'none'}, {'key': 'dataloader_pin_memory', 'value': 'true'}, {'key': 'skip_memory_metrics', 'value': 'true'}, {'key': 'use_legacy_prediction_loop', 'value': 'false'}, {'key': 'push_to_hub', 'value': 'false'}, {'key': 'resume_from_checkpoint', 'value': 'none'}, {'key': 'hub_model_id', 'value': 'none'}, {'key': 'hub_strategy', 'value': 'every_save'}, {'key': 'hub_token', 'value': ''}, {'key': 'hub_private_repo', 'value': 'false'}, {'key': 'gradient_checkpointing', 'value': 'false'}, {'key': 'include_inputs_for_metrics', 'value': 'false'}, {'key': 'fp16_backend', 'value': 'auto'}, {'key': 'push_to_hub_model_id', 'value': 'none'}, {'key': 'push_to_hub_organization', 'value': 'none'}, {'key': 'push_to_hub_token', 'value': ''}, {'key': '_n_gpu', 'value': '1'}, {'key': 'mp_parameters', 'value': ''}, {'key': 'auto_find_batch_size', 'value': 'false'}, {'key': 'full_determinism', 'value': 'false'}, {'key': 'torchdynamo', 'value': 'none'}, {'key': 'ray_scope', 'value': 'last'}] for parameter 'params' supplied. hint: value was of type 'list'. see the api docs for more information about request parameters. ``` training script: ``` import os import numpy as np from datasets import load_dataset, load_metric from transformers import autotokenizer, trainer, trainingarguments, automodelforsequenceclassification train_dataset, test_dataset = load_dataset(\"imdb\", split=['train', 'test']) tokenizer = autotokenizer.from_pretrained(\"distilbert-base-cased\") def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=true) train_dataset = train_dataset.map(tokenize_function, batched=true) test_dataset = test_dataset.map(tokenize_function, batched=true) model = automodelforsequenceclassification.from_pretrained(\"distilbert-base-cased\", num_labels=2) metric = load_metric(\"accuracy\") def compute_metrics(eval_pred): logits, labels = eval_pred predictions = np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) os.environ[\"hf__log_artifacts\"]=\"1\" os.environ[\"_experiment_name\"]=\"trainer--demo\" os.environ[\"_flatten_params\"]=\"1\" #os.environ[\"_tracking_uri\"]= training_args = trainingarguments( num_train_epochs=1, output_dir=\".\/output\", logging_steps=500, save_strategy=\"epoch\", ) trainer = trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, compute_metrics=compute_metrics ) trainer.train() ``` ### expected behavior i would expect logging to work :)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where they were unable to log to a tracking server while running a training job with Transformers.",
        "Issue_preprocessed_content":"Title: fails to log to a tracking server; Content: system info python packaged by who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction . install . configure a vanilla training job to use a tracking server . run the job you should see an error similar to training script expected behavior i would expect logging to work"
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/17066",
        "Issue_title":"Incorrect check for MLFlow active run in MLflowCallback",
        "Issue_creation_time":1651602226000,
        "Issue_closed_time":1651758596000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### System Info\r\n\r\n```shell\r\n- mlflow==1.25.1\r\n- `transformers` version: 4.19.0.dev0\r\n- Platform: Linux-5.10.76-linuxkit-aarch64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- Huggingface_hub version: 0.2.1\r\n- PyTorch version (GPU?): 1.10.2 (False)\r\n```\r\n\r\n\r\n### Who can help?\r\n\r\nShould be fixed by #17067\r\n\r\n### Information\r\n\r\n- [X] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n1. Follow Training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training\r\n2. Change the training arguments to use `TrainingArguments(output_dir=\"test_trainer\", report_to=['mlflow'], run_name=\"run0\")`\r\n3. On `trainer.train()` the MLFlow UI should report a run with a Run Name of `run0` which is not currently the case.\r\n\r\nCause of the Issue:\r\n```\r\n>> import mlflow\r\n>> print(mlflow.active_run is None, mlflow.active_run() is None)\r\nFalse True\r\n```\r\n\r\nIn `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is None:` need to be replaced by `if self._ml_flow.active_run() is None:`\r\n\r\n### Expected behavior\r\n\r\nPR #14894 introduce support for run_name in the MLflowCallback. Though, this does not work as expected since the active run is checked using a method reference that always returns true. Bug introduced by #16131.\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: incorrect check for active run in callback; Content: ### system info ```shell - ==1.25.1 - `transformers` version: 4.19.0.dev0 - platform: linux-5.10.76-linuxkit-aarch64-with-glibc2.31 - python version: 3.9.7 - huggingface_hub version: 0.2.1 - pytorch version (gpu?): 1.10.2 (false) ``` ### who can help? should be fixed by #17067 ### information - [x] the official example scripts - [ ] my own modified scripts ### tasks - [x] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction steps to reproduce: 1. follow training tutorial as per https:\/\/huggingface.co\/docs\/transformers\/training 2. change the training arguments to use `trainingarguments(output_dir=\"test_trainer\", report_to=[''], run_name=\"run0\")` 3. on `trainer.train()` the ui should report a run with a run name of `run0` which is not currently the case. cause of the issue: ``` >> import >> print(.active_run is none, .active_run() is none) false true ``` in `src\/transformers\/integrations.py` the line `if self._ml_flow.active_run is none:` need to be replaced by `if self._ml_flow.active_run() is none:` ### expected behavior pr #14894 introduce support for run_name in the callback. though, this does not work as expected since the active run is checked using a method reference that always returns true. bug introduced by #16131.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the incorrect check for an active run in the callback caused the run name to not be reported correctly.",
        "Issue_preprocessed_content":"Title: incorrect check for active run in callback; Content: system info who can help? should be fixed by information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction steps to reproduce . follow training tutorial as per . change the training arguments to use . on the ui should report a run with a run name of which is not currently the case. cause of the issue in the line need to be replaced by expected behavior pr introduce support for in the callback. though, this does not work as expected since the active run is checked using a method reference that always returns true. bug introduced by ."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/10397",
        "Issue_title":"`MLFlowLogger` does not update its status when `trainer.fit` failed",
        "Issue_creation_time":1636290176000,
        "Issue_closed_time":1640642583000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen an error is raised during training with `MLFlowLogger`, status of a `mlflow.entities.run_info.RunInfo` object should be updated to be 'FAILED', while it remains 'RUNNING'.\r\nDue to the problem, when you look at MLFlow Tracking Server screen, It seams as if training is still in progress even though it has been terminated with an error.\r\n\r\n### To Reproduce\r\n\r\n<!--\r\nPlease reproduce using the BoringModel!\r\n\r\nYou can use the following Colab link:\r\nhttps:\/\/colab.research.google.com\/drive\/1HvWVVTK8j2Nj52qU4Q4YCyzOm0_aLQF3?usp=sharing\r\nIMPORTANT: has to be public.\r\n\r\nor this simple template:\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/master\/pl_examples\/bug_report_model.py\r\n\r\nIf you could not reproduce using the BoringModel and still think there's a bug, please post here\r\nbut remember, bugs with code are fixed faster!\r\n-->\r\n```py\r\nimport os\r\n\r\nimport torch\r\nfrom torch.utils.data import DataLoader, Dataset\r\n\r\nfrom pytorch_lightning import LightningModule, Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger ##### added #####\r\n\r\n\r\nclass RandomDataset(Dataset):\r\n    def __init__(self, size, length):\r\n        self.len = length\r\n        self.data = torch.randn(length, size)\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return self.len\r\n\r\n\r\nclass BoringModel(LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = torch.nn.Linear(32, 2)\r\n\r\n    def forward(self, x):\r\n        return self.layer(x)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"train_loss\", loss)\r\n        raise Exception ##### added #####\r\n        return {\"loss\": loss}\r\n        \r\n    def validation_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"valid_loss\", loss)\r\n\r\n    def test_step(self, batch, batch_idx):\r\n        loss = self(batch).sum()\r\n        self.log(\"test_loss\", loss)\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\r\n\r\n\r\ndef run():\r\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\r\n    \r\n    mlf_logger = MLFlowLogger() ##### added #####\r\n\r\n    model = BoringModel()\r\n    trainer = Trainer(\r\n        default_root_dir=os.getcwd(),\r\n        limit_train_batches=1,\r\n        limit_val_batches=1,\r\n        num_sanity_val_steps=0,\r\n        max_epochs=1,\r\n        # enable_model_summary=False,\r\n        logger=mlf_logger ##### added #####\r\n    )\r\n    try:\r\n        trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\r\n        trainer.test(model, dataloaders=test_data)\r\n    finally:\r\n        print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # This should be 'FAILED'\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\nStatus of each MLFlow's run is correctly updated when `pl.Trainer.fit` failed.\r\n\r\n### Environment\r\n\r\n<!--\r\nPlease copy and paste the output from our environment collection script:\r\nhttps:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\n(For security purposes, please check the contents of the script before running it)\r\n\r\nYou can get the script and run it with:\r\n```bash\r\nwget https:\/\/raw.githubusercontent.com\/PyTorchLightning\/pytorch-lightning\/master\/requirements\/collect_env_details.py\r\npython collect_env_details.py\r\n```\r\n\r\nYou can also fill out the list below manually.\r\n-->\r\n\r\n- PyTorch Lightning Version: 1.4.9\r\n- MLFlow Version: 1.12.0\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: `logger` does not update its status when `trainer.fit` failed; Content: ## bug when an error is raised during training with `logger`, status of a `.entities.run_info.runinfo` object should be updated to be 'failed', while it remains 'running'. due to the problem, when you look at tracking server screen, it seams as if training is still in progress even though it has been terminated with an error. ### to reproduce ```py import os import torch from torch.utils.data import dataloader, dataset from pytorch_lightning import lightningmodule, trainer from pytorch_lightning.loggers import logger ##### added ##### class randomdataset(dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.len class boringmodel(lightningmodule): def __init__(self): super().__init__() self.layer = torch.nn.linear(32, 2) def forward(self, x): return self.layer(x) def training_step(self, batch, batch_idx): loss = self(batch).sum() self.log(\"train_loss\", loss) raise exception ##### added ##### return {\"loss\": loss} def validation_step(self, batch, batch_idx): loss = self(batch).sum() self.log(\"valid_loss\", loss) def test_step(self, batch, batch_idx): loss = self(batch).sum() self.log(\"test_loss\", loss) def configure_optimizers(self): return torch.optim.sgd(self.layer.parameters(), lr=0.1) def run(): train_data = dataloader(randomdataset(32, 64), batch_size=2) val_data = dataloader(randomdataset(32, 64), batch_size=2) test_data = dataloader(randomdataset(32, 64), batch_size=2) mlf_logger = logger() ##### added ##### model = boringmodel() trainer = trainer( default_root_dir=os.getcwd(), limit_train_batches=1, limit_val_batches=1, num_sanity_val_steps=0, max_epochs=1, # enable_model_summary=false, logger=mlf_logger ##### added ##### ) try: trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data) trainer.test(model, dataloaders=test_data) finally: print(trainer.logger.experiment.get_run(trainer.logger._run_id).info.status) # this should be 'failed' if __name__ == \"__main__\": run() ``` ### expected behavior status of each 's run is correctly updated when `pl.trainer.fit` failed. ### environment - pytorch lightning version: 1.4.9 - version: 1.12.0 ### additional context",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the `logger` does not update its status when `trainer.fit` failed, resulting in the tracking server screen appearing as if training is still in progress even though it has been terminated with an error.",
        "Issue_preprocessed_content":"Title: does not update its status when failed; Content: bug a clear and concise description of what the bug is. when an error is raised during training with , status of a object should be updated to be 'failed', while it remains 'running'. due to the problem, when you look at tracking server screen, it seams as if training is still in progress even though it has been terminated with an error. to reproduce please reproduce using the boringmodel! you can use the following colab link important has to be public. or this simple template if you could not reproduce using the boringmodel and still think there's a bug, please post here but remember, bugs with code are fixed faster! expected behavior fill in status of each 's run is correctly updated when failed. environment please copy and paste the output from our environment collection script for security purposes, please check the contents of the script before running it you can get the script and run it with you can also fill out the list below manually. pytorch lightning version version additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/9497",
        "Issue_title":"Inconsistency in MLFlowLogger.log_metrics within steps",
        "Issue_creation_time":1631568762000,
        "Issue_closed_time":1635553585000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## \ud83d\udc1b Inconsistency in MLFlowLogger.log_metrics within steps\r\n\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers.mlflow.html) for MLFlowLogger states that it has a method log_metrics which signature is as follows:\r\n\r\n`log_metrics(metrics, step=None)`\r\n\r\nwhere **metrics** (Dict[str, float]) \u2013 Dictionary with metric names as keys and measured quantities as values and \r\n**step** (Optional[int]) \u2013 Step number at which the metrics should be recorded.\r\n\r\nWhen within a training\/validation\/test _step method of a LightningModule:\r\n- Setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `AttributeError: 'MlflowClient' object has no attribute 'log_metrics'`\r\n- Setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `TypeError: log_metric() missing 2 required positional arguments: 'key' and 'value'`\r\n- Setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `TypeError: log_metric() missing 1 required positional argument: 'value'`\r\n\r\nFound the behavior from the last two options by luck because of a typo. The logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. Even if I use `log_metric` the method expects parameters other than the Dict[str, float] stated in the documentation.\r\n\r\n### To Reproduce\r\n\r\nThis is the minimum code I found that reproduces the bug:\r\n\r\nhttps:\/\/github.com\/mmazuecos\/pytorch_lightning_mlflow_bug\/blob\/main\/pytorch_lightning_mlflow_bug.py\r\n\r\n### Expected behavior\r\n\r\nThe code should work with the `log_metrics` signature from the documentation.\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.21.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.7.1.post2\r\n\t- pytorch-lightning: 1.4.5\r\n\t- tqdm:              4.62.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- ELF\r\n\t- processor:         x86_64\r\n\t- python:            3.8.11\r\n\t- version:           #148-Ubuntu SMP Sat May 8 02:33:43 UTC 2021\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: inconsistency in logger.log_metrics within steps; Content: ## inconsistency in logger.log_metrics within steps the [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.loggers..html) for logger states that it has a method log_metrics which signature is as follows: `log_metrics(metrics, step=none)` where **metrics** (dict[str, float]) dictionary with metric names as keys and measured quantities as values and **step** (optional[int]) step number at which the metrics should be recorded. when within a training\/validation\/test _step method of a lightningmodule: - setting `self.logger.experiment.log_metrics({\"train_loss\": loss})` results in the fit method raising `attributeerror: 'client' object has no attribute 'log_metrics'` - setting `self.logger.experiment.log_metric({\"train_loss\": loss})` results in the fit method raising `typeerror: log_metric() missing 2 required positional arguments: 'key' and 'value'` - setting `self.logger.experiment.log_metric(\"train_loss\", loss)` results in the fit method raising `typeerror: log_metric() missing 1 required positional argument: 'value'` found the behavior from the last two options by luck because of a typo. the logger would expect `log_metric` despite the documentation saying the method is called `log_metrics`. even if i use `log_metric` the method expects parameters other than the dict[str, float] stated in the documentation. ### to reproduce this is the minimum code i found that reproduces the bug: https:\/\/github.com\/mmazuecos\/pytorch_lightning__bug\/blob\/main\/pytorch_lightning__bug.py ### expected behavior the code should work with the `log_metrics` signature from the documentation. ### environment * cuda: - gpu: - available: false - version: none * packages: - numpy: 1.21.2 - pytorch_debug: false - pytorch_version: 1.7.1.post2 - pytorch-lightning: 1.4.5 - tqdm: 4.62.2 * system: - os: linux - architecture: - 64bit - elf - processor: x86_64 - python: 3.8.11 - version: #148-ubuntu smp sat may 8 02:33:43 utc 2021",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with inconsistency in the logger.log_metrics within steps, where the documentation states one signature but the code requires different parameters.",
        "Issue_preprocessed_content":"Title: inconsistency in within steps; Content: inconsistency in within steps the for logger states that it has a method which signature is as follows where metrics dictionary with metric names as keys and measured quantities as values and step step number at which the metrics should be recorded. when within a method of a lightningmodule setting results in the fit method raising setting results in the fit method raising setting results in the fit method raising found the behavior from the last two options by luck because of a typo. the logger would expect despite the documentation saying the method is called . even if i use the method expects parameters other than the dict stated in the documentation. to reproduce this is the minimum code i found that reproduces the bug expected behavior the code should work with the signature from the documentation. environment cuda gpu available false version none packages numpy false tqdm system os linux architecture bit elf processor python version smp sat may utc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/8431",
        "Issue_title":"mlflow run_name unexpected argument error",
        "Issue_creation_time":1626357881000,
        "Issue_closed_time":1626409391000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nThe [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers.mlflow.html?highlight=logger#mlflow-logger) mentions there is an argument called run_name for the mlflow logger, where the run_name of a given experiment can be provided. Although,run_name is an unknown argument to the mlflow logger\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'run_name'`\r\n\r\n## Please reproduce using the BoringModel\r\nColab link:  https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n### To Reproduce\r\n\r\n```\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nimport mlflow\r\n\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name=\"test\",\r\n    run_name=\"testrun\",\r\n)\r\n```\r\n### Environment\r\nColab - https:\/\/colab.research.google.com\/drive\/1thcmInx6tQDOnkxk2Ir8hoOUx1UrOpIx?usp=sharing\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: run_name unexpected argument error; Content: ## bug the [documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/api\/pytorch_lightning.loggers..html?highlight=logger#-logger) mentions there is an argument called run_name for the logger, where the run_name of a given experiment can be provided. although,run_name is an unknown argument to the logger `typeerror: __init__() got an unexpected keyword argument 'run_name'` ## please reproduce using the boringmodel colab link: https:\/\/colab.research.google.com\/drive\/1thcminx6tqdonkxk2ir8hooux1uropix?usp=sharing ### to reproduce ``` from pytorch_lightning.loggers import logger import mlf_logger = logger( experiment_name=\"test\", run_name=\"testrun\", ) ``` ### environment colab - https:\/\/colab.research.google.com\/drive\/1thcminx6tqdonkxk2ir8hooux1uropix?usp=sharing",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected argument error when attempting to use the run_name argument for the logger in PyTorch Lightning.",
        "Issue_preprocessed_content":"Title: unexpected argument error; Content: bug the mentions there is an argument called for the logger, where the of a given experiment can be provided. is an unknown argument to the logger please reproduce using the boringmodel colab link to reproduce environment colab"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6745",
        "Issue_title":"mlflow run context is not logged when using MLFlowLogger",
        "Issue_creation_time":1617115654000,
        "Issue_closed_time":1617875123000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nWhen we use the basic mlflow logging via `with mlflow.start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the Tracking UI ([system tags](https:\/\/mlflow.org\/docs\/latest\/tracking.html#system-tags))\r\n\r\nBut when we use `MLFlowLogger` as a logger in pytorch_lightning, this info is not logged. As a user, I'd like to have a mirrored functionality out-of-the-box.\r\n\r\nI inspected the `start_run()` method of mlflow and deduced that the only thing is left while creating the run via MLflowClient is to add `resolve_tags` from the `context` package:\r\n```python\r\n# pytorch_lightning\/loggers\/mlflow.py\r\nfrom mlflow.tracking.context.registry import resolve_tags\r\n...\r\n    def experiment(self) -> MLflowClient:\r\n        if self._run_id is None:\r\n            run = self._mlflow_client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags))\r\n```\r\n\r\nI think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default API, secondly - it's the pytorch_lightning that manages the mlflow's run anyways.\r\n\r\n**PR is following ...**",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: run context is not logged when using logger; Content: ## bug when we use the basic logging via `with .start_run(): ...` context manager, we get a better supplementary info about the run (git commit sha, user, filename) rendered in the tracking ui ([system tags](https:\/\/.org\/docs\/latest\/tracking.html#system-tags)) but when we use `logger` as a logger in pytorch_lightning, this info is not logged. as a user, i'd like to have a mirrored functionality out-of-the-box. i inspected the `start_run()` method of and deduced that the only thing is left while creating the run via client is to add `resolve_tags` from the `context` package: ```python # pytorch_lightning\/loggers\/.py from .tracking.context.registry import resolve_tags ... def experiment(self) -> client: if self._run_id is none: run = self.__client.create_run(experiment_id=self._experiment_id, tags=resolve_tags(self.tags)) ``` i think it's a better idea to add those tags internally (meaning not to expect users doing that manually) as first - it's as seamless as in the default api, secondly - it's the pytorch_lightning that manages the 's run anyways. **pr is following ...**",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the run context is not logged when using the logger in PyTorch Lightning, and proposed a solution to add the tags internally to ensure the run context is logged.",
        "Issue_preprocessed_content":"Title: run context is not logged when using logger; Content: bug when we use the basic logging via context manager, we get a better supplementary info about the run rendered in the tracking ui but when we use as a logger in this info is not logged. as a user, i'd like to have a mirrored functionality i inspected the method of and deduced that the only thing is left while creating the run via client is to add from the package i think it's a better idea to add those tags internally as first it's as seamless as in the default api, secondly it's the that manages the 's run anyways. pr is following"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6641",
        "Issue_title":"External MLFlow logging failures cause training job to fail",
        "Issue_creation_time":1616445327000,
        "Issue_closed_time":1619815518000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nI am using a `pytorch_lightning.loggers.mlflow.MLFlowLogger` during training, with the MLFlow tracking URI hosted in Databricks. When Databricks updates, we sometimes lose access to MLFlow for a brief period. When this happens, logging to MLFlow fails with the following error:\r\n\r\n```python\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=XXX.cloud.databricks.com, port=443): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get?XXX (Caused by NewConnectionError(<urllib3.connection.HTTPSConnection object at 0x7fbbd6096f50>: Failed to establish a new connection: [Errno 111] Connection refused))\r\n```\r\n\r\nNot only does logging fail, but with PyTorch Lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. \r\n\r\nIdeally, there would be flexibility in PyTorch Lightning to allow users to handle logging errors such that it will not always kill the training job. \r\n\r\n## Please reproduce using the BoringModel\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/17TqdKZ8SjcdpiCWc76N5uQc5IKIgNp7g?usp=sharing \r\n\r\n### To Reproduce\r\n\r\nAttempt to use a logger that fails to log. The training job will fail, losing all progress. \r\n\r\n### Expected behavior\r\n\r\nThere is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. \r\n\r\n### Environment\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.19.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.8.0+cu101\r\n\t- pytorch-lightning: 1.2.4\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.7.10\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n\r\n### Additional context\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: external logging failures cause training job to fail; Content: ## bug i am using a `pytorch_lightning.loggers..logger` during training, with the tracking uri hosted in databricks. when databricks updates, we sometimes lose access to for a brief period. when this happens, logging to fails with the following error: ```python urllib3.exceptions.maxretryerror: httpsconnectionpool(host=xxx.cloud.databricks.com, port=443): max retries exceeded with url: \/api\/2.0\/\/runs\/get?xxx (caused by newconnectionerror(: failed to establish a new connection: [errno 111] connection refused)) ``` not only does logging fail, but with pytorch lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially long-running job with limited error handling options currently available. ideally, there would be flexibility in pytorch lightning to allow users to handle logging errors such that it will not always kill the training job. ## please reproduce using the boringmodel https:\/\/colab.research.google.com\/drive\/17tqdkz8sjcdpicwc76n5uqc5ikignp7g?usp=sharing ### to reproduce attempt to use a logger that fails to log. the training job will fail, losing all progress. ### expected behavior there is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. ### environment * cuda: - gpu: - tesla t4 - available: true - version: 10.1 * packages: - numpy: 1.19.5 - pytorch_debug: false - pytorch_version: 1.8.0+cu101 - pytorch-lightning: 1.2.4 - tqdm: 4.41.1 * system: - os: linux - architecture: - 64bit - - processor: x86_64 - python: 3.7.10 - version: #1 smp thu jul 23 08:00:38 pdt 2020 ### additional context",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where external logging failures caused a training job to fail, with limited error handling options available in PyTorch Lightning.",
        "Issue_preprocessed_content":"Title: external logging failures cause training job to fail; Content: bug i am using a during training, with the tracking uri hosted in databricks. when databricks updates, we sometimes lose access to for a brief period. when this happens, logging to fails with the following error not only does logging fail, but with pytorch lightning, an error logging means the entire training pipeline will also fail, losing progress on a potentially job with limited error handling options currently available. ideally, there would be flexibility in pytorch lightning to allow users to handle logging errors such that it will not always kill the training job. please reproduce using the boringmodel to reproduce attempt to use a logger that fails to log. the training job will fail, losing all progress. expected behavior there is an option to handle exceptions from the logger such that the job does not automatically die if logging a parameter fails. environment cuda gpu tesla t available true version packages numpy false tqdm system os linux architecture bit processor python version smp thu jul pdt additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/6205",
        "Issue_title":"MLFlow Logger Makes a New Run When Resuming from hpc Checkpoint",
        "Issue_creation_time":1614282696000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nCurrently the `MLFlowLogger` creates a new run when resuming from an hpc checkpoint, e.g., after preemption by slurm and requeuing. Runs are an MLFlow concept that groups things in their UI, so when resuming after requeue, it should really be reusing the run ID. I think this can be patched into the hpc checkpoint using the logger which I believe exposes the run ID. This can also be seen on the `v_num` on the progress bar which changes after preemption (in general that `v_num` probably shouldnt be changing in this case). I'm happy to attempt to PR this if the owners agree that it's a bug.\r\n\r\n### To Reproduce\r\n\r\nUse `MLFlowLogger` on a slurm cluster and watch the mlflow UI when preemption happens, there will be a new run created.\r\n\r\n### Expected behavior\r\n\r\nRuns are grouped neatly on the MLFlow UI\r\n\r\n### Environment\r\n\r\n* CUDA:\r\n        - GPU:\r\n        - available:         False\r\n        - version:           10.2\r\n* Packages:\r\n        - numpy:             1.20.1\r\n        - pyTorch_debug:     False\r\n        - pyTorch_version:   1.7.1\r\n        - pytorch-lightning: 1.2.0\r\n        - tqdm:              4.57.0\r\n* System:\r\n        - OS:                Linux\r\n        - architecture:\r\n                - 64bit\r\n                - ELF\r\n        - processor:         x86_64\r\n        - python:            3.8.1\r\n        - version:           #1 SMP Thu Jan 21 16:15:07 EST 2021\r\n\n\ncc @awaelchli @ananthsub @ninginthecloud @rohitgr7 @tchaton @akihironitta",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logger makes a new run when resuming from hpc checkpoint; Content: ## bug currently the `logger` creates a new run when resuming from an hpc checkpoint, e.g., after preemption by slurm and requeuing. runs are an concept that groups things in their ui, so when resuming after requeue, it should really be reusing the run id. i think this can be patched into the hpc checkpoint using the logger which i believe exposes the run id. this can also be seen on the `v_num` on the progress bar which changes after preemption (in general that `v_num` probably shouldnt be changing in this case). i'm happy to attempt to pr this if the owners agree that it's a bug. ### to reproduce use `logger` on a slurm cluster and watch the ui when preemption happens, there will be a new run created. ### expected behavior runs are grouped neatly on the ui ### environment * cuda: - gpu: - available: false - version: 10.2 * packages: - numpy: 1.20.1 - pytorch_debug: false - pytorch_version: 1.7.1 - pytorch-lightning: 1.2.0 - tqdm: 4.57.0 * system: - os: linux - architecture: - 64bit - elf - processor: x86_64 - python: 3.8.1 - version: #1 smp thu jan 21 16:15:07 est 2021 cc @awaelchli @ananthsub @ninginthecloud @rohitgr7 @tchaton @akihironitta",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger creates a new run when resuming from an hpc checkpoint, resulting in runs being grouped incorrectly on the UI.",
        "Issue_preprocessed_content":"Title: logger makes a new run when resuming from hpc checkpoint; Content: bug currently the creates a new run when resuming from an hpc checkpoint, after preemption by slurm and requeuing. runs are an concept that groups things in their ui, so when resuming after requeue, it should really be reusing the run id. i think this can be patched into the hpc checkpoint using the logger which i believe exposes the run id. this can also be seen on the on the progress bar which changes after preemption . i'm happy to attempt to pr this if the owners agree that it's a bug. to reproduce use on a slurm cluster and watch the ui when preemption happens, there will be a new run created. expected behavior runs are grouped neatly on the ui environment cuda gpu available false version packages numpy false tqdm system os linux architecture bit elf processor python version smp thu jan est cc"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/5892",
        "Issue_title":"MlflowLogger fail when logging long parameters",
        "Issue_creation_time":1612927107000,
        "Issue_closed_time":1613510526000,
        "Issue_upvote_count":4,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## Please reproduce using the BoringModel\r\n\r\n\r\n<!-- Please paste your BoringModel colab link here. -->\r\n\r\n### To Reproduce\r\n\r\nLog anything  parameters longer than 250 characters\r\n\r\n\r\n<!-- If you could not reproduce using the BoringModel and still think there's a bug, please post here -->\r\n\r\n### Expected behavior\r\n\r\n<!-- FILL IN -->\r\n\r\nMlflowLogger not sending parameters longer than 250 characters to mlflow and log warning to user\r\n\r\n### Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0):\r\n - OS (e.g., Linux): \r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA\/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n### Additional context\r\n\r\nMlflow only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters:\r\nhttps:\/\/www.mlflow.org\/docs\/latest\/rest-api.html#log-param\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/1976\r\nhttps:\/\/github.com\/mlflow\/mlflow\/issues\/3931\r\n\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logger fail when logging long parameters; Content: ## bug ## please reproduce using the boringmodel ### to reproduce log anything parameters longer than 250 characters ### expected behavior logger not sending parameters longer than 250 characters to and log warning to user ### environment - pytorch version (e.g., 1.0): - os (e.g., linux): - how you installed pytorch (`conda`, `pip`, source): pip - build command you used (if compiling from source): - python version: 3.7 - cuda\/cudnn version: - gpu models and configuration: - any other relevant information: ### additional context only allow paramters to be at most 500 bytes (250 unicode characters), their limit in database is 250 characters: https:\/\/www..org\/docs\/latest\/rest-api.html#log-param https:\/\/github.com\/\/\/issues\/1976 https:\/\/github.com\/\/\/issues\/3931",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger fails when logging long parameters, with the expected behavior being that the logger should not send parameters longer than 250 characters and log a warning to the user.",
        "Issue_preprocessed_content":"Title: logger fail when logging long parameters; Content: bug a clear and concise description of what the bug is. please reproduce using the boringmodel please paste your boringmodel colab link here. to reproduce log anything parameters longer than characters if you could not reproduce using the boringmodel and still think there's a bug, please post here expected behavior fill in logger not sending parameters longer than characters to and log warning to user environment pytorch version os how you installed pytorch pip build command you used python version version gpu models and configuration any other relevant information additional context only allow paramters to be at most bytes , their limit in database is characters add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/4411",
        "Issue_title":"Using log_gpu_memory with MLFLow logger causes an exception.",
        "Issue_creation_time":1603900309000,
        "Issue_closed_time":1605009026000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nUsing log_gpu_memory with MLFLow logger causes an error. It appears the name of the metric is not supported by MLFLow.\r\n\r\n    MlflowException: Invalid metric name: 'gpu_id: 0\/memory.used (MB)'. Names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/).\r\n\r\n### To Reproduce\r\nI reproduced the bug with the BoringModel, in the link bellow:\r\nhttps:\/\/colab.research.google.com\/drive\/1P8uhSfjvYhKPMyRZH-QmfbOUOfnePy6G?usp=sharing\r\n\r\n### Expected behavior\r\nlog_gpu_memory should log gpu memory correctly when using an MLFlow logger.\r\n\r\n### Environment\r\nColab environment:\r\n\r\n* CUDA:\r\n\t- GPU:\r\n\t\t- Tesla T4\r\n\t- available:         True\r\n\t- version:           10.1\r\n* Packages:\r\n\t- numpy:             1.18.5\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cu101\r\n\t- pytorch-lightning: 1.0.3\r\n\t- tqdm:              4.41.1\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t- \r\n\t- processor:         x86_64\r\n\t- python:            3.6.9\r\n\t- version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: using log_gpu_memory with logger causes an exception.; Content: ## bug using log_gpu_memory with logger causes an error. it appears the name of the metric is not supported by . exception: invalid metric name: 'gpu_id: 0\/memory.used (mb)'. names may only contain alphanumerics, underscores (_), dashes (-), periods (.), spaces ( ), and slashes (\/). ### to reproduce i reproduced the bug with the boringmodel, in the link bellow: https:\/\/colab.research.google.com\/drive\/1p8uhsfjvyhkpmyrzh-qmfbouofnepy6g?usp=sharing ### expected behavior log_gpu_memory should log gpu memory correctly when using an logger. ### environment colab environment: * cuda: - gpu: - tesla t4 - available: true - version: 10.1 * packages: - numpy: 1.18.5 - pytorch_debug: false - pytorch_version: 1.6.0+cu101 - pytorch-lightning: 1.0.3 - tqdm: 4.41.1 * system: - os: linux - architecture: - 64bit - - processor: x86_64 - python: 3.6.9 - version: #1 smp thu jul 23 08:00:38 pdt 2020",
        "Issue_original_content_gpt_summary":"The user encountered an error when attempting to use log_gpu_memory with a logger, due to an invalid metric name.",
        "Issue_preprocessed_content":"Title: using with logger causes an exception.; Content: bug a clear and concise description of what the bug is. using with logger causes an error. it appears the name of the metric is not supported by . exception invalid metric name '. names may only contain alphanumerics, underscores , dashes , periods , spaces , and slashes . to reproduce i reproduced the bug with the boringmodel, in the link bellow expected behavior should log gpu memory correctly when using an logger. environment colab environment cuda gpu tesla t available true version packages numpy false tqdm system os linux architecture bit processor python version smp thu jul pdt"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3964",
        "Issue_title":"mlflow logger complains about missing run_id",
        "Issue_creation_time":1602116192000,
        "Issue_closed_time":1602141183000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\nWhen using MLflow logger, log_param() function require `run_id`\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-23-d048545e1854> in <module>\r\n      9 trainer.fit(model=experiment, \r\n     10            train_dataloader=train_dl,\r\n---> 11            val_dataloaders=test_dl)\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)\r\n    452         self.call_hook('on_fit_start')\r\n    453 \r\n--> 454         results = self.accelerator_backend.train()\r\n    455         self.accelerator_backend.teardown()\r\n    456 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self)\r\n     51 \r\n     52         # train or test\r\n---> 53         results = self.train_or_test()\r\n     54         return results\r\n     55 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self)\r\n     48             results = self.trainer.run_test()\r\n     49         else:\r\n---> 50             results = self.trainer.train()\r\n     51         return results\r\n     52 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self)\r\n    499 \r\n    500                 # run train epoch\r\n--> 501                 self.train_loop.run_training_epoch()\r\n    502 \r\n    503                 if self.max_steps and self.max_steps <= self.global_step:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self)\r\n    525             # TRAINING_STEP + TRAINING_STEP_END\r\n    526             # ------------------------------------\r\n--> 527             batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\r\n    528 \r\n    529             # when returning -1 from train_step, we end epoch early\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx)\r\n    660                     opt_idx,\r\n    661                     optimizer,\r\n--> 662                     self.trainer.hiddens\r\n    663                 )\r\n    664 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\r\n    739         \"\"\"\r\n    740         # lightning module hook\r\n--> 741         result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\r\n    742 \r\n    743         if result is None:\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens)\r\n    300         with self.trainer.profiler.profile('model_forward'):\r\n    301             args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens)\r\n--> 302             training_step_output = self.trainer.accelerator_backend.training_step(args)\r\n    303             training_step_output = self.trainer.call_hook('training_step_end', training_step_output)\r\n    304 \r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args)\r\n     59                 output = self.__training_step(args)\r\n     60         else:\r\n---> 61             output = self.__training_step(args)\r\n     62 \r\n     63         return output\r\n\r\n~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args)\r\n     67         batch = self.to_device(batch)\r\n     68         args[0] = batch\r\n---> 69         output = self.trainer.model.training_step(*args)\r\n     70         return output\r\n     71 \r\n\r\n<ipython-input-21-31b6dc3ffd67> in training_step(self, batch, batch_idx, optimizer_idx)\r\n     28         for key, val in train_loss.items():\r\n     29             self.log(key, val.item())\r\n---> 30             self.logger.experiment.log_param(key=key, value=val.item())\r\n     31 \r\n     32         return train_loss\r\n\r\nTypeError: log_param() missing 1 required positional argument: 'run_id'\r\n```\r\n#### Expected behavior\r\nThe MlflowLogger should behave the same as the mlflow api where only key and value argment is needed for log_param() function\r\n\r\n#### Code sample\r\n```python\r\nmlf_logger = MLFlowLogger(\r\n    experiment_name='test',\r\n    tracking_uri=\"file:.\/ml-runs\"\r\n)\r\n\r\nCllass VAEexperiment(LightningModule):\r\n...\r\n    def training_step(self, batch, batch_idx, optimizer_idx = 0):\r\n        ....\r\n        for key, val in train_loss.items():\r\n            self.logger.experiment.log_param(key=key, value=val.item())\r\n       ....\r\n       return train_loss\r\n\r\ntrainer = Trainer(logger=mlf_logger,\r\n                  default_root_dir='..\/logs',\r\n                  early_stop_callback=False,\r\n                  gpus=1, \r\n                  auto_select_gpus=True,\r\n                  max_epochs=40)\r\n\r\ntrainer.fit(model=experiment, \r\n           train_dataloader=train_dl, \r\n           val_dataloaders=test_dl)\r\n```\r\n\r\n\r\n### Environment\r\n\r\npytorch-lightning==0.10.0\r\ntorch==1.6.0\r\ntorchsummary==1.5.1\r\ntorchvision==0.7.0\r\n\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logger complains about missing run_id; Content: ## bug when using logger, log_param() function require `run_id` ``` --------------------------------------------------------------------------- typeerror traceback (most recent call last) in 9 trainer.fit(model=experiment, 10 train_dataloader=train_dl, ---> 11 val_dataloaders=test_dl) ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule) 452 self.call_hook('on_fit_start') 453 --> 454 results = self.accelerator_backend.train() 455 self.accelerator_backend.teardown() 456 ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in train(self) 51 52 # train or test ---> 53 results = self.train_or_test() 54 return results 55 ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/base_accelerator.py in train_or_test(self) 48 results = self.trainer.run_test() 49 else: ---> 50 results = self.trainer.train() 51 return results 52 ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py in train(self) 499 500 # run train epoch --> 501 self.train_loop.run_training_epoch() 502 503 if self.max_steps and self.max_steps <= self.global_step: ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_epoch(self) 525 # training_step + training_step_end 526 # ------------------------------------ --> 527 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx) 528 529 # when returning -1 from train_step, we end epoch early ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in run_training_batch(self, batch, batch_idx, dataloader_idx) 660 opt_idx, 661 optimizer, --> 662 self.trainer.hiddens 663 ) 664 ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step_and_backward(self, split_batch, batch_idx, opt_idx, optimizer, hiddens) 739 \"\"\" 740 # lightning module hook --> 741 result = self.training_step(split_batch, batch_idx, opt_idx, hiddens) 742 743 if result is none: ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py in training_step(self, split_batch, batch_idx, opt_idx, hiddens) 300 with self.trainer.profiler.profile('model_forward'): 301 args = self.build_train_args(split_batch, batch_idx, opt_idx, hiddens) --> 302 training_step_output = self.trainer.accelerator_backend.training_step(args) 303 training_step_output = self.trainer.call_hook('training_step_end', training_step_output) 304 ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in training_step(self, args) 59 output = self.__training_step(args) 60 else: ---> 61 output = self.__training_step(args) 62 63 return output ~\/anaconda3\/envs\/ns_dl_2020_torch\/lib\/python3.7\/site-packages\/pytorch_lightning\/accelerators\/gpu_backend.py in __training_step(self, args) 67 batch = self.to_device(batch) 68 args[0] = batch ---> 69 output = self.trainer.model.training_step(*args) 70 return output 71 in training_step(self, batch, batch_idx, optimizer_idx) 28 for key, val in train_loss.items(): 29 self.log(key, val.item()) ---> 30 self.logger.experiment.log_param(key=key, value=val.item()) 31 32 return train_loss typeerror: log_param() missing 1 required positional argument: 'run_id' ``` #### expected behavior the logger should behave the same as the api where only key and value argment is needed for log_param() function #### code sample ```python mlf_logger = logger( experiment_name='test', tracking_uri=\"file:.\/ml-runs\" ) cllass vaeexperiment(lightningmodule): ... def training_step(self, batch, batch_idx, optimizer_idx = 0): .... for key, val in train_loss.items(): self.logger.experiment.log_param(key=key, value=val.item()) .... return train_loss trainer = trainer(logger=mlf_logger, default_root_dir='..\/logs', early_stop_callback=false, gpus=1, auto_select_gpus=true, max_epochs=40) trainer.fit(model=experiment, train_dataloader=train_dl, val_dataloaders=test_dl) ``` ### environment pytorch-lightning==0.10.0 torch==1.6.0 torchsummary==1.5.1 torchvision==0.7.0",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logger was complaining about a missing run_id when using the log_param() function in a LightningModule.",
        "Issue_preprocessed_content":"Title: logger complains about missing ; Content: bug when using logger, function require expected behavior the logger should behave the same as the api where only key and value argment is needed for function code sample environment"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3444",
        "Issue_title":"TypeError: can't pickle _thread.lock objects - Error while logging model into mlflow in multi gpu scenario",
        "Issue_creation_time":1599740214000,
        "Issue_closed_time":1605489870000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":15.0,
        "Issue_body":"## \u2753 Questions and Help\r\n\r\n#### What is your question?\r\nTrying to log model into mlflow using `mlflow.pytorch.log_model` in train end. Getting the above error only in multi gpu scenario. \r\n\r\n#### Code\r\n\r\n\r\nmnist script file - \r\n\r\n```\r\nimport pytorch_lightning as pl\r\nimport torch\r\nfrom argparse import ArgumentParser\r\n#from mlflow.pytorch.pytorch_autolog import __MLflowPLCallback\r\nfrom pytorch_lightning.logging import MLFlowLogger\r\nfrom sklearn.metrics import accuracy_score\r\nfrom torch.nn import functional as F\r\nfrom torch.utils.data import DataLoader, random_split\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nclass LightningMNISTClassifier(pl.LightningModule):\r\n    def __init__(self):\r\n        \"\"\"\r\n        Initializes the network\r\n        \"\"\"\r\n        super(LightningMNISTClassifier, self).__init__()\r\n\r\n        # mnist images are (1, 28, 28) (channels, width, height)\r\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\r\n        self.layer_2 = torch.nn.Linear(128, 256)\r\n        self.layer_3 = torch.nn.Linear(256, 10)\r\n\r\n        # transforms for images\r\n        self.transform = transforms.Compose(\r\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\r\n        )\r\n\r\n    @staticmethod\r\n    def add_model_specific_args(parent_parser):\r\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\r\n        parser.add_argument(\r\n            \"--batch-size\",\r\n            type=int,\r\n            default=64,\r\n            metavar=\"N\",\r\n            help=\"input batch size for training (default: 64)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--num-workers\",\r\n            type=int,\r\n            default=0,\r\n            metavar=\"N\",\r\n            help=\"number of workers (default: 0)\",\r\n        )\r\n        parser.add_argument(\r\n            \"--lr\",\r\n            type=float,\r\n            default=1e-3,\r\n            metavar=\"LR\",\r\n            help=\"learning rate (default: 1e-3)\",\r\n        )\r\n        return parser\r\n\r\n    def forward(self, x):\r\n        \"\"\"\r\n        Forward Function\r\n        \"\"\"\r\n        batch_size, channels, width, height = x.size()\r\n\r\n        # (b, 1, 28, 28) -> (b, 1*28*28)\r\n        x = x.view(batch_size, -1)\r\n\r\n        # layer 1 (b, 1*28*28) -> (b, 128)\r\n        x = self.layer_1(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 2 (b, 128) -> (b, 256)\r\n        x = self.layer_2(x)\r\n        x = torch.relu(x)\r\n\r\n        # layer 3 (b, 256) -> (b, 10)\r\n        x = self.layer_3(x)\r\n\r\n        # probability distribution over labels\r\n        x = torch.log_softmax(x, dim=1)\r\n\r\n        return x\r\n\r\n    def cross_entropy_loss(self, logits, labels):\r\n        \"\"\"\r\n        Loss Fn to compute loss\r\n        \"\"\"\r\n        return F.nll_loss(logits, labels)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        \"\"\"\r\n        training the data as batches and returns training loss on each batch\r\n        \"\"\"\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"loss\": loss}\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        \"\"\"\r\n        Performs validation of data in batches\r\n        \"\"\"\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n        loss = self.cross_entropy_loss(logits, y)\r\n        return {\"val_loss\": loss}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average validation accuracy\r\n        \"\"\"\r\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\r\n        tensorboard_logs = {\"val_loss\": avg_loss}\r\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs}\r\n\r\n    def test_step(self, test_batch, batch_idx):\r\n        \"\"\"\r\n        Performs test and computes test accuracy\r\n        \"\"\"\r\n        x, y = test_batch\r\n        output = self.forward(x)\r\n        a, y_hat = torch.max(output, dim=1)\r\n        test_acc = accuracy_score(y_hat.cpu(), y.cpu())\r\n        return {\"test_acc\": torch.tensor(test_acc)}\r\n\r\n    def test_epoch_end(self, outputs):\r\n        \"\"\"\r\n        Computes average test accuracy score\r\n        \"\"\"\r\n        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\r\n        return {\"avg_test_acc\": avg_test_acc}\r\n\r\n    def prepare_data(self):\r\n        \"\"\"\r\n        Preprocess the input data.\r\n        \"\"\"\r\n        return {}\r\n\r\n    def train_dataloader(self):\r\n        \"\"\"\r\n        Loading training data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_train,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def val_dataloader(self):\r\n        \"\"\"\r\n        Loading validation data as batches\r\n        \"\"\"\r\n        mnist_train = datasets.MNIST(\r\n            \"dataset\", download=True, train=True, transform=self.transform\r\n        )\r\n        mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\r\n\r\n        return DataLoader(\r\n            mnist_val,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def test_dataloader(self):\r\n        \"\"\"\r\n        Loading test data as batches\r\n        \"\"\"\r\n        mnist_test = datasets.MNIST(\r\n            \"dataset\", download=True, train=False, transform=self.transform\r\n        )\r\n        return DataLoader(\r\n            mnist_test,\r\n            batch_size=64,\r\n            num_workers=1\r\n        )\r\n\r\n    def configure_optimizers(self):\r\n        \"\"\"\r\n        Creates and returns Optimizer\r\n        \"\"\"\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\r\n        self.scheduler = {\r\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\r\n                self.optimizer,\r\n                mode=\"min\",\r\n                factor=0.2,\r\n                patience=2,\r\n                min_lr=1e-6,\r\n                verbose=True,\r\n            )\r\n        }\r\n        return [self.optimizer], [self.scheduler]\r\n\r\n    def optimizer_step(\r\n        self,\r\n        epoch,\r\n        batch_idx,\r\n        optimizer,\r\n        optimizer_idx,\r\n        second_order_closure=None,\r\n        on_tpu=False,\r\n        using_lbfgs=False,\r\n        using_native_amp=False,\r\n    ):\r\n        self.optimizer.step()\r\n        self.optimizer.zero_grad()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    from pytorch_autolog import autolog\r\n    autolog()\r\n    model = LightningMNISTClassifier()\r\n    mlflow_logger = MLFlowLogger(\r\n        experiment_name=\"Default\", tracking_uri=\"http:\/\/localhost:5000\/\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        logger=mlflow_logger,\r\n        gpus=2,\r\n        distributed_backend=\"ddp\",\r\n        max_epochs=1\r\n    )\r\n    trainer.fit(model)\r\n    trainer.test()\r\n\r\n```\r\n\r\nSample code from autolog - Callback class. \r\n\r\n```\r\n    class __MLflowPLCallback(pl.Callback):\r\n\r\n        def __init__(self):\r\n            super().__init__()\r\n\r\n        def on_train_end(self, trainer, pl_module):\r\n            \"\"\"\r\n            Logs the model checkpoint into mlflow - models folder on the training end\r\n            \"\"\"\r\n\r\n            mlflow.set_tracking_uri(trainer.logger._tracking_uri )\r\n            mlflow.set_experiment(trainer.logger._experiment_name)\r\n            mlflow.start_run(trainer.logger.run_id)\r\n            mlflow.pytorch.log_model(trainer.model, \"models\")\r\n            mlflow.end_run()\r\n\r\n\r\n```\r\n\r\n\r\n\r\n\r\nStack Trace\r\n\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                          \r\n  File \"mnist.py\", line 231, in <module>\r\n    trainer.fit(model)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 218, in fit\r\n    return _run_and_log_function(self, original, args, kwargs)\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 209, in _run_and_log_function\r\n    result = original(self, *args, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 992, in fit\r\n    results = self.spawn_ddp_children(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 462, in spawn_ddp_children\r\n    results = self.ddp_train(local_rank, q=None, model=model, is_master=True)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_data_parallel.py\", line 560, in ddp_train\r\n    results = self.run_pretrain_routine(model)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1213, in run_pretrain_routine\r\n    self.train()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 392, in train\r\n    self.run_training_teardown()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/training_loop.py\", line 872, in run_training_teardown\r\n    self.on_train_end()\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/callback_hook.py\", line 72, in on_train_end\r\n    callback.on_train_end(self, self.get_model())\r\n  File \"\/home\/ubuntu\/mnist\/pytorch_autolog.py\", line 120, in on_train_end\r\n    mlflow.pytorch.log_model(trainer.model, \"models\")\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 179, in log_model\r\n    signature=signature, input_example=input_example, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/models\/model.py\", line 154, in log\r\n    **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/mlflow\/pytorch\/__init__.py\", line 300, in save_model\r\n    torch.save(pytorch_model, model_path, pickle_module=pickle_module, **kwargs)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 370, in save\r\n    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/torch\/serialization.py\", line 443, in _legacy_save\r\n    pickler.dump(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/site-packages\/cloudpickle\/cloudpickle.py\", line 491, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 659, in save_reduce\r\n    self._batch_setitems(dictitems)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"\/home\/ubuntu\/anaconda3\/lib\/python3.7\/pickle.py\", line 524, in save\r\n    rv = reduce(self.proto)\r\nTypeError: can't pickle _thread.lock objects\r\n\r\n\r\n```\r\n\r\n\r\n\r\n#### What have you tried?\r\nTried out the possibilities mentioned in the similar thread - https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/2186\r\n\r\nTried  wrapping the code inside, `trainer.is_global_zero`  . And also tried `trainer.global_rank == 0`. Also tried decorating the method as `@rank_zero_only`. But no luck. Getting the same error. \r\n\r\n#### What's your environment?\r\n\r\n - OS: Ubuntu\r\n - Packaging - torch, pytorch-lightning, torchvision, mlflow",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: typeerror: can't pickle _thread.lock objects - error while logging model into in multi gpu scenario; Content: ## questions and help #### what is your question? trying to log model into using `.pytorch.log_model` in train end. getting the above error only in multi gpu scenario. ``` stack trace ``` typeerror: can't pickle _thread.lock objects ``` #### what have you tried? tried out the possibilities mentioned in the similar thread - https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/issues\/2186 tried wrapping the code inside, `trainer.is_global_zero` . and also tried `trainer.global_rank == 0`. also tried decorating the method as `@rank_zero_only`. but no luck. getting the same error. #### what's your environment? - os: ubuntu - packaging - torch, pytorch-lightning, torchvision,",
        "Issue_original_content_gpt_summary":"The user is encountering a TypeError when attempting to log a model into a multi-GPU scenario using PyTorch Lightning, despite trying various solutions.",
        "Issue_preprocessed_content":"Title: typeerror can't pickle objects error while logging model into in multi gpu scenario; Content: questions and help what is your question? trying to log model into using in train end. getting the above error only in multi gpu scenario. code mnist script file sample code from autolog callback class. stack trace what have you tried? tried out the possibilities mentioned in the similar thread tried wrapping the code inside, . and also tried . also tried decorating the method as . but no luck. getting the same error. what's your environment? os ubuntu packaging torch, torchvision,"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3393",
        "Issue_title":"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",
        "Issue_creation_time":1599546516000,
        "Issue_closed_time":1599644307000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.\r\nI have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.\r\n\r\n### To Reproduce\r\n1. Start an MLFlow server locally\r\n```\r\nmlflow ui\r\n```\r\n2. Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)\r\n3. Uncomment out the `tracking_uri` to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.\r\n\r\n#### Code sample\r\n```\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader\r\nimport pytorch_lightning as pl\r\n\r\nclass MyModel(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_examples = 5000\r\n        self.num_valid = 1000\r\n        self.batch_size = 64\r\n        self.lr = 1e-3\r\n        self.wd = 1e-2\r\n        self.num_features = 2\r\n        self.linear = torch.nn.Linear(self.num_features, 1)\r\n        self.loss_func = torch.nn.MSELoss()\r\n        self.X = torch.rand(self.num_examples, self.num_features)\r\n        self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)\r\n        \r\n    def forward(self, x):\r\n        return self.linear(x)\r\n\r\n    def train_dataloader(self): \r\n        ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def val_dataloader(self): \r\n        ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])\r\n        dl = DataLoader(ds, batch_size=self.batch_size)\r\n        return dl\r\n\r\n    def configure_optimizers(self):\r\n        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\r\n\r\n    def training_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.TrainResult(minimize=loss)\r\n        result.log('train_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\n    def validation_step(self, batch, batch_idx):\r\n        x, y = batch\r\n        yhat = self(x)\r\n        loss = self.loss_func(yhat, y)\r\n        result = pl.EvalResult(early_stop_on=loss)\r\n        result.log('val_loss', loss, on_epoch=True, on_step=False)\r\n        return result\r\n\r\nif __name__ == '__main__':\r\n    from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger\r\n    mlf_logger = MLFlowLogger(\r\n        experiment_name=f\"MyModel\",\r\n        # tracking_uri=\"http:\/\/localhost:5000\"\r\n    )\r\n    trainer = pl.Trainer(\r\n        min_epochs=5,\r\n        max_epochs=50,\r\n        early_stop_callback=True,\r\n        logger=mlf_logger\r\n    )\r\n    model = MyModel()\r\n    trainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen using the TrainResult and EvalResult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop. \r\nThis would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.\r\n\r\n### Environment\r\n```\r\n* CUDA:\r\n\t- GPU:\r\n\t- available:         False\r\n\t- version:           None\r\n* Packages:\r\n\t- numpy:             1.18.2\r\n\t- pyTorch_debug:     False\r\n\t- pyTorch_version:   1.6.0+cpu\r\n\t- pytorch-lightning: 0.9.0\r\n\t- tensorboard:       2.2.0\r\n\t- tqdm:              4.48.2\r\n* System:\r\n\t- OS:                Linux\r\n\t- architecture:\r\n\t\t- 64bit\r\n\t\t-\r\n\t- processor:         x86_64\r\n\t- python:            3.7.9\r\n\t- version:           #1 SMP Tue May 26 11:42:35 UTC 2020\r\n```\r\n### Additional context\r\n\r\nWe host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed. \r\nIt appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.\r\n\r\n### Solution\r\nI've done a bit of debugging in the codebase and have been able to isolate the cause in two places\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L125-L129\r\nHere `self.experiment` is called regardless of whether `self._run_id` exists. If we add an `if not self._run_id` here we avoid calling `self._mlflow_client.get_experiment_by_name(self._experiment_name)` on each step.\r\nHowever we still call it each time we log metrics to MFlow, because of the property `self.experiment`.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/mlflow.py#L100-L112\r\nHere if we store `expt` within the logger and only call `self._mlflow_client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.\r\n\r\nI'd be happy to raise a PR for this fix.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logger slows training steps dramatically, despite only setting metrics to be logged on epoch; Content: ## bug when using the logger, with a remote server, logging per step introduces latency which slows the training loop. i have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. i suspect the logger is still communicating with the server on each training step. ### to reproduce 1. start an server locally ``` ui ``` 2. run the minimal code example below as is, (with logger set to use the default file uri.) 3. uncomment out the `tracking_uri` to use the local server and run the code again. you will see a 2-3 times drop in the iterations per second. #### code sample ``` import torch from torch.utils.data import tensordataset, dataloader import pytorch_lightning as pl class mymodel(pl.lightningmodule): def __init__(self): super().__init__() self.num_examples = 5000 self.num_valid = 1000 self.batch_size = 64 self.lr = 1e-3 self.wd = 1e-2 self.num_features = 2 self.linear = torch.nn.linear(self.num_features, 1) self.loss_func = torch.nn.mseloss() self.x = torch.rand(self.num_examples, self.num_features) self.y = self.x.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples) def forward(self, x): return self.linear(x) def train_dataloader(self): ds = tensordataset(self.x[:-self.num_valid], self.x[:-self.num_valid]) dl = dataloader(ds, batch_size=self.batch_size) return dl def val_dataloader(self): ds = tensordataset(self.x[-self.num_valid:], self.x[-self.num_valid:]) dl = dataloader(ds, batch_size=self.batch_size) return dl def configure_optimizers(self): return torch.optim.adam(self.parameters(), lr=self.lr, weight_decay=self.wd) def training_step(self, batch, batch_idx): x, y = batch yhat = self(x) loss = self.loss_func(yhat, y) result = pl.trainresult(minimize=loss) result.log('train_loss', loss, on_epoch=true, on_step=false) return result def validation_step(self, batch, batch_idx): x, y = batch yhat = self(x) loss = self.loss_func(yhat, y) result = pl.evalresult(early_stop_on=loss) result.log('val_loss', loss, on_epoch=true, on_step=false) return result if __name__ == '__main__': from pytorch_lightning.loggers import tensorboardlogger, logger mlf_logger = logger( experiment_name=f\"mymodel\", # tracking_uri=\"http:\/\/localhost:5000\" ) trainer = pl.trainer( min_epochs=5, max_epochs=50, early_stop_callback=true, logger=mlf_logger ) model = mymodel() trainer.fit(model) ``` ### expected behavior when using the trainresult and evalresult, or manually handling metric logging using the `training_epoch_end` and `validation_epoch_end` callbacks. it should be possible to avoid the logger from communicating with the server in each training loop. this would make it feasible to implement the when a remote server is used for experiment tracking. ### environment ``` * cuda: - gpu: - available: false - version: none * packages: - numpy: 1.18.2 - pytorch_debug: false - pytorch_version: 1.6.0+cpu - pytorch-lightning: 0.9.0 - tensorboard: 2.2.0 - tqdm: 4.48.2 * system: - os: linux - architecture: - 64bit - - processor: x86_64 - python: 3.7.9 - version: #1 smp tue may 26 11:42:35 utc 2020 ``` ### additional context we host a instance in aws and would like to be able to track experiments without affecting the training speed. it appears that in general the logger is much less performant than the default tensorboard logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop. ### solution i've done a bit of debugging in the codebase and have been able to isolate the cause in two places https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/.py#l125-l129 here `self.experiment` is called regardless of whether `self._run_id` exists. if we add an `if not self._run_id` here we avoid calling `self.__client.get_experiment_by_name(self._experiment_name)` on each step. however we still call it each time we log metrics to mflow, because of the property `self.experiment`. https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/d438ad8a8db3e76d3ed4e3c6bc9b91d6b3266b8e\/pytorch_lightning\/loggers\/.py#l100-l112 here if we store `expt` within the logger and only call `self.__client.get_experiment_by_name` when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the logging appears to be working as expected. i'd be happy to raise a pr for this fix.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where using the logger with a remote server to log metrics per step introduces latency which slows the training loop, despite configuring logging of metrics only per epoch.",
        "Issue_preprocessed_content":"Title: logger slows training steps dramatically, despite only setting metrics to be logged on epoch; Content: bug when using the logger, with a remote server, logging per step introduces latency which slows the training loop. i have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. i suspect the logger is still communicating with the server on each training step. to reproduce . start an server locally . run the minimal code example below as is, . uncomment out the to use the local server and run the code again. you will see a times drop in the iterations per second. code sample expected behavior when using the trainresult and evalresult, or manually handling metric logging using the and callbacks. it should be possible to avoid the logger from communicating with the server in each training loop. this would make it feasible to implement the when a remote server is used for experiment tracking. environment additional context we host a instance in aws and would like to be able to track experiments without affecting the training speed. it appears that in general the logger is much less performant than the default tensorboard logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop. solution i've done a bit of debugging in the codebase and have been able to isolate the cause in two places here is called regardless of whether exists. if we add an here we avoid calling on each step. however we still call it each time we log metrics to mflow, because of the property . here if we store within the logger and only call when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the logging appears to be working as expected. i'd be happy to raise a pr for this fix."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3392",
        "Issue_title":"mlflow training loss not reported until end of run",
        "Issue_creation_time":1599521969000,
        "Issue_closed_time":1599634019000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"I think I'm logging correctly, this is my `training_step`\r\n\r\n        result = pl.TrainResult(loss)\r\n        result.log('loss\/train', loss)\r\n        return result\r\n\r\nand `validation_step`\r\n\r\n        result = pl.EvalResult(loss)\r\n        result.log('loss\/validation', loss)\r\n        return result\r\n\r\nThe validation loss is updated in mlflow each epoch, however the training loss isn't displayed until training has finished. Then it's available for every step. This may be a mlflow rather than pytorch-lighting issue - somewhere along the line it seems to be buffered?\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/5028974\/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png)\r\n\r\nVersions:\r\n\r\npytorch-lightning==0.9.0\r\nmlflow==1.11.0\r\n\r\nEdit: logging TrainResult with on_epoch=True results in the metric appearing in mlflow during training, it's only the default train logging which gets delayed. i.e.\r\n\r\n        result.log('accuracy\/train', acc, on_epoch=True)\r\n\r\nis fine\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: training loss not reported until end of run; Content: i think i'm logging correctly, this is my `training_step` result = pl.trainresult(loss) result.log('loss\/train', loss) return result and `validation_step` result = pl.evalresult(loss) result.log('loss\/validation', loss) return result the validation loss is updated in each epoch, however the training loss isn't displayed until training has finished. then it's available for every step. this may be a rather than pytorch-lighting issue - somewhere along the line it seems to be buffered? ![image](https:\/\/user-images.githubusercontent.com\/5028974\/92420471-d5b56c00-f1b6-11ea-9296-db075c3dcf87.png) versions: pytorch-lightning==0.9.0 ==1.11.0 edit: logging trainresult with on_epoch=true results in the metric appearing in during training, it's only the default train logging which gets delayed. i.e. result.log('accuracy\/train', acc, on_epoch=true) is fine",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the training loss was not reported until the end of the run, despite being logged correctly, which may be a PyTorch-Lightning issue.",
        "Issue_preprocessed_content":"Title: training loss not reported until end of run; Content: i think i'm logging correctly, this is my result loss return result and result loss return result the validation loss is updated in each epoch, however the training loss isn't displayed until training has finished. then it's available for every step. this may be a rather than issue somewhere along the line it seems to be buffered? versions edit logging trainresult with results in the metric appearing in during training, it's only the default train logging which gets delayed. acc, is fine"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/3046",
        "Issue_title":"MLFlowLogger throws a JSONDecodeError",
        "Issue_creation_time":1597814570000,
        "Issue_closed_time":1597848054000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n#### Code sample\r\n<!-- Ideally attach a minimal code sample to reproduce the decried issue. \r\nMinimal means having the shortest code but still preserving the bug. -->\r\n\r\n```python\r\nfrom pytorch_lightning import Trainer\r\nfrom pytorch_lightning.loggers import MLFlowLogger\r\nmlflow_logger = MLFlowLogger(experiment_name=\"test-experiment\", tracking_uri=\"URI_HERE\")\r\nt = Trainer(logger=mlflow_logger)\r\nt.logger.experiment_id\r\n```\r\nthrows a `JSONDecodeError` exception.\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 120, in experiment_id\r\n    _ = self.experiment\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment\r\n    return fn(self)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 98, in experiment\r\n    expt = self._mlflow_client.get_experiment_by_name(self._experiment_name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 154, in get_experiment_by_name\r\n    return self._tracking_client.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name\r\n    return self.store.get_experiment_by_name(name)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name\r\n    response_proto = self._call_endpoint(GetExperimentByName, req_body)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/mlflow\/utils\/rest_utils.py\", line 145, in call_endpoint\r\n    js_dict = json.loads(response.text)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode\r\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\r\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n```\r\n### Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n### Environment\r\nEnvironment details\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.6.0\r\n - PyTorch Lightning Version: 0.9.0rc12\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7.7\r\n - CUDA\/cuDNN version: Not relevant\r\n - GPU models and configuration: Not relevant\r\n - Any other relevant information: Not relevant\r\n\r\n### Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logger throws a jsondecodeerror; Content: ## bug ### to reproduce steps to reproduce the behavior: #### code sample ```python from pytorch_lightning import trainer from pytorch_lightning.loggers import logger _logger = logger(experiment_name=\"test-experiment\", tracking_uri=\"uri_here\") t = trainer(logger=_logger) t.logger.experiment_id ``` throws a `jsondecodeerror` exception. ```python traceback (most recent call last): file \"\", line 1, in file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/.py\", line 120, in experiment_id _ = self.experiment file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 421, in experiment return get_experiment() or dummyexperiment() file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 13, in wrapped_fn return fn(*args, **kwargs) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 420, in get_experiment return fn(self) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/.py\", line 98, in experiment expt = self.__client.get_experiment_by_name(self._experiment_name) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/tracking\/client.py\", line 154, in get_experiment_by_name return self._tracking_client.get_experiment_by_name(name) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 114, in get_experiment_by_name return self.store.get_experiment_by_name(name) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/store\/tracking\/rest_store.py\", line 219, in get_experiment_by_name response_proto = self._call_endpoint(getexperimentbyname, req_body) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/store\/tracking\/rest_store.py\", line 32, in _call_endpoint return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto) file \"\/envs\/pl_env\/lib\/python3.7\/site-packages\/\/utils\/rest_utils.py\", line 145, in call_endpoint js_dict = json.loads(response.text) file \"\/envs\/pl_env\/lib\/python3.7\/json\/__init__.py\", line 348, in loads return _default_decoder.decode(s) file \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 337, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) file \"\/envs\/pl_env\/lib\/python3.7\/json\/decoder.py\", line 355, in raw_decode raise jsondecodeerror(\"expecting value\", s, err.value) from none json.decoder.jsondecodeerror: expecting value: line 1 column 1 (char 0) ``` ### expected behavior ### environment environment details ``` - pytorch version (e.g., 1.0): 1.6.0 - pytorch lightning version: 0.9.0rc12 - os (e.g., linux): linux - how you installed pytorch (`conda`, `pip`, source): conda - build command you used (if compiling from source): - python version: 3.7.7 - cuda\/cudnn version: not relevant - gpu models and configuration: not relevant - any other relevant information: not relevant ### additional context",
        "Issue_original_content_gpt_summary":"The user encountered a JSONDecodeError when attempting to use the logger in PyTorch Lightning, which was caused by an issue with the tracking_uri.",
        "Issue_preprocessed_content":"Title: logger throws a jsondecodeerror; Content: bug a clear and concise description of what the bug is. to reproduce steps to reproduce the behavior code sample ideally attach a minimal code sample to reproduce the decried issue. minimal means having the shortest code but still preserving the bug. throws a exception. expected behavior a clear and concise description of what you expected to happen. environment environment details pytorch version pytorch lightning version os linux how you installed pytorch conda build command you used python version version not relevant gpu models and configuration not relevant any other relevant information not relevant additional context add any other context about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2939",
        "Issue_title":"mlflow checkpoints in the wrong location ",
        "Issue_creation_time":1597273128000,
        "Issue_closed_time":1597488847000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"I'm not sure if I'm doing something wrong, I'm using mlflow instead of tensorboard as a logger. I've used the defaults i.e.\r\n\r\n```\r\nmlflow = loggers.MLFlowLogger()\r\ntrainer = pl.Trainer.from_argparse_args(args, logger=mlflow)\r\n```\r\n\r\nI'm ending up with the following folder structure\r\n\r\n\\mlflow\r\n\\mlflow\\1\r\n\\mlflow\\1\\\\{guid}\\artifacts\r\n\\mlflow\\1\\\\{guid}\\metrics\r\n\\mlflow\\1\\\\{guid}\\params\r\n\\mlflow\\1\\\\{guid}\\meta.yaml\r\n**\\1\\\\{guid}\\checkpoints**\r\n\r\ni.e. the checkpoints are in the wrong location, they should be in the `\\mlflow` folder. \r\n\r\nPerhaps this is an mlflow rather than pytorch-lightning issue? \r\n\r\nI'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: checkpoints in the wrong location ; Content: i'm not sure if i'm doing something wrong, i'm using instead of tensorboard as a logger. i've used the defaults i.e. ``` = loggers.logger() trainer = pl.trainer.from_argparse_args(args, logger=) ``` i'm ending up with the following folder structure \\ \\\\1 \\\\1\\\\{guid}\\artifacts \\\\1\\\\{guid}\\metrics \\\\1\\\\{guid}\\params \\\\1\\\\{guid}\\meta.yaml **\\1\\\\{guid}\\checkpoints** i.e. the checkpoints are in the wrong location, they should be in the `\\` folder. perhaps this is an rather than pytorch-lightning issue? i'm using pytorch-lightning 0.8.5 on macos running in python 3.7.6",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with checkpoints being stored in the wrong location when using PyTorch-Lightning 0.8.5 on MacOS running in Python 3.7.6.",
        "Issue_preprocessed_content":"Title: checkpoints in the wrong location ; Content: i'm not sure if i'm doing something wrong, i'm using instead of tensorboard as a logger. i've used the defaults i'm ending up with the following folder structure the checkpoints are in the wrong location, they should be in the folder. perhaps this is an rather than issue? i'm using on macos running in python"
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/2058",
        "Issue_title":"Hydra MLFlow Clash",
        "Issue_creation_time":1591172197000,
        "Issue_closed_time":1592925645000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"<!-- \r\n### Common bugs:\r\n1. Tensorboard not showing in Jupyter-notebook see [issue 79](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/issues\/79).    \r\n2. PyTorch 1.1.0 vs 1.2.0 support [see FAQ](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning#faq)    \r\n-->\r\n\r\n## \ud83d\udc1b Bug\r\n\r\nWhen using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a `DictConfig`, the condition in the `logger\/base.py` is not met.\r\n\r\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#L177\r\n\r\n### To Reproduce\r\n\r\nUse Hydra and MLFlow together. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in <module>\r\n    main()\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main\r\n    strict=strict,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra\r\n    overrides=args.overrides,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run\r\n    job_subdir_key=None,\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File \"\/home\/siavash\/KroniKare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main\r\n    trainer.fit(wound_seg_pl)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit\r\n    self.single_gpu_train(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train\r\n    self.run_pretrain_routine(model)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine\r\n    self.logger.log_hyperparams(ref_model.hparams)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in <listcomp>\r\n    [logger.log_hyperparams(params) for logger in self._logger_iterable]\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/mlflow.py\", line 105, in log_hyperparams\r\n    self.experiment.log_param(self.run_id, k, v)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/client.py\", line 206, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 177, in log_param\r\n    _validate_param_name(key)\r\n  File \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/mlflow\/utils\/validation.py\", line 120, in _validate_param_name\r\n    INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'\r\n```\r\n\r\n### Expected behavior\r\n\r\nCheck whether the instance if `dict` or `DictConfig` in the given line. \r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: hydra clash; Content: ## bug when using the logger with hydra, because the parameters passed to the lightningmodule is a `dictconfig`, the condition in the `logger\/base.py` is not met. https:\/\/github.com\/pytorchlightning\/pytorch-lightning\/blob\/8211256c46430e43e0c27e4f078c72085bb4ea34\/pytorch_lightning\/loggers\/base.py#l177 ### to reproduce use hydra and together. ```python traceback (most recent call last): file \"\/home\/siavash\/kronikare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 115, in main() file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/main.py\", line 24, in decorated_main strict=strict, file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/utils.py\", line 174, in run_hydra overrides=args.overrides, file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/_internal\/hydra.py\", line 86, in run job_subdir_key=none, file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/hydra\/plugins\/common\/utils.py\", line 109, in run_job ret.return_value = task_function(task_cfg) file \"\/home\/siavash\/kronikare\/kwae2\/kwae_ma\/models\/pl_train_segmentation_model.py\", line 111, in main trainer.fit(wound_seg_pl) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 765, in fit self.single_gpu_train(model) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/distrib_parts.py\", line 492, in single_gpu_train self.run_pretrain_routine(model) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 843, in run_pretrain_routine self.logger.log_hyperparams(ref_model.hparams) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in log_hyperparams [logger.log_hyperparams(params) for logger in self._logger_iterable] file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/base.py\", line 275, in [logger.log_hyperparams(params) for logger in self._logger_iterable] file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/utilities\/distributed.py\", line 10, in wrapped_fn return fn(*args, **kwargs) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/pytorch_lightning\/loggers\/.py\", line 105, in log_hyperparams self.experiment.log_param(self.run_id, k, v) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/\/tracking\/client.py\", line 206, in log_param self._tracking_client.log_param(run_id, key, value) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 177, in log_param _validate_param_name(key) file \"\/home\/siavash\/anaconda3\/envs\/kwae-ma\/lib\/python3.7\/site-packages\/\/utils\/validation.py\", line 120, in _validate_param_name invalid_parameter_value) .exceptions.exception: invalid parameter name: ''. names may be treated as files in certain cases, and must not resolve to other names when treated as such. this name would resolve to '.' ``` ### expected behavior check whether the instance if `dict` or `dictconfig` in the given line.",
        "Issue_original_content_gpt_summary":"The user encountered challenges when using the logger with hydra, due to the parameters passed to the lightningmodule being a `dictconfig`, which caused the condition in the `logger\/base.py` to not be met.",
        "Issue_preprocessed_content":"Title: hydra clash; Content: bug when using the logger with hydra, because the parameters passed to the lightningmodule is a , the condition in the is not met. to reproduce use hydra and together. if you have a code sample, error messages, stack traces, please provide it here as well expected behavior check whether the instance if or in the given line."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning\/issues\/630",
        "Issue_title":"Pickle error from Trainer.fit when using MLFlowLogger and distributed data parallel without SLURM",
        "Issue_creation_time":1576464430000,
        "Issue_closed_time":1583540837000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nTrainer.fit fails with a pickle error when the logger is MLFlowLogger, and distributed_backend='ddp' on GPUs but without SLURM.\r\n\r\n### To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Instantiate MLFlowLogger in Pytorch 0.5.3.2 with Pytorch 1.3.1 and MLFlow 1.4.0. The execution environment has environment variables MLFLOW_TRACKING_URI, and also MLFLOW_TRACKING_USERNAME and MLFLOW_TRACKING_PASSWORD to connect to the MLflow tracking server with HTTP Basic Authentication. The MLflow tracking server is also v1.4.0.\r\n2. Instantiate Trainer with MLFlowLogger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with NVIDIA GPUs but without SLURM.\r\n3. Run Trainer.fit\r\n\r\nFrom the error output, it looks like multiprocessing is attempting to pickle the nested function in MLflow function [_get_rest_store](https:\/\/github.com\/mlflow\/mlflow\/blob\/v1.4.0\/mlflow\/tracking\/_tracking_service\/utils.py#L81):\r\n```\r\nayla.khan@gpu12:~\/photosynthetic$ python test_mlflow.py\r\nTraceback (most recent call last):\r\n  File \"test_mlflow.py\", line 71, in <module>\r\n    trainer.fit(model)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit\r\n    mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn\r\n    process.start()\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/mnt\/unihome\/home\/CORP\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_get_rest_store.<locals>.get_default_host_creds'\r\n```\r\n\r\n#### Code sample\r\nSample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)):\r\n\r\n```\r\ntest_hparams = Namespace()\r\nmodel = XORGateModel(test_hparams)\r\n\r\nlogger = MLFlowLogger(experiment_name='test_lightning_logger',\r\n                                          tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\r\ntrainer = pl.Trainer(logger=logger, distributed_backend='ddp', gpus='-1')\r\ntrainer.fit(model)\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer.fit runs without error.\r\n\r\n### Environment\r\n\r\n```\r\n(photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\nGPU 4: GeForce GTX 1080 Ti\r\nGPU 5: GeForce GTX 1080 Ti\r\nGPU 6: GeForce GTX 1080 Ti\r\nGPU 7: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] pytorch-lightning==0.5.3.2\r\n[pip] pytorch-toolbelt==0.2.1\r\n[pip] torch==1.3.1\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] pytorch-lightning         0.5.3.2                  pypi_0    pypi\r\n[conda] pytorch-toolbelt          0.2.1                    pypi_0    pypi\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: pickle error from trainer.fit when using logger and distributed data parallel without slurm; Content: ## bug trainer.fit fails with a pickle error when the logger is logger, and distributed_backend='ddp' on gpus but without slurm. ### to reproduce steps to reproduce the behavior: 1. instantiate logger in pytorch 0.5.3.2 with pytorch 1.3.1 and 1.4.0. the execution environment has environment variables _tracking_uri, and also _tracking_username and _tracking_password to connect to the tracking server with http basic authentication. the tracking server is also v1.4.0. 2. instantiate trainer with logger instance as logger, distributed_backend='ddp' and with the gpus parameter on a machine with nvidia gpus but without slurm. 3. run trainer.fit from the error output, it looks like multiprocessing is attempting to pickle the nested function in function [_get_rest_store](https:\/\/github.com\/\/\/blob\/v1.4.0\/\/tracking\/_tracking_service\/utils.py#l81): ``` ayla.khan@gpu12:~\/photosynthetic$ python test_.py traceback (most recent call last): file \"test_.py\", line 71, in trainer.fit(model) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 343, in fit mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,)) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/spawn.py\", line 162, in spawn process.start() file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/process.py\", line 105, in start self._popen = self._popen(self) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/context.py\", line 284, in _popen return popen(process_obj) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__ super().__init__(process_obj) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_fork.py\", line 19, in __init__ self._launch(process_obj) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch reduction.dump(process_obj, fp) file \"\/mnt\/unihome\/home\/corp\/ayla.khan\/miniconda2\/envs\/photosynthetic\/lib\/python3.6\/multiprocessing\/reduction.py\", line 60, in dump forkingpickler(file, protocol).dump(obj) attributeerror: can't pickle local object '_get_rest_store..get_default_host_creds' ``` #### code sample sample code tested with a very simple test model ([gist](https:\/\/gist.github.com\/a-y-khan\/8693d2b186227561a4baf4d03ce75c34)): ``` test_hparams = namespace() model = xorgatemodel(test_hparams) logger = logger(experiment_name='test_lightning_logger', tracking_uri=os.environ['_tracking_uri']) trainer = pl.trainer(logger=logger, distributed_backend='ddp', gpus='-1') trainer.fit(model) ``` ### expected behavior trainer.fit runs without error. ### environment ``` (photosynthetic) ayla.khan@gpu12:~\/photosynthetic$ python collect_env.py collecting environment information... pytorch version: 1.3.1 is debug build: no cuda used to build pytorch: 10.1.243 os: centos linux 7 (core) gcc version: (gcc) 4.8.5 20150623 (red hat 4.8.5-39) cmake version: could not collect python version: 3.6 is cuda available: yes cuda runtime version: 10.0.130 gpu models and configuration: gpu 0: geforce gtx 1080 ti gpu 1: geforce gtx 1080 ti gpu 2: geforce gtx 1080 ti gpu 3: geforce gtx 1080 ti gpu 4: geforce gtx 1080 ti gpu 5: geforce gtx 1080 ti gpu 6: geforce gtx 1080 ti gpu 7: geforce gtx 1080 ti nvidia driver version: 440.33.01 cudnn version: \/usr\/local\/cuda-10.0\/lib64\/libcudnn.so.7 versions of relevant libraries: [pip] numpy==1.16.4 [pip] pytorch-lightning==0.5.3.2 [pip] pytorch-toolbelt==0.2.1 [pip] torch==1.3.1 [pip] torchsummary==1.5.1 [pip] torchvision==0.4.2 [conda] blas 1.0 mkl [conda] mkl 2019.4 243 [conda] mkl-service 2.3.0 py36he904b0f_0 [conda] mkl_fft 1.0.15 py36ha843d7b_0 [conda] mkl_random 1.1.0 py36hd6b4f25_0 [conda] pytorch 1.3.1 py3.6_cuda10.1.243_cudnn7.6.3_0 pytorch [conda] pytorch-lightning 0.5.3.2 pypi_0 pypi [conda] pytorch-toolbelt 0.2.1 pypi_0 pypi [conda] torchsummary 1.5.1 pypi_0 pypi [conda] torchvision 0.4.2 py36_cu101 pytorch ```",
        "Issue_original_content_gpt_summary":"The user encountered a pickle error when using logger, distributed_backend='ddp' on gpus without slurm while running trainer.fit in PyTorch 0.5.3.2 with PyTorch 1.3.1 and 1.4.0.",
        "Issue_preprocessed_content":"Title: pickle error from when using logger and distributed data parallel without slurm; Content: bug fails with a pickle error when the logger is logger, and on gpus but without slurm. to reproduce steps to reproduce the behavior . instantiate logger in pytorch with pytorch and the execution environment has environment variables and also and to connect to the tracking server with http basic authentication. the tracking server is also . instantiate trainer with logger instance as logger, and with the gpus parameter on a machine with nvidia gpus but without slurm. . run from the error output, it looks like multiprocessing is attempting to pickle the nested function in function code sample sample code tested with a very simple test model expected behavior runs without error. environment"
    },
    {
        "Issue_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/7232",
        "Issue_title":"Mlflow UI deployment error",
        "Issue_creation_time":1662389970000,
        "Issue_closed_time":1662467440000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"```\r\nAttributeError: 'DatabaseServiceMetadataPipeline' object has no attribute 'mlModelFilterPattern'\r\n```\r\n\r\nWe need to review which configuration param is being sent here",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: ui deployment error; Content: ``` attributeerror: 'databaseservicemetadatapipeline' object has no attribute 'mlmodelfilterpattern' ``` we need to review which configuration param is being sent here",
        "Issue_original_content_gpt_summary":"The user encountered an AttributeError when attempting to deploy a UI, and needs to review the configuration parameters being sent.",
        "Issue_preprocessed_content":"Title: ui deployment error; Content: we need to review which configuration param is being sent here"
    },
    {
        "Issue_link":"https:\/\/github.com\/open-metadata\/OpenMetadata\/issues\/5688",
        "Issue_title":"MlFlow deployment fails from UI",
        "Issue_creation_time":1656394659000,
        "Issue_closed_time":1656421119000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":null,
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: deployment fails from ui; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered challenges when attempting to deploy from the UI.",
        "Issue_preprocessed_content":"Title: deployment fails from ui; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4130",
        "Issue_title":"error creating a triton deployment mlflow plugin",
        "Issue_creation_time":1648621921000,
        "Issue_closed_time":1649449895000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Description**\r\nError when a MLflow registry model is deployed to triton using **mlflow-triton-plugin** with `--falvor=onnx` flag.\r\nThe plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself.\r\nEasy fix is to create that folder beforehand, but could also be handled from the plugin side.\r\n\r\n```\r\n# create a dir if not exists  \r\nif not os.exists(triton_deployment_dir):\r\n  os.mkdir(triton_deployment_dir)\r\n# then write config to that dir\r\nwith open(os.path.join(triton_deployment_dir, \"config.pbtxt\"),\r\n            \"w\") as cfile:\r\n    cfile.write(config)\r\n```\r\n\r\n**Triton Information**\r\nDocker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3`\r\n\r\n**To Reproduce**\r\n\r\n0. Install mlflow-triton-plugin\r\n1. Log and register an ONNX model to MLflow model registry.\r\n2. Run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false`\r\n3. Create a deployment from mlflow:\r\n `mlflow deployments create -t triton --flavor onnx --name <model-name> -m \"models:\/<model-name>\/1\"`\r\n\r\nError is raised:\r\n```\r\nFile \"mlflow_triton\/deployments.py\", line 105, in create_deployment\r\n  File \"mlflow_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo\r\n  File \"mlflow_triton\/deployments.py\", line 326, in _get_copy_paths\r\nFileNotFoundError: [Errno 2] No such file or directory: '<dest-folder>\/<model-name>\/config.pbtxt'\r\n```\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: error creating a triton deployment plugin; Content: **description** error when a registry model is deployed to triton using **-triton-plugin** with `--falvor=onnx` flag. the plugin is trying to create a `config.pbtxt` in the destination folder before creating that model folder itself. easy fix is to create that folder beforehand, but could also be handled from the plugin side. ``` # create a dir if not exists if not os.exists(triton_deployment_dir): os.mkdir(triton_deployment_dir) # then write config to that dir with open(os.path.join(triton_deployment_dir, \"config.pbtxt\"), \"w\") as cfile: cfile.write(config) ``` **triton information** docker image: `nvcr.io\/nvidia\/tritonserver:21.12-py3` **to reproduce** 0. install -triton-plugin 1. log and register an onnx model to model registry. 2. run a triton inference server with these flags: `--model-control-mode=explicit --strict-model-config=false` 3. create a deployment from : ` deployments create -t triton --flavor onnx --name -m \"models:\/\/1\"` error is raised: ``` file \"_triton\/deployments.py\", line 105, in create_deployment file \"_triton\/deployments.py\", line 332, in _copy_files_to_triton_repo file \"_triton\/deployments.py\", line 326, in _get_copy_paths filenotfounderror: [errno 2] no such file or directory: '\/\/config.pbtxt' ```",
        "Issue_original_content_gpt_summary":"The user encountered an error when deploying a registry model to Triton using the \"-triton-plugin\" with the \"--flavor=onnx\" flag, which was caused by the plugin trying to create a \"config.pbtxt\" in the destination folder before creating the model folder itself.",
        "Issue_preprocessed_content":"Title: error creating a triton deployment plugin; Content: description error when a registry model is deployed to triton using with flag. the plugin is trying to create a in the destination folder before creating that model folder itself. easy fix is to create that folder beforehand, but could also be handled from the plugin side. triton information docker image to reproduce . install . log and register an onnx model to model registry. . run a triton inference server with these flags . create a deployment from error is raised"
    },
    {
        "Issue_link":"https:\/\/github.com\/triton-inference-server\/server\/issues\/4089",
        "Issue_title":"Input to the script for publishing models to mlflow is overly particular with inputs",
        "Issue_creation_time":1647959057000,
        "Issue_closed_time":1650643135000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Description**\r\nWhen using the `publish_model_to_mlflow.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways.\r\n\r\n**Triton Information**\r\nWhat version of Triton are you using? 2.19.0\r\n\r\nAre you using the Triton container or did you build it yourself? container\r\n\r\n**To Reproduce**\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\\r\n    --flavor triton\r\n```\r\n\r\nThis gives the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"publish_model_to_mlflow.py\", line 71, in <module>\r\n    publish_to_mlflow()\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"publish_model_to_mlflow.py\", line 56, in publish_to_mlflow\r\n    triton_flavor.log_model(\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model\r\n    Model.log(\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 282, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"\/mlflow\/triton-inference-server\/server\/deploy\/mlflow-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model\r\n    shutil.copytree(triton_model_path, model_data_path)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 557, in copytree\r\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/shutil.py\", line 458, in _copytree\r\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\r\n  File \"\/opt\/conda\/envs\/mlflow\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nFileExistsError: [Errno 17] File exists: '\/tmp\/tmpdg2r5f0_\/model\/'\r\ncommand terminated with exit code 1\r\n```\r\n\r\nThe model being used seems to have no effect on the error.\r\n\r\n**Expected behavior**\r\nThe input provided is syntactically identical to:\r\n```\r\npython publish_model_to_mlflow.py \\\r\n    --model_name abp-nvsmi-xgb \\\r\n    --model_directory \/common\/models\/abp-nvsmi-xgb \\\r\n    --flavor triton\r\n```\r\n\r\nand should provide the same outcome.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: input to the script for publishing models to is overly particular with inputs; Content: **description** when using the `publish_model_to_.py` script, if the value given for the `--model_directory` argument has a trailing `\/`, the script will bomb in interesting ways. **triton information** what version of triton are you using? 2.19.0 are you using the triton container or did you build it yourself? container **to reproduce** ``` python publish_model_to_.py \\ --model_name abp-nvsmi-xgb \\ --model_directory \/common\/models\/abp-nvsmi-xgb\/ \\ --flavor triton ``` this gives the following error: ``` traceback (most recent call last): file \"publish_model_to_.py\", line 71, in publish_to_() file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 1128, in __call__ return self.main(*args, **kwargs) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 1053, in main rv = self.invoke(ctx) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 1395, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/click\/core.py\", line 754, in invoke return __callback(*args, **kwargs) file \"publish_model_to_.py\", line 56, in publish_to_ triton_flavor.log_model( file \"\/\/triton-inference-server\/server\/deploy\/-triton-plugin\/scripts\/triton_flavor.py\", line 100, in log_model model.log( file \"\/opt\/conda\/envs\/\/lib\/python3.8\/site-packages\/\/models\/model.py\", line 282, in log flavor.save_model(path=local_path, _model=_model, **kwargs) file \"\/\/triton-inference-server\/server\/deploy\/-triton-plugin\/scripts\/triton_flavor.py\", line 73, in save_model shutil.copytree(triton_model_path, model_data_path) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/shutil.py\", line 557, in copytree return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks, file \"\/opt\/conda\/envs\/\/lib\/python3.8\/shutil.py\", line 458, in _copytree os.makedirs(dst, exist_ok=dirs_exist_ok) file \"\/opt\/conda\/envs\/\/lib\/python3.8\/os.py\", line 223, in makedirs mkdir(name, mode) fileexistserror: [errno 17] file exists: '\/tmp\/tmpdg2r5f0_\/model\/' command terminated with exit code 1 ``` the model being used seems to have no effect on the error. **expected behavior** the input provided is syntactically identical to: ``` python publish_model_to_.py \\ --model_name abp-nvsmi-xgb \\ --model_directory \/common\/models\/abp-nvsmi-xgb \\ --flavor triton ``` and should provide the same outcome.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using the `publish_model_to_.py` script, where if the value given for the `--model_directory` argument had a trailing `\/`, the script would fail in unexpected ways.",
        "Issue_preprocessed_content":"Title: input to the script for publishing models to is overly particular with inputs; Content: description when using the script, if the value given for the argument has a trailing , the script will bomb in interesting ways. triton information what version of triton are you using? are you using the triton container or did you build it yourself? container to reproduce this gives the following error the model being used seems to have no effect on the error. expected behavior the input provided is syntactically identical to and should provide the same outcome."
    },
    {
        "Issue_link":"https:\/\/github.com\/mindsdb\/mindsdb\/issues\/2043",
        "Issue_title":"[ BYOM MLflow ] Check valid URL when creating predictor",
        "Issue_creation_time":1646758611000,
        "Issue_closed_time":1656947080000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"At the moment, an mlflow byom predictor with arbitrary URLs can be created. We should first check whether an actual mlflow model is served at that URL before creating\/linking said model.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [ byom ] check valid url when creating predictor; Content: at the moment, an byom predictor with arbitrary urls can be created. we should first check whether an actual model is served at that url before creating\/linking said model.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to check if a valid model is served at a given URL before creating\/linking the model when creating a BYOM predictor.",
        "Issue_preprocessed_content":"Title: check valid url when creating predictor; Content: at the moment, an byom predictor with arbitrary urls can be created. we should first check whether an actual model is served at that url before said model."
    },
    {
        "Issue_link":"https:\/\/github.com\/deepset-ai\/haystack\/issues\/2244",
        "Issue_title":"MLFlowLogging always disabled for training `FARMReader` models",
        "Issue_creation_time":1645701717000,
        "Issue_closed_time":1651060598000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nWhen training a Reader model, a user might want to log training statistics and metrics to MLFlow. However, when initializing a `FARMReader`, we initialize an `Inferencer`. There, we call `MLFlowLogger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#L77), which disables all logging to MLFlow. Therefore, when a user is calling the Reader's `train` method after initializing the Reader, no tranining statistics wil be logged.\r\n\r\nAs a workaround, the user can manually set `MLFlowLogger.disable_logging = False` before calling the `train` method.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logging always disabled for training `farmreader` models; Content: **describe the bug** when training a reader model, a user might want to log training statistics and metrics to . however, when initializing a `farmreader`, we initialize an `inferencer`. there, we call `logger.disable()` on [this line](https:\/\/github.com\/deepset-ai\/haystack\/blob\/15c70bdb9f8cd16511d1eb9ed9b2e9466de65cbf\/haystack\/modeling\/infer.py#l77), which disables all logging to . therefore, when a user is calling the reader's `train` method after initializing the reader, no tranining statistics wil be logged. as a workaround, the user can manually set `logger.disable_logging = false` before calling the `train` method.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where logging was always disabled for training `farmreader` models, requiring a workaround of manually setting `logger.disable_logging = false` before calling the `train` method.",
        "Issue_preprocessed_content":"Title: logging always disabled for training models; Content: describe the bug when training a reader model, a user might want to log training statistics and metrics to . however, when initializing a , we initialize an . there, we call on , which disables all logging to . therefore, when a user is calling the reader's method after initializing the reader, no tranining statistics wil be logged. as a workaround, the user can manually set before calling the method."
    },
    {
        "Issue_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/3146",
        "Issue_title":"bug: failed to containerize when using mlflow",
        "Issue_creation_time":1666803000000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":13.0,
        "Issue_body":"Trying to integrate Mlflow with my current bentoml workflow and following this example\r\n`https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch`\r\nBut getting error when i try to deploy model with docker, \r\n\r\nwhen i run  `bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\n```Building docker image for Bento(tag=\"mlflow_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")...\r\nERROR: failed to solve: executor failed running [\/bin\/sh -c bash <<EOF\r\nset -euxo pipefail\r\n\r\nif [ -f \/home\/bentoml\/bento\/env\/conda\/environment.yml ]; then\r\n   set pip_interop_enabled to improve conda-pip interoperability. Conda can use\r\n   pip-installed packages to satisfy dependencies.\r\n  echo \"Updating conda base environment with environment.yml\"\r\n  \/opt\/conda\/bin\/conda config --set pip_interop_enabled True\r\n  \/opt\/conda\/bin\/conda env update -n base -f \/home\/bentoml\/bento\/env\/conda\/environment.yml\r\n  \/opt\/conda\/bin\/conda clean --all\r\nfi\r\nEOF]: exit code: 1\r\nFailed building docker image: Command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', 'mlflow_pytfile', 'env\\\\docker\\\\Dockerfile', '--load', '.']' returned non-zero exit status 1.\r\n```\r\n### To reproduce\r\n\r\nBug recreation steps:\r\nClone the repo `https:\/\/github.com\/bentoml\/BentoML\/tree\/main\/examples\/mlflow\/pytorch` \r\nGoto the folder `examples\/mlflow\/pytorch`\r\n`python mnist.py`\r\n`bentoml build`\r\n`bentoml containerize mlflow_pytorch_mnist_demo:latest`\r\n\r\nP.S. `bentoml serve service.py:svc`  works fine`\r\n\r\n\r\n### Environment\r\n\r\nbentoml version 1.0.7\r\nPython version  3.9.12\r\nDocker Engine 20.10.17",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: bug: failed to containerize when using ; trying to integrate with my current bentoml workflow and following this example `https:\/\/github.com\/bentoml\/bentoml\/tree\/main\/examples\/\/pytorch` but getting error when i try to deploy model with docker, when i run `bentoml containerize _pytorch_mnist_demo:latest` ```building docker image for bento(tag=\"_pytorch_mnist_demo:3utxjn2vbgxh5gbc\")... error: failed to solve: executor failed running [\/bin\/sh -c bash <<eof set -euxo pipefail if [ -f \/home\/bentoml\/bento\/env\/conda\/environment.yml ]; Content: then set pip_interop_enabled to improve conda-pip interoperability. conda can use pip-installed packages to satisfy dependencies. echo \"updating conda base environment with environment.yml\" \/opt\/conda\/bin\/conda config --set pip_interop_enabled true \/opt\/conda\/bin\/conda env update -n base -f \/home\/bentoml\/bento\/env\/conda\/environment.yml \/opt\/conda\/bin\/conda clean --all fi eof]: exit code: 1 failed building docker image: command '['docker', 'buildx', 'build', '--progress', 'auto', '--tag', '_pytfile', 'env\\\\docker\\\\dockerfile', '--load', '.']' returned non-zero exit status 1. ``` ### to reproduce bug recreation steps: clone the repo `https:\/\/github.com\/bentoml\/bentoml\/tree\/main\/examples\/\/pytorch` goto the folder `examples\/\/pytorch` `python mnist.py` `bentoml build` `bentoml containerize _pytorch_mnist_demo:latest` p.s. `bentoml serve service.py:svc` works fine` ### environment bentoml version 1.0.7 python version 3.9.12 docker engine 20.10.17",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to containerize a BentoML model using the example from the BentoML GitHub repository, resulting in an error code of 1.",
        "Issue_preprocessed_content":"Title: bug failed to containerize when using ; Content: trying to integrate with my current bentoml workflow and following this example but getting error when i try to deploy model with docker, when i run to reproduce bug recreation steps clone the repo goto the folder works fine` environment bentoml version python version docker engine"
    },
    {
        "Issue_link":"https:\/\/github.com\/bentoml\/BentoML\/issues\/2160",
        "Issue_title":"MLflow pyfunc model can't be loaded",
        "Issue_creation_time":1641482199000,
        "Issue_closed_time":1642711286000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nI can't load a *mlflow* model in the beta version of BentoML 1.0\r\n\r\n**To Reproduce**\r\n1. Train & log a pyfunc model to mflow\r\n```\r\nfrom sklearn import svm, datasets\r\n\r\nimport mlflow\r\n\r\n\r\n# Load training data\r\niris = datasets.load_iris()\r\nX, y = iris.data, iris.target\r\n\r\n# Model Training\r\nclf = svm.SVC()\r\nclf.fit(X, y)\r\n\r\n# Wrap up as a custom pyfunc model\r\nclass ModelPyfunc(mlflow.pyfunc.PythonModel):\r\n    \r\n    def load_context(self, context):\r\n        self.model = clf\r\n    \r\n    def predict(self, context, model_input):\r\n        return self.model.predict(model_input)      \r\n      \r\n# Log model\r\nwith mlflow.start_run() as run:\r\n    model = ModelPyfunc()\r\n    mlflow.pyfunc.log_model(\"model\", python_model=model)\r\n    print(\"run_id: {}\".format(run.info.run_id))\r\n```\r\n\r\n2. Load it into BentoML\r\n```\r\nimport bentoml\r\n\r\nmodel_uri = f\"runs:\/{run.info.run_id}\/model\"\r\n\r\ntag = bentoml.mlflow.import_from_uri(\"model\", model_uri)\r\n\r\nmodel = bentoml.mlflow.load(tag)\r\n```\r\n3. The model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml.mlflow.load(tag)` is failing to **AttributeError** `module 'mlflow.pyfunc.model' has no attribute 'load_model'`\r\n\r\n**Expected behavior**\r\nPyfunc model should load without issues\r\n\r\n**Screenshots\/Logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"sandbox.py\", line 11, in <module>\r\n    bentoml.mlflow.load(tag)\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _\r\n    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\r\n  File \"\/Users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/mlflow.py\", line 85, in load\r\n    return loader_module.load_model(mlflow_folder)  # noqa\r\nAttributeError: module 'mlflow.pyfunc.model' has no attribute 'load_model'\r\n```\r\n\r\n**Environment:**\r\n - OS: MacOS 11.6\r\n - Python Version Python 3.8.5\r\n - BentoML Version BentoML-1.0.0a1",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: pyfunc model can't be loaded; Content: **describe the bug** i can't load a ** model in the beta version of bentoml 1.0 **to reproduce** 1. train & log a pyfunc model to mflow ``` from sklearn import svm, datasets import # load training data iris = datasets.load_iris() x, y = iris.data, iris.target # model training clf = svm.svc() clf.fit(x, y) # wrap up as a custom pyfunc model class modelpyfunc(.pyfunc.pythonmodel): def load_context(self, context): self.model = clf def predict(self, context, model_input): return self.model.predict(model_input) # log model with .start_run() as run: model = modelpyfunc() .pyfunc.log_model(\"model\", python_model=model) print(\"run_id: {}\".format(run.info.run_id)) ``` 2. load it into bentoml ``` import bentoml model_uri = f\"runs:\/{run.info.run_id}\/model\" tag = bentoml..import_from_uri(\"model\", model_uri) model = bentoml..load(tag) ``` 3. the model gets successfully stored in the local model store (listed in `bentoml models list`), however the loading `model = bentoml..load(tag)` is failing to **attributeerror** `module '.pyfunc.model' has no attribute 'load_model'` **expected behavior** pyfunc model should load without issues **screenshots\/logs** ``` traceback (most recent call last): file \"sandbox.py\", line 11, in bentoml..load(tag) file \"\/users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/simple_di\/__init__.py\", line 124, in _ return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs)) file \"\/users\/e056232\/opt\/miniconda3\/lib\/python3.8\/site-packages\/bentoml\/_internal\/frameworks\/.py\", line 85, in load return loader_module.load_model(_folder) # noqa attributeerror: module '.pyfunc.model' has no attribute 'load_model' ``` **environment:** - os: macos 11.6 - python version python 3.8.5 - bentoml version bentoml-1.0.0a1",
        "Issue_original_content_gpt_summary":"The user encountered an AttributeError when attempting to load a pyfunc model in the beta version of BentoML 1.0.",
        "Issue_preprocessed_content":"Title: pyfunc model can't be loaded; Content: describe the bug i can't load a model in the beta version of bentoml to reproduce . train & log a pyfunc model to mflow . load it into bentoml . the model gets successfully stored in the local model store , however the loading is failing to attributeerror expected behavior pyfunc model should load without issues environment os macos python version python bentoml version"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/3059",
        "Issue_title":"[BUG]: Runs recorded in MLflow nests all recursively when [full] installed",
        "Issue_creation_time":1667105333000,
        "Issue_closed_time":1669124369000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"### pycaret version checks\r\n\r\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\r\n\r\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\r\n\r\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\r\n\r\n\r\n### Issue Description\r\n\r\nWhen pycaret is installed with [full], all runs executed in one script are shown nested recursively in MLflow dashboard.\r\nThis happens only with [full] installation.\r\n\r\n### Reproducible Example\r\n\r\n```python\r\n%pip install -U pip wheel\r\n%pip install --pre pycaret[full]\r\n\r\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\n```\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpected display: (when installed without [full])\r\n![OK](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png)\r\n\r\nActual display: (when installed with [full])\r\n![NG](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png)\r\n\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nAttached the figure also in 'Expected Behavior'.\r\n```\r\n\r\n\r\n### Installed Versions\r\n\r\n<details>\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/home\/ak\/sample\/.venv\/bin\/python\r\n   machine: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n\r\nPyCaret required dependencies:\r\n                 pip: 22.3\r\n          setuptools: 44.0.0\r\n             pycaret: 3.0.0rc4\r\n             IPython: 8.5.0\r\n          ipywidgets: 8.0.2\r\n                tqdm: 4.64.1\r\n               numpy: 1.22.4\r\n              pandas: 1.4.4\r\n              jinja2: 3.1.2\r\n               scipy: 1.8.1\r\n              joblib: 1.2.0\r\n             sklearn: 1.1.3\r\n                pyod: 1.0.6\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.1.post0\r\n            lightgbm: 3.3.3\r\n               numba: 0.55.2\r\n            requests: 2.28.1\r\n          matplotlib: 3.5.3\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.5\r\n              plotly: 5.11.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.13.4\r\n               tbats: 1.1.1\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.3\r\n\r\nPyCaret optional dependencies:\r\n                shap: 0.41.0\r\n           interpret: 0.2.7\r\n                umap: 0.5.3\r\n    pandas_profiling: 3.4.0\r\n  explainerdashboard: 0.4.0\r\n             autoviz: 0.1.58\r\n           fairlearn: 0.8.0\r\n             xgboost: 1.7.0rc1\r\n            catboost: 1.1\r\n              kmodes: 0.12.2\r\n             mlxtend: 0.21.0\r\n       statsforecast: 1.1.3\r\n        tune_sklearn: 0.4.4\r\n                 ray: 2.0.1\r\n            hyperopt: 0.2.7\r\n              optuna: 3.0.3\r\n               skopt: 0.9.0\r\n              mlflow: 1.30.0\r\n              gradio: 3.8\r\n             fastapi: 0.85.1\r\n             uvicorn: 0.19.0\r\n              m2cgen: 0.10.0\r\n           evidently: 0.1.59.dev2\r\n                nltk: 3.7\r\n            pyLDAvis: Not installed\r\n              gensim: Not installed\r\n               spacy: Not installed\r\n           wordcloud: 1.8.2.2\r\n            textblob: 0.17.1\r\n               fugue: 0.6.6\r\n           streamlit: Not installed\r\n             prophet: Not installed\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: runs recorded in nests all recursively when [full] installed; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master). ### issue description when pycaret is installed with [full], all runs executed in one script are shown nested recursively in dashboard. this happens only with [full] installation. ### reproducible example ```python %pip install -u pip wheel %pip install --pre pycaret[full] import from pycaret.classification import * from pycaret.datasets import get_data .set_tracking_uri(\"http:\/\/localhost:5000\") data = get_data(\"diabetes\") setup(data, target=\"class variable\", log_experiment=true) compare_models(include=[\"lr\", \"svm\", \"rf\"]) setup(data, target=\"class variable\", log_experiment=true) compare_models(include=[\"lr\", \"svm\", \"rf\"]) ``` ### expected behavior expected display: (when installed without [full]) ![ok](https:\/\/user-images.githubusercontent.com\/1991802\/198862894-7a459755-5b94-4abc-a00b-be8d42e1f71c.png) actual display: (when installed with [full]) ![ng](https:\/\/user-images.githubusercontent.com\/1991802\/198862906-a26034b1-e22b-4d36-a0e5-1f0c5ccdad8c.png) ### actual results ```python-traceback attached the figure also in 'expected behavior'. ``` ### installed versions system: python: 3.9.5 (default, nov 23 2021, 15:27:38) [gcc 9.3.0] executable: \/home\/ak\/sample\/.venv\/bin\/python machine: linux-5.10.102.1-microsoft-standard-wsl2-x86_64-with-glibc2.31 pycaret required dependencies: pip: 22.3 setuptools: 44.0.0 pycaret: 3.0.0rc4 ipython: 8.5.0 ipywidgets: 8.0.2 tqdm: 4.64.1 numpy: 1.22.4 pandas: 1.4.4 jinja2: 3.1.2 scipy: 1.8.1 joblib: 1.2.0 sklearn: 1.1.3 pyod: 1.0.6 imblearn: 0.9.1 category_encoders: 2.5.1.post0 lightgbm: 3.3.3 numba: 0.55.2 requests: 2.28.1 matplotlib: 3.5.3 scikitplot: 0.3.7 yellowbrick: 1.5 plotly: 5.11.0 kaleido: 0.2.1 statsmodels: 0.13.2 sktime: 0.13.4 tbats: 1.1.1 pmdarima: 1.8.5 psutil: 5.9.3 pycaret optional dependencies: shap: 0.41.0 interpret: 0.2.7 umap: 0.5.3 pandas_profiling: 3.4.0 explainerdashboard: 0.4.0 autoviz: 0.1.58 fairlearn: 0.8.0 xgboost: 1.7.0rc1 catboost: 1.1 kmodes: 0.12.2 mlxtend: 0.21.0 statsforecast: 1.1.3 tune_sklearn: 0.4.4 ray: 2.0.1 hyperopt: 0.2.7 optuna: 3.0.3 skopt: 0.9.0 : 1.30.0 gradio: 3.8 fastapi: 0.85.1 uvicorn: 0.19.0 m2cgen: 0.10.0 evidently: 0.1.59.dev2 nltk: 3.7 pyldavis: not installed gensim: not installed spacy: not installed wordcloud: 1.8.2.2 textblob: 0.17.1 fugue: 0.6.6 streamlit: not installed prophet: not installed",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where installing pycaret with the [full] option caused all runs executed in one script to be shown nested recursively in the dashboard.",
        "Issue_preprocessed_content":"Title: runs recorded in nests all recursively when installed; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description when pycaret is installed with , all runs executed in one script are shown nested recursively in dashboard. this happens only with installation. reproducible example expected behavior expected display actual display actual results installed versions details system python executable machine pycaret required dependencies pip setuptools pycaret ipython ipywidgets tqdm numpy pandas jinja scipy joblib sklearn pyod imblearn lightgbm numba requests matplotlib scikitplot yellowbrick plotly kaleido statsmodels sktime tbats pmdarima psutil pycaret optional dependencies shap interpret umap explainerdashboard autoviz fairlearn xgboost catboost kmodes mlxtend statsforecast ray hyperopt optuna skopt gradio fastapi uvicorn m cgen evidently nltk pyldavis not installed gensim not installed spacy not installed wordcloud textblob fugue streamlit not installed prophet not installed"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2975",
        "Issue_title":"[BUG]: Runs recorded in MLflow nests all recursively",
        "Issue_creation_time":1663557360000,
        "Issue_closed_time":1663775400000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nAs started runs in MlflowLogger are never ended, all runs shown in MLflow dashboard seem to be nested recursively.\r\nMLflow 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic.\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nfrom pycaret.datasets import get_data\r\n\r\nmlflow.set_tracking_uri(\"http:\/\/localhost:5000\")\r\n\r\ndata = get_data(\"diabetes\")\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\r\nsetup(data, target=\"Class variable\", log_experiment=True, silent=True)\r\ncompare_models(include=[\"lr\", \"svm\", \"rf\"])\n```\n\n\n### Expected Behavior\n\nExpected display:\r\n![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png)\r\n\r\nActual display:\r\n![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png)\r\n\n\n### Actual Results\n\n```python-traceback\nAttached the figure also in 'Expected Behavior'.\n```\n\n\n### Installed Versions\n\n<details>\r\n'2.3.10'\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: runs recorded in nests all recursively; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [x] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master). ### issue description as started runs in logger are never ended, all runs shown in dashboard seem to be nested recursively. 1.28.0 fixed the display of deeply nested runs correctly, so the bug is now problematic. ### reproducible example ```python import from pycaret.classification import * from pycaret.datasets import get_data .set_tracking_uri(\"http:\/\/localhost:5000\") data = get_data(\"diabetes\") setup(data, target=\"class variable\", log_experiment=true, silent=true) compare_models(include=[\"lr\", \"svm\", \"rf\"]) setup(data, target=\"class variable\", log_experiment=true, silent=true) compare_models(include=[\"lr\", \"svm\", \"rf\"]) ``` ### expected behavior expected display: ![p2](https:\/\/user-images.githubusercontent.com\/1991802\/190944134-3490628a-4eca-490a-af11-c4cdfe41953e.png) actual display: ![p1](https:\/\/user-images.githubusercontent.com\/1991802\/190944304-08c41ae2-93fd-4b79-b3ff-ebb594ad2664.png) ### actual results ```python-traceback attached the figure also in 'expected behavior'. ``` ### installed versions '2.3.10'",
        "Issue_original_content_gpt_summary":"The user encountered a bug in pycaret where runs started in the logger were never ended, resulting in all runs shown in the dashboard being nested recursively.",
        "Issue_preprocessed_content":"Title: runs recorded in nests all recursively; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description as started runs in logger are never ended, all runs shown in dashboard seem to be nested recursively. fixed the display of deeply nested runs correctly, so the bug is now problematic. reproducible example expected behavior expected display actual display actual results installed versions details"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2856",
        "Issue_title":"MlFlow not logging metrics",
        "Issue_creation_time":1660651109000,
        "Issue_closed_time":1660653306000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nHi,\r\n\r\nI am trying to integrate pycaret with mlflow using your parameter `log_experiment` in `setup()`. When I set it to true, everything is stores as planned in my local MlFlow server, but not the metrics.\r\n\r\nIn the documentation is says the `log_experiment=True` should control everything. So I am not sure if I do something wrong here of if it is a bug from your side.\r\n\r\nWould be glad if you could help!\n\n### Reproducible Example\n\n```python\nfrom pycaret.datasets import get_data\r\nfrom pycaret.regression import *\r\ndf = get_data('bike')\r\nexp = RegressionExperiment()\r\nexp.setup(data=df, log_experiment=True)\r\nmodel = exp.create_model(\"lr\")\r\npred = exp.predict_model(estimator=model)\r\nexp.finalize_model(estimator=model)\n```\n\n\n### Expected Behavior\n\nshould log metrics\n\n### Actual Results\n\n```python-traceback\nNo metrics logged.\n```\n\n\n### Installed Versions\n\n<details>\r\nPyCaret 3.0.0rc3\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: not logging metrics; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master). ### issue description hi, i am trying to integrate pycaret with using your parameter `log_experiment` in `setup()`. when i set it to true, everything is stores as planned in my local server, but not the metrics. in the documentation is says the `log_experiment=true` should control everything. so i am not sure if i do something wrong here of if it is a bug from your side. would be glad if you could help! ### reproducible example ```python from pycaret.datasets import get_data from pycaret.regression import * df = get_data('bike') exp = regressionexperiment() exp.setup(data=df, log_experiment=true) model = exp.create_model(\"lr\") pred = exp.predict_model(estimator=model) exp.finalize_model(estimator=model) ``` ### expected behavior should log metrics ### actual results ```python-traceback no metrics logged. ``` ### installed versions pycaret 3.0.0rc3",
        "Issue_original_content_gpt_summary":"The user is encountering an issue with PyCaret where metrics are not being logged when the parameter 'log_experiment' is set to true.",
        "Issue_preprocessed_content":"Title: not logging metrics; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description hi, i am trying to integrate pycaret with using your parameter in . when i set it to true, everything is stores as planned in my local server, but not the metrics. in the documentation is says the should control everything. so i am not sure if i do something wrong here of if it is a bug from your side. would be glad if you could help! reproducible example expected behavior should log metrics actual results installed versions details pycaret"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2838",
        "Issue_title":"[BUG]: MLflow server integration",
        "Issue_creation_time":1660026191000,
        "Issue_closed_time":1660286399000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nI have a problem saving xgboost run in mlflow server. The run has a status of UNFINISHED, no metrics or artifacts are created. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/101572186\/183577670-53398204-debf-428b-8b0c-3c7ca83f4785.png)\r\n\r\nWhen I use `mlflow ui` everything is fine, but when I run mlflow server with SQLite as backend store the problem occurs.\r\nCommand used to run mlflow server- `mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root \/mlflow\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/mlflow\/experiments\/mlflow.db`\n\n### Reproducible Example\n\n```python\nimport mlflow\r\nfrom pycaret.classification import *\r\nimport pandas as pd\r\n\r\nmlflow.set_tracking_uri('http:\/\/localhost:5000')\r\n\r\ndata = pd.DataFrame({'V1': [-1.34419, -1.89211, 1.69421, 0.263328, 0.107918, 0.154241, 0.33468, 1.447778, -0.918269, 0.86319, -1.630049, 1.643798, 1.274341, -1.296742, -0.193585, 1.627422, -0.66805, -1.664491, -1.86911, 0.892885],\r\n                     'V2': [0.85556, -1.70503, -0.02896, 1.746258, -0.084151, 1.673185, 1.113326, -0.23231, 1.054817, -1.407584, 0.474997, 0.150687, -0.738246, -0.045513, 1.58637, 0.984249, 0.624333, 0.298866, 0.662204, 0.967942],\r\n                     'V3': [1.768638, -0.503169, -0.25622, -0.937752, -0.062189, -0.820652, -1.786942, -1.770495, 1.808681, -0.280286, -1.389736, 0.182212, -0.602959, -0.354683, -1.065631, 1.649264, 0.389538, -1.674815, 0.281824, -1.683662],\r\n                     'V4': [1.512828, 1.177697, -1.156862, -1.877876, 1.526013, 1.644001, -1.282481, -0.720543, 0.323963, -1.931616, 1.632839, 1.706752, 1.895627, 1.860705, -1.559702, 1.517466, 1.254323, 1.84415, -1.175013, -1.600652],\r\n                     'V5': [0.820483, -1.20923, -0.012221, 1.682836, 0.104248, 1.258085, 0.404062, 0.18019, 1.352545, -0.497071, 0.771277, 1.614052, -0.693854, 0.002655, 0.277743, -0.977744, -0.97259, -1.501586, -0.731194, -0.551264],\r\n                     'V6': [1.079115, -0.734152, -1.630816, -1.877664, 1.577477, -1.902078, 1.012828, -1.107726, 1.742781, -1.338595, 1.788969, -0.851507, 1.061596, -0.635559, -1.171469, -1.001642, 1.493507, 0.732088, 1.565327, -1.845441],\r\n                     'V7': [1.165929, 1.804607, 0.886589, -0.027458, -1.444197, -0.415643, 0.863924, -1.177661, 1.684514, 1.023797, -1.234116, -0.989024, 0.815575, -0.668453, 0.591911, -0.798925, 1.024032, -1.983963, 1.900752, 1.201001],\r\n                     'V8': [-0.536923, 0.641581, -0.585228, 1.061145, -0.303192, -0.652068, 0.858556, 0.11012, 1.839738, -1.51798, -0.942028, -0.736386, -0.098261, 0.699127, 0.173854, -1.16775, -0.417662, 0.021639, 1.745042, -1.119667],\r\n                     'V9': [0.643498, -1.090347, 0.120182, -0.819219, -1.296763, 0.530723, -1.367664, -0.708116, -1.304274, 1.486166, 1.656498, 1.645308, -0.257558, 0.400849, 1.356781, 1.693433, 0.42606, 0.370683, -0.239278, -0.541334],\r\n                     'V10': [-0.744989, 0.506658, 1.15586, 1.461127, 1.928769, -0.330472, 1.514159, -1.209056, -0.741453, -1.479674, 1.92057, -1.148481, 0.949433, 0.674107, -1.410627, 1.497083, -1.262624, -0.856706, -1.708155, 0.93153],\r\n                     'V11': [0.967242, 1.968385, -1.362337, -0.46194, 0.809224, 0.226177, 1.782128, -0.114595, 0.698243, -0.141743, -0.117251, 1.762656, -0.068839, 0.648945, -1.497037, -1.455443, -0.291242, 1.806048, -1.945438, 0.251282],\r\n                     'V12': [0.010432, -0.101522, -1.764095, 1.326967, -1.299122, -0.549148, 0.807092, -0.75387, 0.955056, 0.640369, -0.917832, 0.250338, 0.624729, 1.566922, 0.118619, 1.907585, -0.919995, 0.868393, -1.103909, 0.347108],\r\n                     'V13': [0.122315, -1.140017, -0.876424, -1.075771, 0.668814, 1.916654, -0.864906, 0.132892, 0.740058, 0.94469, -0.260381, 0.92833, -1.186423, -0.18321, 1.99266, -0.779091, -1.649025, -1.688821, 1.075145, -1.988603],\r\n                     'V14': [-1.494, 0.679776, 0.813194, 1.8687, -0.20273, -0.363265, 1.98902, 0.100025, 1.462866, 0.561017, 0.418922, 1.981837, -1.834009, -1.657952, 0.585069, -0.898764, 0.683234, 0.743215, -0.050289, -0.668302], \r\n                     'V15': [0.199787, 0.81829, 1.200156, -1.684249, 0.847466, 1.326102, 0.323103, -1.010648, -1.868355, -1.204467, 1.777393, 0.375692, -1.654002, 0.50357, -1.372448, -0.522425, 0.360716, 1.007605, 1.009369, -0.353638],\r\n                     'V16': [1.535552, -0.082278, -0.083154, 0.069432, 1.356735, -0.042527, -0.462543, 1.813852, -1.664882, 0.408013, -1.802172, -1.920202, 1.987332, -1.126771, 1.485496, 1.972345, -0.33345, 1.414685, -0.06674, 1.383197],\r\n                     'V17': [-0.249929, 1.668129, 0.860046, 0.013955, 0.085628, 1.285539, -0.754444, -0.306815, -1.244118, -0.61328, 0.711952, 1.384674, 1.710264, 1.337836, -0.029678, -1.382343, -1.963618, 0.088497, -0.110544, 0.954066],\r\n                     'V18': [0.665032, -1.214589, 0.486172, 1.184611, 1.152936, -0.192168, -1.096281, -0.762198, -0.338583, 0.170551, -0.045797, -0.897271, 0.433204, -0.986375, 0.430157, 1.846751, -0.905146, -1.398763, 1.790667, -1.580808],\r\n                     'V19': [1.347637, -0.356925, 0.414118, 0.277104, 0.41587, -1.237646, 0.580625, 1.468221, -0.254781, 0.245683, -1.25356, 0.241325, 1.15677, -1.74525, 1.970698, -0.038675, -0.314979, 0.114507, 1.378524, -0.139709],\r\n                     'V20': [-1.291686, -1.714475, 0.012188, 1.002238, -1.587334, 1.408967, 1.055095, -1.356865, 1.307388, 0.697003, -0.112676, 1.762375, 0.82697, 1.084934, 1.656421, 0.786079, -1.580991, 1.753751, -0.242525, 1.854008],\r\n                     'Class': [1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1]})\r\n\r\nsetup(data = data,\r\ntarget = 'Class', \r\nexperiment_name = 'xgb_test', \r\nfix_imbalance = True,\r\nlog_experiment = True, \r\nsilent=True, \r\nuse_gpu=True,\r\nfold=5,\r\npreprocess=False)\r\n\r\nmodels = ['xgboost','knn','rf']\r\ntop_models = compare_models(include = model)\r\ndd = pull()\n```\n\n\n### Expected Behavior\n\nArtifacts and metrics should be crated. \n\n### Actual Results\n\n```python-traceback\nError from logs.log:\r\n\r\n2022-08-09 06:11:05,384:ERROR:dashboard_logger.log_model() for XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\r\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\r\n              early_stopping_rounds=None, enable_categorical=False,\r\n              eval_metric=None, feature_types=None, gamma=0, gpu_id=0,\r\n              grow_policy='depthwise', importance_type=None,\r\n              interaction_constraints='', learning_rate=0.300000012,\r\n              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6,\r\n              max_leaves=0, min_child_weight=1, missing=nan,\r\n              monotone_constraints='()', n_estimators=100, n_jobs=-1,\r\n              num_parallel_tree=1, objective='binary:logistic',\r\n              predictor='auto', random_state=989, ...) raised an exception:\r\n2022-08-09 06:11:05,385:ERROR:Traceback (most recent call last):\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models\r\n    dashboard_logger.log_model(\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model\r\n    logger.log_params(params, model_name=full_name)\r\n  File \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/mlflow_logger.py\", line 46, in log_params\r\n    mlflow.log_params(params)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py\", line 675, in log_params\r\n    MlflowClient().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[])\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py\", line 918, in log_batch\r\n    self._tracking_client.log_batch(run_id, metrics, params, tags)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py\", line 315, in log_batch\r\n    self.store.log_batch(\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 309, in log_batch\r\n    self._call_endpoint(LogBatch, req_body)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint\r\n    return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 256, in call_endpoint\r\n    response = verify_rest_response(response, endpoint)\r\n  File \"\/usr\/local\/envs\/Jun_24_2022\/lib\/python3.8\/site-packages\/mlflow\/utils\/rest_utils.py\", line 185, in verify_rest_response\r\n    raise RestException(json.loads(response.text))\r\nmlflow.exceptions.RestException: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'None'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'None'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'None'}, {'key': 'enable_categorical', 'value': 'False'}, {'key': 'eval_metric', 'value': 'None'}, {'key': 'feature_types', 'value': 'None'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'None'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n```\n\n\n### Installed Versions\n\n<details>\r\npycaret- Version: 2.3.10 <\/br>\r\nmlflow- Version: 1.27.0 <\/br>\r\nxgboost-  Version: 2.0.0.dev0 <\/br>\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: server integration; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master). ### issue description i have a problem saving xgboost run in server. the run has a status of unfinished, no metrics or artifacts are created. when i use ` ui` everything is fine, but when i run server with sqlite as backend store the problem occurs. command used to run server- ` server --host 0.0.0.0 --port 5000 --default-artifact-root \/\/artifacts\/ --backend-store-uri sqlite:\/\/\/\/\/\/experiments\/.db` ### expected behavior artifacts and metrics should be crated. ### actual results ```python-traceback error from logs.log: 2022-08-09 06:11:05,384:error:dashboard_logger.log_model() for xgbclassifier(base_score=0.5, booster='gbtree', callbacks=none, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=none, enable_categorical=false, eval_metric=none, feature_types=none, gamma=0, gpu_id=0, grow_policy='depthwise', importance_type=none, interaction_constraints='', learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=-1, num_parallel_tree=1, objective='binary:logistic', predictor='auto', random_state=989, ...) raised an exception: 2022-08-09 06:11:05,385:error:traceback (most recent call last): file \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/internal\/tabular.py\", line 2362, in compare_models dashboard_logger.log_model( file \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/__init__.py\", line 93, in log_model logger.log_params(params, model_name=full_name) file \"\/home\/vscode\/.local\/lib\/python3.8\/site-packages\/pycaret\/loggers\/_logger.py\", line 46, in log_params .log_params(params) file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/tracking\/fluent.py\", line 675, in log_params client().log_batch(run_id=run_id, metrics=[], params=params_arr, tags=[]) file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/tracking\/client.py\", line 918, in log_batch self._tracking_client.log_batch(run_id, metrics, params, tags) file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 315, in log_batch self.store.log_batch( file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/store\/tracking\/rest_store.py\", line 309, in log_batch self._call_endpoint(logbatch, req_body) file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/store\/tracking\/rest_store.py\", line 56, in _call_endpoint return call_endpoint(self.get_host_creds(), endpoint, method, json_body, response_proto) file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/utils\/rest_utils.py\", line 256, in call_endpoint response = verify_rest_response(response, endpoint) file \"\/usr\/local\/envs\/jun_24_2022\/lib\/python3.8\/site-packages\/\/utils\/rest_utils.py\", line 185, in verify_rest_response raise restexception(json.loads(response.text)) .exceptions.restexception: invalid_parameter_value: invalid value [{'key': 'objective', 'value': 'binary:logistic'}, {'key': 'use_label_encoder', 'value': 'none'}, {'key': 'base_score', 'value': '0.5'}, {'key': 'booster', 'value': 'gbtree'}, {'key': 'callbacks', 'value': 'none'}, {'key': 'colsample_bylevel', 'value': '1'}, {'key': 'colsample_bynode', 'value': '1'}, {'key': 'colsample_bytree', 'value': '1'}, {'key': 'early_stopping_rounds', 'value': 'none'}, {'key': 'enable_categorical', 'value': 'false'}, {'key': 'eval_metric', 'value': 'none'}, {'key': 'feature_types', 'value': 'none'}, {'key': 'gamma', 'value': '0'}, {'key': 'gpu_id', 'value': '0'}, {'key': 'grow_policy', 'value': 'depthwise'}, {'key': 'importance_type', 'value': 'none'}, {'key': 'interaction_constraints', 'value': ''}, {'key': 'learning_rate', 'value': '0.300000012'}, {'key': 'max_bin', 'value': '256'}, {'key': 'max_cat_to_onehot', 'value': '4'}, {'key': 'max_delta_step', 'value': '0'}, {'key': 'max_depth', 'value': '6'}, {'key': 'max_leaves', 'value': '0'}, {'key': 'min_child_weight', 'value': '1'}, {'key': 'missing', 'value': 'nan'}, {'key': 'monotone_constraints', 'value': '()'}, {'key': 'n_estimators', 'value': '100'}, {'key': 'n_jobs', 'value': '-1'}, {'key': 'num_parallel_tree', 'value': '1'}, {'key': 'predictor', 'value': 'auto'}, {'key': 'random_state', 'value': '989'}, {'key': 'reg_alpha', 'value': '0'}, {'key': 'reg_lambda', 'value': '1'}, {'key': 'sampling_method', 'value': 'uniform'}, {'key': 'scale_pos_weight', 'value': '1'}, {'key': 'subsample', 'value': '1'}, {'key': 'tree_method', 'value': 'gpu_hist'}, {'key': 'validate_parameters', 'value': '1'}, {'key': 'verbosity', 'value': '0'}] for parameter 'params' supplied. hint: value was of type 'list'. see the api docs for more information about request parameters. ``` ### installed versions pycaret- version: 2.3.10 - version: 1.27.0 xgboost- version: 2.0.0.dev0",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with server integration when attempting to save an XGBoost run, resulting in an unfinished status with no metrics or artifacts created.",
        "Issue_preprocessed_content":"Title: server integration; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description i have a problem saving xgboost run in server. the run has a status of unfinished, no metrics or artifacts are created. when i use everything is fine, but when i run server with sqlite as backend store the problem occurs. command used to run server reproducible example expected behavior artifacts and metrics should be crated. actual results installed versions details pycaret version version xgboost version"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2811",
        "Issue_title":"[BUG]: mlflow incorrectly logging models \"Lasso Least Angle Regression\" and \"Least Angle Regression\"",
        "Issue_creation_time":1659210438000,
        "Issue_closed_time":1670522892000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nmlflow logs the name of both models \"Least Angle Regression\" and \"Lasso Least Angle Regression\" as \"Least Angle Regression\".\r\n\r\nWhen looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags.mlflow.runName`.\r\n\r\nPython Version: 3.9.5\r\nPyCaret Version: '3.0.0.rc3'\r\nPandas Version: 1.4.3\r\n\n\n### Reproducible Example\n\n```python\nimport pandas as pd\r\nfrom pycaret.regression import *\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('diamond')\r\n\r\nEXPERIMENT_NAME = 'diamond_experiment'\r\ns = setup(data=dataset, target='Price', log_experiment=True, experiment_name=EXPERIMENT_NAME, session_id=42, verbose=True)\r\n\r\nmodel = compare_models(verbose=False)\r\n\r\nprint(f\"Notice Least Angle Regression is not unique:\\n{get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'].value_counts()}\")\r\n\r\n# Loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs\r\n# There should be a single unique value for each model\r\nfor model in pull().Model.tolist():\r\n    print(f\"{model} - {len(get_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == model])}\")\r\n\r\n# Further investigation: model Least Angle Regression has 2 instances (should be Lasso Least Angle Regression and Least Angle Regression)\r\nget_logs(experiment_name=EXPERIMENT_NAME)[get_logs(experiment_name=EXPERIMENT_NAME)['tags.mlflow.runName'] == 'Least Angle Regression']\r\n```\n```\n\n\n### Expected Behavior\n\n`tags.mlflow.runName` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()`\n\n### Actual Results\n\n```python-traceback\nWhen looking into the `tags.mlflow.runName` you can see they are all unique but Least Angle Regression is there twice and Lasso Least Angle Regression isn't there at all. Could this be logged incorrectly?\r\n\r\nGradient Boosting Regressor - 1\r\nCatBoost Regressor - 1\r\nLight Gradient Boosting Machine - 1\r\nExtreme Gradient Boosting - 1\r\nLasso Regression - 1\r\nRidge Regression - 1\r\nLinear Regression - 1\r\nLasso Least Angle Regression - 0\r\nLeast Angle Regression - 2\r\nExtra Trees Regressor - 1\r\nRandom Forest Regressor - 1\r\nAdaBoost Regressor - 1\r\nDecision Tree Regressor - 1\r\nOrthogonal Matching Pursuit - 1\r\nElastic Net - 1\r\nHuber Regressor - 1\r\nBayesian Ridge - 1\r\nK Neighbors Regressor - 1\r\nDummy Regressor - 1\r\nPassive Aggressive Regressor - 1\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.9.5 (v3.9.5:0a7dcbdb13, May  3 2021, 13:17:02)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: PATH_TO_ENV\/venv\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.1.1\r\n          setuptools: 56.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 8.4.0\r\n          ipywidgets: 8.0.0rc0\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.5.4\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.9.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.2\r\n            requests: 2.28.0\r\n          matplotlib: 3.5.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.13.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.5\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: incorrectly logging models \"lasso least angle regression\" and \"least angle regression\"; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [x] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master). ### issue description logs the name of both models \"least angle regression\" and \"lasso least angle regression\" as \"least angle regression\". when looking into the `get_logs()` you can see both of those models have unique `run_id` but both have the same `tags..runname`. python version: 3.9.5 pycaret version: '3.0.0.rc3' pandas version: 1.4.3 ### reproducible example ```python import pandas as pd from pycaret.regression import * from pycaret.datasets import get_data dataset = get_data('diamond') experiment_name = 'diamond_experiment' s = setup(data=dataset, target='price', log_experiment=true, experiment_name=experiment_name, session_id=42, verbose=true) model = compare_models(verbose=false) print(f\"notice least angle regression is not unique:\\n{get_logs(experiment_name=experiment_name)['tags..runname'].value_counts()}\") # loop through all models in the `compare_models()` (20 models) function and get the length of the dataframe of that specific model in the logs # there should be a single unique value for each model for model in pull().model.tolist(): print(f\"{model} - {len(get_logs(experiment_name=experiment_name)[get_logs(experiment_name=experiment_name)['tags..runname'] == model])}\") # further investigation: model least angle regression has 2 instances (should be lasso least angle regression and least angle regression) get_logs(experiment_name=experiment_name)[get_logs(experiment_name=experiment_name)['tags..runname'] == 'least angle regression'] ``` ``` ### expected behavior `tags..runname` parameter from `get_logs()` is unique (given a single experiment) and contains all model names from `compare_models()` ### actual results ```python-traceback when looking into the `tags..runname` you can see they are all unique but least angle regression is there twice and lasso least angle regression isn't there at all. could this be logged incorrectly? gradient boosting regressor - 1 catboost regressor - 1 light gradient boosting machine - 1 extreme gradient boosting - 1 lasso regression - 1 ridge regression - 1 linear regression - 1 lasso least angle regression - 0 least angle regression - 2 extra trees regressor - 1 random forest regressor - 1 adaboost regressor - 1 decision tree regressor - 1 orthogonal matching pursuit - 1 elastic net - 1 huber regressor - 1 bayesian ridge - 1 k neighbors regressor - 1 dummy regressor - 1 passive aggressive regressor - 1 ``` ### installed versions system: python: 3.9.5 (v3.9.5:0a7dcbdb13, may 3 2021, 13:17:02) [clang 6.0 (clang-600.0.57)] executable: path_to_env\/venv\/bin\/python machine: macos-10.16-x86_64-i386-64bit pycaret required dependencies: pip: 21.1.1 setuptools: 56.0.0 pycaret: 3.0.0.rc3 ipython: 8.4.0 ipywidgets: 8.0.0rc0 tqdm: 4.64.0 numpy: 1.21.6 pandas: 1.4.3 jinja2: 3.1.2 scipy: 1.5.4 joblib: 1.1.0 sklearn: 1.1.1 pyod: installed but version unavailable imblearn: 0.9.1 category_encoders: 2.5.0 lightgbm: 3.3.2 numba: 0.55.2 requests: 2.28.0 matplotlib: 3.5.2 scikitplot: 0.3.7 yellowbrick: 1.4 plotly: 5.9.0 kaleido: 0.2.1 statsmodels: 0.13.2 sktime: 0.11.4 tbats: installed but version unavailable pmdarima: 1.8.5 psutil: 5.9.1",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the logging of models \"lasso least angle regression\" and \"least angle regression\" were incorrectly logged as \"least angle regression\" in the `get_logs()` function.",
        "Issue_preprocessed_content":"Title: incorrectly logging models lasso least angle regression and least angle regression ; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description logs the name of both models least angle regression and lasso least angle regression as least angle regression . when looking into the you can see both of those models have unique but both have the same . python version pycaret version pandas version reproducible example when looking into the you can see they are all unique but least angle regression is there twice and lasso least angle regression isn't there at all. could this be logged incorrectly? gradient boosting regressor catboost regressor light gradient boosting machine extreme gradient boosting lasso regression ridge regression linear regression lasso least angle regression least angle regression extra trees regressor random forest regressor adaboost regressor decision tree regressor orthogonal matching pursuit elastic net huber regressor bayesian ridge k neighbors regressor dummy regressor passive aggressive regressor installed versions details system python executable machine pycaret required dependencies pip setuptools pycaret ipython ipywidgets tqdm numpy pandas jinja scipy joblib sklearn pyod installed but version unavailable imblearn lightgbm numba requests matplotlib scikitplot yellowbrick plotly kaleido statsmodels sktime tbats installed but version unavailable pmdarima psutil"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2801",
        "Issue_title":"[BUG]: pycaret + mlflow integration does not allow probabilities for classification and binary response models",
        "Issue_creation_time":1658846256000,
        "Issue_closed_time":1669247416000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [ ] I have confirmed this bug exists on the master branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@master).\n\n\n### Issue Description\n\nWe have been using pycaret 2.2 for the model training procedure and registration to the mlflow server. (python based) My company uses a managed version of this in Azure Databricks. After the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. We would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. This is not working in pycaret and appears to be a bug of some sort. It is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. \n\n### Reproducible Example\n\n```python\n# import mlflow.sklearn\r\n# model = mlflow.sklearn.load_model(production_algorithm)\r\n# model.predict_prob(X)\n```\n\n\n### Expected Behavior\n\nwe should see the probabilities model.predict_prob(X) but this code errors out. Another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.DataFrame(X))\r\n\n\n### Actual Results\n\n```python-traceback\nthe end result of the prediction should be a numeric value between 0 and 1. Ex. 0.4278\n```\n\n\n### Installed Versions\n\n<details>\r\nSystem:\r\n    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python\r\n   machine: Linux-5.4.0-1083-azure-x86_64-with-glibc2.29\r\n\r\nPyCaret required dependencies:\r\n                 pip: 21.0.1\r\n          setuptools: 52.0.0\r\n             pycaret: 3.0.0.rc3\r\n             IPython: 7.22.0\r\n          ipywidgets: 7.7.1\r\n                tqdm: 4.64.0\r\n               numpy: 1.21.6\r\n              pandas: 1.4.3\r\n              jinja2: 3.1.2\r\n               scipy: 1.6.2\r\n              joblib: 1.1.0\r\n             sklearn: 1.1.1\r\n                pyod: Installed but version unavailable\r\n            imblearn: 0.8.1\r\n   category_encoders: 2.5.0\r\n            lightgbm: 3.3.2\r\n               numba: 0.55.1\r\n            requests: 2.28.1\r\n          matplotlib: 3.4.2\r\n          scikitplot: 0.3.7\r\n         yellowbrick: 1.4\r\n              plotly: 5.9.0\r\n             kaleido: 0.2.1\r\n         statsmodels: 0.12.2\r\n              sktime: 0.11.4\r\n               tbats: Installed but version unavailable\r\n            pmdarima: 1.8.4\r\n              psutil: 5.9.1\r\n<\/details>\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: pycaret + integration does not allow probabilities for classification and binary response models; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [ ] i have confirmed this bug exists on the master branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@master). ### issue description we have been using pycaret 2.2 for the model training procedure and registration to the server. (python based) my company uses a managed version of this in azure databricks. after the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response 0|1. we would also like to leverage the scikit learn function \"predict_model\" to create the probabilities in addition to the predicted value. this is not working in pycaret and appears to be a bug of some sort. it is also important to note that we are able to see the \"predict_model\" during the model training but not when we call the algorithm for a separate scoring function. ### reproducible example ```python # import .sklearn # model = .sklearn.load_model(production_algorithm) # model.predict_prob(x) ``` ### expected behavior we should see the probabilities model.predict_prob(x) but this code errors out. another example would be the following: predictions_prob = production_algorithm.predict_prob(pd.dataframe(x)) ### actual results ```python-traceback the end result of the prediction should be a numeric value between 0 and 1. ex. 0.4278 ``` ### installed versions system: python: 3.8.10 (default, mar 15 2022, 12:22:08) [gcc 9.4.0] executable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonenv-ca5e1db9-faed-4291-83c9-f55dfcbb8112\/bin\/python machine: linux-5.4.0-1083-azure-x86_64-with-glibc2.29 pycaret required dependencies: pip: 21.0.1 setuptools: 52.0.0 pycaret: 3.0.0.rc3 ipython: 7.22.0 ipywidgets: 7.7.1 tqdm: 4.64.0 numpy: 1.21.6 pandas: 1.4.3 jinja2: 3.1.2 scipy: 1.6.2 joblib: 1.1.0 sklearn: 1.1.1 pyod: installed but version unavailable imblearn: 0.8.1 category_encoders: 2.5.0 lightgbm: 3.3.2 numba: 0.55.1 requests: 2.28.1 matplotlib: 3.4.2 scikitplot: 0.3.7 yellowbrick: 1.4 plotly: 5.9.0 kaleido: 0.2.1 statsmodels: 0.12.2 sktime: 0.11.4 tbats: installed but version unavailable pmdarima: 1.8.4 psutil: 5.9.1",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with pycaret + integration not allowing probabilities for classification and binary response models, resulting in an error when attempting to predict probabilities.",
        "Issue_preprocessed_content":"Title: pycaret + integration does not allow probabilities for classification and binary response models; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the master branch of pycaret . issue description we have been using pycaret for the model training procedure and registration to the server. my company uses a managed version of this in azure databricks. after the registration has been completed, we call the calibrated algorithm in a separate notebook and are trying to score new data with a binary response . we would also like to leverage the scikit learn function to create the probabilities in addition to the predicted value. this is not working in pycaret and appears to be a bug of some sort. it is also important to note that we are able to see the during the model training but not when we call the algorithm for a separate scoring function. reproducible example expected behavior we should see the probabilities but this code errors out. another example would be the following actual results installed versions details system python executable machine pycaret required dependencies pip setuptools pycaret ipython ipywidgets tqdm numpy pandas jinja scipy joblib sklearn pyod installed but version unavailable imblearn lightgbm numba requests matplotlib scikitplot yellowbrick plotly kaleido statsmodels sktime tbats installed but version unavailable pmdarima psutil"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2581",
        "Issue_title":"mlflow ui doesn't show any models",
        "Issue_creation_time":1653399055000,
        "Issue_closed_time":1653501835000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### pycaret version checks\n\n- [X] I have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues).\n\n- [X] I have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret.\n\n- [X] I have confirmed this bug exists on the develop branch of pycaret (pip install -U git+https:\/\/github.com\/pycaret\/pycaret.git@develop).\n\n\n### Issue Description\n\nI have tried both the nightly and the release branch, and read the issues posted here:\r\nhttps:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3Aissue+mlflow+ui+is%3Aclosed\r\n\r\nI do not see any models in the `mlflow ui` *during training*, while several models have already converged and logged to the file system. I see some models have already reported AUC, MSE, etc. but as shows below, nothing is present in the dashboard\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png)\r\n\r\nThanks!\r\n\n\n### Reproducible Example\n\n```python\ntraining_data = pd.read_pickle(\"\/cached_db\")\r\n\r\n\r\nexp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123,\r\n                                  preprocess=True, feature_selection=True, fix_imbalance=True, \r\n                                  remove_perfect_collinearity=False,\r\n                                  log_experiment=True, \r\n                                  log_plots=True, profile=False, log_profile=False,\r\n                                  silent=True,\r\n                                  n_jobs=-1,\r\n                                  fold=2,\r\n                                  )\r\n\r\nbest_models = classification.compare_models(turbo=True, n_select=3,errors='raise')\n```\n\n\n### Expected Behavior\n\nBeing able to see the models that have already converged\n\n### Actual Results\n\n```python-traceback\nNo model is present in the `mlflow ui` dashboard\n```\n\n\n### Installed Versions\n\n2.3.10",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: ui doesn't show any models; Content: ### pycaret version checks - [x] i have checked that this issue has not already been reported [here](https:\/\/github.com\/pycaret\/pycaret\/issues). - [x] i have confirmed this bug exists on the [latest version](https:\/\/github.com\/pycaret\/pycaret\/releases) of pycaret. - [x] i have confirmed this bug exists on the develop branch of pycaret (pip install -u git+https:\/\/github.com\/pycaret\/pycaret.git@develop). ### issue description i have tried both the nightly and the release branch, and read the issues posted here: https:\/\/github.com\/pycaret\/pycaret\/issues?q=is%3aissue++ui+is%3aclosed i do not see any models in the ` ui` *during training*, while several models have already converged and logged to the file system. i see some models have already reported auc, mse, etc. but as shows below, nothing is present in the dashboard ![image](https:\/\/user-images.githubusercontent.com\/31047807\/170046175-c0a85a9b-21e4-4a07-891f-7d58fcc5f579.png) thanks! ### reproducible example ```python training_data = pd.read_pickle(\"\/cached_db\") exp_reg102 = classification.setup(data=training_data, target=args.label, session_id=123, preprocess=true, feature_selection=true, fix_imbalance=true, remove_perfect_collinearity=false, log_experiment=true, log_plots=true, profile=false, log_profile=false, silent=true, n_jobs=-1, fold=2, ) best_models = classification.compare_models(turbo=true, n_select=3,errors='raise') ``` ### expected behavior being able to see the models that have already converged ### actual results ```python-traceback no model is present in the ` ui` dashboard ``` ### installed versions 2.3.10",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the UI dashboard of PyCaret did not show any models, despite several models having already converged and logged to the file system.",
        "Issue_preprocessed_content":"Title: ui doesn't show any models; Content: pycaret version checks i have checked that this issue has not already been reported . i have confirmed this bug exists on the of pycaret. i have confirmed this bug exists on the develop branch of pycaret . issue description i have tried both the nightly and the release branch, and read the issues posted here i do not see any models in the during training , while several models have already converged and logged to the file system. i see some models have already reported auc, mse, etc. but as shows below, nothing is present in the dashboard thanks! reproducible example expected behavior being able to see the models that have already converged actual results installed versions"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2439",
        "Issue_title":"plots are not saving through MLFLOW",
        "Issue_creation_time":1650342119000,
        "Issue_closed_time":1650470329000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"This is the error I get  \"plot_model() got an unexpected keyword argument 'system'\"\r\n\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: plots are not saving through ; Content: this is the error i get \"plot_model() got an unexpected keyword argument 'system'\"",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected keyword argument error when attempting to save a plot using the plot_model() function.",
        "Issue_preprocessed_content":"Title: plots are not saving through ; Content: this is the error i get got an unexpected keyword argument 'system'"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/2425",
        "Issue_title":"[BUG] mlflow ui never runs",
        "Issue_creation_time":1650277642000,
        "Issue_closed_time":1650449956000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"**Describe the bug**\r\nGetting error \"FileNotFoundError: [WinError 2] The system cannot find the file specified\" while running \"mlflow ui\".\r\n\r\n**To Reproduce**\r\nRun \"mlflow ui\"\r\n\r\n\r\n**Expected behavior**\r\nIt should run without any issues\r\n\r\n\r\n**Versions**\r\n2.3.10\r\n\r\nNot sure if this is the right forum to post this issue. If it is not, please ignore.\r\n<!-- Thanks for contributing! -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] ui never runs; Content: **describe the bug** getting error \"filenotfounderror: [winerror 2] the system cannot find the file specified\" while running \" ui\". **to reproduce** run \" ui\" **expected behavior** it should run without any issues **versions** 2.3.10 not sure if this is the right forum to post this issue. if it is not, please ignore.",
        "Issue_original_content_gpt_summary":"The user encountered an error while running \"ui\" which prevented it from running, and was unsure if this was the right forum to post the issue.",
        "Issue_preprocessed_content":"Title: ui never runs; Content: describe the bug getting error filenotfounderror the system cannot find the file specified while running ui . to reproduce run ui expected behavior it should run without any issues versions not sure if this is the right forum to post this issue. if it is not, please ignore. thanks for contributing!"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1736",
        "Issue_title":"[BUG] Issue with Mlflow Timeseries_beta branch",
        "Issue_creation_time":1634814038000,
        "Issue_closed_time":1635811087000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"if in setup log_plot set True then it is giving error in self._mlflow_log_model() as \r\nfor plot in log_plots:\r\nTypeError: 'bool' object is not iterable",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] issue with timeseries_beta branch; Content: if in setup log_plot set true then it is giving error in self.__log_model() as for plot in log_plots: typeerror: 'bool' object is not iterable",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when setting log_plot to true in the setup of the timeseries_beta branch.",
        "Issue_preprocessed_content":"Title: issue with branch; Content: if in setup set true then it is giving error in as for plot in typeerror 'bool' object is not iterable"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1674",
        "Issue_title":"[BUG] some types plot types are not getting saved to the MLFlow experiment artifacts dir",
        "Issue_creation_time":1634052175000,
        "Issue_closed_time":1635405096000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nThank you for creating such a helpful tool!\r\nThe problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the MLFlow experiment artifacts dir. I think the issue is with inconsistent naming for the saved png for certain plot types.\r\nThank you for your help!\r\n<!--\r\n-->\r\n\r\n**To Reproduce**\r\n<!--\r\nAdd a Minimal, Complete, and Verifiable example (for more details, see e.g. https:\/\/stackoverflow.com\/help\/mcve\r\n\r\nIf the code is too long, feel free to put it in a public gist and link it in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom pycaret.classification import *\r\n\r\nfrom pycaret.datasets import get_data\r\ndataset = get_data('credit')\r\n\r\n  pycaret_env = setup(\r\n      data = data, \r\n      target = 'default', \r\n      html=False, \r\n      silent=True,\r\n      verbose=False,\r\n      # for MLFlow logging:\r\n      experiment_name=\"plot_test\",\r\n      log_experiment = True, \r\n      log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'],\r\n  )\r\n\r\n  model = create_model(\"lightgbm\")\r\n```\r\n\r\n**Expected behavior**\r\n<!--\r\n-->\r\nI expect ALL of the plot types to be logged under the MLFlow artifacts dir i.e. \/mlruns\/{experiment number}\/{id}\/artifacts\/\r\nHowever, \"feature.png\" and \"calibration.png\" are saved to the working directory.\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\r\nI think the issue is with inconsistent naming of the file. Here is a printout of the log when it tries to save the calibration plot:\r\n```\r\n2021-10-11 19:03:19,845:INFO:Saving 'calibration.png'\r\n2021-10-11 19:03:20,064:INFO:Visual Rendered Successfully\r\n2021-10-11 19:03:20,213:INFO:plot_model() succesfully completed......................................\r\n2021-10-11 19:03:20,217:WARNING:[Errno 2] No such file or directory: 'Calibration Curve.png'\r\n```\r\nSo you can see that it is looking for 'Calibration Curve.png', but what actually gets produced is 'calibration.png'.\r\n\r\n**Versions**\r\nPython 3.8.11\r\n\r\n<!--\r\nPlease run the following code snippet and paste the output here:\r\n \r\nimport pycaret\r\npycaret.__version__\r\n\r\n-->\r\nPycaret 2.3.4\r\n\r\n<\/details>\r\n\r\n<!-- Thanks for contributing! -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] some types plot types are not getting saved to the experiment artifacts dir; Content: **describe the bug** thank you for creating such a helpful tool! the problem i'm facing is that some types plot types (e.g. \"calibration\" and \"feature\") are not getting saved to the experiment artifacts dir. i think the issue is with inconsistent naming for the saved png for certain plot types. thank you for your help! **to reproduce** ```python from pycaret.classification import * from pycaret.datasets import get_data dataset = get_data('credit') pycaret_env = setup( data = data, target = 'default', html=false, silent=true, verbose=false, # for logging: experiment_name=\"plot_test\", log_experiment = true, log_plots=['auc', 'feature', 'parameter', 'pr', 'calibration', 'confusion_matrix'], ) model = create_model(\"lightgbm\") ``` **expected behavior** i expect all of the plot types to be logged under the artifacts dir i.e. \/s\/{experiment number}\/{id}\/artifacts\/ however, \"feature.png\" and \"calibration.png\" are saved to the working directory. **additional context** i think the issue is with inconsistent naming of the file. here is a printout of the log when it tries to save the calibration plot: ``` 2021-10-11 19:03:19,845:info:saving 'calibration.png' 2021-10-11 19:03:20,064:info:visual rendered successfully 2021-10-11 19:03:20,213:info:plot_model() succesfully completed...................................... 2021-10-11 19:03:20,217:warning:[errno 2] no such file or directory: 'calibration curve.png' ``` so you can see that it is looking for 'calibration curve.png', but what actually gets produced is 'calibration.png'. **versions** python 3.8.11 pycaret 2.3.4",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge where some types of plots (e.g. \"calibration\" and \"feature\") are not getting saved to the experiment artifacts directory, due to inconsistent naming of the file.",
        "Issue_preprocessed_content":"Title: some types plot types are not getting saved to the experiment artifacts dir; Content: describe the bug thank you for creating such a helpful tool! the problem i'm facing is that some types plot types are not getting saved to the experiment artifacts dir. i think the issue is with inconsistent naming for the saved png for certain plot types. thank you for your help! to reproduce add a minimal, complete, and verifiable example setup model , info saving , info visual rendered successfully succesfully , warning errno no such file or directory 'calibration so you can see that it is looking for 'calibration but what actually gets produced is versions python please run the following code snippet and paste the output here import pycaret pycaret thanks for contributing!"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1568",
        "Issue_title":"MLFlow logging issue in compare_models of time_series",
        "Issue_creation_time":1631459045000,
        "Issue_closed_time":1634125992000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"MLFlow logging issue in compare_models of time_series\r\n\r\n```\r\nfrom pycaret.datasets import get_data\r\ndata = get_data('airline')\r\n\r\nfrom pycaret.time_series import *\r\ns = setup(data, fold = 5, fh = 12, session_id = 123, log_experiment=True, experiment_name = 'airline')\r\n\r\nbest = compare_models()\r\n\r\n!mlflow ui\r\n```\r\n\r\nCheck localhost:5000:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992636-db293fe7-5461-43d9-baed-97d8790fd9bd.png)\r\n\r\nAll the runs fail in `compare_models`. Parameters are logged but metrics and artifacts didn't. \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992655-e0d81e16-9a59-49a5-ace3-d22ea2ea10b8.png)\r\n\r\nI cannot reproduce this with `create_model` it means `create_model` works just fine! ",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: logging issue in compare_models of time_series; Content: logging issue in compare_models of time_series ``` from pycaret.datasets import get_data data = get_data('airline') from pycaret.time_series import * s = setup(data, fold = 5, fh = 12, session_id = 123, log_experiment=true, experiment_name = 'airline') best = compare_models() ! ui ``` check localhost:5000: ![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992636-db293fe7-5461-43d9-baed-97d8790fd9bd.png) all the runs fail in `compare_models`. parameters are logged but metrics and artifacts didn't. ![image](https:\/\/user-images.githubusercontent.com\/54699234\/132992655-e0d81e16-9a59-49a5-ace3-d22ea2ea10b8.png) i cannot reproduce this with `create_model` it means `create_model` works just fine!",
        "Issue_original_content_gpt_summary":"The user encountered a logging issue in the compare_models function of the PyCaret time_series module, where parameters were logged but metrics and artifacts were not.",
        "Issue_preprocessed_content":"Title: logging issue in of ; Content: logging issue in of check localhost all the runs fail in . parameters are logged but metrics and artifacts didn't. i cannot reproduce this with it means works just fine!"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/1411",
        "Issue_title":"[BUG]Can we store mlflow artifacts in the Azure blob storage for new experiments in pycaret?",
        "Issue_creation_time":1624984139000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"I was exploring mlflow with pycaret today. And tried to store the artifacts in the Azure blob using the --default-artifact-root tag. It's working fine when I am not giving a experiment name to pycaret setup function. When a experiment name is given the artifacts are getting stored in the local directory.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]can we store artifacts in the azure blob storage for new experiments in pycaret?; Content: i was exploring with pycaret today. and tried to store the artifacts in the azure blob using the --default-artifact-root tag. it's working fine when i am not giving a experiment name to pycaret setup function. when a experiment name is given the artifacts are getting stored in the local directory.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to store artifacts in the Azure Blob Storage for new experiments in PyCaret.",
        "Issue_preprocessed_content":"Title: can we store artifacts in the azure blob storage for new experiments in pycaret?; Content: i was exploring with pycaret today. and tried to store the artifacts in the azure blob using the tag. it's working fine when i am not giving a experiment name to pycaret setup function. when a experiment name is given the artifacts are getting stored in the local directory."
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/931",
        "Issue_title":"MLFlow doesn't save model artifact and some plots - Clustering",
        "Issue_creation_time":1607745205000,
        "Issue_closed_time":1620299767000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":"I'm using clustering module of pycaret and the integration with mlflow but I have problems because I think it doesn't save all artifacs and the status is always failed.\r\n![image](https:\/\/user-images.githubusercontent.com\/12554263\/101971863-66f70d00-3c02-11eb-9710-01cf228fca1b.png)\r\n\r\nThis is my code:\r\n\r\n```python\r\nfrom pycaret.clustering import *\r\n\r\npostpaid_exp = setup(postpaid_sample,\r\n                     ignore_features=ignore_features,\r\n                     numeric_features=numeric_features,\r\n                     normalize=True,\r\n                     normalize_method='robust',\r\n                     remove_multicollinearity=True,\r\n                     multicollinearity_threshold=0.7,\r\n                     log_experiment=True,\r\n                     log_plots=True,\r\n                     log_profile=True,\r\n                     log_data=True,\r\n                     profile=False,\r\n                     experiment_name='pospatid_segmentation',\r\n                     session_id=123)\r\n\r\n# Create model with six clusters\r\nmodel_kmeans =  create_model(model='kmeans', num_clusters=6)\r\n```\r\nMy logs are the following\r\n\r\n```\r\n2020-12-11 22:39:07,118:INFO:PyCaret Supervised Module\r\n2020-12-11 22:39:07,118:INFO:ML Usecase: clustering\r\n2020-12-11 22:39:07,118:INFO:version 2.2.0\r\n2020-12-11 22:39:07,118:INFO:Initializing setup()\r\n2020-12-11 22:39:07,119:INFO:setup(target=None, ml_usecase=clustering, available_plots={'cluster': 'Cluster PCA Plot (2d)', 'tsne': 'Cluster TSnE (3d)', 'elbow': 'Elbow', 'silhouette': 'Silhouette', 'distance': 'Distance', 'distribution': 'Distribution'}, train_size=0.7, test_data=None, preprocess=True, imputation_type=simple, iterative_imputation_iters=5, categorical_features=None, categorical_imputation=mode, categorical_iterative_imputer=lightgbm, ordinal_features=None, high_cardinality_features=None, high_cardinality_method=frequency, numeric_features=['avg_dias_bancos_3m', 'avg_dias_app_pagos_3m', 'avg_dias_viajes_3m', 'avg_dias_compras_3m', 'avg_dias_mb_total_3m', 'avg_mb_total_3m', 'avg_q_apps_3m', 'ate_wh_sum_dias_3m', 'LEADs_tot_3m', 'tot_dias_appmov_movil_3m', 'avg_days_out_voice_tot_3m', 'meses_pagodig_3m'], numeric_imputation=mean, numeric_iterative_imputer=lightgbm, date_features=None, ignore_features=['periodo', 'telefono', 'anexo', 'tot_dias_appmov_fija_3m', 'avg_dias_vid_mus_3m'], normalize=True, normalize_method=robust, transformation=False, transformation_method=yeo-johnson, handle_unknown_categorical=True, unknown_categorical_method=least_frequent, pca=False, pca_method=linear, pca_components=None, ignore_low_variance=False, combine_rare_levels=False, rare_level_threshold=0.1, bin_numeric_features=None, remove_outliers=False, outliers_threshold=0.05, remove_multicollinearity=True, multicollinearity_threshold=0.7, remove_perfect_collinearity=False, create_clusters=False, cluster_iter=20, polynomial_features=False, polynomial_degree=2, trigonometry_features=False, polynomial_threshold=0.1, group_features=None, group_names=None, feature_selection=False, feature_selection_threshold=0.8, feature_selection_method=classic, feature_interaction=False, feature_ratio=False, interaction_threshold=0.01, fix_imbalance=False, fix_imbalance_method=None, transform_target=False, transform_target_method=box-cox, data_split_shuffle=False, data_split_stratify=False, fold_strategy=kfold, fold=10, fold_shuffle=False, fold_groups=None, n_jobs=-1, use_gpu=False, custom_pipeline=None, html=True, session_id=123, log_experiment=True, experiment_name=pospatid_segmentation, log_plots=['cluster', 'distribution', 'elbow'], log_profile=True, log_data=True, silent=False, verbose=True, profile=False, display=None)\r\n2020-12-11 22:39:07,119:INFO:Checking environment\r\n2020-12-11 22:39:07,119:INFO:python_version: 3.8.5\r\n2020-12-11 22:39:07,119:INFO:python_build: ('default', 'Aug  5 2020 09:44:06')\r\n2020-12-11 22:39:07,119:INFO:machine: AMD64\r\n2020-12-11 22:39:07,120:INFO:platform: Windows-10-10.0.18362-SP0\r\n2020-12-11 22:39:07,121:WARNING:cannot find psutil installation. memory not traceable. Install psutil using pip to enable memory logging.\r\n2020-12-11 22:39:07,122:INFO:Checking libraries\r\n2020-12-11 22:39:07,122:INFO:pd==1.1.4\r\n2020-12-11 22:39:07,122:INFO:numpy==1.19.4\r\n2020-12-11 22:39:07,122:INFO:sklearn==0.23.2\r\n2020-12-11 22:39:07,156:INFO:xgboost==1.2.0\r\n2020-12-11 22:39:07,156:INFO:lightgbm==3.0.0\r\n2020-12-11 22:39:07,170:INFO:catboost==0.24.1\r\n2020-12-11 22:39:07,901:INFO:mlflow==1.11.0\r\n2020-12-11 22:39:07,901:INFO:Checking Exceptions\r\n2020-12-11 22:39:07,901:INFO:Declaring global variables\r\n2020-12-11 22:39:07,901:INFO:USI: cd5c\r\n2020-12-11 22:39:07,901:INFO:pycaret_globals: {'_available_plots', 'master_model_container', 'display_container', 'imputation_classifier', 'logging_param', 'seed', 'transform_target_param', 'experiment__', 'transform_target_method_param', 'iterative_imputation_iters_param', 'fold_groups_param', 'fix_imbalance_param', 'prep_pipe', 'exp_name_log', '_all_metrics', 'html_param', '_ml_usecase', 'USI', 'imputation_regressor', 'stratify_param', 'fold_generator', 'fix_imbalance_method_param', '_all_models', 'gpu_param', 'target_param', '_gpu_n_jobs_param', 'log_plots_param', 'pycaret_globals', 'fold_shuffle_param', '_all_models_internal', 'fold_param', 'create_model_container', 'data_before_preprocess', '_internal_pipeline', 'X', 'n_jobs_param'}\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,901:INFO:Preparing display monitor\r\n2020-12-11 22:39:07,914:INFO:Importing libraries\r\n2020-12-11 22:39:07,914:INFO:Copying data for preprocessing\r\n2020-12-11 22:39:07,927:INFO:Declaring preprocessing parameters\r\n2020-12-11 22:39:07,940:INFO:Creating preprocessing pipeline\r\n2020-12-11 22:39:08,059:INFO:Preprocessing pipeline created successfully\r\n2020-12-11 22:39:08,060:ERROR:(Process Exit): setup has been interupted with user command 'quit'. setup must rerun.\r\n2020-12-11 22:39:08,060:INFO:Creating global containers\r\n2020-12-11 22:39:08,061:INFO:Internal pipeline: Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False)\r\n2020-12-11 22:39:10,064:INFO:Creating grid variables\r\n2020-12-11 22:39:10,101:INFO:Logging experiment in MLFlow\r\n2020-12-11 22:39:10,108:WARNING:Couldn't create mlflow experiment. Exception:\r\n2020-12-11 22:39:10,185:WARNING:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 1668, in setup\r\n    mlflow.create_experiment(exp_name_log)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\fluent.py\", line 365, in create_experiment\r\n    return MlflowClient().create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\client.py\", line 184, in create_experiment\r\n    return self._tracking_client.create_experiment(name, artifact_location)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\", line 142, in create_experiment\r\n    return self.store.create_experiment(name=name, artifact_location=artifact_location,)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 288, in create_experiment\r\n    self._validate_experiment_name(name)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 281, in _validate_experiment_name\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Experiment 'pospatid_segmentation' already exists.\r\n\r\n2020-12-11 22:39:10,490:INFO:SubProcess save_model() called ==================================\r\n2020-12-11 22:39:10,501:INFO:Initializing save_model()\r\n2020-12-11 22:39:10,501:INFO:save_model(model=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), model_name=Transformation Pipeline, prep_pipe_=Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False), verbose=False)\r\n2020-12-11 22:39:10,501:INFO:Adding model into prep_pipe\r\n2020-12-11 22:39:10,506:WARNING:Only Model saved as it was a pipeline.\r\n2020-12-11 22:39:10,530:INFO:Transformation Pipeline.pkl saved in current working directory\r\n2020-12-11 22:39:10,535:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:39:10,535:INFO:save_model() succesfully completed......................................\r\n2020-12-11 22:39:10,536:INFO:SubProcess save_model() end ==================================\r\n2020-12-11 22:40:03,332:INFO:create_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:master_model_container: 0\r\n2020-12-11 22:40:03,332:INFO:display_container: 0\r\n2020-12-11 22:40:03,336:INFO:Pipeline(memory=None,\r\n         steps=[('dtypes',\r\n                 DataTypes_Auto_infer(categorical_features=[],\r\n                                      display_types=True,\r\n                                      features_todrop=['periodo', 'telefono',\r\n                                                       'anexo',\r\n                                                       'tot_dias_appmov_fija_3m',\r\n                                                       'avg_dias_vid_mus_3m'],\r\n                                      id_columns=[],\r\n                                      ml_usecase='classification',\r\n                                      numerical_features=['avg_dias_bancos_3m',\r\n                                                          'avg_dias_app_pagos_3m',\r\n                                                          'avg_dias_viajes_3m',\r\n                                                          'avg_dias_compras_3m',\r\n                                                          'av...\r\n                ('dummy', Dummify(target='UNSUPERVISED_DUMMY_TARGET')),\r\n                ('fix_perfect', 'passthrough'),\r\n                ('clean_names', Clean_Colum_Names()),\r\n                ('feature_select', 'passthrough'),\r\n                ('fix_multi',\r\n                 Fix_multicollinearity(correlation_with_target_preference=None,\r\n                                       correlation_with_target_threshold=0.0,\r\n                                       target_variable='UNSUPERVISED_DUMMY_TARGET',\r\n                                       threshold=0.7)),\r\n                ('dfs', 'passthrough'), ('pca', 'passthrough')],\r\n         verbose=False)\r\n2020-12-11 22:40:03,336:INFO:setup() succesfully completed......................................\r\n2020-12-11 22:40:07,628:INFO:Initializing create_model()\r\n2020-12-11 22:40:07,628:INFO:create_model(estimator=kmeans, num_clusters=6, fraction=0.05, ground_truth=None, round=4, fit_kwargs=None, verbose=True, system=True, raise_num_clusters=False, display=None, kwargs={})\r\n2020-12-11 22:40:07,628:INFO:Checking exceptions\r\n2020-12-11 22:40:07,629:INFO:Preparing display monitor\r\n2020-12-11 22:40:07,645:INFO:Importing libraries\r\n2020-12-11 22:40:07,652:INFO:Importing untrained model\r\n2020-12-11 22:40:07,662:INFO:K-Means Clustering Imported succesfully\r\n2020-12-11 22:40:07,670:INFO:Fitting Model\r\n2020-12-11 22:42:30,467:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:42:30,467:INFO:create_models() succesfully completed......................................\r\n2020-12-11 22:42:30,467:INFO:Creating MLFlow logs\r\n2020-12-11 22:42:30,481:INFO:Model: K-Means Clustering\r\n2020-12-11 22:42:30,518:INFO:logged params: {'algorithm': 'auto', 'copy_x': True, 'init': 'k-means++', 'max_iter': 300, 'n_clusters': 6, 'n_init': 10, 'n_jobs': -1, 'precompute_distances': 'deprecated', 'random_state': 123, 'tol': 0.0001, 'verbose': 0}\r\n2020-12-11 22:42:30,557:INFO:SubProcess plot_model() called ==================================\r\n2020-12-11 22:42:30,557:INFO:Initializing plot_model()\r\n2020-12-11 22:42:30,557:INFO:plot_model(plot=cluster, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:30,557:INFO:Checking exceptions\r\n2020-12-11 22:42:30,558:INFO:Preloading libraries\r\n2020-12-11 22:42:30,558:INFO:Copying training dataset\r\n2020-12-11 22:42:30,560:INFO:Plot type: cluster\r\n2020-12-11 22:42:31,493:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:31,494:INFO:Initializing assign_model()\r\n2020-12-11 22:42:31,494:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=True, score=True, verbose=False)\r\n2020-12-11 22:42:31,494:INFO:Checking exceptions\r\n2020-12-11 22:42:31,495:INFO:Determining Trained Model\r\n2020-12-11 22:42:31,495:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:31,495:INFO:Copying data\r\n2020-12-11 22:42:31,496:INFO:Transformation param set to True. Assigned clusters are attached on transformed dataset.\r\n2020-12-11 22:42:31,529:INFO:(90000, 12)\r\n2020-12-11 22:42:31,529:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:31,530:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:31,541:INFO:Fitting PCA()\r\n2020-12-11 22:42:31,908:INFO:Sorting dataframe\r\n2020-12-11 22:42:31,974:INFO:Rendering Visual\r\n2020-12-11 22:42:41,765:INFO:Saving 'Cluster PCA Plot (2d).html' in current active directory\r\n2020-12-11 22:42:41,765:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:42,286:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:42,739:INFO:Initializing plot_model()\r\n2020-12-11 22:42:42,739:INFO:plot_model(plot=distribution, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:42,739:INFO:Checking exceptions\r\n2020-12-11 22:42:42,739:INFO:Preloading libraries\r\n2020-12-11 22:42:42,739:INFO:Copying training dataset\r\n2020-12-11 22:42:42,741:INFO:Plot type: distribution\r\n2020-12-11 22:42:42,741:INFO:SubProcess assign_model() called ==================================\r\n2020-12-11 22:42:42,742:INFO:Initializing assign_model()\r\n2020-12-11 22:42:42,742:INFO:assign_model(model=Pipeline(memory=None,\r\n         steps=[('empty_step', 'passthrough'),\r\n                ('actual_estimator',\r\n                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\r\n                        max_iter=300, n_clusters=6, n_init=10, n_jobs=-1,\r\n                        precompute_distances='deprecated', random_state=123,\r\n                        tol=0.0001, verbose=0))],\r\n         verbose=False), transformation=False, score=True, verbose=False)\r\n2020-12-11 22:42:42,742:INFO:Checking exceptions\r\n2020-12-11 22:42:42,742:INFO:Determining Trained Model\r\n2020-12-11 22:42:42,742:INFO:Trained Model : K-Means Clustering\r\n2020-12-11 22:42:42,742:INFO:Copying data\r\n2020-12-11 22:42:42,793:INFO:(90000, 18)\r\n2020-12-11 22:42:42,793:INFO:assign_model() succesfully completed......................................\r\n2020-12-11 22:42:42,794:INFO:SubProcess assign_model() end ==================================\r\n2020-12-11 22:42:42,794:INFO:Sorting dataframe\r\n2020-12-11 22:42:42,925:INFO:Rendering Visual\r\n2020-12-11 22:42:48,837:INFO:Saving 'Distribution.html' in current active directory\r\n2020-12-11 22:42:48,837:INFO:Visual Rendered Successfully\r\n2020-12-11 22:42:48,979:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:42:49,583:INFO:Initializing plot_model()\r\n2020-12-11 22:42:49,584:INFO:plot_model(plot=elbow, fold=None, verbose=False, display=None, estimator=KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0), feature_name=None, fit_kwargs=None, groups=None, label=False, save=True, scale=1, system=False)\r\n2020-12-11 22:42:49,584:INFO:Checking exceptions\r\n2020-12-11 22:42:49,584:INFO:Preloading libraries\r\n2020-12-11 22:42:49,584:INFO:Copying training dataset\r\n2020-12-11 22:42:49,586:INFO:Plot type: elbow\r\n2020-12-11 22:42:49,690:INFO:Fitting Model\r\n2020-12-11 22:43:12,604:INFO:Saving 'Elbow.png' in current active directory\r\n2020-12-11 22:43:13,207:INFO:Visual Rendered Successfully\r\n2020-12-11 22:43:13,325:INFO:plot_model() succesfully completed......................................\r\n2020-12-11 22:43:13,340:INFO:SubProcess plot_model() end ==================================\r\n2020-12-11 22:43:13,341:WARNING:Couldn't infer MLFlow signature.\r\n2020-12-11 22:43:13,352:ERROR:_mlflow_log_model() for KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0) raised an exception:\r\n2020-12-11 22:43:13,431:ERROR:Traceback (most recent call last):\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 2631, in create_model_unsupervised\r\n    _mlflow_log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pycaret\\internal\\tabular.py\", line 9942, in _mlflow_log_model\r\n    mlflow.sklearn.log_model(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 290, in log_model\r\n    return Model.log(\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\model.py\", line 160, in log\r\n    flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\sklearn\\__init__.py\", line 171, in save_model\r\n    _save_example(mlflow_model, input_example, path)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 131, in _save_example\r\n    example = _Example(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\mlflow\\models\\utils.py\", line 67, in __init__\r\n    input_example = pd.DataFrame.from_dict(input_example)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 1309, in from_dict\r\n    return cls(data, index=index, columns=columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\frame.py\", line 468, in __init__\r\n    mgr = init_dict(data, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 283, in init_dict\r\n    return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 78, in arrays_to_mgr\r\n    index = extract_index(arrays)\r\n  File \"C:\\Users\\CARLOS\\Anaconda3\\envs\\dev_models\\lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 387, in extract_index\r\n    raise ValueError(\"If using all scalar values, you must pass an index\")\r\nValueError: If using all scalar values, you must pass an index\r\n\r\n2020-12-11 22:43:13,432:INFO:Uploading results into container\r\n2020-12-11 22:43:13,435:INFO:Uploading model into container now\r\n2020-12-11 22:43:13,440:INFO:create_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:master_model_container: 1\r\n2020-12-11 22:43:13,440:INFO:display_container: 1\r\n2020-12-11 22:43:13,440:INFO:KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\r\n       n_clusters=6, n_init=10, n_jobs=-1, precompute_distances='deprecated',\r\n       random_state=123, tol=0.0001, verbose=0)\r\n2020-12-11 22:43:13,440:INFO:create_model() succesfully completed......................................\r\n\r\n```\r\n\r\nI'm using Pycaret version : 2.2.0",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: doesn't save model artifact and some plots - clustering; Content: i'm using clustering module of pycaret and the integration with but i have problems because i think it doesn't save all artifacs and the status is always failed. this is my code: ```python from pycaret.clustering import * postpaid_exp = setup(postpaid_sample, ignore_features=ignore_features, numeric_features=numeric_features, normalize=true, normalize_method='robust', remove_multicollinearity=true, multicollinearity_threshold=0.7, log_experiment=true, log_plots=true, log_profile=true, log_data=true, profile=false, experiment_name='pospatid_segmentation', session_id=123) # create model with six clusters model_kmeans = create_model(model='kmeans', num_clusters=6) ```",
        "Issue_original_content_gpt_summary":"The user encountered challenges with PyCaret's clustering module, such as not saving model artifacts and some plots, and the status always failing.",
        "Issue_preprocessed_content":"Title: doesn't save model artifact and some plots clustering; Content: i'm using clustering module of pycaret and the integration with but i have problems because i think it doesn't save all artifacs and the status is always failed. this is my code my logs are the following i'm using pycaret version"
    },
    {
        "Issue_link":"https:\/\/github.com\/pycaret\/pycaret\/issues\/566",
        "Issue_title":"Compare models MLFlowException",
        "Issue_creation_time":1598718264000,
        "Issue_closed_time":1598806653000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"Hi. I just upgraded to pycaret 2.1. When I ran the compare_models function with the Titanic dataset, I got the following error:\r\n\r\nMlflowException: Unable to map 'np.object' type to MLflow DataType. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray),  int, float)\r\n\r\nThe same code worked fine in pycaret 2.0.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: compare models exception; Content: hi. i just upgraded to pycaret 2.1. when i ran the compare_models function with the titanic dataset, i got the following error: exception: unable to map 'np.object' type to datatype. np.object canbe mapped iff all values have identical data type which is one of (string, (bytes or byterray), int, float) the same code worked fine in pycaret 2.0.",
        "Issue_original_content_gpt_summary":"The user encountered an error when running the compare_models function with the titanic dataset in PyCaret 2.1 due to an inability to map the 'np.object' type to a datatype.",
        "Issue_preprocessed_content":"Title: compare models exception; Content: hi. i just upgraded to pycaret when i ran the function with the titanic dataset, i got the following error exception unable to map type to datatype. canbe mapped iff all values have identical data type which is one of , int, float the same code worked fine in pycaret"
    },
    {
        "Issue_link":"https:\/\/github.com\/aimhubio\/aim\/issues\/1415",
        "Issue_title":"aim convert mlflow --experiment fails for experiment id, works for experiment name",
        "Issue_creation_time":1645926561000,
        "Issue_closed_time":1649939186000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\ndoing\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 61`\r\n\r\nas described here https:\/\/aimstack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show-mlflow-logs-in-aim\r\n\r\nfails with the following error\r\n\r\n![Screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png)\r\n\r\nusing the experiment name instead of the experiment id\r\n\r\n![Screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png)\r\n\r\ni.e.\r\n\r\n`$ aim convert mlflow --tracking_uri 'file:\/\/\/Users\/aim_user\/mlruns' --experiment 'ai-vengers-collab'` \r\n\r\nworks:\r\n\r\n![Screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png)\r\n\r\n### To reproduce\r\n\r\nsee above\r\n\r\n### Expected behavior\r\n\r\nconvert the experiment by ID\r\n\r\n### Environment\r\n\r\n- Aim Version 3.6\r\n- Python 3.8.1\r\n- pip3\r\n- Ubuntu 20.04.3 LTS\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: convert --experiment fails for experiment id, works for experiment name; Content: ## bug doing `$ convert --tracking_uri 'file:\/\/\/users\/_user\/s' --experiment 61` as described here https:\/\/stack.readthedocs.io\/en\/latest\/quick_start\/convert_data.html#show--logs-in- fails with the following error ![screenshot from 2022-02-27 02-33-17](https:\/\/user-images.githubusercontent.com\/26168435\/155864827-dc7f3acb-0c79-4fab-9c79-a599f1a954ab.png) using the experiment name instead of the experiment id ![screenshot from 2022-02-27 02-33-55](https:\/\/user-images.githubusercontent.com\/26168435\/155864887-63c19423-865e-4540-bfb7-c034e123af80.png) i.e. `$ convert --tracking_uri 'file:\/\/\/users\/_user\/s' --experiment 'ai-vengers-collab'` works: ![screenshot from 2022-02-27 02-31-46](https:\/\/user-images.githubusercontent.com\/26168435\/155864881-03434a11-68f8-47e3-90e3-13465cbe86b4.png) ### to reproduce see above ### expected behavior convert the experiment by id ### environment - version 3.6 - python 3.8.1 - pip3 - ubuntu 20.04.3 lts",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where using the experiment ID to convert an experiment with AIM failed, but using the experiment name instead worked.",
        "Issue_preprocessed_content":"Title: convert fails for experiment id, works for experiment name; Content: bug doing as described here fails with the following error using the experiment name instead of the experiment id works to reproduce see above expected behavior convert the experiment by id environment version python pip ubuntu lts"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1899",
        "Issue_title":"nyc-taxi-mlflow-deployment.yml refers to a folder that doesn't exists",
        "Issue_creation_time":1669062340000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Operating System\n\nLinux\n\n### Version Information\n\nAzure cli v2\n\n### Steps to reproduce\n\nThe azureml-example batch endpoint nyc-taxi-mlflow-deployment.yml file, refers to a  .\/autolog_nyc_taxi folder that doesn't exist\n\n### Expected behavior\n\nIt looks like we need to re-add the folder?\n\n### Actual behavior\n\ncode fails because folder doesn't exist\n\n### Addition information\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: nyc-taxi--deployment.yml refers to a folder that doesn't exists; Content: ### operating system linux ### version information azure cli v2 ### steps to reproduce the -example batch endpoint nyc-taxi--deployment.yml file, refers to a .\/autolog_nyc_taxi folder that doesn't exist ### expected behavior it looks like we need to re-add the folder? ### actual behavior code fails because folder doesn't exist ### addition information _no response_",
        "Issue_original_content_gpt_summary":"The user encountered an issue where the -example batch endpoint nyc-taxi--deployment.yml file referred to a folder that did not exist, causing the code to fail.",
        "Issue_preprocessed_content":"Title: refers to a folder that doesn't exists; Content: operating system linux version information azure cli v steps to reproduce the batch endpoint file, refers to a folder that doesn't exist expected behavior it looks like we need to the folder? actual behavior code fails because folder doesn't exist addition information"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/1897",
        "Issue_title":"MLflow cli endpoint example out of date with new syntax from breaking changes in yaml (last updated May 11)",
        "Issue_creation_time":1669043018000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Operating System\n\nWindows\n\n### Version Information\n\nlatest cli v2 \n\n### Steps to reproduce\n\nhttps:\/\/github.com\/Azure\/azureml-examples\/blob\/main\/cli\/endpoints\/online\/mlflow\/sklearn-deployment.yaml\r\n\r\nThis yaml is out of date, the model yaml config is wrong. \"name\" is no longer required when specifying model.\n\n### Expected behavior\n\nThat the deployment works based on sklearn-deployment.yml using the cli command `az create deployment`, but it fails. \n\n### Actual behavior\n\nIt fails.\n\n### Addition information\n\n_No response_",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: cli endpoint example out of date with new syntax from breaking changes in yaml (last updated may 11); Content: ### operating system windows ### version information latest cli v2 ### steps to reproduce https:\/\/github.com\/azure\/-examples\/blob\/main\/cli\/endpoints\/online\/\/sklearn-deployment.yaml this yaml is out of date, the model yaml config is wrong. \"name\" is no longer required when specifying model. ### expected behavior that the deployment works based on sklearn-deployment.yml using the cli command `az create deployment`, but it fails. ### actual behavior it fails. ### addition information _no response_",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with deploying an endpoint using the Azure CLI, due to the syntax in the YAML file being out of date with the latest version of the CLI.",
        "Issue_preprocessed_content":"Title: cli endpoint example out of date with new syntax from breaking changes in yaml ; Content: operating system windows version information latest cli v steps to reproduce this yaml is out of date, the model yaml config is wrong. name is no longer required when specifying model. expected behavior that the deployment works based on using the cli command , but it fails. actual behavior it fails. addition information"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/937",
        "Issue_title":"Mlflow error on pytorch.log_model but model is saved",
        "Issue_creation_time":1638959064000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Which example? Describe the issue\r\n\r\nexample: [CIFAR pytorch distributed](https:\/\/github.com\/Azure\/azureml-examples\/tree\/main\/cli\/jobs\/single-step\/pytorch\/cifar-distributed)\r\n\r\ndescription: model training shows completed, model is saved as well but driver logs (`70_driver_log..`.) for the model saving driver has:\r\n \r\n`ERROR mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: \/tmp\/tmpvthoxt0n\/model\/data, flavor: pytorch)\r\nTraceback (most recent call last):\r\n  File \"\/azureml-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/mlflow\/utils\/environment.py\", line 194, in infer_pip_requirements\r\n    return _infer_requirements(model_uri, flavor)\r\n  File \"\/azureml-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/mlflow\/utils\/requirements_utils.py\", line 306, in _infer_requirements\r\n    _MODULES_TO_PACKAGES = importlib_metadata.packages_distributions()\r\nAttributeError: module 'importlib_metadata' has no attribute 'packages_distributions'`\r\n\r\n## Additional context\r\n\r\nTried with variations to the environment in `job.yml: azureml:AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:11 and azureml:AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:6`. Same outcome. \r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: error on pytorch.log_model but model is saved; Content: ## which example? describe the issue example: [cifar pytorch distributed](https:\/\/github.com\/azure\/-examples\/tree\/main\/cli\/jobs\/single-step\/pytorch\/cifar-distributed) description: model training shows completed, model is saved as well but driver logs (`70_driver_log..`.) for the model saving driver has: `error .utils.environment: encountered an unexpected error while inferring pip requirements (model uri: \/tmp\/tmpvthoxt0n\/model\/data, flavor: pytorch) traceback (most recent call last): file \"\/-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/\/utils\/environment.py\", line 194, in infer_pip_requirements return _infer_requirements(model_uri, flavor) file \"\/-envs\/pytorch-1.9\/lib\/python3.7\/site-packages\/\/utils\/requirements_utils.py\", line 306, in _infer_requirements _modules_to_packages = importlib_metadata.packages_distributions() attributeerror: module 'importlib_metadata' has no attribute 'packages_distributions'` ## additional context tried with variations to the environment in `job.yml: :-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:11 and :-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu:6`. same outcome.",
        "Issue_original_content_gpt_summary":"The user encountered an unexpected error while attempting to log a model in PyTorch with the CIFAR distributed example.",
        "Issue_preprocessed_content":"Title: error on but model is saved; Content: which example? describe the issue example description model training shows completed, model is saved as well but driver logs for the model saving driver has additional context tried with variations to the environment in . same outcome."
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/azureml-examples\/issues\/318",
        "Issue_title":"MLflow 1.13 probably broke deployment",
        "Issue_creation_time":1609294231000,
        "Issue_closed_time":1609558021000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"\r\n\r\nhttps:\/\/github.com\/Azure\/azureml-examples\/runs\/1618089261?check_suite_focus=true\r\n\r\n@trangevi \r\n\r\nhttps:\/\/github.com\/mlflow\/mlflow\/pull\/3419\/files",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: 1.13 probably broke deployment; Content: https:\/\/github.com\/azure\/-examples\/runs\/1618089261?check_suite_focus=true @trangevi https:\/\/github.com\/\/\/pull\/3419\/files",
        "Issue_original_content_gpt_summary":"The user @trangevi encountered a challenge with deployment after 1.13 was released, as evidenced by a failed run on AzureML-examples.",
        "Issue_preprocessed_content":"Title: probably broke deployment; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1170",
        "Issue_title":"mlflow.projects.run failing consistently with etag error ",
        "Issue_creation_time":1601583593000,
        "Issue_closed_time":1601658581000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"logs: \r\n\r\n```\r\nRun papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python\r\nInput Notebook:  notebooks\/sklearn\/train-diabetes-mlproject.ipynb\r\nOutput Notebook: out.ipynb\r\n\r\nExecuting:   0%|          | 0\/7 [00:00<?, ?cell\/s]Executing notebook with kernel: python\r\n\r\nExecuting:  14%|\u2588\u258d        | 1\/7 [00:01<00:07,  1.33s\/cell]\r\nExecuting:  29%|\u2588\u2588\u258a       | 2\/7 [00:02<00:07,  1.43s\/cell]\r\nExecuting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 4\/7 [00:05<00:03,  1.32s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:07<00:01,  1.34s\/cell]\r\nExecuting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 6\/7 [00:08<00:01,  1.40s\/cell]\r\nTraceback (most recent call last):\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/bin\/papermill\", line 8, in <module>\r\n    sys.exit(papermill())\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill\r\n    execute_notebook(\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook\r\n    raise_for_execution_errors(nb, output_path)\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors\r\n    raise error\r\npapermill.exceptions.PapermillExecutionError: \r\n---------------------------------------------------------------------------\r\nException encountered at \"In [5]\":\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-5-ef514d3992f5> in <module>\r\n----> 1 run = mlflow.projects.run(\r\n      2     uri=str(project_uri),\r\n      3     parameters=***\"alpha\": 0.3***,\r\n      4     backend=\"azureml\",\r\n      5     backend_config=backend_config,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id)\r\n    271     )\r\n    272 \r\n--> 273     submitted_run_obj = _run(\r\n    274         uri=uri,\r\n    275         experiment_id=experiment_id,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/mlflow\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous)\r\n     98         backend = loader.load_backend(backend_name)\r\n     99         if backend:\r\n--> 100             submitted_run = backend.run(\r\n    101                 uri,\r\n    102                 entry_point,\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/mlflow\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id)\r\n    240         if compute and compute != _LOCAL and compute != _LOCAL.upper():\r\n    241             remote_environment = _load_remote_environment(mlproject)\r\n--> 242             remote_environment.register(workspace=workspace)\r\n    243             cpu_cluster = _load_compute_target(workspace, backend_config)\r\n    244             src.run_config.target = cpu_cluster.name\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/environment.py in register(self, workspace)\r\n    803         environment_client = EnvironmentClient(workspace.service_context)\r\n    804         environment_dict = Environment._serialize_to_dict(self)\r\n--> 805         response = environment_client._register_environment_definition(environment_dict)\r\n    806         env = Environment._deserialize_and_add_to_object(response)\r\n    807 \r\n\r\n\/opt\/hostedtoolcache\/Python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/azureml\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict)\r\n     75             message = \"Error registering the environment definition. Code: ***\\n: ***\".format(response.status_code,\r\n     76                                                                                             response.text)\r\n---> 77             raise Exception(message)\r\n     78 \r\n     79     def _get_image_details(self, name, version=None):\r\n\r\nException: Error registering the environment definition. Code: 409\r\n: ***\r\n  \"error\": ***\r\n    \"code\": \"TransientError\",\r\n    \"severity\": null,\r\n    \"message\": \"Etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\",\r\n    \"messageFormat\": null,\r\n    \"messageParameters\": null,\r\n    \"referenceCode\": null,\r\n    \"detailsUri\": null,\r\n    \"target\": null,\r\n    \"details\": [],\r\n    \"innerError\": null,\r\n    \"debugInfo\": null\r\n  ***,\r\n  \"correlation\": ***\r\n    \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\",\r\n    \"request\": \"f470e9430c5ed842\"\r\n  ***,\r\n  \"environment\": \"eastus\",\r\n  \"location\": \"eastus\",\r\n  \"time\": \"2020-10-01T20:17:52.8383774+00:00\",\r\n  \"componentName\": \"environment-management\"\r\n***\r\n\r\nError: Process completed with exit code 1.\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: .projects.run failing consistently with etag error ; Content: logs: ``` run papermill notebooks\/sklearn\/train-diabetes-mlproject.ipynb out.ipynb -k python input notebook: notebooks\/sklearn\/train-diabetes-mlproject.ipynb output notebook: out.ipynb executing: 0%| | 0\/7 [00:00 sys.exit(papermill()) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 829, in __call__ return self.main(*args, **kwargs) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 782, in main rv = self.invoke(ctx) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 1066, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/core.py\", line 610, in invoke return callback(*args, **kwargs) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/click\/decorators.py\", line 21, in new_func return f(get_current_context(), *args, **kwargs) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/cli.py\", line 240, in papermill execute_notebook( file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 110, in execute_notebook raise_for_execution_errors(nb, output_path) file \"\/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/papermill\/execute.py\", line 222, in raise_for_execution_errors raise error papermill.exceptions.papermillexecutionerror: --------------------------------------------------------------------------- exception encountered at \"in [5]\": --------------------------------------------------------------------------- exception traceback (most recent call last) in ----> 1 run = .projects.run( 2 uri=str(project_uri), 3 parameters=***\"alpha\": 0.3***, 4 backend=\"\", 5 backend_config=backend_config, \/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/\/projects\/__init__.py in run(uri, entry_point, version, parameters, docker_args, experiment_name, experiment_id, backend, backend_config, use_conda, storage_dir, synchronous, run_id) 271 ) 272 --> 273 submitted_run_obj = _run( 274 uri=uri, 275 experiment_id=experiment_id, \/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/\/projects\/__init__.py in _run(uri, experiment_id, entry_point, version, parameters, docker_args, backend_name, backend_config, use_conda, storage_dir, synchronous) 98 backend = loader.load_backend(backend_name) 99 if backend: --> 100 submitted_run = backend.run( 101 uri, 102 entry_point, \/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/\/\/_internal\/projects.py in run(self, project_uri, entry_point, params, version, backend_config, tracking_uri, experiment_id) 240 if compute and compute != _local and compute != _local.upper(): 241 remote_environment = _load_remote_environment(mlproject) --> 242 remote_environment.register(workspace=workspace) 243 cpu_cluster = _load_compute_target(workspace, backend_config) 244 src.run_config.target = cpu_cluster.name \/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/\/core\/environment.py in register(self, workspace) 803 environment_client = environmentclient(workspace.service_context) 804 environment_dict = environment._serialize_to_dict(self) --> 805 response = environment_client._register_environment_definition(environment_dict) 806 env = environment._deserialize_and_add_to_object(response) 807 \/opt\/hostedtoolcache\/python\/3.8.5\/x64\/lib\/python3.8\/site-packages\/\/_restclient\/environment_client.py in _register_environment_definition(self, environment_dict) 75 message = \"error registering the environment definition. code: ***\\n: ***\".format(response.status_code, 76 response.text) ---> 77 raise exception(message) 78 79 def _get_image_details(self, name, version=none): exception: error registering the environment definition. code: 409 : *** \"error\": *** \"code\": \"transienterror\", \"severity\": null, \"message\": \"etag conflict on 0e149764-3720-4610-b0f3-3e3f974544ac\/8f54aa7d6c05b2722ba149d8ea3185c263ecf5310eb2d7271569d1918c736972 with etag .\", \"messageformat\": null, \"messageparameters\": null, \"referencecode\": null, \"detailsuri\": null, \"target\": null, \"details\": [], \"innererror\": null, \"debuginfo\": null ***, \"correlation\": *** \"operation\": \"db22e6e6bfa07f499f1749f708b798c9\", \"request\": \"f470e9430c5ed842\" ***, \"environment\": \"eastus\", \"location\": \"eastus\", \"time\": \"2020-10-01t20:17:52.8383774+00:00\", \"componentname\": \"environment-management\" *** error: process completed with exit code 1. ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with .projects.run failing consistently with an etag error.",
        "Issue_preprocessed_content":"Title: failing consistently with etag error ; Content: logs"
    },
    {
        "Issue_link":"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/776",
        "Issue_title":"AttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'",
        "Issue_creation_time":1581064035000,
        "Issue_closed_time":1581065545000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"I receive the following error when running the following [notebook](https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow\/train-local\/train-local.ipynb)\r\n\r\n```python\r\nIn [6]: ws.get_mlflow_tracking_uri()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-6-6c16e13b21e5> in <module>\r\n----> 1 ws.get_mlflow_tracking_uri()\r\n\r\nAttributeError: 'Workspace' object has no attribute 'get_mlflow_tracking_uri'\r\n```",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: attributeerror: 'workspace' object has no attribute 'get__tracking_uri'; Content: i receive the following error when running the following [notebook](https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/4c0cbac8348f18c502a63996fdee59c3fe682b79\/how-to-use-\/track-and-monitor-experiments\/using-\/train-local\/train-local.ipynb) ```python in [6]: ws.get__tracking_uri() --------------------------------------------------------------------------- attributeerror traceback (most recent call last) in ----> 1 ws.get__tracking_uri() attributeerror: 'workspace' object has no attribute 'get__tracking_uri' ```",
        "Issue_original_content_gpt_summary":"The user encountered an AttributeError when attempting to run a notebook from the Azure Machine Learning Notebooks repository.",
        "Issue_preprocessed_content":"Title: attributeerror 'workspace' object has no attribute ; Content: i receive the following error when running the following"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1317",
        "Issue_title":"on qrun:\"mlflow.exceptions.MlflowException: Param value .... had length 780, which exceeded length limit of 500 \"",
        "Issue_creation_time":1665708717000,
        "Issue_closed_time":1667718001000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"## \ud83d\udc1b Bug Description\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nwhen I do the example:\r\nqrun qrun benchmarks\\GATs\\workflow_config_gats_Alpha158.yaml\r\n\r\nI got the error info:\r\n\r\n\r\n\r\n(py38) D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples>qrun benchmarks\\GATs\\workflow_config_gats_Alpha158_full02.yaml\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [config.py:413] - default_conf: client.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.workflow - [expm.py:31] - experiment manager uri is at file:D:\\worksPool\\works2021\\adair2021\\S92\\P4\\qlib-main\\examples\\mlruns\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\r\n[7724:MainThread](2022-10-14 07:53:33,890) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': WindowsPath('C:\/Users\/adair2019\/.qlib\/qlib_data\/cn_data')}\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [expm.py:316] - <mlflow.tracking.client.MlflowClient object at 0x0000017B5D406F40>\r\n[7724:MainThread](2022-10-14 07:53:33,906) INFO - qlib.workflow - [exp.py:260] - Experiment 3 starts running ...\r\n[7724:MainThread](2022-10-14 07:53:34,124) INFO - qlib.workflow - [recorder.py:339] - Recorder 41d40d173e614811bad721127a3204b8 starts running under Experiment 3 ...\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,140) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,158) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git status`\r\n'git' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002\r\n[7724:MainThread](2022-10-14 07:53:34,164) INFO - qlib.workflow - [recorder.py:372] - Fail to log the uncommitted code of $CWD when run `git diff --cached`\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 301, in log_param\r\n    self.store.log_param(run_id, param)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\store\\tracking\\file_store.py\", line 887, in log_param\r\n    _validate_param(param.key, param.value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 148, in _validate_param\r\n    _validate_length_limit(\"Param value\", MAX_PARAM_VAL_LENGTH, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\utils\\validation.py\", line 269, in _validate_length_limit\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner\r\n    self.run()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run\r\n    data()\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\client.py\", line 858, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"d:\\ProgramData\\Anaconda3\\envs\\py38\\lib\\site-packages\\mlflow-1.29.0-py3.8.egg\\mlflow\\tracking\\_tracking_service\\client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 780, which exceeded length limit of 500\r\n\r\nThe cause of this error is typically due to repeated calls\r\nto an individual run_id event logging.\r\n\r\nIncorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    mlflow.log_param(\"depth\", 3)\r\n    mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will throw an MlflowException for overwriting a\r\nlogged parameter.\r\n\r\nCorrect Example:\r\n---------------------------------------\r\nwith mlflow.start_run():\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 3)\r\n    with mlflow.start_run(nested=True):\r\n        mlflow.log_param(\"depth\", 5)\r\n---------------------------------------\r\n\r\nWhich will create a new nested run for each individual\r\nmodel and prevent parameter key collisions within the\r\ntracking store.'\r\n[7724:MainThread](2022-10-14 07:53:35,515) INFO - qlib.GATs - [pytorch_gats_ts.py:81] - GATs pytorch version...\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:100] - GATs parameters setting:\r\nd_feat : 158\r\nhidden_size : 64\r\nnum_layers : 2\r\ndropout : 0.7\r\nn_epochs : 200\r\nlr : 0.0001\r\nmetric : loss\r\nearly_stop : 10\r\noptimizer : adam\r\nloss_type : mse\r\nbase_model : LSTM\r\nmodel_path : None\r\nvisible_GPU : 0\r\nuse_GPU : True\r\nseed : None\r\n[7724:MainThread](2022-10-14 07:53:35,562) INFO - qlib.GATs - [pytorch_gats_ts.py:146] - model:\r\nGATModel(\r\n  (rnn): LSTM(158, 64, num_layers=2, batch_first=True, dropout=0.7)\r\n  (transformation): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc): Linear(in_features=64, out_features=64, bias=True)\r\n  (fc_out): Linear(in_features=64, out_features=1, bias=True)\r\n  (leaky_relu): LeakyReLU(negative_slope=0.01)\r\n  (softmax): Softmax(dim=1)\r\n)\r\n\r\n\r\n\r\n\r\nThen the program re-run again.\r\nI am wondering how to fix it.\r\nThanks a lot.\r\n\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\n - Qlib version:\r\n - 0.8.6.99'\r\n - Python version:\r\n - 3.8.5\r\n - OS (`Windows`, `Linux`, `MacOS`):\r\n - windows 10\r\n - Commit number (optional, please provide it if you are using the dev version):\r\n\r\n## Additional Notes\r\n\r\n<!-- Add any other information about the problem here. -->\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: on qrun:\".exceptions.exception: param value .... had length 780, which exceeded length limit of 500 \"; Content: ## bug description when i do the example: qrun qrun benchmarks\\gats\\workflow_config_gats_alpha158.yaml i got the error info: (py38) d:\\workspool\\works2021\\adair2021\\s92\\p4\\qlib-main\\examples>qrun benchmarks\\gats\\workflow_config_gats_alpha158_full02.yaml [7724:mainthread](2022-10-14 07:53:33,890) info - qlib.initialization - [config.py:413] - default_conf: client. [7724:mainthread](2022-10-14 07:53:33,890) info - qlib.workflow - [expm.py:31] - experiment manager uri is at file:d:\\workspool\\works2021\\adair2021\\s92\\p4\\qlib-main\\examples\\s [7724:mainthread](2022-10-14 07:53:33,890) info - qlib.initialization - [__init__.py:74] - qlib successfully initialized based on client settings. [7724:mainthread](2022-10-14 07:53:33,890) info - qlib.initialization - [__init__.py:76] - data_path={'__default_freq': windowspath('c:\/users\/adair2019\/.qlib\/qlib_data\/cn_data')} [7724:mainthread](2022-10-14 07:53:33,906) info - qlib.workflow - [expm.py:316] - <.tracking.client.client object at 0x0000017b5d406f40> [7724:mainthread](2022-10-14 07:53:33,906) info - qlib.workflow - [exp.py:260] - experiment 3 starts running ... [7724:mainthread](2022-10-14 07:53:34,124) info - qlib.workflow - [recorder.py:339] - recorder 41d40d173e614811bad721127a3204b8 starts running under experiment 3 ... 'git' [7724:mainthread](2022-10-14 07:53:34,140) info - qlib.workflow - [recorder.py:372] - fail to log the uncommitted code of $cwd when run `git diff` 'git' [7724:mainthread](2022-10-14 07:53:34,158) info - qlib.workflow - [recorder.py:372] - fail to log the uncommitted code of $cwd when run `git status` 'git' [7724:mainthread](2022-10-14 07:53:34,164) info - qlib.workflow - [recorder.py:372] - fail to log the uncommitted code of $cwd when run `git diff --cached` exception in thread thread-1: traceback (most recent call last): file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\tracking\\_tracking_service\\client.py\", line 301, in log_param self.store.log_param(run_id, param) file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\store\\tracking\\file_store.py\", line 887, in log_param _validate_param(param.key, param.value) file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\utils\\validation.py\", line 148, in _validate_param _validate_length_limit(\"param value\", max_param_val_length, value) file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\utils\\validation.py\", line 269, in _validate_length_limit raise exception( .exceptions.exception: param value '[{'class': 'signalrecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '', 'dataset': ''}}, {'class': 'siganarecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': false, 'ann_scaler': 25' had length 780, which exceeded length limit of 500 during handling of the above exception, another exception occurred: traceback (most recent call last): file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\threading.py\", line 932, in _bootstrap_inner self.run() file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\threading.py\", line 870, in run self._target(*self._args, **self._kwargs) file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\utils\\paral.py\", line 91, in run data() file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\pyqlib-0.8.6.99-py3.8-win-amd64.egg\\qlib\\workflow\\recorder.py\", line 441, in log_params self.client.log_param(self.id, name, data) file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\tracking\\client.py\", line 858, in log_param self._tracking_client.log_param(run_id, key, value) file \"d:\\programdata\\anaconda3\\envs\\py38\\lib\\site-packages\\-1.29.0-py3.8.egg\\\\tracking\\_tracking_service\\client.py\", line 305, in log_param raise exception(msg, invalid_parameter_value) .exceptions.exception: param value '[{'class': 'signalrecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '', 'dataset': ''}}, {'class': 'siganarecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': false, 'ann_scaler': 25' had length 780, which exceeded length limit of 500 the cause of this error is typically due to repeated calls to an individual run_id event logging. incorrect example: --------------------------------------- with .start_run(): .log_param(\"depth\", 3) .log_param(\"depth\", 5) --------------------------------------- which will throw an exception for overwriting a logged parameter. correct example: --------------------------------------- with .start_run(): with .start_run(nested=true): .log_param(\"depth\", 3) with .start_run(nested=true): .log_param(\"depth\", 5) --------------------------------------- which will create a new nested run for each individual model and prevent parameter key collisions within the tracking store.' [7724:mainthread](2022-10-14 07:53:35,515) info - qlib.gats - [pytorch_gats_ts.py:81] - gats pytorch version... [7724:mainthread](2022-10-14 07:53:35,562) info - qlib.gats - [pytorch_gats_ts.py:100] - gats parameters setting: d_feat : 158 hidden_size : 64 num_layers : 2 dropout : 0.7 n_epochs : 200 lr : 0.0001 metric : loss early_stop : 10 optimizer : adam loss_type : mse base_model : lstm model_path : none visible_gpu : 0 use_gpu : true seed : none [7724:mainthread](2022-10-14 07:53:35,562) info - qlib.gats - [pytorch_gats_ts.py:146] - model: gatmodel( (rnn): lstm(158, 64, num_layers=2, batch_first=true, dropout=0.7) (transformation): linear(in_features=64, out_features=64, bias=true) (fc): linear(in_features=64, out_features=64, bias=true) (fc_out): linear(in_features=64, out_features=1, bias=true) (leaky_relu): leakyrelu(negative_slope=0.01) (softmax): softmax(dim=1) ) then the program re-run again. i am wondering how to fix it. thanks a lot. ## to reproduce steps to reproduce the behavior: 1. 1. 1. ## expected behavior ## screenshot ## environment **note**: user could run `cd scripts && python collect_info.py all` under project directory to get system information and paste them here directly. - qlib version: - 0.8.6.99' - python version: - 3.8.5 - os (`windows`, `linux`, `macos`): - windows 10 - commit number (optional, please provide it if you are using the dev version): ## additional notes",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running qrun with a workflow configuration file, resulting in an error message due to exceeding the length limit of 500 for a parameter value.",
        "Issue_preprocessed_content":"Title: on param value had length , which exceeded length limit of ; Content: bug description a clear and concise description of what the bug is. when i do the example qrun qrun i got the error info py , info client. , info experiment manager uri is at , info qlib successfully initialized based on client settings. , info , info , info experiment starts running , info recorder d d e bad a b starts running under experiment 'git' , info fail to log the uncommitted code of $cwd when run 'git' , info fail to log the uncommitted code of $cwd when run 'git' , info fail to log the uncommitted code of $cwd when run exception in thread traceback file line , in param file line , in file line , in value , value file line , in raise exception file line , in file line , in run file line , in run data file line , in name, data file line , in key, value file line , in raise exception param value ' info gats pytorch , info gats parameters setting dropout lr metric loss optimizer adam mse lstm none true seed none , info model gatmodel lstm bias true bias true bias true softmax then the program again. i am wondering how to fix it. thanks a lot. to reproduce steps to reproduce the behavior . . . expected behavior a clear and concise description of what you expected to happen. screenshot a screenshot of the error message or anything shouldn't environment note user could run under project directory to get system information and paste them here directly. qlib version python version os windows commit number additional notes add any other information about the problem here."
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1298",
        "Issue_title":"not compatible with mlflow v1.28.0",
        "Issue_creation_time":1663557425000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"when run workflow:\r\n```\r\nqrun ALSTM_workflow_config_alstm_Alpha158.yaml\r\n```  \r\nmlflow v1.27.0 work fine,but failed when with mlflow v1.28.0:\r\n```\r\nFile \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/pyqlib-0.8.6.99-py3.8-linux-x86_64.egg\/qlib\/workflow\/recorder.py\", line 441, in log_params\r\n    self.client.log_param(self.id, name, data)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/client.py\", line 852, in log_param\r\n    self._tracking_client.log_param(run_id, key, value)\r\n  File \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/mlflow-1.28.0-py3.8.egg\/mlflow\/tracking\/_tracking_service\/client.py\", line 305, in log_param\r\n    raise MlflowException(msg, INVALID_PARAMETER_VALUE)\r\nmlflow.exceptions.MlflowException: Param value '[{'class': 'SignalRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '<MODEL>', 'dataset': '<DATASET>'}}, {'class': 'SigAnaRecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': False, 'ann_scaler': 25' had length 778, which exceeded length limit of 500\r\n```\r\ni think the new mflow feature cause this bug.mlflow limit param valu lengh to 500,by read code ,it can not be overwrite.\r\nmaybe relate with this [issue](https:\/\/github.com\/mlflow\/mlflow\/commit\/d4109d00079355459a9a3df1821f0878877e42a8)\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: not compatible with v1.28.0; Content: when run workflow: ``` qrun alstm_workflow_config_alstm_alpha158.yaml ``` v1.27.0 work fine,but failed when with v1.28.0: ``` file \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/pyqlib-0.8.6.99-py3.8-linux-x86_64.egg\/qlib\/workflow\/recorder.py\", line 441, in log_params self.client.log_param(self.id, name, data) file \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/-1.28.0-py3.8.egg\/\/tracking\/client.py\", line 852, in log_param self._tracking_client.log_param(run_id, key, value) file \"miniconda3\/envs\/qlibdev\/lib\/python3.8\/site-packages\/-1.28.0-py3.8.egg\/\/tracking\/_tracking_service\/client.py\", line 305, in log_param raise exception(msg, invalid_parameter_value) .exceptions.exception: param value '[{'class': 'signalrecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'model': '', 'dataset': ''}}, {'class': 'siganarecord', 'module_path': 'qlib.workflow.record_temp', 'kwargs': {'ana_long_short': false, 'ann_scaler': 25' had length 778, which exceeded length limit of 500 ``` i think the new mflow feature cause this bug. limit param valu lengh to 500,by read code ,it can not be overwrite. maybe relate with this [issue](https:\/\/github.com\/\/\/commit\/d4109d00079355459a9a3df1821f0878877e42a8)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running a workflow with v1.28.0, where the workflow failed due to a parameter value length limit of 500, which was caused by a new mflow feature.",
        "Issue_preprocessed_content":"Title: not compatible with ; Content: when run workflow work fine,but failed when with i think the new mflow feature cause this limit param valu lengh to ,by read code ,it can not be overwrite. maybe relate with this"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/1035",
        "Issue_title":"run the example workflow_by_code.ipynb, caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' ",
        "Issue_creation_time":1649238776000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"## \ud83d\udc1b Bug Description\r\nwhen I run the code below in qlib-main\/examples\/workflow_by_code.ipynb\uff0cit caused MlflowException: Invalid experiment ID: '.ipynb_checkpoints' \r\n###################################\r\n# train model\r\n###################################\r\ndata_handler_config = {\r\n    \"start_time\": \"2008-01-01\",\r\n    \"end_time\": \"2020-08-01\",\r\n    \"fit_start_time\": \"2008-01-01\",\r\n    \"fit_end_time\": \"2014-12-31\",\r\n    \"instruments\": market,\r\n}\r\n\r\ntask = {\r\n    \"model\": {\r\n        \"class\": \"LGBModel\",\r\n        \"module_path\": \"qlib.contrib.model.gbdt\",\r\n        \"kwargs\": {\r\n            \"loss\": \"mse\",\r\n            \"colsample_bytree\": 0.8879,\r\n            \"learning_rate\": 0.0421,\r\n            \"subsample\": 0.8789,\r\n            \"lambda_l1\": 205.6999,\r\n            \"lambda_l2\": 580.9768,\r\n            \"max_depth\": 8,\r\n            \"num_leaves\": 210,\r\n            \"num_threads\": 20,\r\n        },\r\n    },\r\n    \"dataset\": {\r\n        \"class\": \"DatasetH\",\r\n        \"module_path\": \"qlib.data.dataset\",\r\n        \"kwargs\": {\r\n            \"handler\": {\r\n                \"class\": \"Alpha158\",\r\n                \"module_path\": \"qlib.contrib.data.handler\",\r\n                \"kwargs\": data_handler_config,\r\n            },\r\n            \"segments\": {\r\n                \"train\": (\"2008-01-01\", \"2014-12-31\"),\r\n                \"valid\": (\"2015-01-01\", \"2016-12-31\"),\r\n                \"test\": (\"2017-01-01\", \"2020-08-01\"),\r\n            },\r\n        },\r\n    },\r\n}\r\n\r\n# model initiaiton\r\nmodel = init_instance_by_config(task[\"model\"])\r\ndataset = init_instance_by_config(task[\"dataset\"])\r\n\r\n# start exp to train model\r\nwith R.start(experiment_name=\"train_model\"):\r\n    R.log_params(**flatten_dict(task))\r\n    model.fit(dataset)\r\n    R.save_objects(trained_model=model)\r\n    rid = R.get_recorder().id\r\n\r\n=====================\r\nThe whole error message is below\uff1a\r\n[2607:MainThread](2022-04-06 17:38:12,377) INFO - qlib.timer - [log.py:113] - Time cost: 18.919s | Loading data Done\r\n[2607:MainThread](2022-04-06 17:38:12,737) INFO - qlib.timer - [log.py:113] - Time cost: 0.147s | DropnaLabel Done\r\n\/Users\/yzwu\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/data\/dataset\/processor.py:310: SettingWithCopyWarning: \r\nA value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_indexer,col_indexer] = value instead\r\n\r\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\r\n  df[cols] = df[cols].groupby(\"datetime\").apply(self.zscore_func)\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 1.650s | CSZScoreNorm Done\r\n[2607:MainThread](2022-04-06 17:38:14,387) INFO - qlib.timer - [log.py:113] - Time cost: 2.010s | fit & process data Done\r\n[2607:MainThread](2022-04-06 17:38:14,388) INFO - qlib.timer - [log.py:113] - Time cost: 20.930s | Init data Done\r\n[2607:MainThread](2022-04-06 17:38:14,399) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x2859099a0>\r\n[2607:MainThread](2022-04-06 17:38:14,402) WARNING - qlib.workflow - [expm.py:195] - No valid experiment found. Create a new experiment with name train_model.\r\n---------------------------------------------------------------------------\r\nMlflowException                           Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:391, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    390 try:\r\n--> 391     exp = self.client.get_experiment_by_name(experiment_name)\r\n    392     if exp is None or exp.lifecycle_stage.upper() == \"DELETED\":\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:462, in MlflowClient.get_experiment_by_name(self, name)\r\n    432 \"\"\"\r\n    433 Retrieve an experiment by experiment name from the backend store\r\n    434 \r\n   (...)\r\n    460     Lifecycle_stage: active\r\n    461 \"\"\"\r\n--> 462 return self._tracking_client.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:167, in TrackingServiceClient.get_experiment_by_name(self, name)\r\n    163 \"\"\"\r\n    164 :param name: The experiment name.\r\n    165 :return: :py:class:`mlflow.entities.Experiment`\r\n    166 \"\"\"\r\n--> 167 return self.store.get_experiment_by_name(name)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     67 \"\"\"\r\n     68 Fetch the experiment by name from the backend store.\r\n     69 This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74 :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75 \"\"\"\r\n---> 76 for experiment in self.list_experiments(ViewType.ALL):\r\n     77     if experiment.name == experiment_name:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    259 try:\r\n    260     # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261     experiment = self._get_experiment(exp_id, view_type)\r\n    262     if experiment:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    336 self._check_root_dir()\r\n--> 337 _validate_experiment_id(experiment_id)\r\n    338 experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:189, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    187 try:\r\n    188     return (\r\n--> 189         self._get_exp(experiment_id=experiment_id, experiment_name=experiment_name),\r\n    190         False,\r\n    191     )\r\n    192 except ValueError:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:397, in MLflowExpManager._get_exp(self, experiment_id, experiment_name)\r\n    396 except MlflowException as e:\r\n--> 397     raise ValueError(\r\n    398         \"No valid experiment has been found, please make sure the input experiment name is correct.\"\r\n    399     ) from e\r\n\r\nValueError: No valid experiment has been found, please make sure the input experiment name is correct.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMlflowException                           Traceback (most recent call last)\r\nInput In [6], in <cell line: 51>()\r\n     48 dataset = init_instance_by_config(task[\"dataset\"])\r\n     50 # start exp to train model\r\n---> 51 with R.start(experiment_name=\"train_model\"):\r\n     52     R.log_params(**flatten_dict(task))\r\n     53     model.fit(dataset)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/contextlib.py:113, in _GeneratorContextManager.__enter__(self)\r\n    111 del self.args, self.kwds, self.func\r\n    112 try:\r\n--> 113     return next(self.gen)\r\n    114 except StopIteration:\r\n    115     raise RuntimeError(\"generator didn't yield\") from None\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:69, in QlibRecorder.start(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     25 @contextmanager\r\n     26 def start(\r\n     27     self,\r\n   (...)\r\n     34     resume: bool = False,\r\n     35 ):\r\n     36     \"\"\"\r\n     37     Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\r\n     38 \r\n   (...)\r\n     67         whether to resume the specific recorder with given name under the given experiment.\r\n     68     \"\"\"\r\n---> 69     run = self.start_exp(\r\n     70         experiment_id=experiment_id,\r\n     71         experiment_name=experiment_name,\r\n     72         recorder_id=recorder_id,\r\n     73         recorder_name=recorder_name,\r\n     74         uri=uri,\r\n     75         resume=resume,\r\n     76     )\r\n     77     try:\r\n     78         yield run\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/__init__.py:125, in QlibRecorder.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n     84 def start_exp(\r\n     85     self,\r\n     86     *,\r\n   (...)\r\n     92     resume=False,\r\n     93 ):\r\n     94     \"\"\"\r\n     95     Lower level method for starting an experiment. When use this method, one should end the experiment manually\r\n     96     and the status of the recorder may not be handled properly. Here is the example code:\r\n   (...)\r\n    123     An experiment instance being started.\r\n    124     \"\"\"\r\n--> 125     return self.exp_manager.start_exp(\r\n    126         experiment_id=experiment_id,\r\n    127         experiment_name=experiment_name,\r\n    128         recorder_id=recorder_id,\r\n    129         recorder_name=recorder_name,\r\n    130         uri=uri,\r\n    131         resume=resume,\r\n    132     )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:339, in MLflowExpManager.start_exp(self, experiment_id, experiment_name, recorder_id, recorder_name, uri, resume)\r\n    337 if experiment_name is None:\r\n    338     experiment_name = self._default_exp_name\r\n--> 339 experiment, _ = self._get_or_create_exp(experiment_id=experiment_id, experiment_name=experiment_name)\r\n    340 # Set up active experiment\r\n    341 self.active_experiment = experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:202, in ExpManager._get_or_create_exp(self, experiment_id, experiment_name)\r\n    200 if pr.scheme == \"file\":\r\n    201     with FileLock(os.path.join(pr.netloc, pr.path, \"filelock\")):  # pylint: disable=E0110\r\n--> 202         return self.create_exp(experiment_name), True\r\n    203 # NOTE: for other schemes like http, we double check to avoid create exp conflicts\r\n    204 try:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:362, in MLflowExpManager.create_exp(self, experiment_name)\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n    361         raise ExpAlreadyExistError() from e\r\n--> 362     raise e\r\n    364 experiment = MLflowExperiment(experiment_id, experiment_name, self.uri)\r\n    365 experiment._default_name = self._default_exp_name\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/qlib\/workflow\/expm.py:358, in MLflowExpManager.create_exp(self, experiment_name)\r\n    356 # init experiment\r\n    357 try:\r\n--> 358     experiment_id = self.client.create_experiment(experiment_name)\r\n    359 except MlflowException as e:\r\n    360     if e.error_code == ErrorCode.Name(RESOURCE_ALREADY_EXISTS):\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py:507, in MlflowClient.create_experiment(self, name, artifact_location, tags)\r\n    464 def create_experiment(\r\n    465     self,\r\n    466     name: str,\r\n    467     artifact_location: Optional[str] = None,\r\n    468     tags: Optional[Dict[str, Any]] = None,\r\n    469 ) -> str:\r\n    470     \"\"\"Create an experiment.\r\n    471 \r\n    472     :param name: The experiment name. Must be unique.\r\n   (...)\r\n    505         Lifecycle_stage: active\r\n    506     \"\"\"\r\n--> 507     return self._tracking_client.create_experiment(name, artifact_location, tags)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py:182, in TrackingServiceClient.create_experiment(self, name, artifact_location, tags)\r\n    179 _validate_experiment_name(name)\r\n    180 _validate_experiment_artifact_location(artifact_location)\r\n--> 182 return self.store.create_experiment(\r\n    183     name=name,\r\n    184     artifact_location=artifact_location,\r\n    185     tags=[ExperimentTag(key, value) for (key, value) in tags.items()] if tags else [],\r\n    186 )\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:321, in FileStore.create_experiment(self, name, artifact_location, tags)\r\n    319 def create_experiment(self, name, artifact_location=None, tags=None):\r\n    320     self._check_root_dir()\r\n--> 321     self._validate_experiment_name(name)\r\n    322     # Get all existing experiments and find the one with largest numerical ID.\r\n    323     # len(list_all(..)) would not work when experiments are deleted.\r\n    324     experiments_ids = [\r\n    325         int(e.experiment_id)\r\n    326         for e in self.list_experiments(ViewType.ALL)\r\n    327         if e.experiment_id.isdigit()\r\n    328     ]\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:303, in FileStore._validate_experiment_name(self, name)\r\n    299 if name is None or name == \"\":\r\n    300     raise MlflowException(\r\n    301         \"Invalid experiment name '%s'\" % name, databricks_pb2.INVALID_PARAMETER_VALUE\r\n    302     )\r\n--> 303 experiment = self.get_experiment_by_name(name)\r\n    304 if experiment is not None:\r\n    305     if experiment.lifecycle_stage == LifecycleStage.DELETED:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/abstract_store.py:76, in AbstractStore.get_experiment_by_name(self, experiment_name)\r\n     66 def get_experiment_by_name(self, experiment_name):\r\n     67     \"\"\"\r\n     68     Fetch the experiment by name from the backend store.\r\n     69     This is a base implementation using ``list_experiments``, derived classes may have\r\n   (...)\r\n     74     :return: A single :py:class:`mlflow.entities.Experiment` object if it exists.\r\n     75     \"\"\"\r\n---> 76     for experiment in self.list_experiments(ViewType.ALL):\r\n     77         if experiment.name == experiment_name:\r\n     78             return experiment\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:261, in FileStore.list_experiments(self, view_type, max_results, page_token)\r\n    258 for exp_id in rsl:\r\n    259     try:\r\n    260         # trap and warn known issues, will raise unexpected exceptions to caller\r\n--> 261         experiment = self._get_experiment(exp_id, view_type)\r\n    262         if experiment:\r\n    263             experiments.append(experiment)\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/store\/tracking\/file_store.py:337, in FileStore._get_experiment(self, experiment_id, view_type)\r\n    335 def _get_experiment(self, experiment_id, view_type=ViewType.ALL):\r\n    336     self._check_root_dir()\r\n--> 337     _validate_experiment_id(experiment_id)\r\n    338     experiment_dir = self._get_experiment_path(experiment_id, view_type)\r\n    339     if experiment_dir is None:\r\n\r\nFile ~\/DevEnv\/miniconda3\/envs\/quant_py38_arm\/lib\/python3.8\/site-packages\/mlflow\/utils\/validation.py:267, in _validate_experiment_id(exp_id)\r\n    265 \"\"\"Check that `experiment_id`is a valid string or None, raise an exception if it isn't.\"\"\"\r\n    266 if exp_id is not None and _EXPERIMENT_ID_REGEX.match(exp_id) is None:\r\n--> 267     raise MlflowException(\r\n    268         \"Invalid experiment ID: '%s'\" % exp_id, error_code=INVALID_PARAMETER_VALUE\r\n    269     )\r\n\r\nMlflowException: Invalid experiment ID: '.ipynb_checkpoints'\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. just rerun the code in my envirment\r\n\r\n\r\n## Expected Behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Screenshot\r\n\r\n<!-- A screenshot of the error message or anything shouldn't appear-->\r\n\r\n## Environment\r\n\r\n**Note**: User could run `cd scripts && python collect_info.py all` under project directory to get system information\r\nand paste them here directly.\r\n\r\nDarwin\r\narm64\r\nmacOS-12.2.1-arm64-arm-64bit\r\nDarwin Kernel Version 21.3.0: Wed Jan  5 21:37:58 PST 2022; root:xnu-8019.80.24~20\/RELEASE_ARM64_T6000\r\n\r\nPython version: 3.8.11 (default, Jul 29 2021, 14:57:32)  [Clang 12.0.0 ]\r\n\r\nQlib version: 0.8.4.99\r\nnumpy==1.22.3\r\npandas==1.4.2\r\nscipy==1.8.0\r\nrequests==2.25.1\r\nsacred==0.8.2\r\npython-socketio==5.5.2\r\nredis==4.2.2\r\npython-redis-lock==3.7.0\r\nschedule==1.1.0\r\ncvxpy==1.1.18\r\nhyperopt==0.1.2\r\nfire==0.4.0\r\nstatsmodels==0.13.2\r\nxlrd==2.0.1\r\nplotly==5.6.0\r\nmatplotlib==3.5.1\r\ntables==3.7.0\r\npyyaml==6.0\r\nmlflow==1.24.0\r\ntqdm==4.61.2\r\nloguru==0.6.0\r\nlightgbm==3.3.2\r\ntornado==6.1\r\njoblib==1.1.0\r\nfire==0.4.0\r\nruamel.yaml==0.17.21\r\n\r\n\r\n## Additional Notes\r\n\r\nI installed qlib from source, and my conda env is the version for arm64\r\n",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: run the example workflow_by_code.ipynb, caused exception: invalid experiment id: '.ipynb_checkpoints' ; Content: ## bug description when i run the code below in qlib-main\/examples\/workflow_by_code.ipynbit caused exception: invalid experiment id: '.ipynb_checkpoints'",
        "Issue_original_content_gpt_summary":"The user encountered an exception when running the code in qlib-main\/examples\/workflow_by_code.ipynb, resulting in an invalid experiment id of '.ipynb_checkpoints'.",
        "Issue_preprocessed_content":"Title: run the example caused exception invalid experiment id ; Content: bug description when i run the code below in caused exception invalid experiment id train model task , , dataset , segments , , , model initiaiton model dataset start exp to train model with rid the whole error message is below , info time cost loading data done , info time cost dropnalabel done settingwithcopywarning a value is trying to be set on a copy of a slice from a dataframe. try using value instead see the caveats in the documentation df , info time cost cszscorenorm done , info time cost fit & process data done , info time cost init data done , info , warning no valid experiment found. create a new experiment with name exception traceback file in try exp if exp is none or deleted file in name retrieve an experiment by experiment name from the backend store active return file in name param name the experiment name. return return file in fetch the experiment by name from the backend store. this is a base implementation using , derived classes may have return a single object if it exists. for experiment in if file in try trap and warn known issues, will raise unexpected exceptions to caller experiment if experiment file in file in if is not none and is none raise exception exception invalid experiment id the above exception was the direct cause of the following exception valueerror traceback file in try return , false, except valueerror file in except exception as e raise valueerror from e valueerror no valid experiment has been found, please make sure the input experiment name is correct. during handling of the above exception, another exception occurred exception traceback input in , in dataset start exp to train model with file in del try return except stopiteration raise runtimeerror from none file in uri, resume def start resume bool false, method to start an experiment. this method can only be called within a python's statement. here is the example code whether to resume the specific recorder with given name under the given experiment. run uri uri, resume resume, try yield run file in uri, resume def self, , resume false, lower level method for starting an experiment. when use this method, one should end the experiment manually and the status of the recorder may not be handled properly. here is the example code an experiment instance being started. return uri uri, resume resume, file in uri, resume if is none experiment, _ set up active experiment experiment file in if file with filelock pylint disable e return true note for other schemes like http, we double check to avoid create exp conflicts try file in if raise expalreadyexisterror from e raise e experiment file in init experiment try except exception as e if file in name, tags def self, name str, optional none, tags optional none, str create an experiment. param name the experiment name. must be unique. active return tags file in name, tags return name name, tags experimenttag for in if tags else , file in name, tags def name, tags none get all existing experiments and find the one with largest numerical id. would not work when experiments are deleted. file in name if name is none or name raise exception experiment if experiment is not none if file in def fetch the experiment by name from the backend store. this is a base implementation using , derived classes may have return a single object if it exists. for experiment in if return experiment file in for in rsl try trap and warn known issues, will raise unexpected exceptions to caller experiment if experiment file in def if is none file in check that is a valid string or none, raise an exception if it if is not none and is none raise exception exception invalid experiment id to reproduce steps to reproduce the behavior . just rerun the code in my envirment expected behavior a clear and concise description of what you expected to happen. screenshot a screenshot of the error message or anything shouldn't environment note user could run under project directory to get system information and paste them here directly. darwin arm darwin kernel version wed jan pst ; python version qlib version additional notes i installed qlib from source, and my conda env is the version for arm"
    },
    {
        "Issue_link":"https:\/\/github.com\/microsoft\/qlib\/issues\/369",
        "Issue_title":"Double Ensemble MlflowException",
        "Issue_creation_time":1616595419000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via setup.py and uninstalling \/ reinstalling numpy. but this one i do not know how to solve:\r\nMlflowException: Got invalid value Series([], dtype: float64) for metric 'IC' (timestamp=1616595157552). Please specify value as a valid double (64-bit floating point)\r\n\r\nif i have only sh000300 in my instruments, it's gonna produce the following value error:\r\nValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]).\r\nYou can drop duplicate edges by setting the 'duplicates' kwarg\r\n\r\ni followed instructions on data collector's markdown page to download cn data up until 03\/01\/2021. my yaml file looks like this:\r\nqlib_init:\r\n    provider_uri: \"\/content\/gdrive\/MyDrive\/qlib\/qlib_data\/qlib_cn_1d\"\r\n    region: cn\r\nmarket: &market \r\nbenchmark: &benchmark SH000300\r\ndata_handler_config: &data_handler_config\r\n    start_time: 2008-01-01\r\n    end_time: 2021-03-01\r\n    fit_start_time: 2008-01-01\r\n    fit_end_time: 2018-12-31\r\n    instruments: ['SH000300', 'SH000903']\r\nport_analysis_config: &port_analysis_config\r\n    strategy:\r\n        class: TopkDropoutStrategy\r\n        module_path: qlib.contrib.strategy.strategy\r\n        kwargs:\r\n            topk: 50\r\n            n_drop: 5\r\n    backtest:\r\n        verbose: True\r\n        limit_threshold: 0.095\r\n        account: 50000\r\n        benchmark: *benchmark\r\n        deal_price: close\r\n        open_cost: 0.0005\r\n        close_cost: 0.0015\r\n        min_cost: 5\r\ntask:\r\n    model:\r\n        class: DEnsembleModel\r\n        module_path: qlib.contrib.model.double_ensemble\r\n        kwargs:\r\n            base_model: \"gbm\"\r\n            loss: mse\r\n            num_models: 6\r\n            enable_sr: True\r\n            enable_fs: True\r\n            alpha1: 1\r\n            alpha2: 1\r\n            bins_sr: 10\r\n            bins_fs: 5\r\n            decay: 0.5\r\n            sample_ratios:\r\n                - 0.8\r\n                - 0.7\r\n                - 0.6\r\n                - 0.5\r\n                - 0.4\r\n            sub_weights:\r\n                - 1\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n                - 0.2\r\n            epochs: 28\r\n            colsample_bytree: 0.8879\r\n            learning_rate: 0.2\r\n            subsample: 0.8789\r\n            lambda_l1: 205.6999\r\n            lambda_l2: 580.9768\r\n            max_depth: 8\r\n            num_leaves: 210\r\n            num_threads: 20\r\n            verbosity: -1\r\n    dataset:\r\n        class: DatasetH\r\n        module_path: qlib.data.dataset\r\n        kwargs:\r\n            handler:\r\n                class: Alpha158\r\n                module_path: qlib.contrib.data.handler\r\n                kwargs: *data_handler_config\r\n            segments:\r\n                train: [2008-01-01, 2018-12-31]\r\n                valid: [2019-01-01, 2020-07-31]\r\n                test: [2020-08-01, 2020-03-01]\r\n    record:\r\n        - class: SignalRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs: {}\r\n        - class: SigAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            ana_long_short: False\r\n            ann_scaler: 252\r\n        - class: PortAnaRecord\r\n          module_path: qlib.workflow.record_temp\r\n          kwargs:\r\n            config: *port_analysis_config\r\n\r\nthanks for answering in advance.",
        "Tool":"MLflow",
        "Platform":"Github",
        "Issue_original_content":"Title: double ensemble exception; Content: hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via setup.py and uninstalling \/ reinstalling numpy. but this one i do not know how to solve: exception: got invalid value series([], dtype: float64) for metric 'ic' (timestamp=1616595157552). please specify value as a valid double (64-bit floating point) if i have only sh000300 in my instruments, it's gonna produce the following value error: valueerror: bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]). you can drop duplicate edges by setting the 'duplicates' kwarg i followed instructions on data collector's markdown page to download cn data up until 03\/01\/2021. my yaml file looks like this: qlib_init: provider_uri: \"\/content\/gdrive\/mydrive\/qlib\/qlib_data\/qlib_cn_1d\" region: cn market: &market benchmark: &benchmark sh000300 data_handler_config: &data_handler_config start_time: 2008-01-01 end_time: 2021-03-01 fit_start_time: 2008-01-01 fit_end_time: 2018-12-31 instruments: ['sh000300', 'sh000903'] port_analysis_config: &port_analysis_config strategy: class: topkdropoutstrategy module_path: qlib.contrib.strategy.strategy kwargs: topk: 50 n_drop: 5 backtest: verbose: true limit_threshold: 0.095 account: 50000 benchmark: *benchmark deal_price: close open_cost: 0.0005 close_cost: 0.0015 min_cost: 5 task: model: class: densemblemodel module_path: qlib.contrib.model.double_ensemble kwargs: base_model: \"gbm\" loss: mse num_models: 6 enable_sr: true enable_fs: true alpha1: 1 alpha2: 1 bins_sr: 10 bins_fs: 5 decay: 0.5 sample_ratios: - 0.8 - 0.7 - 0.6 - 0.5 - 0.4 sub_weights: - 1 - 0.2 - 0.2 - 0.2 - 0.2 - 0.2 epochs: 28 colsample_bytree: 0.8879 learning_rate: 0.2 subsample: 0.8789 lambda_l1: 205.6999 lambda_l2: 580.9768 max_depth: 8 num_leaves: 210 num_threads: 20 verbosity: -1 dataset: class: dataseth module_path: qlib.data.dataset kwargs: handler: class: alpha158 module_path: qlib.contrib.data.handler kwargs: *data_handler_config segments: train: [2008-01-01, 2018-12-31] valid: [2019-01-01, 2020-07-31] test: [2020-08-01, 2020-03-01] record: - class: signalrecord module_path: qlib.workflow.record_temp kwargs: {} - class: siganarecord module_path: qlib.workflow.record_temp kwargs: ana_long_short: false ann_scaler: 252 - class: portanarecord module_path: qlib.workflow.record_temp kwargs: config: *port_analysis_config thanks for answering in advance.",
        "Issue_original_content_gpt_summary":"The user encountered multiple challenges while running a CN data on Google Colab, including an invalid value series error and a value error related to bin edges, which were solved by cloning the repo and installing via setup.py, uninstalling\/reinstalling numpy, and following instructions on the data collector's markdown page.",
        "Issue_preprocessed_content":"Title: double ensemble exception; Content: hi, so i ran a cn data on google colab. alstm worked fine but double ensemble keep giving issues, i manage to solve some by cloning the repo and install via and uninstalling \/ reinstalling numpy. but this one i do not know how to solve exception got invalid value series for metric 'ic' . please specify value as a valid double if i have only sh in my instruments, it's gonna produce the following value error valueerror bin edges must be unique array . you can drop duplicate edges by setting the 'duplicates' kwarg i followed instructions on data collector's markdown page to download cn data up until my yaml file looks like this region cn market &market benchmark &benchmark sh instruments strategy class topkdropoutstrategy kwargs topk backtest verbose true account benchmark benchmark close task model class densemblemodel kwargs gbm loss mse true true alpha alpha decay epochs subsample verbosity dataset class dataseth kwargs handler class alpha kwargs segments train valid test record class signalrecord kwargs class siganarecord kwargs false class portanarecord kwargs config thanks for answering in advance."
    },
    {
        "Issue_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/2013",
        "Issue_title":"Bug Report: NeptuneConfig import failing - Flask",
        "Issue_creation_time":1666114324000,
        "Issue_closed_time":1666184788000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"## Expected Behavior\r\nThe metadata service (using Neptune) to start successfully.\r\n\r\n## Current Behavior\r\nFlask application startup fails due to an import error - `ImportError: module 'metadata_service.config' has no attribute 'NeptuneConfig'`\r\n\r\n## Possible Solution\r\nMake the NeptuneConfig discoverable by the service.\r\n\r\n## Steps to Reproduce\r\n1. Deploy a container based on the amundsen-metadata image (latest)\r\n2. Follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-neptune.md) to set up the service to use Neptune i.e. configure env vars\r\n3. Start container and the app is unable to start\r\n\r\n## Screenshots (if appropriate)\r\n![Screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png)\r\n\r\n## Context\r\nI cannot start an ECS task based on this image and therefore can't connect to the Neptune cluster.\r\n\r\n## Your Environment\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: bug report: config import failing - flask; Content: ## expected behavior the metadata service (using ) to start successfully. ## current behavior flask application startup fails due to an import error - `importerror: module 'metadata_service.config' has no attribute 'config'` ## possible solution make the config discoverable by the service. ## steps to reproduce 1. deploy a container based on the amundsen-metadata image (latest) 2. follow this [guide](https:\/\/github.com\/amundsen-io\/amundsen\/blob\/08839140b774acb50018813511db17cb0056500c\/docs\/tutorials\/how-to-use-amundsen-with-aws-.md) to set up the service to use i.e. configure env vars 3. start container and the app is unable to start ## screenshots (if appropriate) ![screenshot 2022-10-18 at 18 31 04](https:\/\/user-images.githubusercontent.com\/36985452\/196503029-9ff2c833-e54f-4be0-a79e-80cfae510fed.png) ## context i cannot start an ecs task based on this image and therefore can't connect to the cluster. ## your environment",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the Flask application startup failed due to an import error, preventing them from connecting to the cluster.",
        "Issue_preprocessed_content":"Title: bug report config import failing flask; Content: expected behavior the metadata service to start successfully. current behavior flask application startup fails due to an import error possible solution make the config discoverable by the service. steps to reproduce . deploy a container based on the image . follow this to set up the service to use configure env vars . start container and the app is unable to start screenshots context i cannot start an ecs task based on this image and therefore can't connect to the cluster. your environment"
    },
    {
        "Issue_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1946",
        "Issue_title":"Neptune MalformedQueryException",
        "Issue_creation_time":1659310549000,
        "Issue_closed_time":1664069854000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\nI started to use amundsen metadata with Neptune database. Initially I used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. So I tested it locally, using a VPN to connect to neptune db, and I found 2 problems. I'll do a PR linked to the issue that solves the problems\r\n## Expected Behavior\r\n<!--- Tell us what should happen -->\r\nWhen calling a route of the metadata api for the neptune service, the server should respond without problem\r\n## Current Behavior\r\n<!--- Tell us what happens instead of the expected behavior -->\r\n1. When calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. This error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382\r\n2. After the correction of 1, another error during the same request\r\n```json\r\n{\r\n    \"detailedMessage\": \"Failed to interpret Gremlin query: Query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\",\r\n    \"code\": \"MalformedQueryException\",\r\n    \"requestId\": \"25542307-96bb-40d2-9585-5a340b8d868c\"\r\n}\r\n```\r\n## Possible Solution\r\n<!--- Not obligatory, but suggest a fix\/reason for the bug -->\r\n1. Initialize `TornadoTransport` class properly, removing `read_timeout` and `write_timeout` in  `gremlin_proxy.py` file\r\n2. Move `Order.decr`to `Order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. The Order.decr and Order.incr are deprecated and don't work with neptune\r\n## Steps to Reproduce\r\n<!--- Provide a link to a live example, or an unambiguous set of steps to -->\r\n<!--- reproduce this bug. Include code to reproduce, if relevant -->\r\n1. Call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with AWS Neptune db\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\n<!--- How has this issue affected you? -->\r\n<!--- Providing context helps us come up with a solution that is most useful in the real world -->\r\n\r\n## Your Environment\r\n<!--- Include as many relevant details about the environment you experienced the bug in -->\r\n* Amunsen version used: last (metadata-3.10.0)\r\n* Data warehouse stores: snowflake\r\n* Deployment (k8s or native):\r\n* Link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: malformedqueryexception; Content: i started to use amundsen metadata with database. initially i used the metadata docker image to interact with the database, but every tested route gave me a 500 internal server error. so i tested it locally, using a vpn to connect to db, and i found 2 problems. i'll do a pr linked to the issue that solves the problems ## expected behavior when calling a route of the metadata api for the service, the server should respond without problem ## current behavior 1. when calling the api to retrieve (for example) a table description, there's an error `got an unexpected keyword argument 'read_timeout'`. this error has already be identified in https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1382 2. after the correction of 1, another error during the same request ```json { \"detailedmessage\": \"failed to interpret gremlin query: query parsing failed at line 1, character position at 208, error message : token recognition error at: 'dec'\", \"code\": \"malformedqueryexception\", \"requestid\": \"25542307-96bb-40d2-9585-5a340b8d868c\" } ``` ## possible solution 1. initialize `tornadotransport` class properly, removing `read_timeout` and `write_timeout` in `gremlin_proxy.py` file 2. move `order.decr`to `order.desc` for `_get_table_columns` and `_get_popular_tables_uris` functions in `gremlin_proxy.py` file. the order.decr and order.incr are deprecated and don't work with ## steps to reproduce 1. call the `\/table\/{table_uri}` metadata route using the gremlin metadata service with aws db ## screenshots (if appropriate) ## context ## your environment * amunsen version used: last (metadata-3.10.0) * data warehouse stores: snowflake * deployment (k8s or native): * link to your fork or repository: https:\/\/github.com\/ggirodda\/amundsen\/tree\/main",
        "Issue_original_content_gpt_summary":"The user encountered two issues when using Amundsen Metadata with a database, resulting in 500 Internal Server Errors, which were solved by initializing the tornadotransport class properly and moving order.decr to order.desc in the gremlin_proxy.py file.",
        "Issue_preprocessed_content":"Title: malformedqueryexception; Content: look through existing open and closed issues to see if someone has reported the issue before i started to use amundsen metadata with database. initially i used the metadata docker image to interact with the database, but every tested route gave me a internal server error. so i tested it locally, using a vpn to connect to db, and i found problems. i'll do a pr linked to the issue that solves the problems expected behavior tell us what should happen when calling a route of the metadata api for the service, the server should respond without problem current behavior tell us what happens instead of the expected behavior . when calling the api to retrieve a table description, there's an error . this error has already be identified in . after the correction of , another error during the same request possible solution not obligatory, but suggest a for the bug . initialize class properly, removing and in file . move to for and functions in file. the and are deprecated and don't work with steps to reproduce provide a link to a live example, or an unambiguous set of steps to reproduce this bug. include code to reproduce, if relevant . call the metadata route using the gremlin metadata service with aws db screenshots context how has this issue affected you? providing context helps us come up with a solution that is most useful in the real world your environment include as many relevant details about the environment you experienced the bug in amunsen version used last data warehouse stores snowflake deployment link to your fork or repository"
    },
    {
        "Issue_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1748",
        "Issue_title":"Bug Report elasticsearch exception for sample_neptune_loader",
        "Issue_creation_time":1646313579000,
        "Issue_closed_time":1649071896000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"<!--- Provide a general summary of the issue in the Title above -->\r\n<!--- Look through existing open and closed issues to see if someone has reported the issue before -->\r\n\r\n## Expected Behavior\r\n<WARNING:elasticsearch:PUT https:\/\/my aws ES endpoint\/table_a54a9a96-c246-4bcd-b417-2d8c005c3290 [status:400 request:0.069s]\r\nINFO:databuilder.callback.call_back:No callbacks to notify\r\nTraceback (most recent call last):\r\n  File \"example\/scripts\/sample_data_loader_neptune.py\", line 403, in <module>\r\n    job_es_table.launch()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 76, in launch\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 72, in launch\r\n    self.publisher.publish()\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n    raise e\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n    self.publish_impl()\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/elasticsearch_publisher.py\", line 93, in publish_impl\r\n    self.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/utils.py\", line 347, in _wrapped\r\n    return func(*args, params=params, headers=headers, **kwargs)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/indices.py\", line 146, in create\r\n    \"PUT\", _make_path(index), params=params, headers=headers, body=body\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 466, in perform_request\r\n    raise e\r\n  File \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 434, in perform_request\r\n    timeout=timeout,\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/http_requests.py\", line 216, in perform_request\r\n    self._raise_error(response.status_code, raw_data)\r\n  File \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/base.py\", line 329, in _raise_error\r\n    status_code, error_message, additional_info\r\n\r\n\r\nelasticsearch.exceptions.RequestError: RequestError(400, 'mapper_parsing_exception', 'Root mapping definition has unsupported parameters:  [schema : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [cluster : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [description : {analyzer=simple, type=text}] [display_name : {type=keyword}] [column_descriptions : {analyzer=simple, type=text}] [programmatic_descriptions : {analyzer=simple, type=text}] [tags : {type=keyword}] [badges : {type=keyword}] [database : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [total_usage : {type=long}] [name : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [last_updated_timestamp : {format=epoch_second, type=date}] [unique_usage : {type=long}] [column_names : {analyzer=simple, type=text, fields={raw={normalizer=column_names_normalizer, type=keyword}}}] [key : {type=keyword}]')->\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n* Amunsen version used: Databuilder: 6.7.1 Common 0.26.0 Amundsen-Gremlin 0.0.13 AWS ES : 6.8\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: bug report elasticsearch exception for sample__loader; Content: ## expected behavior job_es_table.launch() file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 76, in launch raise e file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/job\/job.py\", line 72, in launch self.publisher.publish() file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 40, in publish raise e file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/base_publisher.py\", line 37, in publish self.publish_impl() file \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/databuilder\/publisher\/elasticsearch_publisher.py\", line 93, in publish_impl self.elasticsearch_client.indices.create(index=self.elasticsearch_new_index, body=self.elasticsearch_mapping) file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/utils.py\", line 347, in _wrapped return func(*args, params=params, headers=headers, **kwargs) file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/client\/indices.py\", line 146, in create \"put\", _make_path(index), params=params, headers=headers, body=body file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 466, in perform_request raise e file \"\/tmp\/damundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/transport.py\", line 434, in perform_request timeout=timeout, file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/http_requests.py\", line 216, in perform_request self._raise_error(response.status_code, raw_data) file \"\/tmp\/amundsen\/venv\/lib\/python3.7\/site-packages\/elasticsearch\/connection\/base.py\", line 329, in _raise_error status_code, error_message, additional_info elasticsearch.exceptions.requesterror: requesterror(400, 'mapper_parsing_exception', 'root mapping definition has unsupported parameters: [schema : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [cluster : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [description : {analyzer=simple, type=text}] [display_name : {type=keyword}] [column_descriptions : {analyzer=simple, type=text}] [programmatic_descriptions : {analyzer=simple, type=text}] [tags : {type=keyword}] [badges : {type=keyword}] [database : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [total_usage : {type=long}] [name : {analyzer=simple, type=text, fields={raw={type=keyword}}}] [last_updated_timestamp : {format=epoch_second, type=date}] [unique_usage : {type=long}] [column_names : {analyzer=simple, type=text, fields={raw={normalizer=column_names_normalizer, type=keyword}}}] [key : {type=keyword}]')-> * amunsen version used: databuilder: 6.7.1 common 0.26.0 amundsen-gremlin 0.0.13 aws es : 6.8",
        "Issue_original_content_gpt_summary":"The user encountered an Elasticsearch exception while running job_es_table.launch() with an unsupported parameter list.",
        "Issue_preprocessed_content":"Title: bug report elasticsearch exception for ; Content: look through existing open and closed issues to see if someone has reported the issue before expected behavior warning elasticsearch put aws es callbacks to notify traceback file line , in file line , in launch raise e file line , in launch file line , in publish raise e file line , in publish file line , in file line , in return func file line , in create put , params params, headers headers, body body file line , in raise e file line , in timeout timeout, file line , in file line , in requesterror amunsen version used databuilder common aws es"
    },
    {
        "Issue_link":"https:\/\/github.com\/amundsen-io\/amundsen\/issues\/1430",
        "Issue_title":"Databuilder `NeptuneBulkLoaderApi` constructs wrong IAM role ARN for AWS other than global",
        "Issue_creation_time":1628591009000,
        "Issue_closed_time":1671067447000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"For uploading data to AWS Neptune we use `NeptuneCSVPublisher`, which internally uses `NeptuneBulkLoaderApi`. The current configuration uses config key `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME`, which provides name of IAM role for the loader to be able to use S3 and Neptune. The issue is that `NeptuneBulkLoaderApi` constructs IAM role ARN from name as follows: \r\n\r\n```python\r\naccount_id = self.session.client('sts').get_caller_identity()['Account']\r\nself.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nwhereas, [second element of ARN aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently:\r\n* `aws` -AWS Regions\r\n* `aws-cn` - China Regions\r\n* `aws-us-gov` - AWS GovCloud (US) Regions\r\n\r\nSince we use Amundsen also in AWS China, the above ARN is not valid. \r\n\r\n## Expected Behavior\r\n\r\nIAM role ARN either takes into account AWS partition or there is a possibility of passing IAM role ARN instead of name directly.\r\n\r\n## Current Behavior\r\n\r\nIAM role ARN is constructed incorrectly outside of AWS Global.\r\n\r\n## Possible Solutions\r\n\r\nIAM role ARN should take partition into account. There are two solutions:\r\n1. Add partition into current code\r\n2. Add option of passing IAM role ARN directly which supersedes IAM role name \r\n\r\n### Solution 1\r\n\r\nSince I didn't know or found any good way to get the AWS partition, we can use caller identity and ARN there to get the partition, e.g.:\r\n\r\n```python\r\nidentity = self.session.client('sts').get_caller_identity()\r\naccount_id = identity['Account']\r\npartition = identity['Arn'].split(':')[1]\r\nself.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nThis is smaller fix but it is a bit hacky and I'm not sure it'll work in all situation, but it should I guess.\r\n\r\n### Solution 2\r\n\r\nAdd config key `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN` which either supersedes `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` in a way that in constructor we would have something like:\r\n\r\n```python\r\nif iam_role_arn:\r\n    self.iam_role_arn = iam_role_arn\r\nelse:\r\n   ...\r\n   self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}'\r\n```\r\n\r\nOr even replace `NeptuneCSVPublisher.AWS_IAM_ROLE_NAME` with `NeptuneCSVPublisher.AWS_IAM_ROLE_ARN`, which is IMO cleaner, but would be not backward compatible. \r\n\r\n## Steps to Reproduce\r\nDeploy Amundsen in AWS China with Neptune and try to use Databuilder to upload CSV data from S3. \r\n\r\n## Screenshots (if appropriate)\r\n\r\n## Context\r\nCurrently we are unable to load data into Neptune as the IAM role ARN setting is hidden and we get an error:\r\n\r\n```\r\n[ERROR] Exception: Failed to load csv. Response: {'detailedMessage': \"Failed to start new load from the source s3:\/\/amundsenBucket\/amundsen\/2021_08_10_01_01_28. Couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/RoleForNeptune111111-2222\", 'code': 'InvalidParameterException', 'requestId': 'xxx'}\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler\r\n\u00a0\u00a0\u00a0\u00a0redshift_to_neptune_job.launch()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch\r\n\u00a0\u00a0\u00a0\u00a0self.publisher.publish()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish\r\n\u00a0\u00a0\u00a0\u00a0raise e\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish\r\n\u00a0\u00a0\u00a0\u00a0self.publish_impl()\r\n\u00a0\u00a0File \"\/var\/task\/databuilder\/publisher\/neptune_csv_publisher.py\", line 109, in publish_impl\r\n\u00a0\u00a0\u00a0\u00a0raise Exception(\"Failed to load csv. Response: {0}\".format(str(bulk_upload_response)))\r\n```\r\n\r\n## Your Environment\r\n* Amunsen version used: `amundsen-databuilder==4.3.1`\r\n* Data warehouse stores: AWS Neptune\r\n* Deployment (k8s or native): AWS Step Functions (k8s for backend but unrelated for now)\r\n* Link to your fork or repository:",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: databuilder `bulkloaderapi` constructs wrong iam role arn for aws other than global; Content: for uploading data to aws we use `csvpublisher`, which internally uses `bulkloaderapi`. the current configuration uses config key `csvpublisher.aws_iam_role_name`, which provides name of iam role for the loader to be able to use s3 and . the issue is that `bulkloaderapi` constructs iam role arn from name as follows: ```python account_id = self.session.client('sts').get_caller_identity()['account'] self.iam_role_arn = f'arn:aws:iam::{account_id}:role\/{iam_role_name}' ``` whereas, [second element of arn aka partition](https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/aws-arns-and-namespaces.html) can be currently: * `aws` -aws regions * `aws-cn` - china regions * `aws-us-gov` - aws govcloud (us) regions since we use amundsen also in aws china, the above arn is not valid. ## expected behavior iam role arn either takes into account aws partition or there is a possibility of passing iam role arn instead of name directly. ## current behavior iam role arn is constructed incorrectly outside of aws global. ## possible solutions iam role arn should take partition into account. there are two solutions: 1. add partition into current code 2. add option of passing iam role arn directly which supersedes iam role name ### solution 1 since i didn't know or found any good way to get the aws partition, we can use caller identity and arn there to get the partition, e.g.: ```python identity = self.session.client('sts').get_caller_identity() account_id = identity['account'] partition = identity['arn'].split(':')[1] self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}' ``` this is smaller fix but it is a bit hacky and i'm not sure it'll work in all situation, but it should i guess. ### solution 2 add config key `csvpublisher.aws_iam_role_arn` which either supersedes `csvpublisher.aws_iam_role_name` in a way that in constructor we would have something like: ```python if iam_role_arn: self.iam_role_arn = iam_role_arn else: ... self.iam_role_arn = f'arn:{partition}:iam::{account_id}:role\/{iam_role_name}' ``` or even replace `csvpublisher.aws_iam_role_name` with `csvpublisher.aws_iam_role_arn`, which is imo cleaner, but would be not backward compatible. ## steps to reproduce deploy amundsen in aws china with and try to use databuilder to upload csv data from s3. ## screenshots (if appropriate) ## context currently we are unable to load data into as the iam role arn setting is hidden and we get an error: ``` [error] exception: failed to load csv. response: {'detailedmessage': \"failed to start new load from the source s3:\/\/amundsenbucket\/amundsen\/2021_08_10_01_01_28. couldn't find the aws credential for iam_role_arn: arn:aws:iam::111111111:role\/rolefor111111-2222\", 'code': 'invalidparameterexception', 'requestid': 'xxx'} traceback (most recent call last): file \"\/var\/task\/ctw\/jobs\/synchronize_redshift_metadata.py\", line 49, in lambda_handler redshift_to__job.launch() file \"\/var\/task\/databuilder\/job\/job.py\", line 76, in launch raise e file \"\/var\/task\/databuilder\/job\/job.py\", line 72, in launch self.publisher.publish() file \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 40, in publish raise e file \"\/var\/task\/databuilder\/publisher\/base_publisher.py\", line 37, in publish self.publish_impl() file \"\/var\/task\/databuilder\/publisher\/_csv_publisher.py\", line 109, in publish_impl raise exception(\"failed to load csv. response: {0}\".format(str(bulk_upload_response))) ``` ## your environment * amunsen version used: `amundsen-databuilder==4.3.1` * data warehouse stores: aws * deployment (k8s or native): aws step functions (k8s for backend but unrelated for now) * link to your fork or repository:",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the databuilder `bulkloaderapi` constructs an incorrect IAM role ARN for AWS other than global, preventing them from loading data into their data warehouse.",
        "Issue_preprocessed_content":"Title: databuilder constructs wrong iam role arn for aws other than global; Content: for uploading data to aws we use , which internally uses . the current configuration uses config key , which provides name of iam role for the loader to be able to use s and . the issue is that constructs iam role arn from name as follows whereas, can be currently aws regions china regions aws govcloud regions since we use amundsen also in aws china, the above arn is not valid. expected behavior iam role arn either takes into account aws partition or there is a possibility of passing iam role arn instead of name directly. current behavior iam role arn is constructed incorrectly outside of aws global. possible solutions iam role arn should take partition into account. there are two solutions . add partition into current code . add option of passing iam role arn directly which supersedes iam role name solution since i didn't know or found any good way to get the aws partition, we can use caller identity and arn there to get the partition, this is smaller fix but it is a bit hacky and i'm not sure it'll work in all situation, but it should i guess. solution add config key which either supersedes in a way that in constructor we would have something like or even replace with , which is imo cleaner, but would be not backward compatible. steps to reproduce deploy amundsen in aws china with and try to use databuilder to upload csv data from s . screenshots context currently we are unable to load data into as the iam role arn setting is hidden and we get an error your environment amunsen version used data warehouse stores aws deployment aws step functions link to your fork or repository"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/222",
        "Issue_title":"Configuration options not being set correctly when using CN region Neptune endpoint as host",
        "Issue_creation_time":1635879966000,
        "Issue_closed_time":1635986654000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nThere are several areas in the code where we have an explicit check for the `neptune.amazonaws.com` DNS suffix; this is used to determine if we need to use Neptune-specific configuration options and request URI elements. \r\n\r\nHowever, these checks misidentify endpoints of Neptune clusters in AWS CN regions, which use the `neptune.<region>.amazonaws.com.cn` DNS suffix instead, as non-AWS endpoints. As a result, required config options such as `auth_mode` and `region` are not set correctly.\r\n\r\nAll of the following checks need to be changed to \"amazonaws.com\":\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#L160\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/neptune\/client.py#L129\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#L54\r\nhttps:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#L14",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: configuration options not being set correctly when using cn region endpoint as host; **describe the bug** there are several areas in the code where we have an explicit check for the `.amazonaws.com` dns suffix; Content: this is used to determine if we need to use -specific configuration options and request uri elements. however, these checks misidentify endpoints of clusters in aws cn regions, which use the `..amazonaws.com.cn` dns suffix instead, as non-aws endpoints. as a result, required config options such as `auth_mode` and `region` are not set correctly. all of the following checks need to be changed to \"amazonaws.com\": https:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/magics\/graph_magic.py#l160 https:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/\/client.py#l129 https:\/\/github.com\/aws\/graph-notebook\/blob\/a5818452d152ba51b7f7e26b6cf8e188dca54693\/src\/graph_notebook\/configuration\/generate_config.py#l54 https:\/\/github.com\/aws\/graph-notebook\/blob\/68e888def530be70e08b5250c8146292fb49cfa1\/src\/graph_notebook\/configuration\/get_config.py#l14",
        "Issue_original_content_gpt_summary":"The user encountered a bug where configuration options were not being set correctly when using cn region endpoint as host, due to explicit checks for the `.amazonaws.com` dns suffix misidentifying endpoints of clusters in aws cn regions.",
        "Issue_preprocessed_content":"Title: configuration options not being set correctly when using cn region endpoint as host; Content: describe the bug there are several areas in the code where we have an explicit check for the dns suffix; this is used to determine if we need to use configuration options and request uri elements. however, these checks misidentify endpoints of clusters in aws cn regions, which use the dns suffix instead, as endpoints. as a result, required config options such as and are not set correctly. all of the following checks need to be changed to"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/167",
        "Issue_title":"[BUG] Neptune ML Export widget throwing error",
        "Issue_creation_time":1627938048000,
        "Issue_closed_time":1628716798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\nWhen using the Neptune ML widget to export data like the command below from the 01- Node Classification notebook:\r\n```\r\n%%neptune_ml export start --export-url {neptune_ml.get_export_service_host()} --export-iam --wait --store-to export_results\r\n${export_params}\r\n```\r\nThe following error is thrown\r\n```\r\n{\r\n  \"message\": \"Credential should be scoped to correct service: 'execute-api'. \"\r\n}  \r\n```\r\n\r\n**Expected behavior**\r\nThe export should run to completion\r\n\r\n\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] ml export widget throwing error; Content: **describe the bug** when using the ml widget to export data like the command below from the 01- node classification notebook: ``` %%_ml export start --export-url {_ml.get_export_service_host()} --export-iam --wait --store-to export_results ${export_params} ``` the following error is thrown ``` { \"message\": \"credential should be scoped to correct service: 'execute-api'. \" } ``` **expected behavior** the export should run to completion",
        "Issue_original_content_gpt_summary":"The user encountered an error when using the ML widget to export data from the 01- node classification notebook, where the expected behavior was for the export to run to completion.",
        "Issue_preprocessed_content":"Title: ml export widget throwing error; Content: describe the bug when using the ml widget to export data like the command below from the node classification notebook the following error is thrown expected behavior the export should run to completion"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/144",
        "Issue_title":"Limit issue .with(\"Neptune#ml.limit\",3)",
        "Issue_creation_time":1626912970000,
        "Issue_closed_time":1632957644000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Hi\r\n\r\nAs per the below code It is allowing only default limit as 1 and the limit 3 is not working and throwing error for Introduction to Node Classification Gremlin\r\n\r\n%%gremlin\r\ng.with(\"Neptune#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"Neptune#ml.limit\", 3 ).V().has('title', 'Toy Story (1995)').properties(\"genre\").with(\"Neptune#ml.classification\").value()\r\n\r\nError\r\n{\r\n  \"requestId\": \"fbab9b0a-176c-47f8-accc-969fc4580792\",\r\n  \"detailedMessage\": \"Incompatible data from external service. Please check your service configuration and query again.\",\r\n  \"code\": \"ConstraintViolationException\"\r\n}\r\n\r\nCan some one suggest is there something wrong with the code which was mentioned in the document\r\n\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: limit issue .with(\"#ml.limit\",3); Content: hi as per the below code it is allowing only default limit as 1 and the limit 3 is not working and throwing error for introduction to node classification gremlin %%gremlin g.with(\"#ml.endpoint\",\"node-cla-2021-07-15-15-13-940000-endpoint\").with( \"#ml.limit\", 3 ).v().has('title', 'toy story (1995)').properties(\"genre\").with(\"#ml.classification\").value() error { \"requestid\": \"fbab9b0a-176c-47f8-accc-969fc4580792\", \"detailedmessage\": \"incompatible data from external service. please check your service configuration and query again.\", \"code\": \"constraintviolationexception\" } can some one suggest is there something wrong with the code which was mentioned in the document",
        "Issue_original_content_gpt_summary":"The user encountered a limit issue with the code provided in the document, which only allowed a default limit of 1 and threw an error when attempting to use a limit of 3.",
        "Issue_preprocessed_content":"Title: limit issue ; Content: hi as per the below code it is allowing only default limit as and the limit is not working and throwing error for introduction to node classification gremlin %%gremlin 'toy story error requestid detailedmessage incompatible data from external service. please check your service configuration and query code constraintviolationexception can some one suggest is there something wrong with the code which was mentioned in the document"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/116",
        "Issue_title":"[BUG] Neptune ML notebooks have incorrect Genre stated in the text",
        "Issue_creation_time":1619195852000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"For the  01 notebooks for Neptune ML the text in the notebook incorrectly specifies that the genre returned for a node classification task on `Toy Story` is `Comedy` when it should be `Drama`",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] ml notebooks have incorrect genre stated in the text; Content: for the 01 notebooks for ml the text in the notebook incorrectly specifies that the genre returned for a node classification task on `toy story` is `comedy` when it should be `drama`",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the genre stated in the ML notebooks for a node classification task on 'Toy Story' was incorrectly specified as 'Comedy' instead of 'Drama'.",
        "Issue_preprocessed_content":"Title: ml notebooks have incorrect genre stated in the text; Content: for the notebooks for ml the text in the notebook incorrectly specifies that the genre returned for a node classification task on is when it should be"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/81",
        "Issue_title":"[BUG] Neptune_ML widget error in 2.0.9",
        "Issue_creation_time":1615509404000,
        "Issue_closed_time":1620330541000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":22.0,
        "Issue_body":"**Describe the bug**\r\nStarting in version 2.0.9 the neptune_ml widget is having an issue where the json values being passed in are getting the following error \r\n```\r\n{'error': JSONDecodeError('Expecting value: line 1 column 1 (char 0)',)}\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run through the 01-Introduction-to-Node-Classification-Gremlin notebook\r\n2. When you get to the export step the error occurs\r\n\r\n**Additional context**\r\nThis is not a problem in version 2.0.7",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] _ml widget error in 2.0.9; Content: **describe the bug** starting in version 2.0.9 the _ml widget is having an issue where the json values being passed in are getting the following error ``` {'error': jsondecodeerror('expecting value: line 1 column 1 (char 0)',)} ``` **to reproduce** steps to reproduce the behavior: 1. run through the 01-introduction-to-node-classification-gremlin notebook 2. when you get to the export step the error occurs **additional context** this is not a problem in version 2.0.7",
        "Issue_original_content_gpt_summary":"The user encountered a bug in version 2.0.9 of the _ml widget where json values were resulting in an error, which could be reproduced by running through the 01-introduction-to-node-classification-gremlin notebook and exporting at the end.",
        "Issue_preprocessed_content":"Title: widget error in ; Content: describe the bug starting in version the widget is having an issue where the json values being passed in are getting the following error to reproduce steps to reproduce the behavior . run through the notebook . when you get to the export step the error occurs additional context this is not a problem in version"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/42",
        "Issue_title":"[BUG] Missing documentation on connecting to Neptune from MacOS",
        "Issue_creation_time":1607104372000,
        "Issue_closed_time":1608658157000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nThere are some missing details for how to connect to Neptune from a MacOS device, we should add them to our doc on connecting to neptune via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/neptune)\r\n\r\nOne main piece that we are missing is that a host alias needs to be made in order to get things working properly.\r\n\r\n**Additional context**\r\nThis is coming from a bug report from connectivity not working as found in #40 ",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] missing documentation on connecting to from macos; Content: **describe the bug** there are some missing details for how to connect to from a macos device, we should add them to our doc on connecting to via ssh-tunnel found [here](https:\/\/github.com\/aws\/graph-notebook\/tree\/main\/additional-databases\/) one main piece that we are missing is that a host alias needs to be made in order to get things working properly. **additional context** this is coming from a bug report from connectivity not working as found in #40",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with missing documentation on connecting to a database from a MacOS device, requiring the addition of a host alias to get things working properly.",
        "Issue_preprocessed_content":"Title: missing documentation on connecting to from macos; Content: describe the bug there are some missing details for how to connect to from a macos device, we should add them to our doc on connecting to via found one main piece that we are missing is that a host alias needs to be made in order to get things working properly. additional context this is coming from a bug report from connectivity not working as found in"
    },
    {
        "Issue_link":"https:\/\/github.com\/aws\/graph-notebook\/issues\/40",
        "Issue_title":"[BUG] No documentation on how to connect local notebook to remote Neptune SSL",
        "Issue_creation_time":1606948478000,
        "Issue_closed_time":1607104425000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"**SSL Connection to remote Neptune not working**\r\nI am unable to figure out how can I specify the correct certificate SFSRootCAG2.pem when running queries against SSL-enabled Neptune.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. I set up SSH tunnel via bastion to the Neptune cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -N -L 8182:yourneptuneendpoint:8182_'\r\n2. I start graph-notebook as '_jupyter notebook notebook\/destination_neptune_'. This gives me the output _Jupyter Notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_\r\n3. I open my notebook and run the following magic commands \r\n_'%%graph_notebook_config\r\n{\r\n  \"host\": \"localhost\",\r\n  \"port\": 8182,\r\n  \"auth_mode\": \"DEFAULT\",\r\n  \"iam_credentials_provider_type\": \"ROLE\",\r\n  \"load_from_s3_arn\": \"\",\r\n  \"aws_region\": <myregion>,\r\n  **\"ssl\": true**\r\n}'_\r\n4. I run the command \r\n_%%sparql        \r\nSELECT * WHERE {?s ?p ?o} LIMIT 1_\r\n\r\n5. It gives me the error\r\n**{'error': SSLError(MaxRetryError('HTTPSConnectionPool(host=\\'localhost\\', port=8182): Max retries exceeded with url: \/sparql (Caused by SSLError(SSLCertVerificationError(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............**\r\n\r\n**Expected behavior**\r\nI expect to be able to connect to a remote neptune that has ssl enabled.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Desktop (please complete the following information):**\r\n - macOS 10.15.7 Catalina\r\n - Browser Chrome\r\n - Version 86.0.4240.198 (Official Build) (x86_64)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.None",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] no documentation on how to connect local notebook to remote ssl; Content: **ssl connection to remote not working** i am unable to figure out how can i specify the correct certificate sfsrootcag2.pem when running queries against ssl-enabled . **to reproduce** steps to reproduce the behavior: 1. i set up ssh tunnel via bastion to the cluster '_ssh -i keypairfilename.pem ec2-user@yourec2instanceendpoint -n -l 8182:yourendpoint:8182_' 2. i start graph-notebook as '_jupyter notebook notebook\/destination__'. this gives me the output _jupyter notebook 6.1.5 is running at: http:\/\/localhost:8888\/?token=13b2761a59217f9246aed1dab73e70c3ae42973c4339f328_ 3. i open my notebook and run the following magic commands _'%%graph_notebook_config { \"host\": \"localhost\", \"port\": 8182, \"auth_mode\": \"default\", \"iam_credentials_provider_type\": \"role\", \"load_from_s3_arn\": \"\", \"aws_region\": , **\"ssl\": true** }'_ 4. i run the command _%%sparql select * where {?s ?p ?o} limit 1_ 5. it gives me the error **{'error': sslerror(maxretryerror('httpsconnectionpool(host=\\'localhost\\', port=8182): max retries exceeded with url: \/sparql (caused by sslerror(sslcertverificationerror(\"hostname \\'localhost\\' doesn\\'t match either of \\'*.............** **expected behavior** i expect to be able to connect to a remote that has ssl enabled. **screenshots** none **desktop (please complete the following information):** - macos 10.15.7 catalina - browser chrome - version 86.0.4240.198 (official build) (x86_64) **additional context** add any other context about the problem here.none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to connect to a remote SSL-enabled cluster, as they were unable to figure out how to specify the correct certificate sfsrootcag2.pem when running queries.",
        "Issue_preprocessed_content":"Title: no documentation on how to connect local notebook to remote ssl; Content: ssl connection to remote not working i am unable to figure out how can i specify the correct certificate when running queries against . to reproduce steps to reproduce the behavior . i set up ssh tunnel via bastion to the cluster i n l . i start as notebook this gives me the output notebook is running at . i open my notebook and run the following magic commands host localhost , port , default , role , , , ssl true . i run the command select where limit . it gives me the error 'error' port max retries exceeded with url macos catalina browser chrome version additional context add any other context about the problem"
    },
    {
        "Issue_link":"https:\/\/github.com\/neptune-ai\/examples\/issues\/42",
        "Issue_title":"Neptune_catalyst.ipynb fails",
        "Issue_creation_time":1624893279000,
        "Issue_closed_time":1625030635000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Seems that the Neptune_catalyst.ipynb is failing. \r\nPerhaps there is some type as it seems to be missing the `run` object. \r\nhttps:\/\/github.com\/neptune-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: _catalyst.ipynb fails; Content: seems that the _catalyst.ipynb is failing. perhaps there is some type as it seems to be missing the `run` object. https:\/\/github.com\/-ai\/examples\/runs\/2932574924?check_suite_focus=true",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the _catalyst.ipynb failing, potentially due to a missing `run` object.",
        "Issue_preprocessed_content":"Title: fails; Content: seems that the is failing. perhaps there is some type as it seems to be missing the object."
    },
    {
        "Issue_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/57",
        "Issue_title":"[BUG] neptune minimal launcher link points to full launcher",
        "Issue_creation_time":1625774761000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\nNeptune guide's quicklaunch link points to the full launcher\r\n\r\n**Expected behavior**\r\nShould point to the minimal launcher\r\n\r\n",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] minimal launcher link points to full launcher; Content: **describe the bug** guide's quicklaunch link points to the full launcher **expected behavior** should point to the minimal launcher",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the guide's quicklaunch link points to the full launcher instead of the minimal launcher, which is the expected behavior.",
        "Issue_preprocessed_content":"Title: minimal launcher link points to full launcher; Content: describe the bug guide's quicklaunch link points to the full launcher expected behavior should point to the minimal launcher"
    },
    {
        "Issue_link":"https:\/\/github.com\/graphistry\/graph-app-kit\/issues\/45",
        "Issue_title":"[BUG] AWS neptune templates for p3.2, p3.16 fail to start",
        "Issue_creation_time":1615094175000,
        "Issue_closed_time":1615109545000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"**Describe the bug**\r\n\r\nCloud formation for neptune fails on a p3.2 and p3.16 yet succeeds on a g4dn\r\n\r\nReported by a Neptune user\r\n\r\n**To Reproduce**\r\n\r\nRun through Neptune tutorial and use a p3.16\r\n\r\n**Expected behavior**\r\nIt launches\r\n\r\n**Actual behavior**\r\nFormation template stalls out and auto-deletes\r\n\r\nWorking on getting logs. After 10min, GPU services (forge-etl-python + streamgl) failed to start. V100 issue?\r\n\r\n**Screenshots**\r\n\r\n**Browser environment (please complete the following information):**\r\nall\r\n\r\n**PyGraphistry environment**\r\nAll\r\n\r\n**Additional context**\r\nCurrent graph-app-kit",
        "Tool":"Neptune",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] aws templates for p3.2, p3.16 fail to start; Content: **describe the bug** cloud formation for fails on a p3.2 and p3.16 yet succeeds on a g4dn reported by a user **to reproduce** run through tutorial and use a p3.16 **expected behavior** it launches **actual behavior** formation template stalls out and auto-deletes working on getting logs. after 10min, gpu services (forge-etl-python + streamgl) failed to start. v100 issue? **screenshots** **browser environment (please complete the following information):** all **pygraphistry environment** all **additional context** current graph-app-kit",
        "Issue_original_content_gpt_summary":"The user encountered a bug where Cloud Formation for AWS fails on a p3.2 and p3.16 yet succeeds on a g4dn, with GPU services (forge-etl-python + streamgl) failing to start after 10 minutes.",
        "Issue_preprocessed_content":"Title: aws templates for fail to start; Content: describe the bug cloud formation for fails on a and yet succeeds on a g dn reported by a user to reproduce run through tutorial and use a expected behavior it launches actual behavior formation template stalls out and working on getting logs. after min, gpu services failed to start. v issue? screenshots browser environment all pygraphistry environment all additional context current"
    },
    {
        "Issue_link":"https:\/\/github.com\/ray-project\/ray\/issues\/27203",
        "Issue_title":"[AIR] Fix  \/\/doc\/source\/tune\/examples:sigopt_example",
        "Issue_creation_time":1659034993000,
        "Issue_closed_time":1659051271000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### What happened + What you expected to happen\n\nNotebook is broken due to missing permission first, maybe more issues down the road. @Yard1 looked into it earlier and we're creating this issue to keep track of it.\n\n### Versions \/ Dependencies\n\nmaster\n\n### Reproduction script\n\n`bazel test \/\/doc\/source\/tune\/examples:sigopt_example`\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content":"Title: [air] fix \/\/doc\/source\/tune\/examples:_example; Content: ### what happened + what you expected to happen notebook is broken due to missing permission first, maybe more issues down the road. @yard1 looked into it earlier and we're creating this issue to keep track of it. ### versions \/ dependencies master ### reproduction script `bazel test \/\/doc\/source\/tune\/examples:_example` ### issue severity medium: it is a significant difficulty but i can work around it.",
        "Issue_original_content_gpt_summary":"The user encountered a medium severity issue with the notebook being broken due to missing permission, and is attempting to keep track of it by creating an issue.",
        "Issue_preprocessed_content":"Title: fix ; Content: what happened + what you expected to happen notebook is broken due to missing permission first, maybe more issues down the road. looked into it earlier and we're creating this issue to keep track of it. versions \/ dependencies master reproduction script issue severity medium it is a significant difficulty but i can work around it."
    },
    {
        "Issue_link":"https:\/\/github.com\/ray-project\/ray\/issues\/24864",
        "Issue_title":"[tune] SigOptSearch suggester is not serialisable",
        "Issue_creation_time":1652740461000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### What happened + What you expected to happen\n\nI tried to run a ray tune job using the Sigopt suggester on a remote cluster. The sigopt suggester object was later found to be unserialisable however the stack trace gave no indication of this.\r\n\r\nThe stack trace looks like this\r\n\r\ndiscussion around this issue can be found here https:\/\/ray-distributed.slack.com\/archives\/CNECXMW22\/p1652417782100299\r\n\r\nThanks to Matthew Deng for finding the issue on this one!\n\n### Versions \/ Dependencies\n\nPython 3.8.12\r\nray==1.12.0\n\n### Reproduction script\n\n```\r\nimport ray\r\nimport numpy as np\r\nimport os\r\nos.environ['SIGOPT_KEY'] = APIKEYHERE\r\n\r\nfrom ray.tune.suggest.sigopt import SigOptSearch\r\nfrom ray import tune\r\nWORKING_DIR = os.getcwd()\r\n\r\n\r\n\r\ndef main():\r\n\r\n\tray.init(\r\n\t\taddress = \"ray:\/\/127.0.0.1:10001\",\r\n\t\t# address = \"auto\",\r\n\t\truntime_env = {\r\n\t\t\t\"working_dir\": WORKING_DIR,\r\n\t\t\t\"pip\": [\"sigopt==5.7.0\"]\r\n\t\t}\r\n\t)\r\n\t\r\n\tn_observations = 20\r\n\r\n\thyperparameter_space = [\r\n          {\r\n              'name': 'learning_rate',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'max': np.log(0.01),\r\n                  'min': np.log(0.0001)\r\n              },\r\n          },\r\n          {\r\n              'name': 'momentum',\r\n              'type': 'double',\r\n              'bounds': {\r\n                  'min': 0.85,\r\n                  'max': 0.99\r\n              },\r\n          },\r\n      ]\r\n\t\r\n\tsigopt_search = SigOptSearch(\r\n\t\t# OmegaConf.to_container(config.search_space),\r\n        hyperparameter_space,\r\n\t\tname=\"Tune distributed\",\r\n\t\tmax_concurrent=2, \r\n\t\tobservation_budget=n_observations,\r\n\t\tproject=\"sigopt-ray-integration\",\r\n\t\tmetric=[\"val_loss\"],\r\n\t\tmode=[\"min\"]\r\n\t\t# metric=[\"val_loss\", \"training_loss\"],\r\n\t\t# mode=[\"max\", \"min\"]\r\n\t)\r\n\r\n\ttune_config = {\r\n\t\t# \"config\": config\r\n\t}\r\n\tanalysis = tune.run(\r\n\t\ttrain_model,\r\n\t\tmetric=\"val_loss\",\r\n\t\tmode=\"min\",\r\n\t\tconfig=tune_config,\r\n\t\tnum_samples=n_observations,\r\n\t\tname=\"Tune distributed\",\r\n\t\tresources_per_trial={'gpu': 1},\r\n\t\tsearch_alg=sigopt_search,\r\n\t\t# scheduler=FIFOScheduler(),\r\n\t)\r\n\r\n\r\n\r\n\r\ndef train_model(config):\r\n    pass\r\n\r\nmain()\r\n\r\n```\n\n### Issue Severity\n\nMedium: It is a significant difficulty but I can work around it.",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content":"Title: [tune] search suggester is not serialisable; Content: ### what happened + what you expected to happen i tried to run a ray tune job using the suggester on a remote cluster. the suggester object was later found to be unserialisable however the stack trace gave no indication of this. the stack trace looks like this discussion around this issue can be found here https:\/\/ray-distributed.slack.com\/archives\/cnecxmw22\/p1652417782100299 thanks to matthew deng for finding the issue on this one! ### versions \/ dependencies python 3.8.12 ray==1.12.0 ### reproduction script ``` import ray import numpy as np import os os.environ['_key'] = apikeyhere from ray.tune.suggest. import search from ray import tune working_dir = os.getcwd() def main(): ray.init( address = \"ray:\/\/127.0.0.1:10001\", # address = \"auto\", runtime_env = { \"working_dir\": working_dir, \"pip\": [\"==5.7.0\"] } ) n_observations = 20 hyperparameter_space = [ { 'name': 'learning_rate', 'type': 'double', 'bounds': { 'max': np.log(0.01), 'min': np.log(0.0001) }, }, { 'name': 'momentum', 'type': 'double', 'bounds': { 'min': 0.85, 'max': 0.99 }, }, ] _search = search( # omegaconf.to_container(config.search_space), hyperparameter_space, name=\"tune distributed\", max_concurrent=2, observation_budget=n_observations, project=\"-ray-integration\", metric=[\"val_loss\"], mode=[\"min\"] # metric=[\"val_loss\", \"training_loss\"], # mode=[\"max\", \"min\"] ) tune_config = { # \"config\": config } analysis = tune.run( train_model, metric=\"val_loss\", mode=\"min\", config=tune_config, num_samples=n_observations, name=\"tune distributed\", resources_per_trial={'gpu': 1}, search_alg=_search, # scheduler=fifoscheduler(), ) def train_model(config): pass main() ``` ### issue severity medium: it is a significant difficulty but i can work around it.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when running a Ray Tune job using the search suggester on a remote cluster, as the suggester object was found to be unserialisable, resulting in a medium severity issue.",
        "Issue_preprocessed_content":"Title: search suggester is not serialisable; Content: what happened + what you expected to happen i tried to run a ray tune job using the suggester on a remote cluster. the suggester object was later found to be unserialisable however the stack trace gave no indication of this. the stack trace looks like this discussion around this issue can be found here thanks to matthew deng for finding the issue on this one! versions \/ dependencies python reproduction script issue severity medium it is a significant difficulty but i can work around it."
    },
    {
        "Issue_link":"https:\/\/github.com\/ray-project\/ray\/issues\/11581",
        "Issue_title":"[Tune] Sigopt (multi-metric) api fails with 1.1.0 (tries to hash list)",
        "Issue_creation_time":1603479422000,
        "Issue_closed_time":1603498330000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"<!--Please include [tune], [rllib], [autoscaler] etc. in the issue title if relevant-->\r\nIf you run \r\n\r\npy_test(\r\n name = \"sigopt_prior_beliefs_example\",\r\n size = \"medium\",\r\n srcs = [\"examples\/sigopt_prior_beliefs_example.py\"],\r\n deps = [\":tune_lib\"],\r\n tags = [\"exclusive\", \"example\"],\r\n args = [\"--smoke-test\"]\r\n)\r\n\r\nin python\/ray\/tune\/build (this part of the testing is commented out since you need a sigopt API key...)\r\nYou get an output that looks like this:\r\n\r\n\"\"\"\r\n...\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial\r\n    self._validate_result_metrics(result)\r\n  File \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics\r\n    elif search_metric and search_metric not in result:\r\nTypeError: unhashable type: 'list'\r\n...\r\n\"\"\"\r\nray 1.1.0.dev\r\n\r\n### Reproduction (REQUIRED)\r\nin python\/ray\/tune\/build  run the sigopt sections that are commented out.\r\n",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content":"Title: [tune] (multi-metric) api fails with 1.1.0 (tries to hash list); Content: if you run py_test( name = \"_prior_beliefs_example\", size = \"medium\", srcs = [\"examples\/_prior_beliefs_example.py\"], deps = [\":tune_lib\"], tags = [\"exclusive\", \"example\"], args = [\"--smoke-test\"] ) in python\/ray\/tune\/build (this part of the testing is commented out since you need a api key...) you get an output that looks like this: \"\"\" ... file \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 737, in _process_trial self._validate_result_metrics(result) file \"\/usr\/local\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 818, in _validate_result_metrics elif search_metric and search_metric not in result: typeerror: unhashable type: 'list' ... \"\"\" ray 1.1.0.dev ### reproduction (required) in python\/ray\/tune\/build run the sections that are commented out.",
        "Issue_original_content_gpt_summary":"The user encountered a TypeError when running a multi-metric API in ray 1.1.0.dev, which attempted to hash a list and failed.",
        "Issue_preprocessed_content":"Title: api fails with ; Content: if you run name size medium , srcs , deps , tags , args in you get an output that looks like this file line , in file line , in elif and not in result typeerror unhashable type 'list' ray reproduction in run the sections that are commented out."
    },
    {
        "Issue_link":"https:\/\/github.com\/huggingface\/transformers\/issues\/18145",
        "Issue_title":"the Sigopt api is outdated in transformers trainer.py, the old api could not work",
        "Issue_creation_time":1657875526000,
        "Issue_closed_time":1658150380000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### System Info\r\n\r\n- `transformers` version: 4.21.0.dev0\r\n- Platform: Linux-5.8.0-43-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.7.0\r\n- PyTorch version (GPU?): 1.11.0+cu113 (True)\r\n- Tensorflow version (GPU?): 2.9.1 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): 0.5.0 (cpu)\r\n- Jax version: 0.3.6\r\n- JaxLib version: 0.3.5\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n\r\n### Who can help?\r\n\r\n@sgugger \r\n\r\n### Information\r\n\r\n- [ ] The official example scripts\r\n- [ ] My own modified scripts\r\n\r\n### Tasks\r\n\r\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\r\n- [ ] My own task or dataset (give details below)\r\n\r\n### Reproduction\r\n\r\n1.enable sigopt HPO in example and run.\r\n2. work log like\"UserWarning: You're currently using the old SigOpt Experience. Try out the new and improved SigOpt experience by getting started with the docs today. You have until July 2022 to migrate over without experiencing breaking changes.\"\r\n\r\n### Expected behavior\r\n\r\nHPO with sigopt backend could work correctly without warning",
        "Tool":"SigOpt",
        "Platform":"Github",
        "Issue_original_content":"Title: the api is outdated in transformers trainer.py, the old api could not work; Content: ### system info - `transformers` version: 4.21.0.dev0 - platform: linux-5.8.0-43-generic-x86_64-with-glibc2.29 - python version: 3.8.10 - huggingface_hub version: 0.7.0 - pytorch version (gpu?): 1.11.0+cu113 (true) - tensorflow version (gpu?): 2.9.1 (false) - flax version (cpu?\/gpu?\/tpu?): 0.5.0 (cpu) - jax version: 0.3.6 - jaxlib version: 0.3.5 - using gpu in script?: - using distributed or parallel set-up in script?: ### who can help? @sgugger ### information - [ ] the official example scripts - [ ] my own modified scripts ### tasks - [ ] an officially supported task in the `examples` folder (such as glue\/squad, ...) - [ ] my own task or dataset (give details below) ### reproduction 1.enable hpo in example and run. 2. work log like\"userwarning: you're currently using the old experience. try out the new and improved experience by getting started with the docs today. you have until july 2022 to migrate over without experiencing breaking changes.\" ### expected behavior hpo with backend could work correctly without warning",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the outdated API in the transformers trainer.py, which caused a warning when trying to enable HPO in the example and run.",
        "Issue_preprocessed_content":"Title: the api is outdated in transformers the old api could not work; Content: system info version platform python version version pytorch version tensorflow version flax version jax version jaxlib version using gpu in script? using distributed or parallel in script? who can help? information the official example scripts my own modified scripts tasks an officially supported task in the folder my own task or dataset reproduction .enable hpo in example and run. . work log like userwarning you're currently using the old experience. try out the new and improved experience by getting started with the docs today. you have until july to migrate over without experiencing breaking expected behavior hpo with backend could work correctly without warning"
    },
    {
        "Issue_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526",
        "Issue_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI (Prediction without explanation works fine)",
        "Issue_creation_time":1658320562000,
        "Issue_closed_time":1659550833000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":8.0,
        "Issue_body":"Hi,\r\n\r\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get **explainable** predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\r\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\r\n\r\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\r\n\r\n\r\n#### Environment details\r\n\r\n  - Google Cloud Notebook\r\n  - Python version: 3.7.12\r\n  - pip version: 21.3.1\r\n  - `google-cloud-aiplatform` version: 1.15.0\r\n\r\n#### Reference\r\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\r\n\r\n#### explanation-metadata.json\r\n(_Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key._)\r\n```\r\n{\r\n    \"inputs\": {\r\n        \"A\": {},\r\n        \"B\": {}\r\n    },\r\n    \"outputs\": {\r\n        \"Y\": {}\r\n    }\r\n}\r\n```\r\n#### Model upload with explanation parameters and metadata\r\n```\r\n! gcloud ai models upload \\\r\n  --region=$REGION \\\r\n  --display-name=$MODEL_NAME \\\r\n  --container-image-uri=$PRED_IMAGE_URI \\\r\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\r\n  --explanation-method=sampled-shapley \\\r\n  --explanation-path-count=10 \\\r\n  --explanation-metadata-file=explanation-metadata.json\r\n```\r\n\r\n#### Prediction\/Explanation Input\r\n```\r\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\r\n\r\n# Another example\r\ninstances_2 = [[1.1,20], [2.2,21]]\r\n# Prediction (works fine):\r\nendpoint.predict(instances=instances_2)\r\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\r\nendpoint.explain(instances=instances_2) # Returns error\r\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\r\n```\r\n#### Prediction Server (Flask)\r\n```python\r\n# Custom Flask server to serve online predictions\r\n# Input for prediction\r\nraw_input = request.get_json()\r\ninput = raw_input['instances']\r\ndf = pd.DataFrame(input, columns = ['A', 'B'])\r\n# Prediction from model (loaded from GCP bucket)\r\npredictions = model.predict(df).tolist() # [0, 1]\r\nresponse = jsonify({\"predictions\": predictions})\r\nreturn response\r\n```\r\n\r\n#### Stack trace of error (1)\r\n```\r\n---------------------------------------------------------------------------\r\n_InactiveRpcError                         Traceback (most recent call last)\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     49         try:\r\n---> 50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\r\n    945                                       wait_for_ready, compression)\r\n--> 946         return _end_unary_response_blocking(state, call, False, None)\r\n    947 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\r\n    848     else:\r\n--> 849         raise _InactiveRpcError(state)\r\n    850 \r\n\r\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n\tstatus = StatusCode.INVALID_ARGUMENT\r\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\r\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\r\n>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\r\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\r\n   1563             parameters=parameters,\r\n   1564             deployed_model_id=deployed_model_id,\r\n-> 1565             timeout=timeout,\r\n   1566         )\r\n   1567 \r\n\r\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\r\n    917             retry=retry,\r\n    918             timeout=timeout,\r\n--> 919             metadata=metadata,\r\n    920         )\r\n    921 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\r\n    152             kwargs[\"metadata\"] = metadata\r\n    153 \r\n--> 154         return wrapped_func(*args, **kwargs)\r\n    155 \r\n    156 \r\n\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\r\n     50             return callable_(*args, **kwargs)\r\n     51         except grpc.RpcError as exc:\r\n---> 52             raise exceptions.from_grpc_error(exc) from exc\r\n     53 \r\n     54     return error_remapped_callable\r\n\r\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\r\n---------------------------------------------------------------------------\r\n```",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: error while trying to get explanation from (custom container) model deployed on (prediction without explanation works fine); Content: hi, i created a custom docker container to deploy my model on . the model uses lightgbm, so i can't use the pre-built container images available for tf\/skl\/xgboost. i was able to deploy the model and get predictions, but i get errors while trying to get **explainable** predictions from the model. i have tried to follow the guidelines to configure the model for explanations. the example below shows a simplified version of the model that still reproduces the issue, with only two input features 'a' and 'b'. please take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach. #### environment details - google cloud notebook - python version: 3.7.12 - pip version: 21.3.1 - `google-cloud-aiplatform` version: 1.15.0 #### reference https:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container #### explanation-metadata.json (_model output is unkeyed. the guide suggests using any memorable string for output key._) ``` { \"inputs\": { \"a\": {}, \"b\": {} }, \"outputs\": { \"y\": {} } } ``` #### model upload with explanation parameters and metadata ``` ! gcloud ai models upload \\ --region=$region \\ --display-name=$model_name \\ --container-image-uri=$pred_image_uri \\ --artifact-uri=$artifact_location_gcs \\ --explanation-method=sampled-shapley \\ --explanation-path-count=10 \\ --explanation-metadata-file=explanation-metadata.json ``` #### prediction\/explanation input ``` instances = [{\"a\": 1.1, \"b\": 20}, {\"a\": 2.2, \"b\": 21}] # prediction (works fine): endpoint.predict(instances=instances) # prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=none endpoint.explain(instances=instances) # returns error (1) shown in stack trace below # another example instances_2 = [[1.1,20], [2.2,21]] # prediction (works fine): endpoint.predict(instances=instances_2) # prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=none endpoint.explain(instances=instances_2) # returns error # error: nameless inputs are allowed only if there is a single input in the explanation metadata. ``` #### prediction server (flask) ```python # custom flask server to serve online predictions # input for prediction raw_input = request.get_json() input = raw_input['instances'] df = pd.dataframe(input, columns = ['a', 'b']) # prediction from model (loaded from gcp bucket) predictions = model.predict(df).tolist() # [0, 1] response = jsonify({\"predictions\": predictions}) return response ``` #### stack trace of error (1) ``` --------------------------------------------------------------------------- _inactiverpcerror traceback (most recent call last) \/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs) 49 try: ---> 50 return callable_(*args, **kwargs) 51 except grpc.rpcerror as exc: \/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression) 945 wait_for_ready, compression) --> 946 return _end_unary_response_blocking(state, call, false, none) 947 \/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline) 848 else: --> 849 raise _inactiverpcerror(state) 850 _inactiverpcerror: <_inactiverpcerror of rpc that terminated with: status = statuscode.invalid_argument details = \"{\"error\": \"unable to explain the requested instance(s) because: invalid response from prediction server - the response field predictions is missing. response: {'error': '400 bad request: the browser (or proxy) sent a request that this server could not understand.'}\"}\" debug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"unable to explain the requested instance(s) because: invalid response from prediction server - the response field predictions is missing. response: {'error': '400 bad request: the browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\" > the above exception was the direct cause of the following exception: invalidargument traceback (most recent call last) \/tmp\/ipykernel_2590\/4024017963.py in ----> 3 print(endpoint.explain(instances=instances, parameters={})) ~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout) 1563 parameters=parameters, 1564 deployed_model_id=deployed_model_id, -> 1565 timeout=timeout, 1566 ) 1567 ~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata) 917 retry=retry, 918 timeout=timeout, --> 919 metadata=metadata, 920 ) 921 \/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs) 152 kwargs[\"metadata\"] = metadata 153 --> 154 return wrapped_func(*args, **kwargs) 155 156 \/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs) 50 return callable_(*args, **kwargs) 51 except grpc.rpcerror as exc: ---> 52 raise exceptions.from_grpc_error(exc) from exc 53 54 return error_remapped_callable invalidargument: 400 {\"error\": \"unable to explain the requested instance(s) because: invalid response from prediction server - the response field predictions is missing. response: {'error': '400 bad request: the browser (or proxy) sent a request that this server could not understand.'}\"} --------------------------------------------------------------------------- ```",
        "Issue_original_content_gpt_summary":"The user is encountering challenges while trying to get explainable predictions from a custom container model deployed on Google Cloud Platform, despite being able to get predictions without explanation.",
        "Issue_preprocessed_content":"Title: error while trying to get explanation from model deployed on ; Content: hi, i created a custom docker container to deploy my model on . the model uses lightgbm, so i can't use the container images available for i was able to deploy the model and get predictions, but i get errors while trying to get explainable predictions from the model. i have tried to follow the guidelines to configure the model for explanations. the example below shows a simplified version of the model that still reproduces the issue, with only two input features 'a' and 'b'. please take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach. environment details google cloud notebook python version pip version version reference output is unkeyed. the guide suggests using any memorable string for output model upload with explanation parameters and metadata input prediction server stack trace of error"
    },
    {
        "Issue_link":"https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1078",
        "Issue_title":"Vertex AI Automl training error when training dataset with foreign project's BQ table as source",
        "Issue_creation_time":1647263282000,
        "Issue_closed_time":1649696673000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Issue in Vertex AI Automl training python API while using vertex dataset with source as foreign project's BQ table\r\n- What would you like to achieve: I would like to run automl training job(using automl api python) with vertex dataset created from foreign project's BigQuery table.\r\n- Even though 'vertex AI service agent' and 'Vertex AI Custom code service agent' added to respective bigquery dataset with 'bigquery data editor' role, we are receiving bigquery.tables.get persmission denied error when we try run automl training job. \r\n\r\nError:\r\n`INFO:google.cloud.aiplatform.training_jobs:No column transformations provided, so now retrieving columns from dataset in order to set default column transformations. \r\nTraceback (most recent call last): \r\nFile \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 190, in <module> \r\nautoml_tabular = create_training_pipeline_tabular(train_type, project_id, display_name, int(dataset_id), location, model_display_name, float(training_fraction_split), float(validation_fraction_split), float(test_fraction_split), int(budget_milli_node_hours), disable_early_stopping, sync, target_column) \r\nFile \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 57, in create_training_pipeline_tabular \r\nmodel = tabular_job.run( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3461, in run \r\nreturn self._run( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/base.py\", line 730, in wrapper \r\nreturn method(*args, **kwargs) \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3645, in _run \r\n) = column_transformations_utils.get_default_column_transformations( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/utils\/column_transformations_utils.py\", line 42, in get_default_column_transformations \r\nfor column_name in dataset.column_names \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 81, in column_names \r\nself._retrieve_bq_source_columns( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 241, in _retrieve_bq_source_columns \r\ntable = client.get_table(bq_table_uri) \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 1034, in get_table \r\napi_response = self._call_api( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 782, in _call_api \r\nreturn call() \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 283, in retry_wrapped_func \r\nreturn retry_target( \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 190, in retry_target \r\nreturn target() \r\nFile \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/_http\/__init__.py\", line 480, in api_request \r\nraise exceptions.from_http_response(response) \r\ngoogle.api_core.exceptions.Forbidden: 403 GET https:\/\/bigquery.googleapis.com\/bigquery\/v2\/projects\/<<projectID>>\/datasets\/<<datasetID>>\/tables\/<<tableID>>?prettyPrint=false: Access Denied: Table <<TableID>>: Permission bigquery.tables.get denied on table <<TableID>> (or it may not exist). \r\n`\r\n\r\nAfter raising google support, we got a temporary workaround by adding column specification in API call (reference case# 29535929, 29310889).\r\n**Sample API call of the workaround:**\r\n`job = training_jobs.AutoMLTabularTrainingJob( \r\ndisplay_name=\"my_display_name\", \r\noptimization_prediction_type=\"classification\", \r\noptimization_objective=\"minimize-log-loss\", \r\ncolumn_specs={\"column_1\": \"auto\", \"column_2\": \"numeric\"}, \r\nlabels={'key': 'value'}, \r\n) \r\n`\r\n\r\nWe would want to have this fixed in in SDK. If possible, we would like to have a gcloud command for automl modules.\r\n",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: automl training error when training dataset with foreign project's bq table as source; Content: ### issue in automl training python api while using vertex dataset with source as foreign project's bq table - what would you like to achieve: i would like to run automl training job(using automl api python) with vertex dataset created from foreign project's bigquery table. - even though ' service agent' and ' custom code service agent' added to respective bigquery dataset with 'bigquery data editor' role, we are receiving bigquery.tables.get persmission denied error when we try run automl training job. error: `info:google.cloud.aiplatform.training_jobs:no column transformations provided, so now retrieving columns from dataset in order to set default column transformations. traceback (most recent call last): file \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 190, in automl_tabular = create_training_pipeline_tabular(train_type, project_id, display_name, int(dataset_id), location, model_display_name, float(training_fraction_split), float(validation_fraction_split), float(test_fraction_split), int(budget_milli_node_hours), disable_early_stopping, sync, target_column) file \"\/home\/vsts\/work\/1\/a\/.terraform\/modules\/gcp_automl\/scripts\/train_automl.py\", line 57, in create_training_pipeline_tabular model = tabular_job.run( file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3461, in run return self._run( file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/base.py\", line 730, in wrapper return method(*args, **kwargs) file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/training_jobs.py\", line 3645, in _run ) = column_transformations_utils.get_default_column_transformations( file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/utils\/column_transformations_utils.py\", line 42, in get_default_column_transformations for column_name in dataset.column_names file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 81, in column_names self._retrieve_bq_source_columns( file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/aiplatform\/datasets\/column_names_dataset.py\", line 241, in _retrieve_bq_source_columns table = client.get_table(bq_table_uri) file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 1034, in get_table api_response = self._call_api( file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/bigquery\/client.py\", line 782, in _call_api return call() file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 283, in retry_wrapped_func return retry_target( file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/api_core\/retry.py\", line 190, in retry_target return target() file \"\/home\/vsts\/.local\/lib\/python3.8\/site-packages\/google\/cloud\/_http\/__init__.py\", line 480, in api_request raise exceptions.from_http_response(response) google.api_core.exceptions.forbidden: 403 get https:\/\/bigquery.googleapis.com\/bigquery\/v2\/projects\/<>\/datasets\/<>\/tables\/<>?prettyprint=false: access denied: table <>: permission bigquery.tables.get denied on table <> (or it may not exist). ` after raising google support, we got a temporary workaround by adding column specification in api call (reference case# 29535929, 29310889). **sample api call of the workaround:** `job = training_jobs.automltabulartrainingjob( display_name=\"my_display_name\", optimization_prediction_type=\"classification\", optimization_objective=\"minimize-log-loss\", column_specs={\"column_1\": \"auto\", \"column_2\": \"numeric\"}, labels={'key': 'value'}, ) ` we would want to have this fixed in in sdk. if possible, we would like to have a gcloud command for automl modules.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to run an AutoML training job using a Vertex dataset created from a foreign project's BigQuery table, resulting in a \"permission denied\" error.",
        "Issue_preprocessed_content":"Title: automl training error when training dataset with foreign project's bq table as source; Content: issue in automl training python api while using vertex dataset with source as foreign project's bq table what would you like to achieve i would like to run automl training job with vertex dataset created from foreign project's bigquery table. even though ' service agent' and ' custom code service agent' added to respective bigquery dataset with 'bigquery data editor' role, we are receiving persmission denied error when we try run automl training job. error after raising google support, we got a temporary workaround by adding column specification in api call . sample api call of the workaround we would want to have this fixed in in sdk. if possible, we would like to have a gcloud command for automl modules."
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/1315",
        "Issue_title":"Unable to import aiplatform module when running Vertex AI Matching Engine sample notebook",
        "Issue_creation_time":1669999872000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## Expected Behavior\r\nI expect the notebook here https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/matching_engine\/sdk_matching_engine_for_indexing.ipynb to work\r\n\r\n\r\n\r\n## Actual Behavior\r\n\r\n, but for some reason whenever I try to import the module `aiplatform` inside a cell of the notebook\r\n```\r\nfrom google.cloud import aiplatform\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/protobuf\/descriptor.py in __new__(cls, name, index, number, type, options, serialized_options, create_key)\r\n    753                 type=None,  # pylint: disable=redefined-builtin\r\n    754                 options=None, serialized_options=None, create_key=None):\r\n--> 755       _message.Message._CheckCalledFromGeneratedFile()\r\n    756       # There is no way we can build a complete EnumValueDescriptor with the\r\n    757       # given parameters (the name of the Enum is not known, for example).\r\n\r\nTypeError: Descriptors cannot not be created directly.\r\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\r\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\r\n 1. Downgrade the protobuf package to 3.20.x or lower.\r\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\r\n\r\nMore information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\r\n```\r\n\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1. Clone the sample notebook\r\n1. Import it into a vertex AI Workbench running the Python3 image\r\n1. Try to run through the steps and get stuck in installation issues\r\n\r\n## Specifications\r\n\r\n- Version:\r\n- Platform:",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: unable to import aiplatform module when running matching engine sample notebook; Content: ## expected behavior i expect the notebook here https:\/\/github.com\/googlecloudplatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/matching_engine\/sdk_matching_engine_for_indexing.ipynb to work ## actual behavior , but for some reason whenever i try to import the module `aiplatform` inside a cell of the notebook ``` from google.cloud import aiplatform ``` i get the following error: ``` \/opt\/conda\/lib\/python3.7\/site-packages\/google\/protobuf\/descriptor.py in __new__(cls, name, index, number, type, options, serialized_options, create_key) 753 type=none, # pylint: disable=redefined-builtin 754 options=none, serialized_options=none, create_key=none): --> 755 _message.message._checkcalledfromgeneratedfile() 756 # there is no way we can build a complete enumvaluedescriptor with the 757 # given parameters (the name of the enum is not known, for example). typeerror: descriptors cannot not be created directly. if this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0. if you cannot immediately regenerate your protos, some other possible workarounds are: 1. downgrade the protobuf package to 3.20.x or lower. 2. set protocol_buffers_python_implementation=python (but this will use pure-python parsing and will be much slower). more information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates ``` ## steps to reproduce the problem 1. clone the sample notebook 1. import it into a workbench running the python3 image 1. try to run through the steps and get stuck in installation issues ## specifications - version: - platform:",
        "Issue_original_content_gpt_summary":"The user encountered an issue when attempting to import the aiplatform module when running the matching engine sample notebook, resulting in an error due to an outdated version of the protobuf package.",
        "Issue_preprocessed_content":"Title: unable to import aiplatform module when running matching engine sample notebook; Content: expected behavior i expect the notebook here to work actual behavior , but for some reason whenever i try to import the module inside a cell of the notebook i get the following error steps to reproduce the problem . clone the sample notebook . import it into a workbench running the python image . try to run through the steps and get stuck in installation issues specifications version platform"
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/749",
        "Issue_title":"Vertex AI - Endpoint Call with JSON - Invalid JSON payload received",
        "Issue_creation_time":1658950752000,
        "Issue_closed_time":1659564318000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"I successfully trained and deployed a Tensorflow Recommender model on Vertex AI, Tensorflow 2.8.\r\n\r\nEverything is online and to predict the output. In the notebook I do:\r\n\r\n    loaded = tf.saved_model.load(path)\r\n    scores, titles = loaded([\"doctor\"])\r\n\r\nThat returns:\r\n\r\n    Recommendations: [b'Nelly & Monsieur Arnaud (1995)'\r\n     b'Three Lives and Only One Death (1996)' b'Critical Care (1997)']\r\n\r\nThat is, the payload (input for the neural network) must be `[\"doctor\"]`\r\n\r\nThen I generate the JSON for payload (the error is here):\r\n\r\n    !echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"input_1\"\\\"\" : {[\"\\\"\"doctor\"\\\"\"]}}]} > instances0.json\r\n\r\nAnd submit to the endpoint:\r\n\r\n    !curl -X POST  \\\r\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\r\n    -H \"Content-Type: application\/json\" \\\r\n    https:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\\r\n    -d @instances0.json > results.json\r\n\r\n... as seen here: https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollTo=35348dd21acd\r\n\r\nHowever, when I use this payload, I get error 400:\r\n\r\n    code: 400\r\n    message: \"Invalid JSON payload received. Expected an object key or }. s\" : [{\"input_1\" : {[\"doctor\"]}}]} ^\"\r\n    status: \"INVALID_ARGUMENT\"\r\n\r\nThis below don't work either:\r\n\r\n    !echo {\"inputs\": {\"input_1\": [\"doctor\"]}} > instances0.json\r\n\r\nEven with validated JSON Lint, it does not return the proper prediction.\r\n\r\nRunning:\r\n\r\n    !saved_model_cli show --dir \/home\/jupyter\/model --all\r\n\r\nI get:\r\n\r\n    MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n    \r\n    signature_def['__saved_model_init_op']:\r\n      The given SavedModel SignatureDef contains the following input(s):\r\n      The given SavedModel SignatureDef contains the following output(s):\r\n        outputs['__saved_model_init_op'] tensor_info:\r\n            dtype: DT_INVALID\r\n            shape: unknown_rank\r\n            name: NoOp\r\n      Method name is: \r\n    \r\n    signature_def['serving_default']:\r\n      The given SavedModel SignatureDef contains the following input(s):\r\n        inputs['input_1'] tensor_info:\r\n            dtype: DT_STRING\r\n            shape: (-1)\r\n            name: serving_default_input_1:0\r\n      The given SavedModel SignatureDef contains the following output(s):\r\n        outputs['output_1'] tensor_info:\r\n            dtype: DT_FLOAT\r\n            shape: (-1, 10)\r\n            name: StatefulPartitionedCall_1:0\r\n        outputs['output_2'] tensor_info:\r\n            dtype: DT_STRING\r\n            shape: (-1, 10)\r\n            name: StatefulPartitionedCall_1:1\r\n      Method name is: tensorflow\/serving\/predict\r\n\r\n\r\n    Concrete Functions:\r\n      Function Name: '__call__'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #2\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #3\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #4\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n    \r\n      Function Name: '_default_save_signature'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n    \r\n      Function Name: 'call_and_return_all_conditional_losses'\r\n        Option #1\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #2\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n        Option #3\r\n          Callable with:\r\n            Argument #1\r\n              queries: TensorSpec(shape=(None,), dtype=tf.string, name='queries')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: False\r\n        Option #4\r\n          Callable with:\r\n            Argument #1\r\n              input_1: TensorSpec(shape=(None,), dtype=tf.string, name='input_1')\r\n            Argument #2\r\n              DType: NoneType\r\n              Value: None\r\n            Argument #3\r\n              DType: bool\r\n              Value: True\r\n\r\nThe point is: I'm passing an array and I'm not sure if it must be in b64 format.\r\n\r\nThis Python code works, but returns a different result than expected:\r\n\r\n    import tensorflow as tf\r\n    import base64\r\n    from google.protobuf import json_format\r\n    from google.protobuf.struct_pb2 import Value\r\n    import numpy as np\r\n    from google.cloud import aiplatform\r\n    import os\r\n    vertex_model = tf.saved_model.load(\"gs:\/\/bucket\/model\")\r\n    \r\n    serving_input = list(\r\n        vertex_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\r\n    )[0]\r\n    \r\n    print(\"Serving input :\", serving_input)\r\n    \r\n    aip_endpoint_name = (\r\n        f\"projects\/my-project\/locations\/us-west1\/endpoints\/12345567\"\r\n    )\r\n    endpoint = aiplatform.Endpoint(aip_endpoint_name)\r\n    \r\n    def encode_input(input):\r\n        return base64.b64encode(np.array(input)).decode(\"utf-8\")\r\n    \r\n    instances_list = [{serving_input: {\"b64\": encode_input(np.array([\"doctor\"]))}}]\r\n    instances = [json_format.ParseDict(s, Value()) for s in instances_list]\r\n    \r\n    results = endpoint.predict(instances=instances)\r\n    print(results.predictions[0][\"output_2\"])\r\n\r\n\r\n    ['8 1\/2 (1963)', 'Sword in the Stone, The (1963)', 'Much Ado About Nothing (1993)', 'Jumanji (1995)', 'As Good As It Gets (1997)', 'Age of Innocence, The (1993)', 'Double vie de V\u00e9ronique, La (Double Life of Veronique, The) (1991)', 'Piano, The (1993)', 'Eat Drink Man Woman (1994)', 'Bullets Over Broadway (1994)']\r\n\r\nAny ideas on how to fix \/ encode the payload ?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: - endpoint call with json - invalid json payload received; Content: i successfully trained and deployed a tensorflow recommender model on , tensorflow 2.8. everything is online and to predict the output. in the notebook i do: loaded = tf.saved_model.load(path) scores, titles = loaded([\"doctor\"]) that returns: recommendations: [b'nelly & monsieur arnaud (1995)' b'three lives and only one death (1996)' b'critical care (1997)'] that is, the payload (input for the neural network) must be `[\"doctor\"]` then i generate the json for payload (the error is here): !echo {\"\\\"\"instances\"\\\"\" : [{\"\\\"\"input_1\"\\\"\" : {[\"\\\"\"doctor\"\\\"\"]}}]} > instances0.json and submit to the endpoint: !curl -x post \\ -h \"authorization: bearer $(gcloud auth print-access-token)\" \\ -h \"content-type: application\/json\" \\ https:\/\/us-west1-aiplatform.googleapis.com\/v1\/projects\/my_project\/locations\/us-west1\/endpoints\/123456789:predict \\ -d @instances0.json > results.json ... as seen here: https:\/\/colab.research.google.com\/github\/googlecloudplatform\/vertex-ai-samples\/blob\/master\/notebooks\/community\/vertex_endpoints\/tf_hub_obj_detection\/deploy_tfhub_object_detection_on_vertex_endpoints.ipynb#scrollto=35348dd21acd however, when i use this payload, i get error 400: code: 400 message: \"invalid json payload received. expected an object key or }. s\" : [{\"input_1\" : {[\"doctor\"]}}]} ^\" status: \"invalid_argument\" this below don't work either: !echo {\"inputs\": {\"input_1\": [\"doctor\"]}} > instances0.json even with validated json lint, it does not return the proper prediction. running: !saved_model_cli show --dir \/home\/jupyter\/model --all i get: metagraphdef with tag-set: 'serve' contains the following signaturedefs: signature_def['__saved_model_init_op']: the given savedmodel signaturedef contains the following input(s): the given savedmodel signaturedef contains the following output(s): outputs['__saved_model_init_op'] tensor_info: dtype: dt_invalid shape: unknown_rank name: noop method name is: signature_def['serving_default']: the given savedmodel signaturedef contains the following input(s): inputs['input_1'] tensor_info: dtype: dt_string shape: (-1) name: serving_default_input_1:0 the given savedmodel signaturedef contains the following output(s): outputs['output_1'] tensor_info: dtype: dt_float shape: (-1, 10) name: statefulpartitionedcall_1:0 outputs['output_2'] tensor_info: dtype: dt_string shape: (-1, 10) name: statefulpartitionedcall_1:1 method name is: tensorflow\/serving\/predict concrete functions: function name: '__call__' option #1 callable with: argument #1 input_1: tensorspec(shape=(none,), dtype=tf.string, name='input_1') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: true option #2 callable with: argument #1 queries: tensorspec(shape=(none,), dtype=tf.string, name='queries') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: true option #3 callable with: argument #1 input_1: tensorspec(shape=(none,), dtype=tf.string, name='input_1') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: false option #4 callable with: argument #1 queries: tensorspec(shape=(none,), dtype=tf.string, name='queries') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: false function name: '_default_save_signature' option #1 callable with: argument #1 input_1: tensorspec(shape=(none,), dtype=tf.string, name='input_1') function name: 'call_and_return_all_conditional_losses' option #1 callable with: argument #1 input_1: tensorspec(shape=(none,), dtype=tf.string, name='input_1') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: false option #2 callable with: argument #1 queries: tensorspec(shape=(none,), dtype=tf.string, name='queries') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: true option #3 callable with: argument #1 queries: tensorspec(shape=(none,), dtype=tf.string, name='queries') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: false option #4 callable with: argument #1 input_1: tensorspec(shape=(none,), dtype=tf.string, name='input_1') argument #2 dtype: nonetype value: none argument #3 dtype: bool value: true the point is: i'm passing an array and i'm not sure if it must be in b64 format. this python code works, but returns a different result than expected: import tensorflow as tf import base64 from google.protobuf import json_format from google.protobuf.struct_pb2 import value import numpy as np from google.cloud import aiplatform import os vertex_model = tf.saved_model.load(\"gs:\/\/bucket\/model\") serving_input = list( vertex_model.signatures[\"serving_default\"].structured_input_signature[1].keys() )[0] print(\"serving input :\", serving_input) aip_endpoint_name = ( f\"projects\/my-project\/locations\/us-west1\/endpoints\/12345567\" ) endpoint = aiplatform.endpoint(aip_endpoint_name) def encode_input(input): return base64.b64encode(np.array(input)).decode(\"utf-8\") instances_list = [{serving_input: {\"b64\": encode_input(np.array([\"doctor\"]))}}] instances = [json_format.parsedict(s, value()) for s in instances_list] results = endpoint.predict(instances=instances) print(results.predictions[0][\"output_2\"]) ['8 1\/2 (1963)', 'sword in the stone, the (1963)', 'much ado about nothing (1993)', 'jumanji (1995)', 'as good as it gets (1997)', 'age of innocence, the (1993)', 'double vie de vronique, la (double life of veronique, the) (1991)', 'piano, the (1993)', 'eat drink man woman (1994)', 'bullets over broadway (1994)'] any ideas on how to fix \/ encode the payload ?",
        "Issue_original_content_gpt_summary":"The user is encountering challenges with an endpoint call with a JSON payload, receiving an invalid JSON payload error despite validating the JSON with lint and attempting to encode the payload in b64 format.",
        "Issue_preprocessed_content":"Title: endpoint call with json invalid json payload received; Content: i successfully trained and deployed a tensorflow recommender model on , tensorflow everything is online and to predict the output. in the notebook i do loaded scores, titles loaded that returns recommendations that is, the payload must be then i generate the json for payload !echo and submit to the endpoint !curl x post \\ h authorization bearer $ \\ h \\ \\ d as seen here however, when i use this payload, i get error code message invalid json payload received. expected an object key or . s ^ status this below don't work either !echo even with validated json lint, it does not return the proper prediction. running show i get metagraphdef with 'serve' contains the following signaturedefs the given savedmodel signaturedef contains the following input the given savedmodel signaturedef contains the following output dtype shape name noop method name is the given savedmodel signaturedef contains the following input dtype shape name the given savedmodel signaturedef contains the following output dtype shape name dtype shape name method name is concrete functions function name option callable with argument tensorspec , argument dtype nonetype value none argument dtype bool value true option callable with argument queries tensorspec , name 'queries' argument dtype nonetype value none argument dtype bool value true option callable with argument tensorspec , argument dtype nonetype value none argument dtype bool value false option callable with argument queries tensorspec , name 'queries' argument dtype nonetype value none argument dtype bool value false function name option callable with argument tensorspec , function name option callable with argument tensorspec , argument dtype nonetype value none argument dtype bool value false option callable with argument queries tensorspec , name 'queries' argument dtype nonetype value none argument dtype bool value true option callable with argument queries tensorspec , name 'queries' argument dtype nonetype value none argument dtype bool value false option callable with argument tensorspec , argument dtype nonetype value none argument dtype bool value true the point is i'm passing an array and i'm not sure if it must be in b format. this python code works, but returns a different result than expected import tensorflow as tf import base from import from import value import numpy as np from import aiplatform import os list print endpoint def return instances results any ideas on how to fix \/ encode the payload ?"
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/issues\/349",
        "Issue_title":"ModelUploadOp from \"Vertex AI Pipelines: model upload using google-cloud-pipeline-components\"  does not work",
        "Issue_creation_time":1646182369000,
        "Issue_closed_time":1646371495000,
        "Issue_upvote_count":2,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"## Expected Behavior\r\nCode example  from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended.\r\n\r\n## Actual Behavior\r\nCode example below from \"Vertex AI Pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work\r\n\r\n```\r\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\r\n    from google.cloud import aiplatform\r\n    aiplatform.init(project=project, location=region)\r\n\r\n    # THIS IS THE METHOD THAT DOESN'T APPEAR TO WORK\r\n    model_upload_op = gcc_aip.ModelUploadOp(\r\n            project=project,\r\n            location=region,\r\n            display_name=model_display_name,\r\n            artifact_uri=model.uri,\r\n            serving_container_image_uri=serving_container_image_uri\r\n            )\r\n```\r\nOn the other hand, the method below worked:\r\n```\r\n # THIS METHOD DOES WORK\r\n    # aiplatform.Model.upload(\r\n    #     display_name=model_display_name,\r\n    #     artifact_uri=model.uri,\r\n    #     serving_container_image_uri=serving_container_image_uri,\r\n    # )\r\n```\r\n\r\nI'm currently using Vertex AI Pipelines to train a model and upload to Vertex AI. Currently in the pipeline, I'm attempting to use the ModelUploadOp class to upload a custom model to Vertex AI models. The logs show the job is succeeding, but the model never actually gets uploaded.\r\n\r\n## Steps to Reproduce the Problem\r\n\r\n1.\r\n1.\r\n1.\r\n\r\n## Specifications\r\n\r\nVersion: \r\n- Pipeline SDK (Kubeflow Pipelines\/TFX) Version: kfp\r\n- Pipelines Version: kfp==1.8.11\r\n- Platform: Google Cloud Vertex AI \r\n\r\n[1]: https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: modeluploadop from \" pipelines: model upload using google-cloud-pipeline-components\" does not work; Content: ## expected behavior code example from \" pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] should work as intended. ## actual behavior code example below from \" pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" [1] had issue and does not work ``` from google_cloud_pipeline_components import aiplatform as gcc_aip from google.cloud import aiplatform aiplatform.init(project=project, location=region) # this is the method that doesn't appear to work model_upload_op = gcc_aip.modeluploadop( project=project, location=region, display_name=model_display_name, artifact_uri=model.uri, serving_container_image_uri=serving_container_image_uri ) ``` on the other hand, the method below worked: ``` # this method does work # aiplatform.model.upload( # display_name=model_display_name, # artifact_uri=model.uri, # serving_container_image_uri=serving_container_image_uri, # ) ``` i'm currently using pipelines to train a model and upload to . currently in the pipeline, i'm attempting to use the modeluploadop class to upload a custom model to models. the logs show the job is succeeding, but the model never actually gets uploaded. ## steps to reproduce the problem 1. 1. 1. ## specifications version: - pipeline sdk (kubeflow pipelines\/tfx) version: kfp - pipelines version: kfp==1.8.11 - platform: google cloud [1]: https:\/\/github.com\/googlecloudplatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/pipelines\/google_cloud_pipeline_components_model_train_upload_deploy.ipynb",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the modeluploadop from the \"pipelines: model train, upload, and deploy using google-cloud-pipeline-components\" code example, where the job was succeeding but the model was not being uploaded.",
        "Issue_preprocessed_content":"Title: modeluploadop from pipelines model upload using does not work; Content: expected behavior code example from pipelines model train, upload, and deploy using should work as intended. actual behavior code example below from pipelines model train, upload, and deploy using had issue and does not work on the other hand, the method below worked i'm currently using pipelines to train a model and upload to . currently in the pipeline, i'm attempting to use the modeluploadop class to upload a custom model to models. the logs show the job is succeeding, but the model never actually gets uploaded. steps to reproduce the problem . . . specifications version pipeline sdk version kfp pipelines version platform google cloud"
    },
    {
        "Issue_link":"https:\/\/github.com\/zenml-io\/zenml\/issues\/1001",
        "Issue_title":"[BUG]: Vertex AI blogpost is outdated after 0.20.0 release",
        "Issue_creation_time":1666626693000,
        "Issue_closed_time":1667472145000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":7.0,
        "Issue_body":"### Contact Details [Optional]\n\nfrancogbocci@gmail.com\n\n### System Information\n\nZenML version: 0.20.5\r\nInstall path: \/Users\/f.bocci\/Library\/Caches\/pypoetry\/virtualenvs\/banana-bMSm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml\r\nPython version: 3.9.6\r\nPlatform information: {'os': 'mac', 'mac_version': '10.15.7'}\r\nEnvironment: native\r\nIntegrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn']\n\n### What happened?\n\nTrying to follow the [guide to run a pipeline using Vertex AI](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because ZenML does not now have a `metadata-store` stack category.\r\n\r\n```shell\r\n$ zenml\r\nStack Components:\r\n      alerter                 Commands to interact with alerters.\r\n      annotator               Commands to interact with annotators.\r\n      artifact-store          Commands to interact with artifact stores.\r\n      container-registry      Commands to interact with container registries.\r\n      data-validator          Commands to interact with data validators.\r\n      experiment-tracker      Commands to interact with experiment trackers.\r\n      feature-store           Commands to interact with feature stores.\r\n      model-deployer          Commands to interact with model deployers.\r\n      orchestrator            Commands to interact with orchestrators.\r\n      secrets-manager         Commands to interact with secrets managers.\r\n      step-operator           Commands to interact with step operators.\r\n$ zenml metadata-store\r\nError: No such command 'metadata-store'.\r\n```\n\n### Reproduction steps\n\n1. zenml metadata-store\r\n\r\nIf I don't add it and run the Vertex AI pipeline, it fails.\r\n\n\n### Relevant log output\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug]: blogpost is outdated after 0.20.0 release; Content: ### contact details [optional] francogbocci@gmail.com ### system information zenml version: 0.20.5 install path: \/users\/f.bocci\/library\/caches\/pypoetry\/virtualenvs\/banana-bmsm4ime-py3.9\/lib\/python3.9\/site-packages\/zenml python version: 3.9.6 platform information: {'os': 'mac', 'mac_version': '10.15.7'} environment: native integrations: ['gcp', 'graphviz', 'kubeflow', 'kubernetes', 'scipy', 'sklearn'] ### what happened? trying to follow the [guide to run a pipeline using ](https:\/\/blog.zenml.io\/vertex-ai-blog\/), it fails because zenml does not now have a `metadata-store` stack category. ```shell $ zenml stack components: alerter commands to interact with alerters. annotator commands to interact with annotators. artifact-store commands to interact with artifact stores. container-registry commands to interact with container registries. data-validator commands to interact with data validators. experiment-tracker commands to interact with experiment trackers. feature-store commands to interact with feature stores. model-deployer commands to interact with model deployers. orchestrator commands to interact with orchestrators. secrets-manager commands to interact with secrets managers. step-operator commands to interact with step operators. $ zenml metadata-store error: no such command 'metadata-store'. ``` ### reproduction steps 1. zenml metadata-store if i don't add it and run the pipeline, it fails. ### relevant log output _no response_ ### code of conduct - [x] i agree to follow this project's code of conduct",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when trying to follow a guide to run a pipeline using ZenML, as the command 'metadata-store' was not recognized due to the 0.20.0 release.",
        "Issue_preprocessed_content":"Title: blogpost is outdated after release; Content: contact details system information zenml version install path python version platform information environment native integrations what happened? trying to follow the , it fails because zenml does not now have a stack category. reproduction steps . zenml if i don't add it and run the pipeline, it fails. relevant log output code of conduct i agree to follow this project's code of conduct"
    },
    {
        "Issue_link":"https:\/\/github.com\/googleapis\/java-aiplatform\/issues\/668",
        "Issue_title":"Bug in Vertex AI docs",
        "Issue_creation_time":1631987712000,
        "Issue_closed_time":1652391878000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Vertex AI Java documentation is incorrect.\r\n\r\nI used this code, [copied straight from the Javadocs](https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/PipelineServiceClient.java), to delete a VertexAI Training Pipeline\r\n\r\n```\r\ntry (PipelineServiceClient pipelineServiceClient = PipelineServiceClient.create()) {\r\n  TrainingPipelineName name =\r\n      TrainingPipelineName.of(\"[PROJECT]\", \"[LOCATION]\", \"[TRAINING_PIPELINE]\");\r\n  pipelineServiceClient.deleteTrainingPipelineAsync(name).get();\r\n}\r\n```\r\n\r\nI get this error.\r\n\r\n```\r\nError in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingPipelines\/186468439399187392: \r\njava.util.concurrent.ExecutionException: \r\ncom.google.api.gax.rpc.UnimplementedException: io.grpc.StatusRuntimeException:\r\nUNIMPLEMENTED: HTTP status code 404\r\n```\r\n\r\nAs discussed [here](https:\/\/stackoverflow.com\/questions\/69219230), you have to specify an endpoint -- and then it works.\r\n\r\nI suggest fixing   the documentation to produce working code.",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: bug in docs; Content: java documentation is incorrect. i used this code, [copied straight from the javadocs](https:\/\/github.com\/googleapis\/java-aiplatform\/blob\/master\/google-cloud-aiplatform\/src\/main\/java\/com\/google\/cloud\/aiplatform\/v1\/pipelineserviceclient.java), to delete a vertexai training pipeline ``` try (pipelineserviceclient pipelineserviceclient = pipelineserviceclient.create()) { trainingpipelinename name = trainingpipelinename.of(\"[project]\", \"[location]\", \"[training_pipeline]\"); pipelineserviceclient.deletetrainingpipelineasync(name).get(); } ``` i get this error. ``` error in deleting \/\/aiplatform.googleapis.com\/projects\/746859988231\/locations\/us-central1\/trainingpipelines\/186468439399187392: java.util.concurrent.executionexception: com.google.api.gax.rpc.unimplementedexception: io.grpc.statusruntimeexception: unimplemented: http status code 404 ``` as discussed [here](https:\/\/stackoverflow.com\/questions\/69219230), you have to specify an endpoint -- and then it works. i suggest fixing the documentation to produce working code.",
        "Issue_original_content_gpt_summary":"The user encountered a bug in the Java documentation which caused an error when attempting to delete a VertexAI training pipeline, and had to specify an endpoint to make it work.",
        "Issue_preprocessed_content":"Title: bug in docs; Content: java documentation is incorrect. i used this code, , to delete a vertexai training pipeline i get this error. as discussed , you have to specify an endpoint and then it works. i suggest fixing the documentation to produce working code."
    },
    {
        "Issue_link":"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/issues\/453",
        "Issue_title":"vertex AI endpoint prediction error, 4 DEADLINE_EXCEEDED: Deadline exceeded",
        "Issue_creation_time":1664933940000,
        "Issue_closed_time":1664935217000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"#### Environment details\r\n\r\n  - OS: Mac M1 Pro\r\n  - Node.js version: v16.16.0\r\n  - npm version: 8.11.0\r\n  - `@google-cloud\/aiplatform` version: ^2.3.0\r\n\r\n#### Steps to reproduce\r\n\r\n  1. I've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js\r\n  2. The process paused and shows `4 DEADLINE_EXCEEDED: Deadline exceeded` in the line: `await predictionServiceClient.predict(request);`\r\n\r\n\r\nThanks!\r\n",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: endpoint prediction error, 4 deadline_exceeded: deadline exceeded; Content: #### environment details - os: mac m1 pro - node.js version: v16.16.0 - npm version: 8.11.0 - `@google-cloud\/aiplatform` version: ^2.3.0 #### steps to reproduce 1. i've run this demo on my local computer: https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-text-classification.js 2. the process paused and shows `4 deadline_exceeded: deadline exceeded` in the line: `await predictionserviceclient.predict(request);` thanks!",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with endpoint prediction error, resulting in a \"4 deadline_exceeded: deadline exceeded\" error, when running a demo on their local computer.",
        "Issue_preprocessed_content":"Title: endpoint prediction error, deadline exceeded; Content: environment details os mac m pro version npm version version steps to reproduce . i've run this demo on my local computer . the process paused and shows in the line thanks!"
    },
    {
        "Issue_link":"https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/issues\/171",
        "Issue_title":"[Bug] scikit learn model feature definition doesn't work on Vertex AI Prediction.",
        "Issue_creation_time":1646626569000,
        "Issue_closed_time":1648259081000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## Description\r\n\r\nScikit Learn model in [`kubeflow_pipelines\/pipelines` directory](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/master\/notebooks\/kubeflow_pipelines\/pipelines\/solutions\/trainer_image\/train.py#L46) doesn't work in Vertex AI prediction environment, since it assumes the input as Pandas Dataframe and cannot handle JSON from Web API.\r\n\r\nAfter deploying the model following the labs, this issue can be reproduced with this code snippet.\r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [{'Elevation': [2841.0]},\r\n {'Aspect': [45, 0]},\r\n {'Slope': [0, 0]},\r\n {'Horizontal_Distance_To_Hydrology': [644.0]},\r\n {'Vertical_Distance_To_Hydrology': [282.0]},\r\n {'Horizontal_Distance_To_Roadways': [1376.0]},\r\n {'Hillshade_9am': [218.0]},\r\n {'Hillshade_Noon': [237.0]},\r\n {'Hillshade_3pm': [156.0]},\r\n {'Horizontal_Distance_To_Fire_Points': [1003.0]},\r\n {'Wilderness_Area': ['Commanche']},\r\n {'Soil_Type': ['C4758']}]\r\n\r\nendpoint.predict([instance])\r\n```\r\n\r\nreturns:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/6895245\/156965179-92e4e873-8f60-411c-86b7-df0685509e4c.png)\r\n\r\n## Approach\r\nRewrite feature definition part of `train.py` from:\r\nhttps:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/blob\/e87f3514dda440fb381a78f563bda177aa38ad80\/notebooks\/kubeflow_pipelines\/cicd\/solutions\/trainer_image_vertex\/train.py#L43-L63\r\n\r\nto:\r\n```python\r\n    numeric_feature_indexes = slice(0, 10)\r\n    categorical_feature_indexes = slice(10, 12)\r\n\r\n    preprocessor = ColumnTransformer(\r\n    transformers=[\r\n        ('num', StandardScaler(), numeric_feature_indexes),\r\n        ('cat', OneHotEncoder(), categorical_feature_indexes) \r\n    ])\r\n```\r\n\r\nAnd it should run with this \r\n\r\n```python\r\nendpoint = aiplatform.Endpoint.list()[0]\r\n\r\ninstance = [\r\n    2841.0,\r\n    45.0,\r\n    0.0,\r\n    644.0,\r\n    282.0,\r\n    1376.0,\r\n    218.0,\r\n    237.0,\r\n    156.0,\r\n    1003.0,\r\n    \"Commanche\",\r\n    \"C4758\",\r\n]\r\nendpoint.predict([instance])\r\n```\r\n\r\nOutput:\r\n```\r\nPrediction(predictions=[1.0], deployed_model_id='4516996077043318784', explanations=None)\r\n```\r\n\r\n## Target Files\r\n[These 8 files ](https:\/\/github.com\/GoogleCloudPlatform\/asl-ml-immersion\/search?q=numeric_features+%3D+%5B+++++++++%22Elevation%22%2C) should be update.",
        "Tool":"Vertex AI",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] scikit learn model feature definition doesn't work on prediction.; Content: ## description scikit learn model in [`kubeflow_pipelines\/pipelines` directory](https:\/\/github.com\/googlecloudplatform\/asl-ml-immersion\/blob\/master\/notebooks\/kubeflow_pipelines\/pipelines\/solutions\/trainer_image\/train.py#l46) doesn't work in prediction environment, since it assumes the input as pandas dataframe and cannot handle json from web api. after deploying the model following the labs, this issue can be reproduced with this code snippet. ```python endpoint = aiplatform.endpoint.list()[0] instance = [{'elevation': [2841.0]}, {'aspect': [45, 0]}, {'slope': [0, 0]}, {'horizontal_distance_to_hydrology': [644.0]}, {'vertical_distance_to_hydrology': [282.0]}, {'horizontal_distance_to_roadways': [1376.0]}, {'hillshade_9am': [218.0]}, {'hillshade_noon': [237.0]}, {'hillshade_3pm': [156.0]}, {'horizontal_distance_to_fire_points': [1003.0]}, {'wilderness_area': ['commanche']}, {'soil_type': ['c4758']}] endpoint.predict([instance]) ``` returns: ![image](https:\/\/user-images.githubusercontent.com\/6895245\/156965179-92e4e873-8f60-411c-86b7-df0685509e4c.png) ## approach rewrite feature definition part of `train.py` from: https:\/\/github.com\/googlecloudplatform\/asl-ml-immersion\/blob\/e87f3514dda440fb381a78f563bda177aa38ad80\/notebooks\/kubeflow_pipelines\/cicd\/solutions\/trainer_image_vertex\/train.py#l43-l63 to: ```python numeric_feature_indexes = slice(0, 10) categorical_feature_indexes = slice(10, 12) preprocessor = columntransformer( transformers=[ ('num', standardscaler(), numeric_feature_indexes), ('cat', onehotencoder(), categorical_feature_indexes) ]) ``` and it should run with this ```python endpoint = aiplatform.endpoint.list()[0] instance = [ 2841.0, 45.0, 0.0, 644.0, 282.0, 1376.0, 218.0, 237.0, 156.0, 1003.0, \"commanche\", \"c4758\", ] endpoint.predict([instance]) ``` output: ``` prediction(predictions=[1.0], deployed_model_id='4516996077043318784', explanations=none) ``` ## target files [these 8 files ](https:\/\/github.com\/googlecloudplatform\/asl-ml-immersion\/search?q=numeric_features+%3d+%5b+++++++++%22elevation%22%2c) should be update.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the scikit learn model feature definition in the kubeflow_pipelines\/pipelines directory did not work on prediction, due to the input being a pandas dataframe and not being able to handle json from web api.",
        "Issue_preprocessed_content":"Title: scikit learn model feature definition doesn't work on prediction.; Content: description scikit learn model in doesn't work in prediction environment, since it assumes the input as pandas dataframe and cannot handle json from web api. after deploying the model following the labs, this issue can be reproduced with this code snippet. returns approach rewrite feature definition part of from to and it should run with this output target files these files should be update."
    },
    {
        "Issue_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/993",
        "Issue_title":"Getting Errors with wandb sweeps ",
        "Issue_creation_time":1613369681000,
        "Issue_closed_time":1623275505000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Synced astral-sweep-1: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/peg6pn8y\r\nRun peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: ERROR Run peg6pn8y errored: RuntimeError(\"Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward\",)\r\nwandb: Agent Starting Run: e8d1m877 with config:\r\nwandb: \tlayer_0-6: 2.581652533230976e-05\r\nwandb: \tlayer_12-18: 3.584294374584665e-05\r\nwandb: \tlayer_18-24: 4.488348372658677e-05\r\nwandb: \tlayer_6-12: 1.0161197251306803e-05\r\nwandb: \tnum_train_epochs: 40\r\nwandb: \tparams_classifier.dense.bias: 0.0005874506018709628\r\nwandb: \tparams_classifier.dense.weight: 0.0003389591868569285\r\nwandb: \tparams_classifier.out_proj.bias: 0.0003078179192499977\r\nwandb: \tparams_classifier.out_proj.weight: 0.0006868779346654171\r\nTracking run with wandb version 0.10.19\r\nSyncing run peach-sweep-2 to Weights & Biases (Documentation).\r\nProject page: https:\/\/wandb.ai\/sakrah\/humorize\r\nSweep page: https:\/\/wandb.ai\/sakrah\/humorize\/sweeps\/4sl6uygs\r\nRun page: https:\/\/wandb.ai\/sakrah\/humorize\/runs\/e8d1m877\r\nRun data is saved locally in \/content\/wandb\/run-20210215_055312-e8d1m877",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: getting errors with sweeps ; Content: synced astral-sweep-1: https:\/\/.ai\/sakrah\/humorize\/runs\/peg6pn8y run peg6pn8y errored: runtimeerror(\"expected object of scalar type long but got scalar type float for argument #2 'target' in call to _thnn_nll_loss_forward\",) : error run peg6pn8y errored: runtimeerror(\"expected object of scalar type long but got scalar type float for argument #2 'target' in call to _thnn_nll_loss_forward\",) : agent starting run: e8d1m877 with config: : layer_0-6: 2.581652533230976e-05 : layer_12-18: 3.584294374584665e-05 : layer_18-24: 4.488348372658677e-05 : layer_6-12: 1.0161197251306803e-05 : num_train_epochs: 40 : params_classifier.dense.bias: 0.0005874506018709628 : params_classifier.dense.weight: 0.0003389591868569285 : params_classifier.out_proj.bias: 0.0003078179192499977 : params_classifier.out_proj.weight: 0.0006868779346654171 tracking run with version 0.10.19 syncing run peach-sweep-2 to (documentation). project page: https:\/\/.ai\/sakrah\/humorize sweep page: https:\/\/.ai\/sakrah\/humorize\/sweeps\/4sl6uygs run page: https:\/\/.ai\/sakrah\/humorize\/runs\/e8d1m877 run data is saved locally in \/content\/\/run-20210215_055312-e8d1m877",
        "Issue_original_content_gpt_summary":"The user encountered challenges with sweeps, including runtime errors and syncing issues, while attempting to run a project on a remote server.",
        "Issue_preprocessed_content":"Title: getting errors with sweeps ; Content: synced run peg pn y errored runtimeerror error run peg pn y errored runtimeerror agent starting run e d m with config tracking run with version syncing run to . project page sweep page run page run data is saved locally in"
    },
    {
        "Issue_link":"https:\/\/github.com\/ThilinaRajapakse\/simpletransformers\/issues\/287",
        "Issue_title":"wandb RuntimeError",
        "Issue_creation_time":1585341922000,
        "Issue_closed_time":1591178971000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":6.0,
        "Issue_body":"When training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and wandb enabled:\r\n```\r\nFile \"train.py\", line 101, in <module>\r\n    rc=sklearn.metrics.recall_score)\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in \r\ntrain_model\r\n    **kwargs,\r\n  File \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train\r\n    scaled_loss.backward()\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 256, in <lambda>\r\n    handle = var.register_hook(lambda grad: _callback(grad, log_track))\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 254, in _callback\r\n    self.log_tensor_stats(grad.data, name)\r\n  File \"venv\/lib\/python3.7\/site-packages\/wandb\/wandb_torch.py\", line 165, in log_tensor_stats\r\n    flat = tensor.view(-1)\r\nRuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\r\n```\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: runtimeerror; Content: when training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and enabled: ``` file \"train.py\", line 101, in rc=sklearn.metrics.recall_score) file \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 267, in train_model **kwargs, file \"venv\/lib\/python3.7\/site-packages\/simpletransformers\/classification\/classification_model.py\", line 374, in train scaled_loss.backward() file \"venv\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) file \"venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward allow_unreachable=true) # allow_unreachable flag file \"venv\/lib\/python3.7\/site-packages\/\/_torch.py\", line 256, in handle = var.register_hook(lambda grad: _callback(grad, log_track)) file \"venv\/lib\/python3.7\/site-packages\/\/_torch.py\", line 254, in _callback self.log_tensor_stats(grad.data, name) file \"venv\/lib\/python3.7\/site-packages\/\/_torch.py\", line 165, in log_tensor_stats flat = tensor.view(-1) runtimeerror: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). use .reshape(...) instead. ```",
        "Issue_original_content_gpt_summary":"The user encountered a runtimeerror when training text classification models using xlnet-large-cased, albert-base-v2, xlnet-base-cased and enabled, due to an incompatibility between the view size and the input tensor's size and stride.",
        "Issue_preprocessed_content":"Title: runtimeerror; Content: when training text classification models using and enabled"
    },
    {
        "Issue_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1592",
        "Issue_title":"[BUG] Weird behavior with \"to_wandb\" and confusion matrix",
        "Issue_creation_time":1654717842000,
        "Issue_closed_time":1657703576000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":4.0,
        "Issue_body":"**Describe the bug**\r\nAfter running a model evaluation suite and exprorint to wandb using \"to_wandb\" function, the confusion matrix appears in the w&b page without the values\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n\r\n**Expected behavior**\r\nThe confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown\r\n![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS: linux\r\n - Python Version:3.7.1\r\n - Deepchecks Version:0.7.2\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] weird behavior with \"to_\" and confusion matrix; Content: **describe the bug** after running a model evaluation suite and exprorint to using \"to_\" function, the confusion matrix appears in the w&b page without the values **to reproduce** steps to reproduce the behavior: **expected behavior** the confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown ![1654716717893](https:\/\/user-images.githubusercontent.com\/21197955\/172704682-e1097eaa-5371-48b6-96d7-f0df1006c043.jpeg) **environment (please complete the following information):** - os: linux - python version:3.7.1 - deepchecks version:0.7.2",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the confusion matrix appears in the Weights & Biases page without the values after running a model evaluation suite and exporting to using the \"to_\" function.",
        "Issue_preprocessed_content":"Title: weird behavior with and confusion matrix; Content: describe the bug after running a model evaluation suite and exprorint to using function, the confusion matrix appears in the w&b page without the values to reproduce steps to reproduce the behavior expected behavior the confusion matrix in w&b should appear like the confusion matrix in the notebook which has it values shown environment os linux python deepchecks"
    },
    {
        "Issue_link":"https:\/\/github.com\/deepchecks\/deepchecks\/issues\/1210",
        "Issue_title":"[BUG] to_wandb not sectioning by train\/test and overrides runs by checks",
        "Issue_creation_time":1649320792000,
        "Issue_closed_time":1649580744000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"**Describe the bug**\r\n to_wandb not sectioning by train\/test and overrides runs by checks\r\n\r\n**To Reproduce**\r\nrun a suite with train\/test checks and duplicate checks in suite\r\n\r\n**Expected behavior**\r\nsections for each dataset and being able to run a suite with a couple of checks\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] to_ not sectioning by train\/test and overrides runs by checks; Content: **describe the bug** to_ not sectioning by train\/test and overrides runs by checks **to reproduce** run a suite with train\/test checks and duplicate checks in suite **expected behavior** sections for each dataset and being able to run a suite with a couple of checks **screenshots** if applicable, add screenshots to help explain your problem.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the 'to_' function was not sectioning by train\/test and overriding runs by checks, resulting in an inability to run a suite with a couple of checks.",
        "Issue_preprocessed_content":"Title: not sectioning by and overrides runs by checks; Content: describe the bug not sectioning by and overrides runs by checks to reproduce run a suite with checks and duplicate checks in suite expected behavior sections for each dataset and being able to run a suite with a couple of checks screenshots if applicable, add screenshots to help explain your problem."
    },
    {
        "Issue_link":"https:\/\/github.com\/shagunsodhani\/ml-logger\/issues\/25",
        "Issue_title":"[BUG] Weights & Biases logging does not differentiate between modes",
        "Issue_creation_time":1583449225000,
        "Issue_closed_time":1583456108000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"Logging using Weights and Biases does not differentiate between training and testing modes in `logbook.write_metric_log({  'mode': 'train' ... })`",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] logging does not differentiate between modes; Content: logging using does not differentiate between training and testing modes in `logbook.write_metric_log({ 'mode': 'train' ... })`",
        "Issue_original_content_gpt_summary":"The user encountered a bug where logging using logbook does not differentiate between training and testing modes.",
        "Issue_preprocessed_content":"Title: logging does not differentiate between modes; Content: logging using does not differentiate between training and testing modes in"
    },
    {
        "Issue_link":"https:\/\/github.com\/vlievin\/fz-openqa\/issues\/216",
        "Issue_title":"Wandb logging: auxiliary values (e.g. `retriever\/entropy`) are not logged",
        "Issue_creation_time":1642504357000,
        "Issue_closed_time":1645097047000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: logging: auxiliary values (e.g. `retriever\/entropy`) are not logged; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where auxiliary values such as `retriever\/entropy` were not being logged.",
        "Issue_preprocessed_content":"Title: logging auxiliary values are not logged; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/MathisFederico\/LearnRL\/issues\/96",
        "Issue_title":"Add episode to wandb",
        "Issue_creation_time":1605623692000,
        "Issue_closed_time":1605698611000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"When using wandb, it shows step as X and not episode.\r\n\r\nHence, longer runs have more steps and it makes the comparaison between runs difficult.\r\n\r\n\r\n![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: add episode to ; Content: when using , it shows step as x and not episode. hence, longer runs have more steps and it makes the comparaison between runs difficult. ![photo_2020-11-17_13-47-41](https:\/\/user-images.githubusercontent.com\/13030198\/99403033-5052e400-28ea-11eb-92c0-a3efd14b654a.jpg)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when using a tool that showed the step as x and not episode, making it difficult to compare longer runs.",
        "Issue_preprocessed_content":"Title: add episode to ; Content: when using , it shows step as x and not episode. hence, longer runs have more steps and it makes the comparaison between runs difficult."
    },
    {
        "Issue_link":"https:\/\/github.com\/allenai\/tango\/issues\/152",
        "Issue_title":"Wandb callback prints errors when a training run resumes not from scratch",
        "Issue_creation_time":1642918837000,
        "Issue_closed_time":1644017877000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":9.0,
        "Issue_body":"It prints\r\n```\r\nwandb: WARNING Step must only increase in log calls.  Step 110 < 161; dropping\r\n```",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: callback prints errors when a training run resumes not from scratch; it prints ``` : warning step must only increase in log calls. step 110 < 161; Content: dropping ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where callback prints errors when a training run resumes not from scratch, resulting in a warning that the step must only increase in log calls.",
        "Issue_preprocessed_content":"Title: callback prints errors when a training run resumes not from scratch; Content: it prints"
    },
    {
        "Issue_link":"https:\/\/github.com\/allenai\/tango\/issues\/151",
        "Issue_title":"WandB callback changes the train step's unique ID, but does not change the results",
        "Issue_creation_time":1642916994000,
        "Issue_closed_time":1652740320000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: callback changes the train step's unique id, but does not change the results; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the callback changed the train step's unique id, but did not change the results.",
        "Issue_preprocessed_content":"Title: callback changes the train step's unique id, but does not change the results; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/kaylode\/theseus\/issues\/33",
        "Issue_title":"Resume error in WandB.",
        "Issue_creation_time":1649286157000,
        "Issue_closed_time":1649731891000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Forgot to create an issue in recent days.\r\nWhen tested with ```resume``` argument in ```WandBCallbacks```, i encountered this error. Here's the log:\r\n```python\r\n\r\n[Errno 2] No such file or directory: 'main'\r\n\/content\/main\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:override:78 - Overriding configuration...\r\n2022-04-04 12:21:56 | INFO     | classification\/pipeline.py:__init__:51 - {\r\n    \"global\": {\r\n        \"exp_name\": null,\r\n        \"exist_ok\": false,\r\n        \"debug\": true,\r\n        \"cfg_transform\": \"configs\/classification\/transform.yaml\",\r\n        \"save_dir\": \"\/content\/main\/runs\",\r\n        \"device\": \"cuda:0\",\r\n        \"use_fp16\": true,\r\n        \"pretrained\": null,\r\n        \"resume\": null\r\n    },\r\n    \"trainer\": {\r\n        \"name\": \"SupervisedTrainer\",\r\n        \"args\": {\r\n            \"num_iterations\": 2000,\r\n            \"clip_grad\": 10.0,\r\n            \"evaluate_interval\": 1,\r\n            \"print_interval\": 20,\r\n            \"save_interval\": 500\r\n        }\r\n    },\r\n    \"model\": {\r\n        \"name\": \"BaseTimmModel\",\r\n        \"args\": {\r\n            \"name\": \"convnext_tiny\",\r\n            \"from_pretrained\": true,\r\n            \"num_classes\": 180\r\n        }\r\n    },\r\n    \"loss\": {\r\n        \"name\": \"FocalLoss\"\r\n    },\r\n    \"callbacks\": [\r\n        {\r\n            \"name\": \"LoggerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"CheckpointCallbacks\",\r\n            \"args\": {\r\n                \"best_key\": \"bl_acc\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"VisualizerCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"TensorboardCallbacks\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"WandbCallbacks\",\r\n            \"args\": {\r\n                \"username\": \"lannguyen\",\r\n                \"project_name\": \"theseus_classification\",\r\n                \"resume\": true\r\n            }\r\n        }\r\n    ],\r\n    \"metrics\": [\r\n        {\r\n            \"name\": \"Accuracy\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"BalancedAccuracyMetric\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"F1ScoreMetric\",\r\n            \"args\": {\r\n                \"average\": \"weighted\"\r\n            }\r\n        },\r\n        {\r\n            \"name\": \"ConfusionMatrix\",\r\n            \"args\": null\r\n        },\r\n        {\r\n            \"name\": \"ErrorCases\",\r\n            \"args\": null\r\n        }\r\n    ],\r\n    \"optimizer\": {\r\n        \"name\": \"AdamW\",\r\n        \"args\": {\r\n            \"lr\": 0.001,\r\n            \"weight_decay\": 0.0005,\r\n            \"betas\": [\r\n                0.937,\r\n                0.999\r\n            ]\r\n        }\r\n    },\r\n    \"scheduler\": {\r\n        \"name\": \"SchedulerWrapper\",\r\n        \"args\": {\r\n            \"scheduler_name\": \"cosine2\",\r\n            \"t_initial\": 7,\r\n            \"t_mul\": 0.9,\r\n            \"eta_mul\": 0.9,\r\n            \"eta_min\": 1e-06\r\n        }\r\n    },\r\n    \"data\": {\r\n        \"dataset\": {\r\n            \"train\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/train\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"ImageFolderDataset\",\r\n                \"args\": {\r\n                    \"image_dir\": \"\/content\/main\/data\/food-classification\/val\",\r\n                    \"txt_classnames\": \"configs\/classification\/classes.txt\"\r\n                }\r\n            }\r\n        },\r\n        \"dataloader\": {\r\n            \"train\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": true,\r\n                    \"shuffle\": false,\r\n                    \"collate_fn\": {\r\n                        \"name\": \"MixupCutmixCollator\",\r\n                        \"args\": {\r\n                            \"mixup_alpha\": 0.4,\r\n                            \"cutmix_alpha\": 1.0,\r\n                            \"weight\": [\r\n                                0.2,\r\n                                0.2\r\n                            ]\r\n                        }\r\n                    },\r\n                    \"sampler\": {\r\n                        \"name\": \"BalanceSampler\",\r\n                        \"args\": null\r\n                    }\r\n                }\r\n            },\r\n            \"val\": {\r\n                \"name\": \"DataLoaderWithCollator\",\r\n                \"args\": {\r\n                    \"batch_size\": 32,\r\n                    \"drop_last\": false,\r\n                    \"shuffle\": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n2022-04-04 12:21:56 | DEBUG    | opt.py:load_yaml:36 - Loading config from configs\/classification\/transform.yaml...\r\n2022-04-04 12:21:57 | DEBUG    | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - Calculating class distribution...\r\nDownloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth\r\nTraceback (most recent call last):\r\n  File \"\/content\/main\/configs\/classification\/train.py\", line 9, in <module>\r\n    train_pipeline = Pipeline(opts)\r\n  File \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__\r\n    registry=CALLBACKS_REGISTRY\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in <listcomp>\r\n    out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config]\r\n  File \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively\r\n    return registry.get(config['name'])(**args, **kwargs)\r\nTypeError: type object got multiple values for keyword argument 'resume'\r\n```\r\n\r\nI guess because of the ```resume``` arg is both repeated in ```global``` and ```WandBCallbacks```. Maybe it also happens with ```Tensorboard```.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: resume error in .; Content: forgot to create an issue in recent days. when tested with ```resume``` argument in ```callbacks```, i encountered this error. here's the log: ```python [errno 2] no such file or directory: 'main' \/content\/main 2022-04-04 12:21:56 | debug | opt.py:override:78 - overriding configuration... 2022-04-04 12:21:56 | info | classification\/pipeline.py:__init__:51 - { \"global\": { \"exp_name\": null, \"exist_ok\": false, \"debug\": true, \"cfg_transform\": \"configs\/classification\/transform.yaml\", \"save_dir\": \"\/content\/main\/runs\", \"device\": \"cuda:0\", \"use_fp16\": true, \"pretrained\": null, \"resume\": null }, \"trainer\": { \"name\": \"supervisedtrainer\", \"args\": { \"num_iterations\": 2000, \"clip_grad\": 10.0, \"evaluate_interval\": 1, \"print_interval\": 20, \"save_interval\": 500 } }, \"model\": { \"name\": \"basetimmmodel\", \"args\": { \"name\": \"convnext_tiny\", \"from_pretrained\": true, \"num_classes\": 180 } }, \"loss\": { \"name\": \"focalloss\" }, \"callbacks\": [ { \"name\": \"loggercallbacks\", \"args\": null }, { \"name\": \"checkpointcallbacks\", \"args\": { \"best_key\": \"bl_acc\" } }, { \"name\": \"visualizercallbacks\", \"args\": null }, { \"name\": \"tensorboardcallbacks\", \"args\": null }, { \"name\": \"callbacks\", \"args\": { \"username\": \"lannguyen\", \"project_name\": \"theseus_classification\", \"resume\": true } } ], \"metrics\": [ { \"name\": \"accuracy\", \"args\": null }, { \"name\": \"balancedaccuracymetric\", \"args\": null }, { \"name\": \"f1scoremetric\", \"args\": { \"average\": \"weighted\" } }, { \"name\": \"confusionmatrix\", \"args\": null }, { \"name\": \"errorcases\", \"args\": null } ], \"optimizer\": { \"name\": \"adamw\", \"args\": { \"lr\": 0.001, \"weight_decay\": 0.0005, \"betas\": [ 0.937, 0.999 ] } }, \"scheduler\": { \"name\": \"schedulerwrapper\", \"args\": { \"scheduler_name\": \"cosine2\", \"t_initial\": 7, \"t_mul\": 0.9, \"eta_mul\": 0.9, \"eta_min\": 1e-06 } }, \"data\": { \"dataset\": { \"train\": { \"name\": \"imagefolderdataset\", \"args\": { \"image_dir\": \"\/content\/main\/data\/food-classification\/train\", \"txt_classnames\": \"configs\/classification\/classes.txt\" } }, \"val\": { \"name\": \"imagefolderdataset\", \"args\": { \"image_dir\": \"\/content\/main\/data\/food-classification\/val\", \"txt_classnames\": \"configs\/classification\/classes.txt\" } } }, \"dataloader\": { \"train\": { \"name\": \"dataloaderwithcollator\", \"args\": { \"batch_size\": 32, \"drop_last\": true, \"shuffle\": false, \"collate_fn\": { \"name\": \"mixupcutmixcollator\", \"args\": { \"mixup_alpha\": 0.4, \"cutmix_alpha\": 1.0, \"weight\": [ 0.2, 0.2 ] } }, \"sampler\": { \"name\": \"balancesampler\", \"args\": null } } }, \"val\": { \"name\": \"dataloaderwithcollator\", \"args\": { \"batch_size\": 32, \"drop_last\": false, \"shuffle\": true } } } } } 2022-04-04 12:21:56 | debug | opt.py:load_yaml:36 - loading config from configs\/classification\/transform.yaml... 2022-04-04 12:21:57 | debug | classification\/datasets\/folder_dataset.py:_calculate_classes_dist:71 - calculating class distribution... downloading: \"https:\/\/dl.fbaipublicfiles.com\/convnext\/convnext_tiny_1k_224_ema.pth\" to \/root\/.cache\/torch\/hub\/checkpoints\/convnext_tiny_1k_224_ema.pth traceback (most recent call last): file \"\/content\/main\/configs\/classification\/train.py\", line 9, in train_pipeline = pipeline(opts) file \"\/content\/main\/theseus\/classification\/pipeline.py\", line 159, in __init__ registry=callbacks_registry file \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in get_instance_recursively out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config] file \"\/content\/main\/theseus\/utilities\/getter.py\", line 15, in out = [get_instance_recursively(item, registry=registry, **kwargs) for item in config] file \"\/content\/main\/theseus\/utilities\/getter.py\", line 26, in get_instance_recursively return registry.get(config['name'])(**args, **kwargs) typeerror: type object got multiple values for keyword argument 'resume' ``` i guess because of the ```resume``` arg is both repeated in ```global``` and ```callbacks```. maybe it also happens with ```tensorboard```.",
        "Issue_original_content_gpt_summary":"The user encountered an error when testing with the ```resume``` argument in ```callbacks```, due to the argument being repeated in both ```global``` and ```callbacks```.",
        "Issue_preprocessed_content":"Title: resume error in .; Content: forgot to create an issue in recent days. when tested with argument in , i encountered this error. here's the log i guess because of the arg is both repeated in and . maybe it also happens with ."
    },
    {
        "Issue_link":"https:\/\/github.com\/kuielab\/mdx-net\/issues\/15",
        "Issue_title":"wandb only logs top k val loss",
        "Issue_creation_time":1625581852000,
        "Issue_closed_time":1625620015000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: only logs top k val loss",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of only logging the top k values of the loss function during training.",
        "Issue_preprocessed_content":"Title: only logs top k val loss; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/9",
        "Issue_title":"Fix writing of checkpoints to wandb",
        "Issue_creation_time":1624867819000,
        "Issue_closed_time":1624957138000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## What\r\n\r\nA clear and concise description of what the bug is.\r\n\r\n## How to reproduce\r\n\r\nReproduce by starting a non-dry run via a notebook\r\n\r\n1. Start the run\r\n2. Look at files on the wandb interface. There are no checkpoints\r\n\r\n## Expected\r\n\r\nCheckpoints should be uploaded to wandb whenever there is a better one available during training.\r\n\r\n## Additional context\r\n\r\nI thought I fixed wandb, but it seems that I don't understand the symlinking model of wandb. Apparently you need to have checkpoints under the project root? But this would mean that you can't run multiple experiements at the same time. ",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: fix writing of checkpoints to ; Content: ## what a clear and concise description of what the bug is. ## how to reproduce reproduce by starting a non-dry run via a notebook 1. start the run 2. look at files on the interface. there are no checkpoints ## expected checkpoints should be uploaded to whenever there is a better one available during training. ## additional context i thought i fixed , but it seems that i don't understand the symlinking model of . apparently you need to have checkpoints under the project root? but this would mean that you can't run multiple experiements at the same time.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with writing checkpoints to the interface, where checkpoints were not being uploaded to the interface whenever a better one was available during training, despite the user's attempts to fix the issue.",
        "Issue_preprocessed_content":"Title: fix writing of checkpoints to ; Content: what a clear and concise description of what the bug is. how to reproduce reproduce by starting a run via a notebook . start the run . look at files on the interface. there are no checkpoints expected checkpoints should be uploaded to whenever there is a better one available during training. additional context i thought i fixed , but it seems that i don't understand the symlinking model of . apparently you need to have checkpoints under the project root? but this would mean that you can't run multiple experiements at the same time."
    },
    {
        "Issue_link":"https:\/\/github.com\/feldberlin\/wavenet\/issues\/5",
        "Issue_title":"Loading configs from wandb yields incorrect parameters",
        "Issue_creation_time":1624097966000,
        "Issue_closed_time":1624287268000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## What\r\n\r\nWhen loading configs from wandb, the resulting HParams objects are not correct. This can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on wandb.\r\n\r\n## How to Reproduce\r\n\r\nLoad the configs:\r\n\r\n```python\r\nfrom wavenet import utils, model, train\r\n\r\nrun_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc'\r\np, ptrain = utils.load_wandb_cfg(run_path)\r\np, ptrain = model.HParams(**p), train.HParams(**ptrain)\r\n```\r\n\r\nValidate against the run [on wandb](https:\/\/wandb.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete)\r\n\r\n## Acceptance Criteria\r\n\r\n- [x] Bug has been understood and fixed\r\n- [x] The same config given above can be loaded and is correct",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: loading configs from yields incorrect parameters; Content: ## what when loading configs from , the resulting hparams objects are not correct. this can be seen when attempting to load the model checkpoint with the given parameters (failure), or when comparing the object with the info panel for the run on . ## how to reproduce load the configs: ```python from wavenet import utils, model, train run_path = 'purzelrakete\/feldberlin-wavenet\/21ei0tqc' p, ptrain = utils.load__cfg(run_path) p, ptrain = model.hparams(**p), train.hparams(**ptrain) ``` validate against the run [on ](https:\/\/.ai\/purzelrakete\/feldberlin-wavenet\/runs\/21ei0tqc\/overview?workspace=user-purzelrakete) ## acceptance criteria - [x] bug has been understood and fixed - [x] the same config given above can be loaded and is correct",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when loading configs from , resulting in incorrect hparams objects which could be seen when attempting to load the model checkpoint or when comparing the object with the info panel for the run on .",
        "Issue_preprocessed_content":"Title: loading configs from yields incorrect parameters; Content: what when loading configs from , the resulting hparams objects are not correct. this can be seen when attempting to load the model checkpoint with the given parameters , or when comparing the object with the info panel for the run on . how to reproduce load the configs validate against the run acceptance criteria bug has been understood and fixed the same config given above can be loaded and is correct"
    },
    {
        "Issue_link":"https:\/\/github.com\/ContinualAI\/avalanche\/issues\/797",
        "Issue_title":"Config type in WandBLogger",
        "Issue_creation_time":1635882064000,
        "Issue_closed_time":1635948747000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"This is more like a suggestion than a bug. The `config` parameter to the WandBLogger is supposed to be of type `args.namespace`. Therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. This might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a YAML file). Wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input?\r\n\r\nThanks :)",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: config type in logger; Content: this is more like a suggestion than a bug. the `config` parameter to the logger is supposed to be of type `args.namespace`. therefore it converts it to a dictionary inside its `arge_parse` function using `vars(.)`. this might be restrictive in some cases if someone wants to pass configs directly as a dictionary (for example when hyperparameters are loaded from a yaml file). wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input? thanks :)",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the `config` parameter to the logger, which is supposed to be of type `args.namespace` and is converted to a dictionary inside its `arge_parse` function, potentially restricting the ability to pass configs directly as a dictionary.",
        "Issue_preprocessed_content":"Title: config type in logger; Content: this is more like a suggestion than a bug. the parameter to the logger is supposed to be of type . therefore it converts it to a dictionary inside its function using . this might be restrictive in some cases if someone wants to pass configs directly as a dictionary . wouldn't it be better to do the conversion outside the logger to make it more general in terms of config input? thanks"
    },
    {
        "Issue_link":"https:\/\/github.com\/alvarobartt\/wandbfsspec\/issues\/7",
        "Issue_title":"`WandbFileSystem.ls` not working fine with nested directories",
        "Issue_creation_time":1658474901000,
        "Issue_closed_time":1658480572000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"https:\/\/wandb.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: `filesystem.ls` not working fine with nested directories; Content: https:\/\/.ai\/alvarobartt\/resnet-pytorch\/runs\/39mhvmwp\/files\/this\/is\/just\/for\/testing",
        "Issue_original_content_gpt_summary":"The user encountered an issue with the `filesystem.ls` command not working properly when trying to list the contents of a nested directory.",
        "Issue_preprocessed_content":"Title: not working fine with nested directories; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/boostcampaitech2\/semantic-segmentation-level2-cv-02\/issues\/21",
        "Issue_title":"wandb create index name error",
        "Issue_creation_time":1634804027000,
        "Issue_closed_time":1635155013000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"- [x] wandb index name modify\r\n\r\nwandb create index name error and  change name to \"modelname + save_folder_name\"",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: create index name error; Content: - [x] index name modify create index name error and change name to \"modelname + save_folder_name\"",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to create an index name and had to modify the name to \"modelname + save_folder_name\".",
        "Issue_preprocessed_content":"Title: create index name error; Content: index name modify create index name error and change name to modelname +"
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/173",
        "Issue_title":"Fix Wandb import issue",
        "Issue_creation_time":1602624602000,
        "Issue_closed_time":1604903231000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: fix import issue",
        "Issue_original_content_gpt_summary":"The user encountered an issue with importing a module, which was resolved by updating the module's version.",
        "Issue_preprocessed_content":"Title: fix import issue; Content:"
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/162",
        "Issue_title":"Weird memory problem with sweeps Colab Wandb",
        "Issue_creation_time":1600976683000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"```Problem Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.``` I'm running into this issue with a specific model (e.g. DA-RNN w\/meta-data sweep). If runs truly aren't cleared then sweeps could be corrupting subsequent runs. This behavior hasn't been observed previously however.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: weird memory problem with sweeps colab ; Content: ```problem trying to backward through the graph a second time, but the saved intermediate results have already been freed. specify retain_graph=true when calling backward the first time.``` i'm running into this issue with a specific model (e.g. da-rnn w\/meta-data sweep). if runs truly aren't cleared then sweeps could be corrupting subsequent runs. this behavior hasn't been observed previously however.",
        "Issue_original_content_gpt_summary":"The user encountered a weird memory problem with sweeps colab, where running a specific model (e.g. da-rnn w\/meta-data sweep) could potentially corrupt subsequent runs if the intermediate results were not retained.",
        "Issue_preprocessed_content":"Title: weird memory problem with sweeps colab ; Content: i'm running into this issue with a specific model . if runs truly aren't cleared then sweeps could be corrupting subsequent runs. this behavior hasn't been observed previously however."
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/154",
        "Issue_title":"Wandb Run stalling",
        "Issue_creation_time":1600217910000,
        "Issue_closed_time":1600665871000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Wandb sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgH78bNb9A5JP6NcfFHB189TIjy5c#scrollTo=sTDGweZ0d0QP) advance instead they just stall after the first part of the sweep completes. This is causing problems.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: run stalling; Content: sweep on our [primary notebook don't](https:\/\/colab.research.google.com\/drive\/1vl6tgh78bnb9a5jp6ncffhb189tijy5c#scrollto=stdgwez0d0qp) advance instead they just stall after the first part of the sweep completes. this is causing problems.",
        "Issue_original_content_gpt_summary":"The user is encountering a challenge where their primary notebook is stalling after the first part of the sweep completes, causing problems.",
        "Issue_preprocessed_content":"Title: run stalling; Content: sweep on our advance instead they just stall after the first part of the sweep completes. this is causing problems."
    },
    {
        "Issue_link":"https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/issues\/35",
        "Issue_title":"Wandb bug when running train long",
        "Issue_creation_time":1578034238000,
        "Issue_closed_time":1578199794000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"When running this code https:\/\/github.com\/AIStream-Peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook Wandb views the entire thing as one training session and continue gradient steps indefinitely. Training session should be forced to end when that model stops training not when the meta training loop finishes. Should only be 28 training steps not 80.\r\n<img width=\"1094\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/3865062\/71710653-65567f80-2dcb-11ea-8558-0f3280c4ab7b.png\">\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: bug when running train long; Content: when running this code https:\/\/github.com\/aistream-peelout\/flow-forecast\/commit\/1f67ac4844859e5d60a0f5dba2dbbe8f4c5dbc30 from a colab notebook views the entire thing as one training session and continue gradient steps indefinitely. training session should be forced to end when that model stops training not when the meta training loop finishes. should only be 28 training steps not 80.",
        "Issue_original_content_gpt_summary":"The user encountered a bug when running a long training session, where the training session was viewed as one continuous session and the gradient steps were forced to continue indefinitely, instead of ending when the model stopped training.",
        "Issue_preprocessed_content":"Title: bug when running train long; Content: when running this code from a colab notebook views the entire thing as one training session and continue gradient steps indefinitely. training session should be forced to end when that model stops training not when the meta training loop finishes. should only be training steps not . img width alt image"
    },
    {
        "Issue_link":"https:\/\/github.com\/pstage-ocr-team6\/ocr-teamcode\/issues\/5",
        "Issue_title":"[BUG] wandb value doesn't update",
        "Issue_creation_time":1622372396000,
        "Issue_closed_time":1622440667000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png)\r\n\r\n- lr\uc744 \uc81c\uc678\ud558\uace0 \ub098\uba38\uc9c0 value\uac00 \uc5c5\ub370\uc774\ud2b8\uac00 \ub418\uc9c0 \uc54a\ub294 \ubb38\uc81c \ubc1c\uc0dd\r\n- \uacc4\uc18d \uac12\uc774 \ucd94\uac00\ub418\ub294 \ub9ac\uc2a4\ud2b8\uc778 \uc904 \ubaa8\ub974\uace0 list[0]\uc73c\ub85c \uc778\ub371\uc2f1\ud574\uc11c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub77c\uace0 \uc0dd\uac01\ub428",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] value doesn't update; Content: ![image](https:\/\/user-images.githubusercontent.com\/37505775\/120101493-4f481c80-c181-11eb-8a20-4a044c2bd51c.png) - lr value - list[0]",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the values in a list were not updating, except for the lr value, due to incorrect indexing of the list.",
        "Issue_preprocessed_content":"Title: value doesn't update; Content: lr value list"
    },
    {
        "Issue_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/669",
        "Issue_title":"Test set metrics overwrite validation set metrics in TensorBoard and are rejected for logging by Weights and Biases (W&B)",
        "Issue_creation_time":1663015846000,
        "Issue_closed_time":1663248037000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n\r\nAt model train completion, the test set loss is written as iteration 0 to the TensorBoard \/ W&B chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. As the validation loss and perplexity has already been written to this chart, this results in TensorBoard deleting all the validation metrics, overwriting them with the test loss and perplexity values. W&B refuses to add the test metrics to the charts at all, throwing a warning that looks like `wandb: WARNING Step must only increase in log calls.  Step 0 < 32000; dropping {'validation\/lm_loss': 1.715476632118225}.`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Pip install and setup TensorBoard and W&B\r\n2. Begin training a model with a train, validation, and test set\r\n3. Observe in both TensorBoard and W&B that validation metrics are being logged\r\n4. Allow the model to train to completion\r\n5. Observe that the TensorBoard validation metrics are now gone, overwritten by the test set metrics\r\n6. Observe the W&B error in the text logs \/ program output\r\n\r\n**Expected behavior**\r\nTest metrics should be written to their own charts.\r\n\r\n**Proposed solution**\r\nTest loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively.\r\n\r\n**Screenshots**\r\n![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png)\r\n\r\n**Environment (please complete the following information):**\r\n - GPUs: 4x A100 80 GB\r\n- Configs: (configs that I used to reproduce the bug and test bug fixes are included below)\r\n\r\n```\r\n# GPT-2 pretraining setup\r\n{\r\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\r\n   # across the node boundaries )\r\n   \"pipe-parallel-size\": 1,\r\n   \"model-parallel-size\": 1,\r\n\r\n   # model settings\r\n   \"num-layers\": 24,\r\n   \"hidden-size\": 1024,\r\n   \"num-attention-heads\": 16,\r\n   \"seq-length\": 4096,\r\n   \"max-position-embeddings\": 4096,\r\n   \"norm\": \"layernorm\",\r\n   \"pos-emb\": \"rotary\",\r\n   \"no-weight-tying\": true,\r\n\r\n   # these should provide some speedup but takes a while to build, set to true if desired\r\n   \"scaled-upper-triang-masked-softmax-fusion\": false,\r\n   \"bias-gelu-fusion\": false,\r\n\r\n\r\n\r\n   # optimizer settings\r\n   \"optimizer\": {\r\n     \"type\": \"Adam\",\r\n     \"params\": {\r\n       \"lr\": 0.00003,\r\n       \"betas\": [0.9, 0.999],\r\n       \"eps\": 1.0e-8,\r\n     }\r\n   },\r\n   \"zero_optimization\": {\r\n    \"stage\": 1,\r\n    \"allgather_partitions\": True,\r\n    \"allgather_bucket_size\": 500000000,\r\n    \"overlap_comm\": True,\r\n    \"reduce_scatter\": True,\r\n    \"reduce_bucket_size\": 500000000,\r\n    \"contiguous_gradients\": True,\r\n    \"cpu_offload\": False\r\n  },\r\n   # batch \/ data settings\r\n   \"train_micro_batch_size_per_gpu\": 16,\r\n   \"data-impl\": \"mmap\",\r\n   \"split\": \"949,50,1\",\r\n\r\n   # activation checkpointing\r\n   \"checkpoint-activations\": true,\r\n   \"checkpoint-num-layers\": 1,\r\n   \"partition-activations\": true,\r\n   \"synchronize-each-layer\": true,\r\n\r\n   # regularization\r\n   \"gradient_clipping\": 1.0,\r\n   \"weight-decay\": 0.01,\r\n   \"hidden-dropout\": 0,\r\n   \"attention-dropout\": 0,\r\n\r\n   # precision settings\r\n   \"fp16\": {\r\n     \"fp16\": true,\r\n     \"enabled\": true,\r\n     \"loss_scale\": 0,\r\n     \"loss_scale_window\": 1000,\r\n     \"hysteresis\": 2,\r\n     \"min_loss_scale\": 1\r\n   },\r\n\r\n   # misc. training settings\r\n   \"train-iters\": 100,\r\n   \"lr-decay-iters\": 100,\r\n   \"distributed-backend\": \"nccl\",\r\n   \"lr-decay-style\": \"constant\",\r\n   \"warmup\": 0.1,\r\n   \"save-interval\": 25,\r\n   \"eval-interval\": 25,\r\n   \"eval-iters\": 10,\r\n\r\n   # Checkpoint\r\n   \"finetune\": true,\r\n\r\n   # logging\r\n   \"log-interval\": 10,\r\n   \"steps_per_print\": 10,\r\n   \"keep-last-n-checkpoints\": 4,\r\n   \"wall_clock_breakdown\": true,\r\n}\r\n```\r\n\r\n```\r\n# Suggested data paths when using GPT-NeoX locally\r\n{\r\n  \"train-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/train_text_document\"],\r\n  \"test-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/test_text_document\"],\r\n  \"valid-data-paths\": [\"\/mnt\/4TBNVME\/gpt-neox\/data\/preprocessed\/val_text_document\"],\r\n\r\n  \"vocab-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-vocab.json\",\r\n  \"merge-file\": \"\/mnt\/4TBNVME\/gpt-neox\/data\/gpt2-merges.txt\",\r\n\r\n  \"save\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n  \"load\": \"\/mnt\/4TBNVME\/checkpoints_test\",\r\n\r\n  \"checkpoint_validation_with_forward_pass\": False,\r\n  \r\n  \"tensorboard-dir\": \"\/mnt\/4TBNVME\/logs\/tensorboard\/bug_fix_test\",\r\n  \"log-dir\": \"\/mnt\/4TBNVME\/logs\/gptneox\/bug_fix_test\",\r\n\r\n  \"use_wandb\": True,\r\n  \"wandb_host\": \"https:\/\/api.wandb.ai\",\r\n  \"wandb_project\": \"neox_test\"\r\n}\r\n```\r\n\r\n```\r\n# Add this to your config for sparse attention every other layer\r\n{\r\n  \"attention_config\": [[[\"local\", \"global\"], \"all\"]],\r\n\r\n  # sparsity config:\r\n  # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for\r\n  # illustrative purposes)\r\n  # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for\r\n  # more detailed config instructions and available parameters\r\n\r\n  \"sparsity_config\": {\r\n    \"block\": 16, # block size\r\n    \"num_local_blocks\": 32,\r\n  }\r\n}\r\n```\r\n\r\n**Additional context**\r\n\r\nI have a bug fix ready, will follow up with it.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: test set metrics overwrite validation set metrics in tensorboard and are rejected for logging by (w&b); **describe the bug** at model train completion, the test set loss is written as iteration 0 to the tensorboard \/ w&b chart `validation\/lm_loss`, and the test set perplexity is written as iteration 0 to the chart `validation\/lm_loss_ppl`. as the validation loss and perplexity has already been written to this chart, this results in tensorboard deleting all the validation metrics, overwriting them with the test loss and perplexity values. w&b refuses to add the test metrics to the charts at all, throwing a warning that looks like `: warning step must only increase in log calls. step 0 < 32000; Content: dropping {'validation\/lm_loss': 1.715476632118225}.` **to reproduce** steps to reproduce the behavior: 1. pip install and setup tensorboard and w&b 2. begin training a model with a train, validation, and test set 3. observe in both tensorboard and w&b that validation metrics are being logged 4. allow the model to train to completion 5. observe that the tensorboard validation metrics are now gone, overwritten by the test set metrics 6. observe the w&b error in the text logs \/ program output **expected behavior** test metrics should be written to their own charts. **proposed solution** test loss and perplexity should be written to their own charts `test\/lm_loss` and `test\/lm_loss_ppl` respectively. **screenshots** ![image](https:\/\/user-images.githubusercontent.com\/6119143\/189752970-3b26dd14-475f-48cb-be84-fae23a99ba10.png) **environment (please complete the following information):** - gpus: 4x a100 80 gb - configs: (configs that i used to reproduce the bug and test bug fixes are included below) ``` # gpt-2 pretraining setup { # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages # across the node boundaries ) \"pipe-parallel-size\": 1, \"model-parallel-size\": 1, # model settings \"num-layers\": 24, \"hidden-size\": 1024, \"num-attention-heads\": 16, \"seq-length\": 4096, \"max-position-embeddings\": 4096, \"norm\": \"layernorm\", \"pos-emb\": \"rotary\", \"no-weight-tying\": true, # these should provide some speedup but takes a while to build, set to true if desired \"scaled-upper-triang-masked-softmax-fusion\": false, \"bias-gelu-fusion\": false, # optimizer settings \"optimizer\": { \"type\": \"adam\", \"params\": { \"lr\": 0.00003, \"betas\": [0.9, 0.999], \"eps\": 1.0e-8, } }, \"zero_optimization\": { \"stage\": 1, \"allgather_partitions\": true, \"allgather_bucket_size\": 500000000, \"overlap_comm\": true, \"reduce_scatter\": true, \"reduce_bucket_size\": 500000000, \"contiguous_gradients\": true, \"cpu_offload\": false }, # batch \/ data settings \"train_micro_batch_size_per_gpu\": 16, \"data-impl\": \"mmap\", \"split\": \"949,50,1\", # activation checkpointing \"checkpoint-activations\": true, \"checkpoint-num-layers\": 1, \"partition-activations\": true, \"synchronize-each-layer\": true, # regularization \"gradient_clipping\": 1.0, \"weight-decay\": 0.01, \"hidden-dropout\": 0, \"attention-dropout\": 0, # precision settings \"fp16\": { \"fp16\": true, \"enabled\": true, \"loss_scale\": 0, \"loss_scale_window\": 1000, \"hysteresis\": 2, \"min_loss_scale\": 1 }, # misc. training settings \"train-iters\": 100, \"lr-decay-iters\": 100, \"distributed-backend\": \"nccl\", \"lr-decay-style\": \"constant\", \"warmup\": 0.1, \"save-interval\": 25, \"eval-interval\": 25, \"eval-iters\": 10, # checkpoint \"finetune\": true, # logging \"log-interval\": 10, \"steps_per_print\": 10, \"keep-last-n-checkpoints\": 4, \"wall_clock_breakdown\": true, } ``` ``` # suggested data paths when using gpt-neox locally { \"train-data-paths\": [\"\/mnt\/4tbnvme\/gpt-neox\/data\/preprocessed\/train_text_document\"], \"test-data-paths\": [\"\/mnt\/4tbnvme\/gpt-neox\/data\/preprocessed\/test_text_document\"], \"valid-data-paths\": [\"\/mnt\/4tbnvme\/gpt-neox\/data\/preprocessed\/val_text_document\"], \"vocab-file\": \"\/mnt\/4tbnvme\/gpt-neox\/data\/gpt2-vocab.json\", \"merge-file\": \"\/mnt\/4tbnvme\/gpt-neox\/data\/gpt2-merges.txt\", \"save\": \"\/mnt\/4tbnvme\/checkpoints_test\", \"load\": \"\/mnt\/4tbnvme\/checkpoints_test\", \"checkpoint_validation_with_forward_pass\": false, \"tensorboard-dir\": \"\/mnt\/4tbnvme\/logs\/tensorboard\/bug_fix_test\", \"log-dir\": \"\/mnt\/4tbnvme\/logs\/gptneox\/bug_fix_test\", \"use_\": true, \"_host\": \"https:\/\/api..ai\", \"_project\": \"neox_test\" } ``` ``` # add this to your config for sparse attention every other layer { \"attention_config\": [[[\"local\", \"global\"], \"all\"]], # sparsity config: # (these are the defaults for local sliding window sparsity, training will work without this here, but it's left in for # illustrative purposes) # see https:\/\/www.deepspeed.ai\/tutorials\/sparse-attention\/#how-to-config-sparsity-structures for # more detailed config instructions and available parameters \"sparsity_config\": { \"block\": 16, # block size \"num_local_blocks\": 32, } } ``` **additional context** i have a bug fix ready, will follow up with it.",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the test set metrics overwrite the validation set metrics in Tensorboard and are rejected for logging by Weights & Biases, resulting in the validation metrics being deleted.",
        "Issue_preprocessed_content":"Title: test set metrics overwrite validation set metrics in tensorboard and are rejected for logging by ; Content: describe the bug at model train completion, the test set loss is written as iteration to the tensorboard \/ w&b chart , and the test set perplexity is written as iteration to the chart . as the validation loss and perplexity has already been written to this chart, this results in tensorboard deleting all the validation metrics, overwriting them with the test loss and perplexity values. w&b refuses to add the test metrics to the charts at all, throwing a warning that looks like to reproduce steps to reproduce the behavior . pip install and setup tensorboard and w&b . begin training a model with a train, validation, and test set . observe in both tensorboard and w&b that validation metrics are being logged . allow the model to train to completion . observe that the tensorboard validation metrics are now gone, overwritten by the test set metrics . observe the w&b error in the text logs \/ program output expected behavior test metrics should be written to their own charts. proposed solution test loss and perplexity should be written to their own charts and respectively. screenshots environment gpus x a gb configs additional context i have a bug fix ready, will follow up with it."
    },
    {
        "Issue_link":"https:\/\/github.com\/EleutherAI\/gpt-neox\/issues\/229",
        "Issue_title":"Local wandb logging is borked",
        "Issue_creation_time":1618393073000,
        "Issue_closed_time":1624218172000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"our current wandb logging assumes the presence of an API key, which you don't need if you're running wandb locally.\r\n\r\nWe should configure it so it works with wandb locally, too. ",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: local logging is borked; Content: our current logging assumes the presence of an api key, which you don't need if you're running locally. we should configure it so it works with locally, too.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with local logging, which requires an API key when running locally, and needs to be configured to work without one.",
        "Issue_preprocessed_content":"Title: local logging is borked; Content: our current logging assumes the presence of an api key, which you don't need if you're running locally. we should configure it so it works with locally, too."
    },
    {
        "Issue_link":"https:\/\/github.com\/Lightning-AI\/lightning-hpo\/issues\/17",
        "Issue_title":"Refused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".",
        "Issue_creation_time":1658586252000,
        "Issue_closed_time":1659198312000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"## \ud83d\udc1b Bug\r\n\r\nRefused to frame 'https:\/\/wandb.ai\/' because an ancestor violates the following Content Security Policy directive: \"frame-ancestors 'self'\".\r\n\r\n\r\n### To Reproduce\r\n\r\n`lightning run app app.py --cloud --env xxxx --env xxx`\r\n\r\n<img width=\"1792\" alt=\"Screen Shot 2022-07-23 at 10 23 34 AM\" src=\"https:\/\/user-images.githubusercontent.com\/6315124\/180609239-6093fcc2-7902-4e36-991a-6ae44e5c329c.png\">\r\n\r\n\r\n#### Code sample\r\n\r\n\r\n### Expected behavior\r\n\r\n\r\n### Environment\r\n\r\n\r\n### Additional context\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: refused to frame 'https:\/\/.ai\/' because an ancestor violates the following content security policy directive: \"frame-ancestors 'self'\".; Content: ## bug refused to frame 'https:\/\/.ai\/' because an ancestor violates the following content security policy directive: \"frame-ancestors 'self'\". ### to reproduce `lightning run app app.py --cloud --env xxxx --env xxx` #### code sample ### expected behavior ### environment ### additional context",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where a content security policy directive refused to frame 'https:\/\/.ai\/' due to an ancestor violating the directive.",
        "Issue_preprocessed_content":"Title: refused to frame because an ancestor violates the following content security policy directive 'self' .; Content: bug refused to frame because an ancestor violates the following content security policy directive 'self' . to reproduce img width alt screen shot at am code sample expected behavior environment additional context"
    },
    {
        "Issue_link":"https:\/\/github.com\/icenet-ai\/icenet\/issues\/72",
        "Issue_title":"wandb entity is hardcoded",
        "Issue_creation_time":1669459637000,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"* IceNet version: 0.2.0.dev10\r\n\r\n`icenet\/model\/train.py` has the wandb.init entity hardcoded, oops\r\n\r\nMake this default to $USER, ICENET_WANDB_USER or be overridden by command line (whichever exists right to left... \ud83d\ude09 )\r\n\r\nDo the same for the project too",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: entity is hardcoded; Content: * icenet version: 0.2.0.dev10 `icenet\/model\/train.py` has the .init entity hardcoded, oops make this default to $user, icenet__user or be overridden by command line (whichever exists right to left... ) do the same for the project too",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the entity was hardcoded in the icenet\/model\/train.py file, and needed to be changed to either $user, icenet__user or be overridden by command line.",
        "Issue_preprocessed_content":"Title: entity is hardcoded; Content: icenet version has the entity hardcoded, oops make this default to $user, or be overridden by command line do the same for the project too"
    },
    {
        "Issue_link":"https:\/\/github.com\/ezeeEric\/DiVAE\/issues\/22",
        "Issue_title":"Remove data\/ and wandb\/ directories and rewrite history",
        "Issue_creation_time":1615226514000,
        "Issue_closed_time":1615408051000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"The `data\/MNIST` subdirectory slipped through `.gitignore` and is now part of the repo's history. These binary files should be removed. There's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/).\r\n\r\nAt the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. Let's do that once we have committed all local changes.",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: remove data\/ and \/ directories and rewrite history; Content: the `data\/mnist` subdirectory slipped through `.gitignore` and is now part of the repo's history. these binary files should be removed. there's an open-source tool available to do that called `bfg` (https:\/\/rtyley.github.io\/bfg-repo-cleaner\/). at the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. let's do that once we have committed all local changes.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge of needing to remove the data\/ and \/ directories and rewrite history due to the `data\/mnist` subdirectory slipping through `.gitignore` and becoming part of the repo's history, and needing to use an open-source tool called `bfg` to do so, followed by deleting local clones and cloning a fresh, cleaned version from upstream.",
        "Issue_preprocessed_content":"Title: remove data\/ and \/ directories and rewrite history; Content: the subdirectory slipped through and is now part of the repo's history. these binary files should be removed. there's an tool available to do that called . at the end of the cleaning process, we need to delete our local clones and clone a fresh, cleaned version from upstream. let's do that once we have committed all local changes."
    },
    {
        "Issue_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/90",
        "Issue_title":"wrong wandb dataset file name",
        "Issue_creation_time":1634910579000,
        "Issue_closed_time":1634915290000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"## TL;DR\r\n\uc644\ub514\ube44\uc5d0 golden test dataset\uc744 \uc5c5\ub85c\ub4dc\ud560 \ub54c, \uae30\uc874 \ub370\uc774\ud130\uc14b\uc758 \uad6c\uc870\ub97c train\/ validation\uc73c\ub85c \ubcc0\uacbd\ud588\ub294\ub370 \r\n\uc774\ub984\uc774 training\uc774 \uc544\ub2c8\ub77c train\uc73c\ub85c \ubc14\uafbc\uac8c wisdomify\uc5d0 \uc81c\ub300\ub85c \uc801\uc6a9\ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\ub2e4.\r\n\r\n## WHY?\r\n\ub370\uc774\ud130\uac00 \ub85c\ub4dc\ub418\uc9c0 \uc54a\uc74c.\r\n\r\n## WHAT?\r\n\ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n## TODOs\r\n- [ ] \ub370\uc774\ud130 \ub85c\ub4dc\ud558\ub294 \ubd80\ubd84\uc5d0 \uac00\uc11c \ud30c\uc77c\uc774\ub984\uc744 train.tsv\ub85c \ubcc0\uacbd\ud558\uc790.\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: wrong dataset file name; Content: ## tl;dr golden test dataset , train\/ validation training train wisdomify . ## why? . ## what? train.tsv . ## todos - [ ] train.tsv .",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to upload a golden test dataset to Wandb, where the structure of the existing dataset was changed from train\/validation to training, but the name was not changed to train, resulting in the data not being loaded.",
        "Issue_preprocessed_content":"Title: wrong dataset file name; Content: tl;dr golden test dataset , train\/ validation training train wisdomify . why? . what? . todos ."
    },
    {
        "Issue_link":"https:\/\/github.com\/wisdomify\/wisdomify\/issues\/89",
        "Issue_title":"wandb import failure",
        "Issue_creation_time":1634732461000,
        "Issue_closed_time":1635333937000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"**Describe the bug**\r\n~~~\r\nfrom six.moves.collections_abc import Mapping, Sequence \r\nModuleNotFoundError: No module named 'six.moves.collections_abc'\r\n~~~\r\n\r\n**To Reproduce**\r\nRun on @ohsuz 's server.\r\n(Cannot reproduce on Intel i7 based local condition.)\r\n\r\n**Expected behavior**\r\nwandb should be properly imported.\r\n\r\n**Server (please complete the following information):**\r\n - OS: centOS\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: import failure; Content: **describe the bug** ~~~ from six.moves.collections_abc import mapping, sequence modulenotfounderror: no module named 'six.moves.collections_abc' ~~~ **to reproduce** run on @ohsuz 's server. (cannot reproduce on intel i7 based local condition.) **expected behavior** should be properly imported. **server (please complete the following information):** - os: centos",
        "Issue_original_content_gpt_summary":"The user encountered a ModuleNotFoundError when attempting to import from six.moves.collections_abc on a CentOS server, despite being able to reproduce the import on an Intel i7-based local machine.",
        "Issue_preprocessed_content":"Title: import failure; Content: describe the bug from import mapping, sequence modulenotfounderror no module named to reproduce run on 's server. cannot reproduce on intel i based local expected behavior should be properly imported. server os centos"
    },
    {
        "Issue_link":"https:\/\/github.com\/johannespischinger\/senti_anal\/issues\/51",
        "Issue_title":"wandb api key for github ci",
        "Issue_creation_time":1642148915000,
        "Issue_closed_time":1642173581000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"wandb api key not configured for github ci\r\n\r\nhttps:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: api key for github ci; Content: api key not configured for github ci https:\/\/github.com\/johannespischinger\/senti_anal\/runs\/4808536333?check_suite_focus=true",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with configuring an API key for GitHub CI.",
        "Issue_preprocessed_content":"Title: api key for github ci; Content: api key not configured for github ci"
    },
    {
        "Issue_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/335",
        "Issue_title":"[BUG] Wandb Logger does not work unless pytorch is installed ",
        "Issue_creation_time":1638280869000,
        "Issue_closed_time":1638449992000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"### \ud83d\udc1b Bug Report\n\nWandbLogger throws error while import if etna[torch] is not installed.\n\n### Expected behavior\n\nWandb Logger should work no matter pytorch installation \n\n### How To Reproduce\n\n1. Create new env\r\n2. install etna and etna[wandb]\r\n3. import WandbLogger\r\n\n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: [bug] logger does not work unless pytorch is installed ; Content: ### bug report logger throws error while import if etna[torch] is not installed. ### expected behavior logger should work no matter pytorch installation ### how to reproduce 1. create new env 2. install etna and etna[] 3. import logger ### environment _no response_ ### additional context _no response_ ### checklist - [x] bug appears at the latest library version",
        "Issue_original_content_gpt_summary":"The user encountered a bug where the logger does not work unless PyTorch is installed.",
        "Issue_preprocessed_content":"Title: logger does not work unless pytorch is installed ; Content: bug report logger throws error while import if etna is not installed. expected behavior logger should work no matter pytorch installation how to reproduce . create new env . install etna and etna . import logger environment additional context checklist bug appears at the latest library version"
    },
    {
        "Issue_link":"https:\/\/github.com\/tinkoff-ai\/etna\/issues\/216",
        "Issue_title":"Exception in backtest with `aggregate_metrics=True` when using `WandbLogger`",
        "Issue_creation_time":1634647592000,
        "Issue_closed_time":1635943713000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"### \ud83d\udc1b Bug Report\n\nProgram fails when backtest with `aggregate_metrics=True` is used inside `WandbLogger` (if given). With `aggregate_metrics=False` everything is fine.\r\n\r\nException happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. \r\n\r\nException was caught in `Pipeline.backtest`, but it looks like this bug also appears in `TimeSeriesCrossValidation` class.\n\n### Expected behavior\n\nNo error.\n\n### How To Reproduce\n\nRun backtest with WandLogger while setting `aggregate_metrics=True`. \n\n### Environment\n\n_No response_\n\n### Additional context\n\n_No response_\n\n### Checklist\n\n- [X] Bug appears at the latest library version\n- [X] Bug description added\n- [X] Steps to reproduce added\n- [X] Expected behavior added",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: exception in backtest with `aggregate_metrics=true` when using `logger`; Content: ### bug report program fails when backtest with `aggregate_metrics=true` is used inside `logger` (if given). with `aggregate_metrics=false` everything is fine. exception happens in `tslogger.log_backtest_metrics` while constructing `metrics_df`: it can't make `metrics_df.groupby(\"segment\")`. exception was caught in `pipeline.backtest`, but it looks like this bug also appears in `timeseriescrossvalidation` class. ### expected behavior no error. ### how to reproduce run backtest with wandlogger while setting `aggregate_metrics=true`. ### environment _no response_ ### additional context _no response_ ### checklist - [x] bug appears at the latest library version - [x] bug description added - [x] steps to reproduce added - [x] expected behavior added",
        "Issue_original_content_gpt_summary":"The user encountered a bug when running a backtest with `aggregate_metrics=true` inside `logger`, resulting in an exception in `tslogger.log_backtest_metrics` while constructing `metrics_df`.",
        "Issue_preprocessed_content":"Title: exception in backtest with when using ; Content: bug report program fails when backtest with is used inside . with everything is fine. exception happens in while constructing it can't make . exception was caught in , but it looks like this bug also appears in class. expected behavior no error. how to reproduce run backtest with wandlogger while setting . environment additional context checklist bug appears at the latest library version bug description added steps to reproduce added expected behavior added"
    },
    {
        "Issue_link":"https:\/\/github.com\/Visual-Behavior\/aloception-oss\/issues\/4",
        "Issue_title":"Use tensorboard as default logger and get wandb optional within the project ",
        "Issue_creation_time":1631000506000,
        "Issue_closed_time":1668696973000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":5.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: use tensorboard as default logger and get optional within the project ; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered challenges in using Tensorboard as the default logger and getting optional within the project.",
        "Issue_preprocessed_content":"Title: use tensorboard as default logger and get optional within the project ; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/virasad\/semantic_segmentation_service\/issues\/16",
        "Issue_title":"wandb logger",
        "Issue_creation_time":1653214451000,
        "Issue_closed_time":1653224504000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":null,
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: logger; Content: none",
        "Issue_original_content_gpt_summary":"The user encountered challenges in setting up a logger for their application.",
        "Issue_preprocessed_content":"Title: logger; Content: none"
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/478",
        "Issue_title":"Question: How to save hydra config to wandb config.yaml",
        "Issue_creation_time":1670626715000,
        "Issue_closed_time":1670781696000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Hello, \r\n\r\nI'm using `wandb` logger (and `csv` as well), I found recently `hydra` config no longer save to `wandb` 's`config.yaml` file.\r\nBefore:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.4\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.9.13\r\n    start_time: 1665409636.577166\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 13\r\n      - 23\r\n      4: 3.9.13\r\n      5: 0.13.4\r\n      8:\r\n      - 5\r\ncallbacks\/early_stopping\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.EarlyStopping\r\ncallbacks\/early_stopping\/check_finite:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/check_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/divergence_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/min_delta:\r\n  desc: null\r\n  value: 0.0\r\ncallbacks\/early_stopping\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/early_stopping\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/early_stopping\/patience:\r\n  desc: null\r\n  value: 100\r\ncallbacks\/early_stopping\/stopping_threshold:\r\n  desc: null\r\n  value: None\r\ncallbacks\/early_stopping\/strict:\r\n  desc: null\r\n  value: true\r\ncallbacks\/early_stopping\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.ModelCheckpoint\r\ncallbacks\/model_checkpoint\/auto_insert_metric_name:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/dirpath:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints\r\ncallbacks\/model_checkpoint\/every_n_epochs:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/every_n_train_steps:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/filename:\r\n  desc: null\r\n  value: epoch_{epoch:03d}\r\ncallbacks\/model_checkpoint\/mode:\r\n  desc: null\r\n  value: max\r\ncallbacks\/model_checkpoint\/monitor:\r\n  desc: null\r\n  value: val\/acc\r\ncallbacks\/model_checkpoint\/save_last:\r\n  desc: null\r\n  value: true\r\ncallbacks\/model_checkpoint\/save_on_train_epoch_end:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/save_top_k:\r\n  desc: null\r\n  value: 1\r\ncallbacks\/model_checkpoint\/save_weights_only:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_checkpoint\/train_time_interval:\r\n  desc: null\r\n  value: None\r\ncallbacks\/model_checkpoint\/verbose:\r\n  desc: null\r\n  value: false\r\ncallbacks\/model_summary\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichModelSummary\r\ncallbacks\/model_summary\/max_depth:\r\n  desc: null\r\n  value: -1\r\ncallbacks\/rich_progress_bar\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.callbacks.RichProgressBar\r\nckpt_path:\r\n  desc: null\r\n  value: None\r\ndatamodule\/_target_:\r\n  desc: null\r\n  value: src.datamodules.mnist_datamodule.MNISTDataModule\r\ndatamodule\/batch_size:\r\n  desc: null\r\n  value: 128\r\ndatamodule\/data_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/data\/\r\ndatamodule\/num_workers:\r\n  desc: null\r\n  value: 0\r\ndatamodule\/pin_memory:\r\n  desc: null\r\n  value: false\r\ndatamodule\/train_val_test_split:\r\n  desc: null\r\n  value:\r\n  - 55000\r\n  - 5000\r\n  - 10000\r\nextras\/enforce_tags:\r\n  desc: null\r\n  value: true\r\nextras\/ignore_warnings:\r\n  desc: null\r\n  value: false\r\nextras\/print_config:\r\n  desc: null\r\n  value: true\r\nmodel\/_target_:\r\n  desc: null\r\n  value: src.models.mnist_module.MNISTLitModule\r\nmodel\/net\/_target_:\r\n  desc: null\r\n  value: src.models.components.simple_dense_net.SimpleDenseNet\r\nmodel\/net\/input_size:\r\n  desc: null\r\n  value: 784\r\nmodel\/net\/lin1_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/lin2_size:\r\n  desc: null\r\n  value: 128\r\nmodel\/net\/lin3_size:\r\n  desc: null\r\n  value: 64\r\nmodel\/net\/output_size:\r\n  desc: null\r\n  value: 10\r\nmodel\/optimizer\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/optimizer\/_target_:\r\n  desc: null\r\n  value: torch.optim.Adam\r\nmodel\/optimizer\/lr:\r\n  desc: null\r\n  value: 0.001\r\nmodel\/optimizer\/weight_decay:\r\n  desc: null\r\n  value: 0.0\r\nmodel\/params\/non_trainable:\r\n  desc: null\r\n  value: 0\r\nmodel\/params\/total:\r\n  desc: null\r\n  value: 67978\r\nmodel\/params\/trainable:\r\n  desc: null\r\n  value: 67978\r\nmodel\/scheduler\/_partial_:\r\n  desc: null\r\n  value: true\r\nmodel\/scheduler\/_target_:\r\n  desc: null\r\n  value: torch.optim.lr_scheduler.ReduceLROnPlateau\r\nmodel\/scheduler\/factor:\r\n  desc: null\r\n  value: 0.1\r\nmodel\/scheduler\/mode:\r\n  desc: null\r\n  value: min\r\nmodel\/scheduler\/patience:\r\n  desc: null\r\n  value: 10\r\nseed:\r\n  desc: null\r\n  value: 123\r\ntags:\r\n  desc: null\r\n  value:\r\n  - dev\r\ntask_name:\r\n  desc: null\r\n  value: train\r\ntrainer\/_target_:\r\n  desc: null\r\n  value: pytorch_lightning.Trainer\r\ntrainer\/accelerator:\r\n  desc: null\r\n  value: cpu\r\ntrainer\/check_val_every_n_epoch:\r\n  desc: null\r\n  value: 1\r\ntrainer\/default_root_dir:\r\n  desc: null\r\n  value: \/Users\/caoyu\/Github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\r\ntrainer\/deterministic:\r\n  desc: null\r\n  value: false\r\ntrainer\/devices:\r\n  desc: null\r\n  value: 1\r\ntrainer\/max_epochs:\r\n  desc: null\r\n  value: 3\r\ntrainer\/min_epochs:\r\n  desc: null\r\n  value: 1\r\n```\r\nNow:\r\n```\r\nwandb_version: 1\r\n\r\n_wandb:\r\n  desc: null\r\n  value:\r\n    cli_version: 0.13.6\r\n    framework: lightning\r\n    is_jupyter_run: false\r\n    is_kaggle_kernel: false\r\n    m:\r\n    - 1: trainer\/global_step\r\n      6:\r\n      - 3\r\n    - 1: val\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: val\/acc_best\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: epoch\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: train\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/loss\r\n      5: 1\r\n      6:\r\n      - 1\r\n    - 1: test\/acc\r\n      5: 1\r\n      6:\r\n      - 1\r\n    python_version: 3.8.15\r\n    start_time: 1670583155.275978\r\n    t:\r\n      1:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      2:\r\n      - 1\r\n      - 9\r\n      - 41\r\n      - 50\r\n      - 55\r\n      3:\r\n      - 2\r\n      - 7\r\n      - 23\r\n      4: 3.8.15\r\n      5: 0.13.6\r\n      8:\r\n      - 5\r\n```\r\nThis may related to:\r\nhttps:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#L172\r\nAny idea how to restore to previous state?",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: question: how to save hydra config to config.yaml; Content: hello, i'm using `` logger (and `csv` as well), i found recently `hydra` config no longer save to `` 's`config.yaml` file. before: ``` _version: 1 _: desc: null value: cli_version: 0.13.4 framework: lightning is_jupyter_run: false is_kaggle_kernel: false m: - 1: trainer\/global_step 6: - 3 - 1: val\/loss 5: 1 6: - 1 - 1: val\/acc 5: 1 6: - 1 - 1: val\/acc_best 5: 1 6: - 1 - 1: epoch 5: 1 6: - 1 python_version: 3.9.13 start_time: 1665409636.577166 t: 1: - 1 - 9 - 41 - 50 - 55 2: - 1 - 9 - 41 - 50 - 55 3: - 2 - 7 - 13 - 23 4: 3.9.13 5: 0.13.4 8: - 5 callbacks\/early_stopping\/_target_: desc: null value: pytorch_lightning.callbacks.earlystopping callbacks\/early_stopping\/check_finite: desc: null value: true callbacks\/early_stopping\/check_on_train_epoch_end: desc: null value: none callbacks\/early_stopping\/divergence_threshold: desc: null value: none callbacks\/early_stopping\/min_delta: desc: null value: 0.0 callbacks\/early_stopping\/mode: desc: null value: max callbacks\/early_stopping\/monitor: desc: null value: val\/acc callbacks\/early_stopping\/patience: desc: null value: 100 callbacks\/early_stopping\/stopping_threshold: desc: null value: none callbacks\/early_stopping\/strict: desc: null value: true callbacks\/early_stopping\/verbose: desc: null value: false callbacks\/model_checkpoint\/_target_: desc: null value: pytorch_lightning.callbacks.modelcheckpoint callbacks\/model_checkpoint\/auto_insert_metric_name: desc: null value: false callbacks\/model_checkpoint\/dirpath: desc: null value: \/users\/caoyu\/github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15\/checkpoints callbacks\/model_checkpoint\/every_n_epochs: desc: null value: none callbacks\/model_checkpoint\/every_n_train_steps: desc: null value: none callbacks\/model_checkpoint\/filename: desc: null value: epoch_{epoch:03d} callbacks\/model_checkpoint\/mode: desc: null value: max callbacks\/model_checkpoint\/monitor: desc: null value: val\/acc callbacks\/model_checkpoint\/save_last: desc: null value: true callbacks\/model_checkpoint\/save_on_train_epoch_end: desc: null value: none callbacks\/model_checkpoint\/save_top_k: desc: null value: 1 callbacks\/model_checkpoint\/save_weights_only: desc: null value: false callbacks\/model_checkpoint\/train_time_interval: desc: null value: none callbacks\/model_checkpoint\/verbose: desc: null value: false callbacks\/model_summary\/_target_: desc: null value: pytorch_lightning.callbacks.richmodelsummary callbacks\/model_summary\/max_depth: desc: null value: -1 callbacks\/rich_progress_bar\/_target_: desc: null value: pytorch_lightning.callbacks.richprogressbar ckpt_path: desc: null value: none datamodule\/_target_: desc: null value: src.datamodules.mnist_datamodule.mnistdatamodule datamodule\/batch_size: desc: null value: 128 datamodule\/data_dir: desc: null value: \/users\/caoyu\/github\/lightning-hydra-template\/data\/ datamodule\/num_workers: desc: null value: 0 datamodule\/pin_memory: desc: null value: false datamodule\/train_val_test_split: desc: null value: - 55000 - 5000 - 10000 extras\/enforce_tags: desc: null value: true extras\/ignore_warnings: desc: null value: false extras\/print_config: desc: null value: true model\/_target_: desc: null value: src.models.mnist_module.mnistlitmodule model\/net\/_target_: desc: null value: src.models.components.simple_dense_net.simpledensenet model\/net\/input_size: desc: null value: 784 model\/net\/lin1_size: desc: null value: 64 model\/net\/lin2_size: desc: null value: 128 model\/net\/lin3_size: desc: null value: 64 model\/net\/output_size: desc: null value: 10 model\/optimizer\/_partial_: desc: null value: true model\/optimizer\/_target_: desc: null value: torch.optim.adam model\/optimizer\/lr: desc: null value: 0.001 model\/optimizer\/weight_decay: desc: null value: 0.0 model\/params\/non_trainable: desc: null value: 0 model\/params\/total: desc: null value: 67978 model\/params\/trainable: desc: null value: 67978 model\/scheduler\/_partial_: desc: null value: true model\/scheduler\/_target_: desc: null value: torch.optim.lr_scheduler.reducelronplateau model\/scheduler\/factor: desc: null value: 0.1 model\/scheduler\/mode: desc: null value: min model\/scheduler\/patience: desc: null value: 10 seed: desc: null value: 123 tags: desc: null value: - dev task_name: desc: null value: train trainer\/_target_: desc: null value: pytorch_lightning.trainer trainer\/accelerator: desc: null value: cpu trainer\/check_val_every_n_epoch: desc: null value: 1 trainer\/default_root_dir: desc: null value: \/users\/caoyu\/github\/lightning-hydra-template\/logs\/train\/runs\/2022-10-10_14-47-15 trainer\/deterministic: desc: null value: false trainer\/devices: desc: null value: 1 trainer\/max_epochs: desc: null value: 3 trainer\/min_epochs: desc: null value: 1 ``` now: ``` _version: 1 _: desc: null value: cli_version: 0.13.6 framework: lightning is_jupyter_run: false is_kaggle_kernel: false m: - 1: trainer\/global_step 6: - 3 - 1: val\/loss 5: 1 6: - 1 - 1: val\/acc 5: 1 6: - 1 - 1: val\/acc_best 5: 1 6: - 1 - 1: epoch 5: 1 6: - 1 - 1: train\/loss 5: 1 6: - 1 - 1: train\/acc 5: 1 6: - 1 - 1: test\/loss 5: 1 6: - 1 - 1: test\/acc 5: 1 6: - 1 python_version: 3.8.15 start_time: 1670583155.275978 t: 1: - 1 - 9 - 41 - 50 - 55 2: - 1 - 9 - 41 - 50 - 55 3: - 2 - 7 - 23 4: 3.8.15 5: 0.13.6 8: - 5 ``` this may related to: https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/16fb9a6a807d278d1797ce4dedc885c7e5e1b7fb\/src\/utils\/utils.py#l172 any idea how to restore to previous state?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with saving Hydra config to config.yaml, and was looking for a way to restore to the previous state.",
        "Issue_preprocessed_content":"Title: question how to save hydra config to ; Content: hello, i'm using logger , i found recently config no longer save to file. before now this may related to any idea how to restore to previous state?"
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/362",
        "Issue_title":"hydra-optuna-sweeper and wandb versions conflict",
        "Issue_creation_time":1656590728000,
        "Issue_closed_time":1657912525000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi!\r\n\r\nI have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml):\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example\r\n``` \r\nI faced 2 problems:\r\n\r\n# 1. hydra-optuna-sweeper problem\r\n\r\nI got the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report\r\n    return func()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in <lambda>\r\n    lambda: hydra.multirun(\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun\r\n    ret = sweeper.sweep(arguments=task_overrides)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep\r\n    return self.sweeper.sweep(arguments)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep\r\n    assert self.search_space is None\r\nAssertionError\r\n```\r\nThe same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253).\r\n\r\nFile [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper:\r\n```\r\n# --------- hydra --------- #\r\nhydra-core>=1.1.0\r\nhydra-colorlog>=1.1.0\r\nhydra-optuna-sweeper>=1.1.0\r\n```\r\nBut the latest versions of the packages are installing:\r\n```\r\nhydra-colorlog==1.2.0\r\nhydra-core==1.2.0\r\nhydra-optuna-sweeper==1.2.0\r\n```\r\n\r\nIf I understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. When I change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)):\r\n```yaml\r\nhydra:\r\n  sweeper:\r\n    ...\r\n    params:\r\n      datamodule.batch_size: choice(32,64,128)\r\n      model.lr: interval(0.0001, 0.2)\r\n      model.net.lin1_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin2_size: choice(32, 64, 128, 256, 512)\r\n      model.net.lin3_size: choice(32, 64, 128, 256, 512)\r\n```\r\neverything works without errors.\r\n\r\n# 2. wandb problem\r\nAfter the command `pip install -r requrements.txt` wandb==0.12.20 was installed.\r\nWhen running the training process with this logger:\r\n```\r\ntrain.py -m hparams_search=mnist_optuna experiment=example logger=wandb\r\n```\r\nThe first run with the certian parameters combination finished successfully, the second run had the error:\r\n\r\n```\r\nException in thread StreamThr:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 40, in run\r\n    self._target(**self._kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 85, in wandb_internal\r\n    configure_logging(_settings.log_internal, _settings._log_level)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\internal\\internal.py\", line 189, in configure_logging\r\n    log_handler = logging.FileHandler(log_fname)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__\r\n    StreamHandler.__init__(self, self._open())\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open\r\n    return open(self.baseFilename, self.mode, encoding=self.encoding,\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\yusip\\\\Desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\wandb\\\\run-2022\r\n0630_143648-2vxuij78\\\\logs\\\\debug-internal.log'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\__main__.py\", line 3, in <module>\r\n    cli.cli(prog_name=\"python -m wandb\")\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 96, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\cli\\cli.py\", line 285, in service\r\n    server.serve()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\server.py\", line 140, in serve\r\n    mux.loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 332, in loop\r\n    raise e\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 330, in loop\r\n    self._loop()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 323, in _loop\r\n    self._process_action(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 288, in _process_action\r\n    self._process_add(action)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 208, in _process_add\r\n    stream.start_thread(thread)\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 68, in start_thread\r\n    self._wait_thread_active()\r\n  File \"C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\wandb\\sdk\\service\\streams.py\", line 73, in _wait_thread_active\r\n    assert result\r\nAssertionError\r\nProblem at: C:\\Users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py 357 experiment\r\nwandb: ERROR Error communicating with wandb process\r\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\r\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\r\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\r\nError executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op\r\ntuna', 'experiment=example', 'logger=wandb']\r\nError in call to target 'pytorch_lightning.loggers.wandb.WandbLogger':\r\nUsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: htt\r\nps:\/\/docs.wandb.ai\/library\/init#init-start-error\")\r\nfull_key: logger.wandb\r\n```\r\nIt is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.\r\n\r\n\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: hydra-optuna-sweeper and versions conflict; Content: hi! i have installed all required packages by `pip install -r requrements.txt` and tried to run hyperparametric search using the [file](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/configs\/hparams_search\/mnist_optuna.yaml): ``` train.py -m hparams_search=mnist_optuna experiment=example ``` i faced 2 problems: # 1. hydra-optuna-sweeper problem i got the following error: ``` traceback (most recent call last): file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 213, in run_and_report return func() file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\utils.py\", line 461, in lambda: hydra.multirun( file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra\\_internal\\hydra.py\", line 162, in multirun ret = sweeper.sweep(arguments=task_overrides) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\optuna_sweeper.py\", line 52, in sweep return self.sweeper.sweep(arguments) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\hydra_plugins\\hydra_optuna_sweeper\\_impl.py\", line 289, in sweep assert self.search_space is none assertionerror ``` the same error was reported in [this issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253). file [requrements.txt](https:\/\/github.com\/ashleve\/lightning-hydra-template\/blob\/main\/requirements.txt) contains the following versions for hydra-optuna-sweeper: ``` # --------- hydra --------- # hydra-core>=1.1.0 hydra-colorlog>=1.1.0 hydra-optuna-sweeper>=1.1.0 ``` but the latest versions of the packages are installing: ``` hydra-colorlog==1.2.0 hydra-core==1.2.0 hydra-optuna-sweeper==1.2.0 ``` if i understand correctly, optuna sweeper's syntax has changed in hydra since version 1.2.0. when i change the syntax to the new version (as it was in mentioned above [issue](https:\/\/github.com\/facebookresearch\/hydra\/issues\/2253)): ```yaml hydra: sweeper: ... params: datamodule.batch_size: choice(32,64,128) model.lr: interval(0.0001, 0.2) model.net.lin1_size: choice(32, 64, 128, 256, 512) model.net.lin2_size: choice(32, 64, 128, 256, 512) model.net.lin3_size: choice(32, 64, 128, 256, 512) ``` everything works without errors. # 2. problem after the command `pip install -r requrements.txt` ==0.12.20 was installed. when running the training process with this logger: ``` train.py -m hparams_search=mnist_optuna experiment=example logger= ``` the first run with the certian parameters combination finished successfully, the second run had the error: ``` exception in thread streamthr: traceback (most recent call last): file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\threading.py\", line 973, in _bootstrap_inner self.run() file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 40, in run self._target(**self._kwargs) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\internal\\internal.py\", line 85, in _internal configure_logging(_settings.log_internal, _settings._log_level) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\internal\\internal.py\", line 189, in configure_logging log_handler = logging.filehandler(log_fname) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1146, in __init__ streamhandler.__init__(self, self._open()) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\logging\\__init__.py\", line 1175, in _open return open(self.basefilename, self.mode, encoding=self.encoding, filenotfounderror: [errno 2] no such file or directory: 'c:\\\\users\\\\yusip\\\\desktop\\\\lightning-hydra-template-main\\\\logs\\\\experiments\\\\multiruns\\\\simple_dense_net\\\\2022-06-30_14-36-03\\\\0\\\\\\\\run-2022 0630_143648-2vxuij78\\\\logs\\\\debug-internal.log' traceback (most recent call last): file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 197, in _run_module_as_main return _run_code(code, main_globals, none, file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\runpy.py\", line 87, in _run_code exec(code, run_globals) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\__main__.py\", line 3, in cli.cli(prog_name=\"python -m \") file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1130, in __call__ return self.main(*args, **kwargs) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1055, in main rv = self.invoke(ctx) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1657, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 1404, in invoke return ctx.invoke(self.callback, **ctx.params) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\click\\core.py\", line 760, in invoke return __callback(*args, **kwargs) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\cli\\cli.py\", line 96, in wrapper return func(*args, **kwargs) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\cli\\cli.py\", line 285, in service server.serve() file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\server.py\", line 140, in serve mux.loop() file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 332, in loop raise e file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 330, in loop self._loop() file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 323, in _loop self._process_action(action) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 288, in _process_action self._process_add(action) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 208, in _process_add stream.start_thread(thread) file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 68, in start_thread self._wait_thread_active() file \"c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\\\sdk\\service\\streams.py\", line 73, in _wait_thread_active assert result assertionerror problem at: c:\\users\\yusip\\anaconda3\\envs\\py39\\lib\\site-packages\\pytorch_lightning\\loggers\\.py 357 experiment : error error communicating with process : error try: .init(settings=.settings(start_method='fork')) : error or: .init(settings=.settings(start_method='thread')) : error for more info see: https:\/\/docs..ai\/library\/init#init-start-error error executing job with overrides: ['datamodule.batch_size=32', 'model.lr=0.09357304154313738', 'model.net.lin1_size=256', 'model.net.lin2_size=512', 'model.net.lin3_size=256', 'hparams_search=mnist_op tuna', 'experiment=example', 'logger='] error in call to target 'pytorch_lightning.loggers..logger': usageerror(\"error communicating with process\\ntry: .init(settings=.settings(start_method='fork'))\\nor: .init(settings=.settings(start_method='thread'))\\nfor more info see: htt ps:\/\/docs..ai\/library\/init#init-start-error\") full_key: logger. ``` it is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error.",
        "Issue_original_content_gpt_summary":"The user encountered two challenges when attempting to run a hyperparametric search using hydra-optuna-sweeper: a conflict between the installed versions of the packages and an error when initializing the logger.",
        "Issue_preprocessed_content":"Title: and versions conflict; Content: hi! i have installed all required packages by and tried to run hyperparametric search using the i faced problems . problem i got the following error the same error was reported in . file contains the following versions for but the latest versions of the packages are installing if i understand correctly, optuna sweeper's syntax has changed in hydra since version when i change the syntax to the new version everything works without errors. . problem after the command was installed. when running the training process with this logger the first run with the certian parameters combination finished successfully, the second run had the error it is not clear, which parameters should be passed to pytorch lighting wrapper when initializinig this logger, to avoid this error."
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/328",
        "Issue_title":"wandb logger not working",
        "Issue_creation_time":1654326486000,
        "Issue_closed_time":1654420353000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":2.0,
        "Issue_body":"Hi there, \r\nthank you for this powerful template! \r\nI run into a problem while trying to use wandb as logger\r\nI used the wandb-callbacks branch and after `python train.py logger=wandb` i get (cancelled by user after 130 iterations cause wandb login does not appear)\r\n\r\n````\r\n$ python train.py logger=wandb\r\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502    \u2502 Name          \u2502 Type             \u2502 Params \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502 0  \u2502 model         \u2502 SimpleDenseNet   \u2502  336 K \u2502\r\n\u2502 1  \u2502 model.model   \u2502 Sequential       \u2502  336 K \u2502\r\n\u2502 2  \u2502 model.model.0 \u2502 Linear           \u2502  200 K \u2502\r\n\u2502 3  \u2502 model.model.1 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 4  \u2502 model.model.2 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 5  \u2502 model.model.3 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 6  \u2502 model.model.4 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 7  \u2502 model.model.5 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 8  \u2502 model.model.6 \u2502 Linear           \u2502 65.8 K \u2502\r\n\u2502 9  \u2502 model.model.7 \u2502 BatchNorm1d      \u2502    512 \u2502\r\n\u2502 10 \u2502 model.model.8 \u2502 ReLU             \u2502      0 \u2502\r\n\u2502 11 \u2502 model.model.9 \u2502 Linear           \u2502  2.6 K \u2502\r\n\u2502 12 \u2502 criterion     \u2502 CrossEntropyLoss \u2502      0 \u2502\r\n\u2502 13 \u2502 train_acc     \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 14 \u2502 val_acc       \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 15 \u2502 test_acc      \u2502 Accuracy         \u2502      0 \u2502\r\n\u2502 16 \u2502 val_acc_best  \u2502 MaxMetric        \u2502      0 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\nTrainable params: 336 K\r\nNon-trainable params: 0\r\nTotal params: 336 K\r\nTotal estimated model params size (MB): 1\r\nEpoch 0    ----- ---------------------------------- 130\/939 0:00:04 \u2022 0:00:28 29.28it\/s loss: 0.252\r\nError executing job with overrides: ['logger=wandb']\r\n````\r\n_(Note the last line)_\r\n\r\nChanging `logger: wandb` in train.yaml does not work either. I'm a bit confused because i had it working once before but just don't know what to do anymore. I tried out different conda envs with different torch and pl versions. Does anyboady have an idea?\r\n\r\n\r\n**pip list**\r\n```\r\nPackage                 Version\r\n----------------------- ------------\r\nabsl-py                 1.1.0\r\naiohttp                 3.8.1\r\naiosignal               1.2.0\r\nalembic                 1.8.0\r\nantlr4-python3-runtime  4.8\r\nanyio                   3.6.1\r\nargon2-cffi             21.3.0\r\nargon2-cffi-bindings    21.2.0\r\nasttokens               2.0.5\r\nasync-timeout           4.0.2\r\natomicwrites            1.4.0\r\nattrs                   21.4.0\r\nautopage                0.5.1\r\nBabel                   2.10.1\r\nbackcall                0.2.0\r\nbeautifulsoup4          4.11.1\r\nblack                   22.3.0\r\nbleach                  5.0.0\r\ncachetools              5.2.0\r\ncertifi                 2022.5.18.1\r\ncffi                    1.15.0\r\ncfgv                    3.3.1\r\ncharset-normalizer      2.0.12\r\nclick                   8.1.3\r\ncliff                   3.10.1\r\ncmaes                   0.8.2\r\ncmd2                    2.4.1\r\ncolorama                0.4.4\r\ncolorlog                6.6.0\r\ncommonmark              0.9.1\r\ncycler                  0.11.0\r\ndebugpy                 1.6.0\r\ndecorator               5.1.1\r\ndefusedxml              0.7.1\r\ndistlib                 0.3.4\r\ndocker-pycreds          0.4.0\r\nentrypoints             0.4\r\nexecuting               0.8.3\r\nfastjsonschema          2.15.3\r\nfilelock                3.7.1\r\nflake8                  4.0.1\r\nfonttools               4.33.3\r\nfrozenlist              1.3.0\r\nfsspec                  2022.5.0\r\ngitdb                   4.0.9\r\nGitPython               3.1.27\r\ngoogle-auth             2.6.6\r\ngoogle-auth-oauthlib    0.4.6\r\ngreenlet                1.1.2\r\ngrpcio                  1.46.3\r\nhydra-colorlog          1.2.0\r\nhydra-core              1.1.0\r\nhydra-optuna-sweeper    1.2.0\r\nidentify                2.5.1\r\nidna                    3.3\r\nimportlib-metadata      4.11.4\r\nimportlib-resources     5.7.1\r\niniconfig               1.1.1\r\nipykernel               6.13.0\r\nipython                 8.4.0\r\nipython-genutils        0.2.0\r\nisort                   5.10.1\r\njedi                    0.18.1\r\nJinja2                  3.1.2\r\njoblib                  1.1.0\r\njson5                   0.9.8\r\njsonschema              4.6.0\r\njupyter-client          7.3.1\r\njupyter-core            4.10.0\r\njupyter-server          1.17.0\r\njupyterlab              3.4.2\r\njupyterlab-pygments     0.2.2\r\njupyterlab-server       2.14.0\r\nkiwisolver              1.4.2\r\nMako                    1.2.0\r\nMarkdown                3.3.7\r\nMarkupSafe              2.1.1\r\nmatplotlib              3.5.2\r\nmatplotlib-inline       0.1.3\r\nmccabe                  0.6.1\r\nmistune                 0.8.4\r\nmultidict               6.0.2\r\nmypy-extensions         0.4.3\r\nnbclassic               0.3.7\r\nnbclient                0.6.4\r\nnbconvert               6.5.0\r\nnbformat                5.4.0\r\nnest-asyncio            1.5.5\r\nnodeenv                 1.6.0\r\nnotebook                6.4.11\r\nnotebook-shim           0.1.0\r\nnumpy                   1.22.4\r\noauthlib                3.2.0\r\nomegaconf               2.1.2\r\noptuna                  2.10.0\r\npackaging               21.3\r\npandas                  1.4.2\r\npandocfilters           1.5.0\r\nparso                   0.8.3\r\npathspec                0.9.0\r\npathtools               0.1.2\r\npbr                     5.9.0\r\npickleshare             0.7.5\r\nPillow                  9.1.1\r\npip                     21.2.2\r\nplatformdirs            2.5.2\r\npluggy                  1.0.0\r\npre-commit              2.19.0\r\nprettytable             3.3.0\r\nprometheus-client       0.14.1\r\npromise                 2.3\r\nprompt-toolkit          3.0.29\r\nprotobuf                3.20.1\r\npsutil                  5.9.1\r\npudb                    2022.1.1\r\npure-eval               0.2.2\r\npy                      1.11.0\r\npyasn1                  0.4.8\r\npyasn1-modules          0.2.8\r\npycodestyle             2.8.0\r\npycparser               2.21\r\npyDeprecate             0.3.2\r\npyflakes                2.4.0\r\nPygments                2.12.0\r\npyparsing               3.0.9\r\npyperclip               1.8.2\r\npyreadline3             3.4.1\r\npyrsistent              0.18.1\r\npytest                  7.1.2\r\npython-dateutil         2.8.2\r\npython-dotenv           0.20.0\r\npytorch-lightning       1.6.4\r\npytz                    2022.1\r\npywin32                 304\r\npywinpty                2.0.5\r\nPyYAML                  6.0\r\npyzmq                   23.1.0\r\nrequests                2.27.1\r\nrequests-oauthlib       1.3.1\r\nrich                    12.4.4\r\nrsa                     4.8\r\nscikit-learn            1.1.1\r\nscipy                   1.8.1\r\nseaborn                 0.11.2\r\nSend2Trash              1.8.0\r\nsentry-sdk              1.5.12\r\nsetproctitle            1.2.3\r\nsetuptools              61.2.0\r\nsh                      1.14.2\r\nshortuuid               1.0.9\r\nsix                     1.16.0\r\nsmmap                   5.0.0\r\nsniffio                 1.2.0\r\nsoupsieve               2.3.2.post1\r\nSQLAlchemy              1.4.37\r\nstack-data              0.2.0\r\nstevedore               3.5.0\r\ntensorboard             2.9.0\r\ntensorboard-data-server 0.6.1\r\ntensorboard-plugin-wit  1.8.1\r\nterminado               0.15.0\r\nthreadpoolctl           3.1.0\r\ntinycss2                1.1.1\r\ntoml                    0.10.2\r\ntomli                   2.0.1\r\ntorch                   1.11.0+cu113\r\ntorchaudio              0.11.0+cu113\r\ntorchmetrics            0.9.0\r\ntorchvision             0.12.0+cu113\r\ntornado                 6.1\r\ntqdm                    4.64.0\r\ntraitlets               5.2.2.post1\r\ntyping_extensions       4.2.0\r\nurllib3                 1.26.9\r\nurwid                   2.1.2\r\nurwid-readline          0.13\r\nvirtualenv              20.14.1\r\nwandb                   0.12.17\r\nwcwidth                 0.2.5\r\nwebencodings            0.5.1\r\nwebsocket-client        1.3.2\r\nWerkzeug                2.1.2\r\nwheel                   0.37.1\r\nwincertstore            0.2\r\nyarl                    1.7.2\r\nzipp                    3.8.0\r\n```",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: logger not working; Content: hi there, thank you for this powerful template! i run into a problem while trying to use as logger i used the -callbacks branch and after `python train.py logger=` i get (cancelled by user after 130 iterations cause login does not appear) ```` $ python train.py logger= name type params 0 model simpledensenet 336 k 1 model.model sequential 336 k 2 model.model.0 linear 200 k 3 model.model.1 batchnorm1d 512 4 model.model.2 relu 0 5 model.model.3 linear 65.8 k 6 model.model.4 batchnorm1d 512 7 model.model.5 relu 0 8 model.model.6 linear 65.8 k 9 model.model.7 batchnorm1d 512 10 model.model.8 relu 0 11 model.model.9 linear 2.6 k 12 criterion crossentropyloss 0 13 train_acc accuracy 0 14 val_acc accuracy 0 15 test_acc accuracy 0 16 val_acc_best maxmetric 0 trainable params: 336 k non-trainable params: 0 total params: 336 k total estimated model params size (mb): 1 epoch 0 ----- ---------------------------------- 130\/939 0:00:04 0:00:28 29.28it\/s loss: 0.252 error executing job with overrides: ['logger='] ```` _(note the last line)_ changing `logger: ` in train.yaml does not work either. i'm a bit confused because i had it working once before but just don't know what to do anymore. i tried out different conda envs with different torch and pl versions. does anyboady have an idea? **pip list** ``` package version ----------------------- ------------ absl-py 1.1.0 aiohttp 3.8.1 aiosignal 1.2.0 alembic 1.8.0 antlr4-python3-runtime 4.8 anyio 3.6.1 argon2-cffi 21.3.0 argon2-cffi-bindings 21.2.0 asttokens 2.0.5 async-timeout 4.0.2 atomicwrites 1.4.0 attrs 21.4.0 autopage 0.5.1 babel 2.10.1 backcall 0.2.0 beautifulsoup4 4.11.1 black 22.3.0 bleach 5.0.0 cachetools 5.2.0 certifi 2022.5.18.1 cffi 1.15.0 cfgv 3.3.1 charset-normalizer 2.0.12 click 8.1.3 cliff 3.10.1 cmaes 0.8.2 cmd2 2.4.1 colorama 0.4.4 colorlog 6.6.0 commonmark 0.9.1 cycler 0.11.0 debugpy 1.6.0 decorator 5.1.1 defusedxml 0.7.1 distlib 0.3.4 docker-pycreds 0.4.0 entrypoints 0.4 executing 0.8.3 fastjsonschema 2.15.3 filelock 3.7.1 flake8 4.0.1 fonttools 4.33.3 frozenlist 1.3.0 fsspec 2022.5.0 gitdb 4.0.9 gitpython 3.1.27 google-auth 2.6.6 google-auth-oauthlib 0.4.6 greenlet 1.1.2 grpcio 1.46.3 hydra-colorlog 1.2.0 hydra-core 1.1.0 hydra-optuna-sweeper 1.2.0 identify 2.5.1 idna 3.3 importlib-metadata 4.11.4 importlib-resources 5.7.1 iniconfig 1.1.1 ipykernel 6.13.0 ipython 8.4.0 ipython-genutils 0.2.0 isort 5.10.1 jedi 0.18.1 jinja2 3.1.2 joblib 1.1.0 json5 0.9.8 jsonschema 4.6.0 jupyter-client 7.3.1 jupyter-core 4.10.0 jupyter-server 1.17.0 jupyterlab 3.4.2 jupyterlab-pygments 0.2.2 jupyterlab-server 2.14.0 kiwisolver 1.4.2 mako 1.2.0 markdown 3.3.7 markupsafe 2.1.1 matplotlib 3.5.2 matplotlib-inline 0.1.3 mccabe 0.6.1 mistune 0.8.4 multidict 6.0.2 mypy-extensions 0.4.3 nbclassic 0.3.7 nbclient 0.6.4 nbconvert 6.5.0 nbformat 5.4.0 nest-asyncio 1.5.5 nodeenv 1.6.0 notebook 6.4.11 notebook-shim 0.1.0 numpy 1.22.4 oauthlib 3.2.0 omegaconf 2.1.2 optuna 2.10.0 packaging 21.3 pandas 1.4.2 pandocfilters 1.5.0 parso 0.8.3 pathspec 0.9.0 pathtools 0.1.2 pbr 5.9.0 pickleshare 0.7.5 pillow 9.1.1 pip 21.2.2 platformdirs 2.5.2 pluggy 1.0.0 pre-commit 2.19.0 prettytable 3.3.0 prometheus-client 0.14.1 promise 2.3 prompt-toolkit 3.0.29 protobuf 3.20.1 psutil 5.9.1 pudb 2022.1.1 pure-eval 0.2.2 py 1.11.0 pyasn1 0.4.8 pyasn1-modules 0.2.8 pycodestyle 2.8.0 pycparser 2.21 pydeprecate 0.3.2 pyflakes 2.4.0 pygments 2.12.0 pyparsing 3.0.9 pyperclip 1.8.2 pyreadline3 3.4.1 pyrsistent 0.18.1 pytest 7.1.2 python-dateutil 2.8.2 python-dotenv 0.20.0 pytorch-lightning 1.6.4 pytz 2022.1 pywin32 304 pywinpty 2.0.5 pyyaml 6.0 pyzmq 23.1.0 requests 2.27.1 requests-oauthlib 1.3.1 rich 12.4.4 rsa 4.8 scikit-learn 1.1.1 scipy 1.8.1 seaborn 0.11.2 send2trash 1.8.0 sentry-sdk 1.5.12 setproctitle 1.2.3 setuptools 61.2.0 sh 1.14.2 shortuuid 1.0.9 six 1.16.0 smmap 5.0.0 sniffio 1.2.0 soupsieve 2.3.2.post1 sqlalchemy 1.4.37 stack-data 0.2.0 stevedore 3.5.0 tensorboard 2.9.0 tensorboard-data-server 0.6.1 tensorboard-plugin-wit 1.8.1 terminado 0.15.0 threadpoolctl 3.1.0 tinycss2 1.1.1 toml 0.10.2 tomli 2.0.1 torch 1.11.0+cu113 torchaudio 0.11.0+cu113 torchmetrics 0.9.0 torchvision 0.12.0+cu113 tornado 6.1 tqdm 4.64.0 traitlets 5.2.2.post1 typing_extensions 4.2.0 urllib3 1.26.9 urwid 2.1.2 urwid-readline 0.13 virtualenv 20.14.1 0.12.17 wcwidth 0.2.5 webencodings 0.5.1 websocket-client 1.3.2 werkzeug 2.1.2 wheel 0.37.1 wincertstore 0.2 yarl 1.7.2 zipp 3.8.0 ```",
        "Issue_original_content_gpt_summary":"The user encountered a challenge while trying to use a logger with the -callbacks branch, resulting in an error after 130 iterations.",
        "Issue_preprocessed_content":"Title: logger not working; Content: hi there, thank you for this powerful template! i run into a problem while trying to use as logger i used the branch and after i get logger `"
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/289",
        "Issue_title":"wandb log only 1 run when using ddp and multirun",
        "Issue_creation_time":1651909943000,
        "Issue_closed_time":1657910798000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"When I use DDP, wandb and multirun in `test.py` like this \r\n`python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=wandb`\r\nWandb does not record 3 runs, but only one run.\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: log only 1 run when using ddp and multirun; Content: when i use ddp, and multirun in `test.py` like this `python test.py -m ckpt_path='~~' +seed=1,2,3 +trainer.strategy=ddp logger=` does not record 3 runs, but only one run.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to log multiple runs when using distributed data parallelism (DDP) and multirun in a Python script.",
        "Issue_preprocessed_content":"Title: log only run when using ddp and multirun; Content: when i use ddp, and multirun in like this does not record runs, but only one run."
    },
    {
        "Issue_link":"https:\/\/github.com\/ashleve\/lightning-hydra-template\/issues\/285",
        "Issue_title":"Wandb is not compatible with PL 1.6.1",
        "Issue_creation_time":1650933722000,
        "Issue_closed_time":1654689077000,
        "Issue_upvote_count":1,
        "Issue_downvote_count":0,
        "Issue_comment_count":3.0,
        "Issue_body":"Hi,\r\n\r\nThere may be version conflict between wandb and PL 1.6.1\r\n\r\n**OS:** Ubuntu20.04\r\n**Python:** 3.8.13\r\n**Pytorch:**  1.11.0\r\n**PL:** 1.6.1\r\n**Wandb:** 0.12.11\r\n**hydra-core:** 1.1.2\r\n\r\nwhen I use the Hyperparameter Search, it produces the following error:\r\n\r\n```python\r\nFileNotFoundError: [Errno 2] No such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/wandb\/offline-run-20*\/logs\/debug-internal.log'\r\nProblem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py 357 experiment\r\n```\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: is not compatible with pl 1.6.1; Content: hi, there may be version conflict between and pl 1.6.1 **os:** ubuntu20.04 **python:** 3.8.13 **pytorch:** 1.11.0 **pl:** 1.6.1 **:** 0.12.11 **hydra-core:** 1.1.2 when i use the hyperparameter search, it produces the following error: ```python filenotfounderror: [errno 2] no such file or directory: '\/**\/logs\/experiments\/multiruns\/**\/time\/0\/\/offline-run-20*\/logs\/debug-internal.log' problem at: \/home\/*\/anaconda3\/envs\/*\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py 357 experiment ```",
        "Issue_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to use a hyperparameter search with incompatible versions of os, python, pytorch, pl, and hydra-core.",
        "Issue_preprocessed_content":"Title: is not compatible with pl ; Content: hi, there may be version conflict between and pl os python pytorch pl when i use the hyperparameter search, it produces the following error"
    },
    {
        "Issue_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/316",
        "Issue_title":"WandB fails when config is too large",
        "Issue_creation_time":1666171234000,
        "Issue_closed_time":1666770135000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"I tried to run benchmark.py, with WandB, but got an error because the config is too large, probably due to the train_selection array being too big. `ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)`\r\n\r\nPerhaps the data selections does not need to be uploaded to WandB?\r\n\r\nThe full message is: \r\n```(graphnet) [peter@hep04 northern_tracks]$ python benchmark.py \r\ngraphnet: INFO     2022-10-19 10:33:19 - get_logger - Writing log to logs\/graphnet_20221019-103308.log\r\ngraphnet: WARNING  2022-10-19 10:33:25 - warn_once - `icecube` not available. Some functionality may be missing.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: wandb version 0.13.4 is available!  To upgrade, please run:\r\nwandb:  $ pip install wandb --upgrade\r\nwandb: Tracking run with wandb version 0.13.1\r\nwandb: Run data is saved locally in .\/wandb\/wandb\/run-20221019_103334-47u9ascy\r\nwandb: Run `wandb offline` to turn off syncing.\r\nwandb: Syncing run woven-water-2\r\nwandb: \u2b50\ufe0f View project at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\r\nwandb: \ud83d\ude80 View run at https:\/\/wandb.ai\/graphnet-team\/NortherenTracks_Benchmark\/runs\/47u9ascy\r\nwandb: WARNING Serializing object of type list that is 14743672 bytes\r\nwandb: WARNING Serializing object of type list that is 4914592 bytes\r\nwandb: WARNING Serializing object of type list that is 4914600 bytes\r\nwandb: WARNING Serializing object of type list that is 15673400 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\nwandb: WARNING Serializing object of type list that is 5429640 bytes\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - features: ['dom_x', 'dom_y', 'dom_z', 'dom_time', 'charge', 'rde', 'pmt_area']\r\ngraphnet: INFO     2022-10-19 10:33:54 - train - truth: ['energy', 'energy_track', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith', 'pid', 'elasticity', 'sim_type', 'interaction_type', 'interaction_time', 'inelasticity']\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\ngraphnet: WARNING  2022-10-19 10:33:54 - SQLiteDataset._remove_missing_columns - Removing the following (missing) truth variables: interaction_time\r\n\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/core\/lightning.py:22: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\r\n  rank_zero_deprecation(\r\nGPU available: True (cuda), used: True\r\nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\r\n\r\n  | Name      | Type            | Params\r\n----------------------------------------------\r\n0 | _detector | IceCubeDeepCore | 0     \r\n1 | _gnn      | DynEdge         | 1.3 M \r\n2 | _tasks    | ModuleList      | 258   \r\n----------------------------------------------\r\n1.3 M     Trainable params\r\n0         Non-trainable params\r\n1.3 M     Total params\r\n5.376     Total estimated model params size (MB)\r\nEpoch  0:   0%|                                                                                                            | 0\/4800 [00:00<?, ? batch(es)\/s]wandb: ERROR Error while calling W&B API: run config cannot exceed 15 MB (<Response [400]>)\r\nThread SenderThread:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 25, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 1465, in upsert_run\r\n    response = self.gql(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/retry.py\", line 113, in __call__\r\n    result = self._call_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_api.py\", line 204, in execute\r\n    return self.client.execute(*args, **kwargs)  # type: ignore\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 52, in execute\r\n    result = self._get_result(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/client.py\", line 60, in _get_result\r\n    return self.transport.execute(document, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/vendor\/gql-0.2.0\/wandb_gql\/transport\/requests.py\", line 39, in execute\r\n    request.raise_for_status()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/api.wandb.ai\/graphql\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 51, in run\r\n    self._run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 95, in _run\r\n    self._debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 316, in _debounce\r\n    self._sm.debounce()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 387, in debounce\r\n    self._debounce_config()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 393, in _debounce_config\r\n    self._api.upsert_run(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/apis\/normalize.py\", line 27, in wrapper\r\n    raise CommError(err.response, err)\r\nwandb.errors.CommError: <Response [400]>\r\nwandb: ERROR Internal wandb error: file data was not synced\r\nEpoch  0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4800\/4800 [09:03<00:00,  8.83 batch(es)\/s, loss=-1.22]Traceback (most recent call last):\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1200\/1200 [01:17<00:00, 15.53 batch(es)\/s]\r\n  File \"benchmark.py\", line 204, in <module>\r\n    main()\r\n  File \"benchmark.py\", line 200, in main\r\n    train(config)\r\n  File \"benchmark.py\", line 142, in train\r\n    trainer.fit(model, training_dataloader, validation_dataloader)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 696, in fit\r\n    self._call_and_handle_interrupt(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 650, in _call_and_handle_interrupt\r\n    return trainer_fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 735, in _fit_impl\r\n    results = self._run(model, ckpt_path=self.ckpt_path)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1166, in _run\r\n    results = self._run_stage()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1252, in _run_stage\r\n    return self._run_train()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/trainer.py\", line 1283, in _run_train\r\n    self.fit_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 200, in run\r\n    self.advance(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/fit_loop.py\", line 271, in advance\r\n    self._outputs = self.epoch_loop.run(self._data_fetcher)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 201, in run\r\n    self.on_advance_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 241, in on_advance_end\r\n    self._run_validation()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/epoch\/training_epoch_loop.py\", line 299, in _run_validation\r\n    self.val_loop.run()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/loop.py\", line 207, in run\r\n    output = self.on_run_end()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loops\/dataloader\/evaluation_loop.py\", line 198, in on_run_end\r\n    self.trainer._logger_connector.log_eval_end_metrics(all_logged_outputs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 142, in log_eval_end_metrics\r\n    self.log_metrics(metrics)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/trainer\/connectors\/logger_connector\/logger_connector.py\", line 109, in log_metrics\r\n    logger.log_metrics(metrics=scalar_metrics, step=step)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 390, in log_metrics\r\n    self.experiment.log(dict(metrics, **{\"trainer\/global_step\": step}))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 289, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 255, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1591, in log\r\n    self._log(data=data, step=step, commit=commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1375, in _log\r\n    self._partial_history_callback(data, step, commit)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1259, in _partial_history_callback\r\n    self._backend.interface.publish_partial_history(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 553, in publish_partial_history\r\n    self._publish_partial_history(partial_history)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 67, in _publish_partial_history\r\n    self._publish(rec)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 51, in _publish\r\n    self._sock_client.send_record_publish(record)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 150, in send_record_publish\r\n    self.send_server_request(server_req)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\r\n    self._send_message(msg)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\r\n    self._sendall_with_error_handle(header + data)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\r\n    sent = self._sock.send(data[total_sent:])\r\nBrokenPipeError: [Errno 32] Broken pipe```\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: fails when config is too large; Content: i tried to run benchmark.py, with , but got an error because the config is too large, probably due to the train_selection array being too big. `error error while calling api: run config cannot exceed 15 mb ()` perhaps the data selections does not need to be uploaded to ?",
        "Issue_original_content_gpt_summary":"The user encountered a challenge when attempting to run benchmark.py with a config that was too large, likely due to the train_selection array being too big.",
        "Issue_preprocessed_content":"Title: fails when config is too large; Content: i tried to run with , but got an error because the config is too large, probably due to the array being too big. perhaps the data selections does not need to be uploaded to ? the full message is"
    },
    {
        "Issue_link":"https:\/\/github.com\/graphnet-team\/graphnet\/issues\/270",
        "Issue_title":"Running train_model from examples after install needs directory \"wandb\"",
        "Issue_creation_time":1661861159000,
        "Issue_closed_time":1661948109000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"After installing graphnet from scratch and signing up to WandB, running train_model from examples yields the following error:\r\n\r\n```\r\n(graphnet) [peter@hep04 examples]$ python train_model.py \r\ngraphnet: INFO     2022-08-30 12:21:56 - get_logger - Writing log to logs\/graphnet_20220830-122156.log\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\ngraphnet: WARNING  2022-08-30 12:21:56 - <module> - icecube package not available.\r\nwandb: Currently logged in as: peterandresen (graphnet-team). Use `wandb login --relogin` to force relogin\r\nwandb: WARNING Path .\/wandb\/wandb\/ wasn't writable, using system temp directory.\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\nwandb: ERROR Abnormal program exit\r\nTraceback (most recent call last):\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1040, in init\r\n    wi.setup(kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 287, in setup\r\n    self._log_setup(settings)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 431, in _log_setup\r\n    filesystem._safe_makedirs(os.path.dirname(settings.log_user))\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs\r\n    os.makedirs(dir_name)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs\r\n    makedirs(head, exist_ok=exist_ok)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nPermissionError: [Errno 13] Permission denied: '\/tmp\/wandb\/run-20220830_122200-1qc85fm4'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_model.py\", line 37, in <module>\r\n    wandb_logger = WandbLogger(\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 315, in __init__\r\n    _ = self.experiment\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment\r\n    return get_experiment() or DummyExperiment()\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment\r\n    return fn(self)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/wandb.py\", line 361, in experiment\r\n    self._experiment = wandb.init(**self._wandb_init)\r\n  File \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1081, in init\r\n    raise Exception(\"problem\") from error_seen\r\nException: problem\r\n```\r\n\r\nWhich can be fixed by creating a folder called \"wandb\" in the place where you are running the file from. Would it make sense to automatically create such a folder, if it is not already present?",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: running train_model from examples after install needs directory \"\"; Content: after installing graphnet from scratch and signing up to , running train_model from examples yields the following error: ``` (graphnet) [peter@hep04 examples]$ python train_model.py graphnet: info 2022-08-30 12:21:56 - get_logger - writing log to logs\/graphnet_20220830-122156.log graphnet: warning 2022-08-30 12:21:56 - - icecube package not available. graphnet: warning 2022-08-30 12:21:56 - - icecube package not available. graphnet: warning 2022-08-30 12:21:56 - - icecube package not available. graphnet: warning 2022-08-30 12:21:56 - - icecube package not available. : currently logged in as: peterandresen (graphnet-team). use ` login --relogin` to force relogin : warning path .\/\/\/ wasn't writable, using system temp directory. traceback (most recent call last): file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 1040, in init wi.setup(kwargs) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 287, in setup self._log_setup(settings) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 431, in _log_setup filesystem._safe_makedirs(os.path.dirname(settings.log_user)) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs os.makedirs(dir_name) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs makedirs(head, exist_ok=exist_ok) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs mkdir(name, mode) permissionerror: [errno 13] permission denied: '\/tmp\/\/run-20220830_122200-1qc85fm4' : error abnormal program exit traceback (most recent call last): file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 1040, in init wi.setup(kwargs) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 287, in setup self._log_setup(settings) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 431, in _log_setup filesystem._safe_makedirs(os.path.dirname(settings.log_user)) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/lib\/filesystem.py\", line 10, in _safe_makedirs os.makedirs(dir_name) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 213, in makedirs makedirs(head, exist_ok=exist_ok) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/os.py\", line 223, in makedirs mkdir(name, mode) permissionerror: [errno 13] permission denied: '\/tmp\/\/run-20220830_122200-1qc85fm4' the above exception was the direct cause of the following exception: traceback (most recent call last): file \"train_model.py\", line 37, in _logger = logger( file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py\", line 315, in __init__ _ = self.experiment file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 54, in experiment return get_experiment() or dummyexperiment() file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/utilities\/rank_zero.py\", line 32, in wrapped_fn return fn(*args, **kwargs) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/logger.py\", line 52, in get_experiment return fn(self) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/pytorch_lightning\/loggers\/.py\", line 361, in experiment self._experiment = .init(**self.__init) file \"\/groups\/icecube\/peter\/anaconda3\/envs\/graphnet\/lib\/python3.8\/site-packages\/\/sdk\/_init.py\", line 1081, in init raise exception(\"problem\") from error_seen exception: problem ``` which can be fixed by creating a folder called \"\" in the place where you are running the file from. would it make sense to automatically create such a folder, if it is not already present?",
        "Issue_original_content_gpt_summary":"The user encountered a PermissionError when running train_model from examples after installing graphnet from scratch, which was fixed by creating a folder called \"\" in the place where the file was being run from.",
        "Issue_preprocessed_content":"Title: running from examples after install needs directory ; Content: after installing graphnet from scratch and signing up to , running from examples yields the following error which can be fixed by creating a folder called in the place where you are running the file from. would it make sense to automatically create such a folder, if it is not already present?"
    },
    {
        "Issue_link":"https:\/\/github.com\/neuro-inc\/mlops-wandb-bucket-ref\/issues\/16",
        "Issue_title":"WandB output overwrites wabucketref's output in case of artifact upload",
        "Issue_creation_time":1625736474000,
        "Issue_closed_time":1625736868000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":0.0,
        "Issue_body":"Example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0\r\n\r\nAt the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job.\r\nHowever, we have:\r\n```\r\n...\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\nwandb: Waiting for W&B process to finish, PID 75\r\n...\r\n```\r\n\r\nWhile it should be:\r\n```\r\nINFO:wabucketref.api:Uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ...\r\nINFO:wabucketref.api:Artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nINFO:botocore.credentials:Found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline\r\nwandb: Generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... Done. 0.0s\r\n::set-output name=artifact_name::texture-maps\r\n::set-output name=artifact_type::dataset\r\n::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1\r\nwandb: Waiting for W&B process to finish, PID 75\r\nwandb: Program ended successfully.\r\nwandb:                                                                                \r\n```\r\n\r\nOne line was overwritten by the `wandb: Waiting for W&B process to finish, PID 75`, which, apparently is running in a separate process (`wandb.Settings(start_method=\"fork\")`). ",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: output overwrites wabucketref's output in case of artifact upload; Content: example job: job-7acb5d09-e580-46a2-aa11-03ce72ddc0f0 at the end of the job run, we upload the artifact, where `set-output` happens, and terminate the job. however, we have: ``` ... info:wabucketref.api:uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ... info:wabucketref.api:artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 info:botocore.credentials:found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline : generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... done. 0.0s ::set-output name=artifact_name::texture-maps ::set-output name=artifact_type::dataset : waiting for w&b process to finish, pid 75 ... ``` while it should be: ``` info:wabucketref.api:uploading artifact from '\/tmp\/tmpqkuqrluh' to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 ... info:wabucketref.api:artifact uploaded to s3:\/\/pca-pipeline\/dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1 info:botocore.credentials:found credentials in shared credentials file: \/var\/secrets\/aws\/credentials-pca-pipeline : generating checksum for up to 100000 objects with prefix \"dataset\/texture-maps\/8154311a-ab2e-45cd-adeb-f7e5270122c1\"... done. 0.0s ::set-output name=artifact_name::texture-maps ::set-output name=artifact_type::dataset ::set-output name=artifact_alias::8154311a-ab2e-45cd-adeb-f7e5270122c1 : waiting for w&b process to finish, pid 75 : program ended successfully. : ``` one line was overwritten by the `: waiting for w&b process to finish, pid 75`, which, apparently is running in a separate process (`.settings(start_method=\"fork\")`).",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the output of the wabucketref was overwritten by a separate process, resulting in a missing line of output.",
        "Issue_preprocessed_content":"Title: output overwrites wabucketref's output in case of artifact upload; Content: example job at the end of the job run, we upload the artifact, where happens, and terminate the job. however, we have while it should be one line was overwritten by the , which, apparently is running in a separate process ` ."
    },
    {
        "Issue_link":"https:\/\/github.com\/rdnfn\/beobench\/issues\/67",
        "Issue_title":"In random agent script wandb full episode data logging skips a few steps",
        "Issue_creation_time":1649933570000,
        "Issue_closed_time":1650034426000,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":1.0,
        "Issue_body":"### Problem\r\n\r\n In random agent script wandb full episode data logging skips a few steps. This is because wandb counts the epsiode reward logging steps made prior to the full data logging.\r\n\r\n### Potential Solution\r\n\r\nAdd another metric to log that shows timestep and day (proportional).\r\n",
        "Tool":"Weights & Biases",
        "Platform":"Github",
        "Issue_original_content":"Title: in random agent script full episode data logging skips a few steps; Content: ### problem in random agent script full episode data logging skips a few steps. this is because counts the epsiode reward logging steps made prior to the full data logging. ### potential solution add another metric to log that shows timestep and day (proportional).",
        "Issue_original_content_gpt_summary":"The user encountered a challenge with the random agent script where full episode data logging was skipping a few steps, and proposed a potential solution of adding another metric to log that shows timestep and day (proportional).",
        "Issue_preprocessed_content":"Title: in random agent script full episode data logging skips a few steps; Content: problem in random agent script full episode data logging skips a few steps. this is because counts the epsiode reward logging steps made prior to the full data logging. potential solution add another metric to log that shows timestep and day ."
    },
    {
        "Issue_link":"https:\/\/gitlab.com\/fluidattacks\/universe\/-\/issues\/8382",
        "Issue_title":"[Sorts] Add sagemaker dependencies",
        "Issue_creation_time":1671481698985,
        "Issue_closed_time":null,
        "Issue_upvote_count":0,
        "Issue_downvote_count":0,
        "Issue_comment_count":null,
        "Issue_body":"<!-- Issues are public, they should not contain confidential information -->\n\n### What is the current _bug_ behavior? how can we reproduce it?\nThe requirements.txt file does not have the entire dependency tree defined\n\n### Possible fixes\nModify the requirements.txt file so that it has the complete tree of dependencies and their respective versions\n### Steps\n\n- [x] Make sure that the\n      [code contributions checklist](https:\/\/docs.fluidattacks.com\/development\/contributing#checklist)\n      has been followed.",
        "Tool":"Amazon SageMaker",
        "Platform":"Gitlab",
        "Issue_original_content":"Title: [sorts] add dependencies; Content: ### what is the current _bug_ behavior? how can we reproduce it? the requirements.txt file does not have the entire dependency tree defined ### possible fixes modify the requirements.txt file so that it has the complete tree of dependencies and their respective versions ### steps - [x] make sure that the [code contributions checklist](https:\/\/docs.fluidattacks.com\/development\/contributing#checklist) has been followed.",
        "Issue_original_content_gpt_summary":"The user encountered a challenge where the requirements.txt file did not have the entire dependency tree defined, and needed to modify the file to include the complete tree of dependencies and their respective versions.",
        "Issue_preprocessed_content":"Title: add dependencies; Content: what is the current behavior? how can we reproduce it? the file does not have the entire dependency tree defined possible fixes modify the file so that it has the complete tree of dependencies and their respective versions steps make sure that the has been followed."
    }
]